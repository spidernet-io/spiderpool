{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Introduction The Spiderpool is an IP Address Management (IPAM) CNI plugin that assigns IP addresses for kubernetes clusters. Currently, it is under developing stage, not ready for production environment yet. Any Container Network Interface (CNI) plugin supporting third-party IPAM plugins can use the Spiderpool, such as MacVLAN CNI , VLAN CNI , IPVLAN CNI etc. The Spiderpool also supports Multus CNI case to assign IP for multiple interfaces. More CNIs will be tested for integration with Spiderpool. Why Spiderpool Most overlay CNIs, like Cilium and Calico , have a good implementation of IPAM, so the Spiderpool is not intentionally designed for these cases, but may be integrated with them. The Spiderpool is specifically designed to use with underlay network, where administrators can accurately manage each IP. Currently, the community already has some IPAM plugins such as whereabout , kube-ipam , static , dhcp , and host-local , but few of them could help solve complex underlay-network issues, so we decide to develop the Spiderpool. BTW, there are also some CNI plugins that could work in the underlay mode, such as kube-ovn and coil . But the Spiderpool provides lots of different features. See Features for details. Features The Spiderpool provides a large number of different features as follows. Based on CRD storage, all operation could be done with kubernetes API-server. Support for assigning IP addresses with three options: IPv4-only, IPv6-only, and dual-stack. Support for working on the clusters with three options: IPv4-only, IPv6-only, and dual-stack. Support for creating multiple ippools. Different namespaces and applications could monopolize or share an ippool. An application could specify multiple backup ippool resources in case IP addresses in an ippool are out of use. Therefore, you neither need to scale up the IP resources in a fixed ippool, nor need to modify the application yaml to change an ippool. Support to bind a range of IP addresses to a single application. No need to hard code an IP list in a deployment yaml, which is not easy to modify. With Spiderpool, you only need to set the selector field of ippool and scale up or down the IP resource of an ippool dynamically. Support for always assigning the same IP address to a StatefulSet pod. Different pods in a single controller could get IP addresses from different subnets for an application deployed in different subnets or zones. Administrator could safely edit ippool resources, where the Spiderpool will help validate the modification and prevent from data race. Collect resources in real time, especially for solving IP leakage or slow collection, which may make new pod fail to assign IP addresses. Support ranges of CNI plugin that supports third-party IPAM plugins. Especially, the Spiderpool could help much for CNI like spiderflat , macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI . Especially support for Multus CNI case to assign IP for multiple interfaces. Have a good performance for assigning and collecting IP. Support to reserve IP that will not be assigned to any pod. Included metrics for looking into IP usage and issues. By CIDR Manager, it could automatically scale up and down the IP address of the ippool, to distribute IP resource more reasonable between ippool. Support for both ARM64 and ARM64. Components Refer to architecture for components. Installation Refer to installation . Quick Start Refer to demo . Development Development guide is a reference point for development helper commands. License Spiderpool is licensed under the Apache License, Version 2.0.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#introduction","text":"The Spiderpool is an IP Address Management (IPAM) CNI plugin that assigns IP addresses for kubernetes clusters. Currently, it is under developing stage, not ready for production environment yet. Any Container Network Interface (CNI) plugin supporting third-party IPAM plugins can use the Spiderpool, such as MacVLAN CNI , VLAN CNI , IPVLAN CNI etc. The Spiderpool also supports Multus CNI case to assign IP for multiple interfaces. More CNIs will be tested for integration with Spiderpool.","title":"Introduction"},{"location":"#why-spiderpool","text":"Most overlay CNIs, like Cilium and Calico , have a good implementation of IPAM, so the Spiderpool is not intentionally designed for these cases, but may be integrated with them. The Spiderpool is specifically designed to use with underlay network, where administrators can accurately manage each IP. Currently, the community already has some IPAM plugins such as whereabout , kube-ipam , static , dhcp , and host-local , but few of them could help solve complex underlay-network issues, so we decide to develop the Spiderpool. BTW, there are also some CNI plugins that could work in the underlay mode, such as kube-ovn and coil . But the Spiderpool provides lots of different features. See Features for details.","title":"Why Spiderpool"},{"location":"#features","text":"The Spiderpool provides a large number of different features as follows. Based on CRD storage, all operation could be done with kubernetes API-server. Support for assigning IP addresses with three options: IPv4-only, IPv6-only, and dual-stack. Support for working on the clusters with three options: IPv4-only, IPv6-only, and dual-stack. Support for creating multiple ippools. Different namespaces and applications could monopolize or share an ippool. An application could specify multiple backup ippool resources in case IP addresses in an ippool are out of use. Therefore, you neither need to scale up the IP resources in a fixed ippool, nor need to modify the application yaml to change an ippool. Support to bind a range of IP addresses to a single application. No need to hard code an IP list in a deployment yaml, which is not easy to modify. With Spiderpool, you only need to set the selector field of ippool and scale up or down the IP resource of an ippool dynamically. Support for always assigning the same IP address to a StatefulSet pod. Different pods in a single controller could get IP addresses from different subnets for an application deployed in different subnets or zones. Administrator could safely edit ippool resources, where the Spiderpool will help validate the modification and prevent from data race. Collect resources in real time, especially for solving IP leakage or slow collection, which may make new pod fail to assign IP addresses. Support ranges of CNI plugin that supports third-party IPAM plugins. Especially, the Spiderpool could help much for CNI like spiderflat , macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI . Especially support for Multus CNI case to assign IP for multiple interfaces. Have a good performance for assigning and collecting IP. Support to reserve IP that will not be assigned to any pod. Included metrics for looking into IP usage and issues. By CIDR Manager, it could automatically scale up and down the IP address of the ippool, to distribute IP resource more reasonable between ippool. Support for both ARM64 and ARM64.","title":"Features"},{"location":"#components","text":"Refer to architecture for components.","title":"Components"},{"location":"#installation","text":"Refer to installation .","title":"Installation"},{"location":"#quick-start","text":"Refer to demo .","title":"Quick Start"},{"location":"#development","text":"Development guide is a reference point for development helper commands.","title":"Development"},{"location":"#license","text":"Spiderpool is licensed under the Apache License, Version 2.0.","title":"License"},{"location":"cmdref/spiderpool-agent/","text":"spiderpool-agent This page describes CLI options and ENV of spiderpool-agent. spiderpool-agent daemon Run the spiderpool agent daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin ENV SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5711) SPIDERPOOL_HEALTH_PORT http port (default to 5710) spiderpool-agent shutdown Notify of stopping the spiderpool-agent daemon. spiderpool-agent metric Get local metrics. Options --port string http server port of local metric (default to 5711)","title":"spiderpool-agent"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent","text":"This page describes CLI options and ENV of spiderpool-agent.","title":"spiderpool-agent"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-daemon","text":"Run the spiderpool agent daemon.","title":"spiderpool-agent daemon"},{"location":"cmdref/spiderpool-agent/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin","title":"Options"},{"location":"cmdref/spiderpool-agent/#env","text":"SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5711) SPIDERPOOL_HEALTH_PORT http port (default to 5710)","title":"ENV"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-shutdown","text":"Notify of stopping the spiderpool-agent daemon.","title":"spiderpool-agent shutdown"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-metric","text":"Get local metrics.","title":"spiderpool-agent metric"},{"location":"cmdref/spiderpool-agent/#options_1","text":"--port string http server port of local metric (default to 5711)","title":"Options"},{"location":"cmdref/spiderpool-controller/","text":"spiderpool-controller This page describes CLI options and ENV of spiderpool-controller. spiderpool-controller daemon Run the spiderpool controller daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) ENV SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5721) SPIDERPOOL_GC_IPPOOL_ENABLED enable GC ip in ippool, prior to other GC environment (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED enable GC ip of terminating pod whose graceful-time times out (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY delay to GC ip after graceful-time times out (second, default to 0) SPIDERPOOL_HEALTH_PORT http port (default to 5710) SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION all intervals of GC (second, default to 600) spiderpool-controller shutdown Notify of stopping spiderpool-controller daemon. spiderpool-controller metric Get local metrics. Options --port string http server port of local metric (default to 5721) spiderpool-controller status Show status: Whether local is controller leader ... Options --port string http server port of local metric (default to 5720)","title":"spiderpool-controller"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller","text":"This page describes CLI options and ENV of spiderpool-controller.","title":"spiderpool-controller"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-daemon","text":"Run the spiderpool controller daemon.","title":"spiderpool-controller daemon"},{"location":"cmdref/spiderpool-controller/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map)","title":"Options"},{"location":"cmdref/spiderpool-controller/#env","text":"SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5721) SPIDERPOOL_GC_IPPOOL_ENABLED enable GC ip in ippool, prior to other GC environment (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED enable GC ip of terminating pod whose graceful-time times out (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY delay to GC ip after graceful-time times out (second, default to 0) SPIDERPOOL_HEALTH_PORT http port (default to 5710) SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION all intervals of GC (second, default to 600)","title":"ENV"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-shutdown","text":"Notify of stopping spiderpool-controller daemon.","title":"spiderpool-controller shutdown"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-metric","text":"Get local metrics.","title":"spiderpool-controller metric"},{"location":"cmdref/spiderpool-controller/#options_1","text":"--port string http server port of local metric (default to 5721)","title":"Options"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-status","text":"Show status: Whether local is controller leader ...","title":"spiderpool-controller status"},{"location":"cmdref/spiderpool-controller/#options_2","text":"--port string http server port of local metric (default to 5720)","title":"Options"},{"location":"cmdref/spiderpoolctl/","text":"spiderpoolctl This page describes CLI usage of spiderpoolctl for debug. spiderpoolctl gc Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address) spiderpoolctl ip show Show a pod that is taking this IP. Options --ip string [required] ip spiderpoolctl ip release Try to release an IP. Options --ip string [optional] ip --force [optional] force release ip spiderpoolctl ip set Set IP to be taken by a pod. This will update ippool and workload endpoint resource. Options --ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"spiderpoolctl"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl","text":"This page describes CLI usage of spiderpoolctl for debug.","title":"spiderpoolctl"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-gc","text":"Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address)","title":"spiderpoolctl gc"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-show","text":"Show a pod that is taking this IP.","title":"spiderpoolctl ip show"},{"location":"cmdref/spiderpoolctl/#options","text":"--ip string [required] ip","title":"Options"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-release","text":"Try to release an IP.","title":"spiderpoolctl ip release"},{"location":"cmdref/spiderpoolctl/#options_1","text":"--ip string [optional] ip --force [optional] force release ip","title":"Options"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-set","text":"Set IP to be taken by a pod. This will update ippool and workload endpoint resource.","title":"spiderpoolctl ip set"},{"location":"cmdref/spiderpoolctl/#options_2","text":"--ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"Options"},{"location":"concepts/allocation/","text":"IP Allocation When a pod is creating, it will follow steps below to get an IP. Get all ippool candidates. For which ippool is used by a pod, the following rules are listed from high to low priority. SpiderSubnet annotation. \"ipam.spidernet.io/subnets\" and \"ipam.spidernet.io/subnet\" will choose to use auto-created ippool if the SpiderSubnet feature is enabled. See SpiderSubnet for detail. Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used to specify an ippool. See Pod Annotation for detail. Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used to specify an ippool. See namespace annotation for detail. CNI configuration file. It can be set to \"default_ipv4_ippool\" and \"default_ipv6_ippool\" in the CNI configuration file. See configuration for detail. Cluster default subnet.(you can do not specify any annotations and it will try to use cluster default subnet auto-created ippool if the SpiderSubnet feature it enabled) It can be set to \"clusterDefaultIPv4Subnet\" and \"clusterDefaultIPv6Subnet\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail. Cluster default ippool. It can be set to \"clusterDefaultIPv4IPPool\" and \"clusterDefaultIPv6IPPool\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail. Filter valid ippool candidates. After getting IPv4 and IPv6 ippool candidates, it looks into each ippool and figures out whether it meets following rules, and learns which candidate ippool is available. The \"disable\" field of the ippool is \"false\" The \"ipversion\" field of the ippool must meet the claim The \"namespaceSelector\" field of the ippool must meet the namespace of the pod The \"podSelector\" field of the ippool must meet the pod The \"nodeSelector\" field of the ippool must meet the scheduled node of the pod The available IP resource of the ippool is not exhausted Assign IP from valid ippool candidates. When trying to assign IP from the ippool candidates, it follows rules as below. The IP is not reserved by the \"exclude_ips\" field of the ippool and all ReservedIP instances When the pod controller is a StatefulSet, the pod will get an IP in sequence","title":"IP Allocation"},{"location":"concepts/allocation/#ip-allocation","text":"When a pod is creating, it will follow steps below to get an IP. Get all ippool candidates. For which ippool is used by a pod, the following rules are listed from high to low priority. SpiderSubnet annotation. \"ipam.spidernet.io/subnets\" and \"ipam.spidernet.io/subnet\" will choose to use auto-created ippool if the SpiderSubnet feature is enabled. See SpiderSubnet for detail. Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used to specify an ippool. See Pod Annotation for detail. Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used to specify an ippool. See namespace annotation for detail. CNI configuration file. It can be set to \"default_ipv4_ippool\" and \"default_ipv6_ippool\" in the CNI configuration file. See configuration for detail. Cluster default subnet.(you can do not specify any annotations and it will try to use cluster default subnet auto-created ippool if the SpiderSubnet feature it enabled) It can be set to \"clusterDefaultIPv4Subnet\" and \"clusterDefaultIPv6Subnet\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail. Cluster default ippool. It can be set to \"clusterDefaultIPv4IPPool\" and \"clusterDefaultIPv6IPPool\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail. Filter valid ippool candidates. After getting IPv4 and IPv6 ippool candidates, it looks into each ippool and figures out whether it meets following rules, and learns which candidate ippool is available. The \"disable\" field of the ippool is \"false\" The \"ipversion\" field of the ippool must meet the claim The \"namespaceSelector\" field of the ippool must meet the namespace of the pod The \"podSelector\" field of the ippool must meet the pod The \"nodeSelector\" field of the ippool must meet the scheduled node of the pod The available IP resource of the ippool is not exhausted Assign IP from valid ippool candidates. When trying to assign IP from the ippool candidates, it follows rules as below. The IP is not reserved by the \"exclude_ips\" field of the ippool and all ReservedIP instances When the pod controller is a StatefulSet, the pod will get an IP in sequence","title":"IP Allocation"},{"location":"concepts/annotation/","text":"Annotations Spiderpool provides annotations for configuring custom ippools and routes. Application annotations Once you set up SpiderSubnet feature, you can specify SpiderSubnet annotations for the application templates. For more details, please refer to SpiderSubnet Notice: The current version only supports to use one SpiderSubnet V4/V6 CR for one interface, you shouldn't specify 2 or more SpiderSubnet V4 CRs, and it will choose the first one to use. ipam.spidernet.io/subnets Here's an example for multiple interfaces to use SpiderSubnet. ipam.spidernet.io/subnets: |- [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}] ipam.spidernet.io/subnet This is used for SpiderSubnet with single interface. ipam.spidernet.io/subnet: '{\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]}' Pod annotations For a pod, you can specify Spiderpool annotations for a special request. ipam.spidernet.io/ippool Specify the ippools used to allocate IP. ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-ippool1\"], \"ipv6\": [\"v6-ippool1\", \"v6-ippool2\"] } ipv4 (array, optional): Specify which ippool is used to allocate the IPv4 address. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6 (array, optional): Specify which ippool is used to allocate the IPv6 address. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required. ipam.spidernet.io/ippools It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4\": [\"v4-ippool1\"], \"ipv6\": [\"v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"eth1\", \"ipv4\": [\"v4-ippool2\"], \"ipv6\": [\"v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the interface field shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which ippool is used to allocate the IPv4 address. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6 (array, optional): Specify which ippool is used to allocate the IPv6 address. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required. cleangateway (bool, optional): If set to true, the IPAM plugin will not return the default gateway route recorded in the ippool. default to false For different interfaces, it is not recommended to use ippools of the same subnet. ipam.spidernet.io/routes You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes: |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address. ipam.spidernet.io/assigned-{INTERFACE} It is the IP allocation result of the interface. It is only used by Spiderpool, not reserved for users. ipam.spidernet.io/assigned-eth0: |- { \"interface\": \"eth0\", \"ipv4pool\": \"v4-ippool1\", \"ipv6pool\": \"v6-ippool1\", \"ipv4\": \"172.16.0.100/16\", \"ipv6\": \"fd00::100/64\", \"vlan\": 100 } Namespace annotations Namespace could set following annotations to specify default ippools. They are valid for all Pods under the Namespace. ipam.spidernet.io/default-ipv4-ippool ipam.spidernet.io/default-ipv4-ippool: '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' If multiple ippools are listed, it will try to allocate IP from the later ippool when the former one is not allocatable. ipam.spidernet.io/default-ipv6-ippool ipam.spidernet.io/default-ipv6-ippool: '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' For other procedure, similar to Pod Annotations described above.","title":"Annotations"},{"location":"concepts/annotation/#annotations","text":"Spiderpool provides annotations for configuring custom ippools and routes.","title":"Annotations"},{"location":"concepts/annotation/#application-annotations","text":"Once you set up SpiderSubnet feature, you can specify SpiderSubnet annotations for the application templates. For more details, please refer to SpiderSubnet Notice: The current version only supports to use one SpiderSubnet V4/V6 CR for one interface, you shouldn't specify 2 or more SpiderSubnet V4 CRs, and it will choose the first one to use.","title":"Application annotations"},{"location":"concepts/annotation/#ipamspidernetiosubnets","text":"Here's an example for multiple interfaces to use SpiderSubnet. ipam.spidernet.io/subnets: |- [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}]","title":"ipam.spidernet.io/subnets"},{"location":"concepts/annotation/#ipamspidernetiosubnet","text":"This is used for SpiderSubnet with single interface. ipam.spidernet.io/subnet: '{\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]}'","title":"ipam.spidernet.io/subnet"},{"location":"concepts/annotation/#pod-annotations","text":"For a pod, you can specify Spiderpool annotations for a special request.","title":"Pod annotations"},{"location":"concepts/annotation/#ipamspidernetioippool","text":"Specify the ippools used to allocate IP. ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-ippool1\"], \"ipv6\": [\"v6-ippool1\", \"v6-ippool2\"] } ipv4 (array, optional): Specify which ippool is used to allocate the IPv4 address. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6 (array, optional): Specify which ippool is used to allocate the IPv6 address. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required.","title":"ipam.spidernet.io/ippool"},{"location":"concepts/annotation/#ipamspidernetioippools","text":"It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4\": [\"v4-ippool1\"], \"ipv6\": [\"v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"eth1\", \"ipv4\": [\"v4-ippool2\"], \"ipv6\": [\"v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the interface field shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which ippool is used to allocate the IPv4 address. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6 (array, optional): Specify which ippool is used to allocate the IPv6 address. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required. cleangateway (bool, optional): If set to true, the IPAM plugin will not return the default gateway route recorded in the ippool. default to false For different interfaces, it is not recommended to use ippools of the same subnet.","title":"ipam.spidernet.io/ippools"},{"location":"concepts/annotation/#ipamspidernetioroutes","text":"You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes: |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address.","title":"ipam.spidernet.io/routes"},{"location":"concepts/annotation/#ipamspidernetioassigned-interface","text":"It is the IP allocation result of the interface. It is only used by Spiderpool, not reserved for users. ipam.spidernet.io/assigned-eth0: |- { \"interface\": \"eth0\", \"ipv4pool\": \"v4-ippool1\", \"ipv6pool\": \"v6-ippool1\", \"ipv4\": \"172.16.0.100/16\", \"ipv6\": \"fd00::100/64\", \"vlan\": 100 }","title":"ipam.spidernet.io/assigned-{INTERFACE}"},{"location":"concepts/annotation/#namespace-annotations","text":"Namespace could set following annotations to specify default ippools. They are valid for all Pods under the Namespace.","title":"Namespace annotations"},{"location":"concepts/annotation/#ipamspidernetiodefault-ipv4-ippool","text":"ipam.spidernet.io/default-ipv4-ippool: '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' If multiple ippools are listed, it will try to allocate IP from the later ippool when the former one is not allocatable.","title":"ipam.spidernet.io/default-ipv4-ippool"},{"location":"concepts/annotation/#ipamspidernetiodefault-ipv6-ippool","text":"ipam.spidernet.io/default-ipv6-ippool: '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' For other procedure, similar to Pod Annotations described above.","title":"ipam.spidernet.io/default-ipv6-ippool"},{"location":"concepts/arch/","text":"Architecture Spiderpool consists of following components: Spiderpool IPAM plugin, a binary installed on each host. It is called by a CNI plugin to assign and release IP for a pod. spiderpool-agent, deployed as a daemonset. It receives IPAM requests from the IPAM plugin, assigns and releases IP from the ippool resource. spiderpool-controller, deployed as a deployment. It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. Refer to Resource Reclaim for details. It uses a webhook to watch the ippool resource, help the administrator to validate creation, modification, and deletion. spiderpoolctl, a CLI tool for debugging. CRDs Spiderpool supports for the following CRDs: SpiderSubnet CRD. It is used to represent a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. Refer to SpiderSubnet for detail. SpiderReservedIP CRD. It is used to represent a collection of IP addresses that Spiderpool expects not to be allocated. Refer to SpiderReservedIP for detail. SpiderIPPool CRD. It is used to represent a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. Refer to SpiderIPPool for detail. SpiderEndpoint CRD. It is used to represent IP address allocation details for a specific endpoint object. Refer to SpiderEndpoint for detail.","title":"Architecture"},{"location":"concepts/arch/#architecture","text":"Spiderpool consists of following components: Spiderpool IPAM plugin, a binary installed on each host. It is called by a CNI plugin to assign and release IP for a pod. spiderpool-agent, deployed as a daemonset. It receives IPAM requests from the IPAM plugin, assigns and releases IP from the ippool resource. spiderpool-controller, deployed as a deployment. It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. Refer to Resource Reclaim for details. It uses a webhook to watch the ippool resource, help the administrator to validate creation, modification, and deletion. spiderpoolctl, a CLI tool for debugging.","title":"Architecture"},{"location":"concepts/arch/#crds","text":"Spiderpool supports for the following CRDs: SpiderSubnet CRD. It is used to represent a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. Refer to SpiderSubnet for detail. SpiderReservedIP CRD. It is used to represent a collection of IP addresses that Spiderpool expects not to be allocated. Refer to SpiderReservedIP for detail. SpiderIPPool CRD. It is used to represent a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. Refer to SpiderIPPool for detail. SpiderEndpoint CRD. It is used to represent IP address allocation details for a specific endpoint object. Refer to SpiderEndpoint for detail.","title":"CRDs"},{"location":"concepts/config/","text":"Configuration Instructions for global configuration and environment arguments of Spiderpool. IPAM Plugin Configuration Here is an example of IPAM configuration. { \"cniVersion\":\"0.3.1\", \"name\":\"macvlan-pod-network\", \"plugins\":[ { \"name\":\"macvlan-pod-network\", \"type\":\"macvlan\", \"master\":\"ens256\", \"mode\":\"bridge\", \"mtu\":1500, \"ipam\":{ \"type\":\"spiderpool\", \"log_file_path\":\"/var/log/spidernet/spiderpool.log\", \"log_file_max_size\":\"100\", \"log_file_max_age\":\"30\", \"log_file_max_count\":7, \"log_level\":\"INFO\", \"default_ipv4_ippool\": [\"default-ipv4-pool1\",\"default-ipv4-pool2\"], \"default_ipv6_ippool\": [\"default-ipv6-pool1\",\"default-ipv6-pool2\"] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use. Configmap Configuration Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: conf.yml: | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock networkMode: legacy enableIPv4: true enableIPv6: true enableStatefulSet: true enableSpiderSubnet: true clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [default-v6-ippool] clusterDefaultIPv4Subnet: [default-v4-subnet] clusterDefaultIPv6Subnet: [default-v6-subnet] clusterSubnetDefaultFlexibleIPNumber: 1 ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. networkMode : legacy : Applicable to the traditional physical machine network. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet capability of Spiderpool. false : Disable StatefulSet capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. clusterDefaultIPv4IPPool (array): Global default IPv4 ippools. It takes effect across the cluster. clusterDefaultIPv6IPPool (array): Global default IPv6 ippools. It takes effect across the cluster. clusterDefaultIPv4Subnet (array): Global default IPv4 subnets. It takes effect across the cluster. clusterDefaultIPv6Subnet (array): Global default IPv6 subnets. It takes effect across the cluster. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster. Spiderpool-agent env env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide. Spiderpool-controller env env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listening on. Disabled if empty.","title":"Configuration"},{"location":"concepts/config/#configuration","text":"Instructions for global configuration and environment arguments of Spiderpool.","title":"Configuration"},{"location":"concepts/config/#ipam-plugin-configuration","text":"Here is an example of IPAM configuration. { \"cniVersion\":\"0.3.1\", \"name\":\"macvlan-pod-network\", \"plugins\":[ { \"name\":\"macvlan-pod-network\", \"type\":\"macvlan\", \"master\":\"ens256\", \"mode\":\"bridge\", \"mtu\":1500, \"ipam\":{ \"type\":\"spiderpool\", \"log_file_path\":\"/var/log/spidernet/spiderpool.log\", \"log_file_max_size\":\"100\", \"log_file_max_age\":\"30\", \"log_file_max_count\":7, \"log_level\":\"INFO\", \"default_ipv4_ippool\": [\"default-ipv4-pool1\",\"default-ipv4-pool2\"], \"default_ipv6_ippool\": [\"default-ipv6-pool1\",\"default-ipv6-pool2\"] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM Plugin Configuration"},{"location":"concepts/config/#configmap-configuration","text":"Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: conf.yml: | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock networkMode: legacy enableIPv4: true enableIPv6: true enableStatefulSet: true enableSpiderSubnet: true clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [default-v6-ippool] clusterDefaultIPv4Subnet: [default-v4-subnet] clusterDefaultIPv6Subnet: [default-v6-subnet] clusterSubnetDefaultFlexibleIPNumber: 1 ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. networkMode : legacy : Applicable to the traditional physical machine network. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet capability of Spiderpool. false : Disable StatefulSet capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. clusterDefaultIPv4IPPool (array): Global default IPv4 ippools. It takes effect across the cluster. clusterDefaultIPv6IPPool (array): Global default IPv6 ippools. It takes effect across the cluster. clusterDefaultIPv4Subnet (array): Global default IPv4 subnets. It takes effect across the cluster. clusterDefaultIPv6Subnet (array): Global default IPv6 subnets. It takes effect across the cluster. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster.","title":"Configmap Configuration"},{"location":"concepts/config/#spiderpool-agent-env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide.","title":"Spiderpool-agent env"},{"location":"concepts/config/#spiderpool-controller-env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listening on. Disabled if empty.","title":"Spiderpool-controller env"},{"location":"concepts/gc/","text":"Resource Reclaim IP garbage collection Context When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod. when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call CNI plugin. A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: the new pod may fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue. Solution For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time. The spiderpool controller takes charge of this responsibility. For more details, please refer to IP GC . SpiderIPPool garbage collection To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted. SpiderEndpoint garbage collection Once a pod is created and gets IPs from SpiderIPPool , Spiderpool will create a corresponding SpiderEndpoint object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to OwnerReference with the pod. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then spiderpool controller will remove the Current data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up)","title":"Resource Reclaim"},{"location":"concepts/gc/#resource-reclaim","text":"","title":"Resource Reclaim"},{"location":"concepts/gc/#ip-garbage-collection","text":"","title":"IP garbage collection"},{"location":"concepts/gc/#context","text":"When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod. when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call CNI plugin. A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: the new pod may fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue.","title":"Context"},{"location":"concepts/gc/#solution","text":"For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time. The spiderpool controller takes charge of this responsibility. For more details, please refer to IP GC .","title":"Solution"},{"location":"concepts/gc/#spiderippool-garbage-collection","text":"To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted.","title":"SpiderIPPool garbage collection"},{"location":"concepts/gc/#spiderendpoint-garbage-collection","text":"Once a pod is created and gets IPs from SpiderIPPool , Spiderpool will create a corresponding SpiderEndpoint object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to OwnerReference with the pod. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then spiderpool controller will remove the Current data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up)","title":"SpiderEndpoint garbage collection"},{"location":"concepts/metrics/","text":"Metric Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller. spiderpool controller The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 spiderpool agent The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 Get Started Enable Metric support Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer install for details Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true . Metric reference Spiderpool Agent Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter Spiderpool Controller Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter spiderpool_subnet_ippool_counts Number of SpiderSubnet corresponding IPPools number, prometheus type: gauge","title":"Metric"},{"location":"concepts/metrics/#metric","text":"Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller.","title":"Metric"},{"location":"concepts/metrics/#spiderpool-controller","text":"The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721","title":"spiderpool controller"},{"location":"concepts/metrics/#spiderpool-agent","text":"The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721","title":"spiderpool agent"},{"location":"concepts/metrics/#get-started","text":"","title":"Get Started"},{"location":"concepts/metrics/#enable-metric-support","text":"Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer install for details Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true .","title":"Enable Metric support"},{"location":"concepts/metrics/#metric-reference","text":"","title":"Metric reference"},{"location":"concepts/metrics/#spiderpool-agent_1","text":"Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter","title":"Spiderpool Agent"},{"location":"concepts/metrics/#spiderpool-controller_1","text":"Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter spiderpool_subnet_ippool_counts Number of SpiderSubnet corresponding IPPools number, prometheus type: gauge","title":"Spiderpool Controller"},{"location":"concepts/roadmap/","text":"Spiderpool Roadmap Features [ ] (alpha) a pod use multiple ippools, in case that an ippool is out of use [ ] (alpha) cooperate with macvlan [ ] (alpha) support pod / namespace annotations to customize ip [ ] (alpha) dual stack support. For ipv4-only, ipv6-only, dual-stack cluster, [ ] assign ipv4/ipv6 ip to pod [ ] all component could service on ipv4/ipv6 ip [ ] (alpha) ippool selector [ ] select namespace. Each namespace could occupy non-shared ippool [ ] select pod. For deployment and statefulset case, pod could occupy some ip range [ ] select node. For different zone, pod have specified ip range [ ] (beta) cooperate with multus. multus may call multiple CNI to assign different interface and assign ip [ ] (beta) fixed ip for application, especially for statefulset [ ] (beta) health check [ ] (beta) CLI for debug [ ] (beta) retrieve leaked ip [ ] for graceful timeout of terminating state [ ] for finished job [ ] trigger by hand or automatically [ ] trigger by pod event and happen at interval [ ] (beta) DCE5 integration [ ] (GA) metrics [ ] (GA) reserved ip [ ] (GA) administrator edit ip safely, preventing from race with IPAM CNI, and avoid ip conflicting between ippools [ ] (GA) good performance [ ] take xxx second at most, to assign a ip [ ] take xxx second at most, to assign 1000 ip [ ] (GA) good reliability [ ] (GA) cooperate with spiderflat [ ] Unit-Test [ ] (alpha) 40% coverage at least [ ] (beta) 70% coverage at least [ ] (GA) 90% coverage at least [ ] e2e test [ ] (alpha) 30% coverage for test case of alpha feature [ ] (beta) 80% coverage for test case of beta/alpha feature [ ] (GA) 100% coverage for test case of all feature [ ] (GA) chaos test case, performance test case [ ] All CICD pipeline. nightly ci, auto release chart/image/release, code lint, doc lint, unitest, e2e test [X] (alpha) 80% CICD pipeline [ ] (GA) 100% CICD pipeline [ ] documentation [ ] (alpha) architecture, contributing [ ] (beta) concept, get started, configuration [ ] (GA) command reference Goals of April [x] \u7b2c\u4e00\u4e2a Helm Release\uff0c\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\uff08\u53ef\u6301\u7eed CICD\uff09 [x] \u8f93\u51fa Road Map\uff0c\u5b8c\u6210 6\u4e2a\u6708\u4ee5\u4e0a\u7684\u89c4\u5212 [x] Unit-Test \u8986\u76d6\u7387 10 % [x] \u7b2c\u4e00\u4e2a \u81ea\u52a8\u5316\u7684 e2e \u6d4b\u8bd5\uff08\u6bcf\u5929\u665a\u4e0a\u90fd\u8981\u81ea\u52a8\u8dd1\uff09 [x] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 10% [x] GitHub (Star) 100 [x] webhook \u7cbe\u901a\u53ca\u5206\u4eab [ ] go builder \u7684 SDK \u751f\u6210\uff0c\u6240\u6709\u7684 client \u90fd\u80fd\u5de5\u4f5c\uff0c\u751f\u6210 CRD yaml\u3002 \u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 openapi \u63a5\u53e3\u548c SDK \uff0c\u9a8c\u8bc1\u90fd\u80fd\u5de5\u4f5c\u3002\u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 spiderpool ipam plugin \uff0cagent\u3001 controller\u8fdb\u7a0b\u80fd\u591f\u8dd1 \uff0c\u786e\u4fdd helm apply \u80fd\u591f\u90e8\u7f72\uff08\u4e0d\u8981\u6c42\u80fd\u8dd1\u4e1a\u52a1\uff09 [x] \u642d\u5efa e2e \u6846\u67b6 [x] \u7cbe\u8bfb golang\u3001ginkgo\u3001\u5f00\u6e90\u9879\u76ee e2e\uff0c \u4ee5macvlan + whereabout \u5b8c\u6210\u7b2c\u4e00\u4e2ae2e\u6d4b\u8bd5 Goals of May [ ] \u5b8c\u6210 IPAM plug in [ ] \u5b8c\u6210 Agent \u4e3b\u8981\u4ee3\u7801\uff0c\u80fd\u5206\u914d\u51fa ipv4/ipv6 \u5730\u5740 [ ] \u4ee5 macvlan + whereabout \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 50% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u5b8c\u6210 100% alpha doc [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100% [ ] 5 \u4e2a\u5916\u90e8\u53cd\u9988\u7528\u6237\uff0c10\u4e2a ISSUE ????? [ ] 3\u4e2a\u5916\u90e8\u8d21\u732e\u8005 ???? [ ] GitHub (Star) 200 ??? [ ] Unit-Test \u8986\u76d6\u7387 30% [ ] \u5f00\u653e\u7684 \u72ec\u7acb\u7f51\u7ad9 \u548c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ???? [ ] \u5f00\u53d1\u7684 \u72ec\u7acb\u7f51\u7ad9 QuickStart\uff0c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ??? [ ] Spider Pool e2e \u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1 (\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545\u3002(\u8bc4\u5ba1\u4f1a\u4e4b\u524d\uff0c\u5f00\u4f1areview) [ ] \u5145\u5206\u7684 e2e \u6d4b\u8bd5 \u8bbe\u8ba1(\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545 ?? Goals of June [ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 alpha feature \u7684\u4e1a\u52a1\u4ee3\u7801 [ ] \u4ee5 macvlan + spiderpool \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 100% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u6536\u96c6 10\u4e2a\u4ee5\u4e0a alpha \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] Unit-Test \u8986\u76d6\u7387 80% [ ] \u6027\u80fd\u6d4b\u8bd5\uff0c\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c??? [ ] \u5145\u5206\u7684\u707e\u96be\u573a\u666f\u6d4b\u8bd5\u3002\u548c \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 ?? [ ] \u517c\u5bb9\u6027\u6d4b\u8bd5\u9a8c\u8bc1\uff08\u6d89\u53ca\u7684\u5f00\u6e90\u8f6f\u4ef6\uff1a\u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u6e90\u9879\u76ee \u548c\u5404\u4e2a\u516c\u6709\u4e91\u73af\u5883\uff09 [ ] \u5ba1\u89c6\u201cL3\u4e8b\u6545\u201d\uff0c\u786e\u8ba4\u4e0d\u4f1a\u51fa\u73b0\u4e4b\u524d\u53d1\u751f\u8fc7\u7684\u4e8b\u6545 [ ] \u707e\u96be\u6062\u590d\u6f14\u7ec3 ?? beta [ ] \u5b8c\u6210 \u548c KubeGrid \u7684 \u4f1a\u5e08 ?? beta [ ] \u89e3\u51b3\u6240\u6709\u91cd\u5927bug [ ] \u5408\u4f5c\u4f19\u4f34 1 \u4e2a\uff0c\u53cd\u9988\u7528\u6237 1\u4e2a Goals of July [ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 beta feature \u7684\u4ee3\u7801 [ ] \u5b8c\u6210 100% beta doc Goals of August [ ] finish most of GA feature , wait for spiderflat ready to debug [ ] \u6536\u96c6 10 \u4e2a\u4ee5\u4e0a beta \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] \u5c06\u6240\u6709\u5df2\u77e5 BUG \u89e3\u51b3 [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100 % Goals of September [ ] start spiderflat","title":"Spiderpool Roadmap"},{"location":"concepts/roadmap/#spiderpool-roadmap","text":"","title":"Spiderpool Roadmap"},{"location":"concepts/roadmap/#features","text":"[ ] (alpha) a pod use multiple ippools, in case that an ippool is out of use [ ] (alpha) cooperate with macvlan [ ] (alpha) support pod / namespace annotations to customize ip [ ] (alpha) dual stack support. For ipv4-only, ipv6-only, dual-stack cluster, [ ] assign ipv4/ipv6 ip to pod [ ] all component could service on ipv4/ipv6 ip [ ] (alpha) ippool selector [ ] select namespace. Each namespace could occupy non-shared ippool [ ] select pod. For deployment and statefulset case, pod could occupy some ip range [ ] select node. For different zone, pod have specified ip range [ ] (beta) cooperate with multus. multus may call multiple CNI to assign different interface and assign ip [ ] (beta) fixed ip for application, especially for statefulset [ ] (beta) health check [ ] (beta) CLI for debug [ ] (beta) retrieve leaked ip [ ] for graceful timeout of terminating state [ ] for finished job [ ] trigger by hand or automatically [ ] trigger by pod event and happen at interval [ ] (beta) DCE5 integration [ ] (GA) metrics [ ] (GA) reserved ip [ ] (GA) administrator edit ip safely, preventing from race with IPAM CNI, and avoid ip conflicting between ippools [ ] (GA) good performance [ ] take xxx second at most, to assign a ip [ ] take xxx second at most, to assign 1000 ip [ ] (GA) good reliability [ ] (GA) cooperate with spiderflat [ ] Unit-Test [ ] (alpha) 40% coverage at least [ ] (beta) 70% coverage at least [ ] (GA) 90% coverage at least [ ] e2e test [ ] (alpha) 30% coverage for test case of alpha feature [ ] (beta) 80% coverage for test case of beta/alpha feature [ ] (GA) 100% coverage for test case of all feature [ ] (GA) chaos test case, performance test case [ ] All CICD pipeline. nightly ci, auto release chart/image/release, code lint, doc lint, unitest, e2e test [X] (alpha) 80% CICD pipeline [ ] (GA) 100% CICD pipeline [ ] documentation [ ] (alpha) architecture, contributing [ ] (beta) concept, get started, configuration [ ] (GA) command reference","title":"Features"},{"location":"concepts/roadmap/#goals-of-april","text":"[x] \u7b2c\u4e00\u4e2a Helm Release\uff0c\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\uff08\u53ef\u6301\u7eed CICD\uff09 [x] \u8f93\u51fa Road Map\uff0c\u5b8c\u6210 6\u4e2a\u6708\u4ee5\u4e0a\u7684\u89c4\u5212 [x] Unit-Test \u8986\u76d6\u7387 10 % [x] \u7b2c\u4e00\u4e2a \u81ea\u52a8\u5316\u7684 e2e \u6d4b\u8bd5\uff08\u6bcf\u5929\u665a\u4e0a\u90fd\u8981\u81ea\u52a8\u8dd1\uff09 [x] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 10% [x] GitHub (Star) 100 [x] webhook \u7cbe\u901a\u53ca\u5206\u4eab [ ] go builder \u7684 SDK \u751f\u6210\uff0c\u6240\u6709\u7684 client \u90fd\u80fd\u5de5\u4f5c\uff0c\u751f\u6210 CRD yaml\u3002 \u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 openapi \u63a5\u53e3\u548c SDK \uff0c\u9a8c\u8bc1\u90fd\u80fd\u5de5\u4f5c\u3002\u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 spiderpool ipam plugin \uff0cagent\u3001 controller\u8fdb\u7a0b\u80fd\u591f\u8dd1 \uff0c\u786e\u4fdd helm apply \u80fd\u591f\u90e8\u7f72\uff08\u4e0d\u8981\u6c42\u80fd\u8dd1\u4e1a\u52a1\uff09 [x] \u642d\u5efa e2e \u6846\u67b6 [x] \u7cbe\u8bfb golang\u3001ginkgo\u3001\u5f00\u6e90\u9879\u76ee e2e\uff0c \u4ee5macvlan + whereabout \u5b8c\u6210\u7b2c\u4e00\u4e2ae2e\u6d4b\u8bd5","title":"Goals of April"},{"location":"concepts/roadmap/#goals-of-may","text":"[ ] \u5b8c\u6210 IPAM plug in [ ] \u5b8c\u6210 Agent \u4e3b\u8981\u4ee3\u7801\uff0c\u80fd\u5206\u914d\u51fa ipv4/ipv6 \u5730\u5740 [ ] \u4ee5 macvlan + whereabout \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 50% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u5b8c\u6210 100% alpha doc [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100% [ ] 5 \u4e2a\u5916\u90e8\u53cd\u9988\u7528\u6237\uff0c10\u4e2a ISSUE ????? [ ] 3\u4e2a\u5916\u90e8\u8d21\u732e\u8005 ???? [ ] GitHub (Star) 200 ??? [ ] Unit-Test \u8986\u76d6\u7387 30% [ ] \u5f00\u653e\u7684 \u72ec\u7acb\u7f51\u7ad9 \u548c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ???? [ ] \u5f00\u53d1\u7684 \u72ec\u7acb\u7f51\u7ad9 QuickStart\uff0c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ??? [ ] Spider Pool e2e \u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1 (\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545\u3002(\u8bc4\u5ba1\u4f1a\u4e4b\u524d\uff0c\u5f00\u4f1areview) [ ] \u5145\u5206\u7684 e2e \u6d4b\u8bd5 \u8bbe\u8ba1(\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545 ??","title":"Goals of May"},{"location":"concepts/roadmap/#goals-of-june","text":"[ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 alpha feature \u7684\u4e1a\u52a1\u4ee3\u7801 [ ] \u4ee5 macvlan + spiderpool \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 100% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u6536\u96c6 10\u4e2a\u4ee5\u4e0a alpha \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] Unit-Test \u8986\u76d6\u7387 80% [ ] \u6027\u80fd\u6d4b\u8bd5\uff0c\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c??? [ ] \u5145\u5206\u7684\u707e\u96be\u573a\u666f\u6d4b\u8bd5\u3002\u548c \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 ?? [ ] \u517c\u5bb9\u6027\u6d4b\u8bd5\u9a8c\u8bc1\uff08\u6d89\u53ca\u7684\u5f00\u6e90\u8f6f\u4ef6\uff1a\u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u6e90\u9879\u76ee \u548c\u5404\u4e2a\u516c\u6709\u4e91\u73af\u5883\uff09 [ ] \u5ba1\u89c6\u201cL3\u4e8b\u6545\u201d\uff0c\u786e\u8ba4\u4e0d\u4f1a\u51fa\u73b0\u4e4b\u524d\u53d1\u751f\u8fc7\u7684\u4e8b\u6545 [ ] \u707e\u96be\u6062\u590d\u6f14\u7ec3 ?? beta [ ] \u5b8c\u6210 \u548c KubeGrid \u7684 \u4f1a\u5e08 ?? beta [ ] \u89e3\u51b3\u6240\u6709\u91cd\u5927bug [ ] \u5408\u4f5c\u4f19\u4f34 1 \u4e2a\uff0c\u53cd\u9988\u7528\u6237 1\u4e2a","title":"Goals of June"},{"location":"concepts/roadmap/#goals-of-july","text":"[ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 beta feature \u7684\u4ee3\u7801 [ ] \u5b8c\u6210 100% beta doc","title":"Goals of July"},{"location":"concepts/roadmap/#goals-of-august","text":"[ ] finish most of GA feature , wait for spiderflat ready to debug [ ] \u6536\u96c6 10 \u4e2a\u4ee5\u4e0a beta \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] \u5c06\u6240\u6709\u5df2\u77e5 BUG \u89e3\u51b3 [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100 %","title":"Goals of August"},{"location":"concepts/roadmap/#goals-of-september","text":"[ ] start spiderflat","title":"Goals of September"},{"location":"concepts/spiderendpoint/","text":"SpiderEndpoint A SpiderEndpoint resource represents IP address allocation details for a specific endpoint object. CRD definition The SpiderEndpoint custom resource is modeled after a standard Kubernetes resource and is split into the status section: type SpiderEndpoint struct { [...] // Status is the status of the SpiderEndpoint Status WorkloadEndpointStatus `json:\"status,omitempty\"` } SpiderEndpoint status The status section contains some fields to describe details about the current Endpoint allocation. // WorkloadEndpointStatus defines the observed state of SpiderEndpoint type WorkloadEndpointStatus struct { // the endpoint current allocation details Current *PodIPAllocation `json:\"current,omitempty\"` // the endpoint history allocation details History []PodIPAllocation `json:\"history,omitempty\"` // kubernetes controller owner reference OwnerControllerType string `json:\"ownerControllerType\"` } type PodIPAllocation struct { // container ID ContainerID string `json:\"containerID\"` // node name Node *string `json:\"node,omitempty\"` // allocated IPs IPs []IPAllocationDetail `json:\"ips,omitempty\"` // created time CreationTime *metav1.Time `json:\"creationTime,omitempty\"` } type IPAllocationDetail struct { // interface name NIC string `json:\"interface\"` // IPv4 address IPv4 *string `json:\"ipv4,omitempty\"` // IPv6 address IPv6 *string `json:\"ipv6,omitempty\"` // IPv4 SpiderIPPool name IPv4Pool *string `json:\"ipv4Pool,omitempty\"` // IPv6 SpiderIPPool name IPv6Pool *string `json:\"ipv6Pool,omitempty\"` // vlan ID Vlan *int64 `json:\"vlan,omitempty\"` // IPv4 gateway IPv4Gateway *string `json:\"ipv4Gateway,omitempty\"` // IPv6 gateway IPv6Gateway *string `json:\"ipv6Gateway,omitempty\"` CleanGateway *bool `json:\"cleanGateway,omitempty\"` // route Routes []Route `json:\"routes,omitempty\"` }","title":"SpiderEndpoint"},{"location":"concepts/spiderendpoint/#spiderendpoint","text":"A SpiderEndpoint resource represents IP address allocation details for a specific endpoint object.","title":"SpiderEndpoint"},{"location":"concepts/spiderendpoint/#crd-definition","text":"The SpiderEndpoint custom resource is modeled after a standard Kubernetes resource and is split into the status section: type SpiderEndpoint struct { [...] // Status is the status of the SpiderEndpoint Status WorkloadEndpointStatus `json:\"status,omitempty\"` }","title":"CRD definition"},{"location":"concepts/spiderendpoint/#spiderendpoint-status","text":"The status section contains some fields to describe details about the current Endpoint allocation. // WorkloadEndpointStatus defines the observed state of SpiderEndpoint type WorkloadEndpointStatus struct { // the endpoint current allocation details Current *PodIPAllocation `json:\"current,omitempty\"` // the endpoint history allocation details History []PodIPAllocation `json:\"history,omitempty\"` // kubernetes controller owner reference OwnerControllerType string `json:\"ownerControllerType\"` } type PodIPAllocation struct { // container ID ContainerID string `json:\"containerID\"` // node name Node *string `json:\"node,omitempty\"` // allocated IPs IPs []IPAllocationDetail `json:\"ips,omitempty\"` // created time CreationTime *metav1.Time `json:\"creationTime,omitempty\"` } type IPAllocationDetail struct { // interface name NIC string `json:\"interface\"` // IPv4 address IPv4 *string `json:\"ipv4,omitempty\"` // IPv6 address IPv6 *string `json:\"ipv6,omitempty\"` // IPv4 SpiderIPPool name IPv4Pool *string `json:\"ipv4Pool,omitempty\"` // IPv6 SpiderIPPool name IPv6Pool *string `json:\"ipv6Pool,omitempty\"` // vlan ID Vlan *int64 `json:\"vlan,omitempty\"` // IPv4 gateway IPv4Gateway *string `json:\"ipv4Gateway,omitempty\"` // IPv6 gateway IPv6Gateway *string `json:\"ipv6Gateway,omitempty\"` CleanGateway *bool `json:\"cleanGateway,omitempty\"` // route Routes []Route `json:\"routes,omitempty\"` }","title":"SpiderEndpoint status"},{"location":"concepts/spiderippool/","text":"SpiderIPPool A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. CRD definition The SpiderIPPool custom resource is modeled after a standard Kubernetes resource and is split into a spec and a status section: type SpiderIPPool struct { [...] // Spec is the specification of the IPPool Spec IPPoolSpec `json:\"spec,omitempty\"` // Status is the status of the IPPool Status IPPoolStatus `json:\"status,omitempty\"` } IPPool spec The spec section embeds a specific IPPool field which allows to define the list of all IPs, ExcludeIPs, Routes, and some other data to the IPPool object for allocation: // IPPoolSpec defines the desired state of SpiderIPPool type IPPoolSpec struct { // specify the IPPool's IP version IPVersion *int64 `json:\"ipVersion,omitempty\"` // specify the IPPool's subnet Subnet string `json:\"subnet\"` // specify the IPPool's IP ranges IPs []string `json:\"ips\"` // determine whether ths IPPool could be used or not Disable *bool `json:\"disable,omitempty\"` // specify the exclude IPs for the IPPool ExcludeIPs []string `json:\"excludeIPs,omitempty\"` // specify the gateway Gateway *string `json:\"gateway,omitempty\"` // specify the vlan Vlan *int64 `json:\"vlan,omitempty\"` //specify the routes Routes []Route `json:\"routes,omitempty\"` PodAffinity *metav1.LabelSelector `json:\"podAffinity,omitempty\"` NamesapceAffinity *metav1.LabelSelector `json:\"namespaceAffinity,omitempty\"` NodeAffinity *metav1.LabelSelector `json:\"nodeAffinity,omitempty\"` } type Route struct { // destination Dst string `json:\"dst\"` // gateway Gw string `json:\"gw\"` } IPPool status The status section contains some fields to describe details about the current IPPool allocation. The IPPool status reports all used addresses. // IPPoolStatus defines the observed state of SpiderIPPool type IPPoolStatus struct { // all used addresses details AllocatedIPs PoolIPAllocations `json:\"allocatedIPs,omitempty\"` // the IPPool total addresses counts TotalIPCount *int64 `json:\"totalIPCount,omitempty\"` // the IPPool used addresses counts AllocatedIPCount *int64 `json:\"allocatedIPCount,omitempty\"` } // PoolIPAllocations is a map of allocated IPs indexed by IP type PoolIPAllocations map[string]PoolIPAllocation // PoolIPAllocation is an IP already has been allocated type PoolIPAllocation struct { // container ID ContainerID string `json:\"containerID\"` // interface name NIC string `json:\"interface\"` // node name Node string `json:\"node\"` // namespace Namespace string `json:\"namespace\"` // pod name Pod string `json:\"pod\"` // kubernetes controller owner reference OwnerControllerType string `json:\"ownerControllerType\"` }","title":"SpiderIPPool"},{"location":"concepts/spiderippool/#spiderippool","text":"A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned.","title":"SpiderIPPool"},{"location":"concepts/spiderippool/#crd-definition","text":"The SpiderIPPool custom resource is modeled after a standard Kubernetes resource and is split into a spec and a status section: type SpiderIPPool struct { [...] // Spec is the specification of the IPPool Spec IPPoolSpec `json:\"spec,omitempty\"` // Status is the status of the IPPool Status IPPoolStatus `json:\"status,omitempty\"` }","title":"CRD definition"},{"location":"concepts/spiderippool/#ippool-spec","text":"The spec section embeds a specific IPPool field which allows to define the list of all IPs, ExcludeIPs, Routes, and some other data to the IPPool object for allocation: // IPPoolSpec defines the desired state of SpiderIPPool type IPPoolSpec struct { // specify the IPPool's IP version IPVersion *int64 `json:\"ipVersion,omitempty\"` // specify the IPPool's subnet Subnet string `json:\"subnet\"` // specify the IPPool's IP ranges IPs []string `json:\"ips\"` // determine whether ths IPPool could be used or not Disable *bool `json:\"disable,omitempty\"` // specify the exclude IPs for the IPPool ExcludeIPs []string `json:\"excludeIPs,omitempty\"` // specify the gateway Gateway *string `json:\"gateway,omitempty\"` // specify the vlan Vlan *int64 `json:\"vlan,omitempty\"` //specify the routes Routes []Route `json:\"routes,omitempty\"` PodAffinity *metav1.LabelSelector `json:\"podAffinity,omitempty\"` NamesapceAffinity *metav1.LabelSelector `json:\"namespaceAffinity,omitempty\"` NodeAffinity *metav1.LabelSelector `json:\"nodeAffinity,omitempty\"` } type Route struct { // destination Dst string `json:\"dst\"` // gateway Gw string `json:\"gw\"` }","title":"IPPool spec"},{"location":"concepts/spiderippool/#ippool-status","text":"The status section contains some fields to describe details about the current IPPool allocation. The IPPool status reports all used addresses. // IPPoolStatus defines the observed state of SpiderIPPool type IPPoolStatus struct { // all used addresses details AllocatedIPs PoolIPAllocations `json:\"allocatedIPs,omitempty\"` // the IPPool total addresses counts TotalIPCount *int64 `json:\"totalIPCount,omitempty\"` // the IPPool used addresses counts AllocatedIPCount *int64 `json:\"allocatedIPCount,omitempty\"` } // PoolIPAllocations is a map of allocated IPs indexed by IP type PoolIPAllocations map[string]PoolIPAllocation // PoolIPAllocation is an IP already has been allocated type PoolIPAllocation struct { // container ID ContainerID string `json:\"containerID\"` // interface name NIC string `json:\"interface\"` // node name Node string `json:\"node\"` // namespace Namespace string `json:\"namespace\"` // pod name Pod string `json:\"pod\"` // kubernetes controller owner reference OwnerControllerType string `json:\"ownerControllerType\"` }","title":"IPPool status"},{"location":"concepts/spiderreservedip/","text":"SpiderReservedIP A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. CRD definition The SpiderReservedIP custom resource is modeled after a standard Kubernetes resource and is split into a spec section: // SpiderReservedIP is the Schema for the spiderreservedips API type SpiderReservedIP struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ReservedIPSpec `json:\"spec,omitempty\"` } SpiderReservedIP spec The spec section embeds a specific ReservedIP field which allows to define the list of all reserved IPs: // ReservedIPSpec defines the desired state of SpiderReservedIP type ReservedIPSpec struct { // IP version IPVersion *int64 `json:\"ipVersion,omitempty\"` ni // reserved IPs IPs []string `json:\"ips\"` }","title":"SpiderReservedIP"},{"location":"concepts/spiderreservedip/#spiderreservedip","text":"A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated.","title":"SpiderReservedIP"},{"location":"concepts/spiderreservedip/#crd-definition","text":"The SpiderReservedIP custom resource is modeled after a standard Kubernetes resource and is split into a spec section: // SpiderReservedIP is the Schema for the spiderreservedips API type SpiderReservedIP struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ReservedIPSpec `json:\"spec,omitempty\"` }","title":"CRD definition"},{"location":"concepts/spiderreservedip/#spiderreservedip-spec","text":"The spec section embeds a specific ReservedIP field which allows to define the list of all reserved IPs: // ReservedIPSpec defines the desired state of SpiderReservedIP type ReservedIPSpec struct { // IP version IPVersion *int64 `json:\"ipVersion,omitempty\"` ni // reserved IPs IPs []string `json:\"ips\"` }","title":"SpiderReservedIP spec"},{"location":"concepts/spidersubnet/","text":"SpiderSubnet A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. CRD definition The SpiderSubnet custom resource is modeled after a standard Kubernetes resource and is split into a spec and a status section: type SpiderSubnet struct { [...] // Spec is the specification of the Subnet Spec SubnetSpec `json:\"spec,omitempty\"` // Status is the status of the SpiderSubnet Status SubnetStatus `json:\"status,omitempty\"` } Subnet spec The spec section embeds a specific Subnet field which allows to define the list of all IPs, ExcludeIPs, Routes, and some other data to the Subnet object for allocation: // SubnetSpec defines the desired state of SpiderSubnet type SubnetSpec struct { // specify the SpiderSubnet's IP version IPVersion *int64 `json:\"ipVersion,omitempty\"` // specify the SpiderSubnet's subnet Subnet string `json:\"subnet\"` // specify the SpiderSubnet's IP ranges IPs []string `json:\"ips\"` // specify the exclude IPs for the SpiderSubnet ExcludeIPs []string `json:\"excludeIPs,omitempty\"` // specify the gateway Gateway *string `json:\"gateway,omitempty\"` // specify the vlan Vlan *int64 `json:\"vlan,omitempty\"` //specify the routes Routes []Route `json:\"routes,omitempty\"` } Subnet status The status section contains some fields to describe details about the current IPPool allocation. The IPPool status reports all used addresses. // SubnetStatus defines the observed state of SpiderSubnet type SubnetStatus struct { // the SpiderSubnet IPPool pre-allocations ControlledIPPools PoolIPPreAllocations `json:\"controlledIPPools,omitempty\"` // the SpiderSubnet total addresses counts TotalIPCount *int64 `json:\"totalIPCount,omitempty\"` // the SpiderSubnet allocated addresses counts AllocatedIPCount *int64 `json:\"allocatedIPCount,omitempty\"` } // PoolIPPreAllocations is a map of pool IP pre-allocation details indexed by pool name. type PoolIPPreAllocations map[string]PoolIPPreAllocation type PoolIPPreAllocation struct { // specify the SpiderSubnet's IPPool allocation IP ranges IPs []string `json:\"ips\"` }","title":"SpiderSubnet"},{"location":"concepts/spidersubnet/#spidersubnet","text":"A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned.","title":"SpiderSubnet"},{"location":"concepts/spidersubnet/#crd-definition","text":"The SpiderSubnet custom resource is modeled after a standard Kubernetes resource and is split into a spec and a status section: type SpiderSubnet struct { [...] // Spec is the specification of the Subnet Spec SubnetSpec `json:\"spec,omitempty\"` // Status is the status of the SpiderSubnet Status SubnetStatus `json:\"status,omitempty\"` }","title":"CRD definition"},{"location":"concepts/spidersubnet/#subnet-spec","text":"The spec section embeds a specific Subnet field which allows to define the list of all IPs, ExcludeIPs, Routes, and some other data to the Subnet object for allocation: // SubnetSpec defines the desired state of SpiderSubnet type SubnetSpec struct { // specify the SpiderSubnet's IP version IPVersion *int64 `json:\"ipVersion,omitempty\"` // specify the SpiderSubnet's subnet Subnet string `json:\"subnet\"` // specify the SpiderSubnet's IP ranges IPs []string `json:\"ips\"` // specify the exclude IPs for the SpiderSubnet ExcludeIPs []string `json:\"excludeIPs,omitempty\"` // specify the gateway Gateway *string `json:\"gateway,omitempty\"` // specify the vlan Vlan *int64 `json:\"vlan,omitempty\"` //specify the routes Routes []Route `json:\"routes,omitempty\"` }","title":"Subnet spec"},{"location":"concepts/spidersubnet/#subnet-status","text":"The status section contains some fields to describe details about the current IPPool allocation. The IPPool status reports all used addresses. // SubnetStatus defines the observed state of SpiderSubnet type SubnetStatus struct { // the SpiderSubnet IPPool pre-allocations ControlledIPPools PoolIPPreAllocations `json:\"controlledIPPools,omitempty\"` // the SpiderSubnet total addresses counts TotalIPCount *int64 `json:\"totalIPCount,omitempty\"` // the SpiderSubnet allocated addresses counts AllocatedIPCount *int64 `json:\"allocatedIPCount,omitempty\"` } // PoolIPPreAllocations is a map of pool IP pre-allocation details indexed by pool name. type PoolIPPreAllocations map[string]PoolIPPreAllocation type PoolIPPreAllocation struct { // specify the SpiderSubnet's IPPool allocation IP ranges IPs []string `json:\"ips\"` }","title":"Subnet status"},{"location":"develop/changelog/","text":"Changelog How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Changelog"},{"location":"develop/changelog/#changelog","text":"How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Changelog"},{"location":"develop/pullrequest/","text":"Submit Pull Request A pull request will be checked by following workflow, which is required for merging. Action: your PR should be signed off When you commit your modification, add -s in your commit command: git commit -s Action: check yaml files If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml . Action: check golang source code It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label Action: check licenses Any golang or shell file should be licensed correctly. Action: check markdown file Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling . Action: lint yaml file If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml . Action: lint chart Action: lint openapi.yaml Action: check code spell Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Submit Pull Request"},{"location":"develop/pullrequest/#submit-pull-request","text":"A pull request will be checked by following workflow, which is required for merging.","title":"Submit Pull Request"},{"location":"develop/pullrequest/#action-your-pr-should-be-signed-off","text":"When you commit your modification, add -s in your commit command: git commit -s","title":"Action: your PR should be signed off"},{"location":"develop/pullrequest/#action-check-yaml-files","text":"If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml .","title":"Action: check yaml files"},{"location":"develop/pullrequest/#action-check-golang-source-code","text":"It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label","title":"Action: check golang source code"},{"location":"develop/pullrequest/#action-check-licenses","text":"Any golang or shell file should be licensed correctly.","title":"Action: check licenses"},{"location":"develop/pullrequest/#action-check-markdown-file","text":"Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling .","title":"Action: check markdown file"},{"location":"develop/pullrequest/#action-lint-yaml-file","text":"If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml .","title":"Action: lint yaml file"},{"location":"develop/pullrequest/#action-lint-chart","text":"","title":"Action: lint chart"},{"location":"develop/pullrequest/#action-lint-openapiyaml","text":"","title":"Action: lint openapi.yaml"},{"location":"develop/pullrequest/#action-check-code-spell","text":"Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Action: check code spell"},{"location":"develop/release/","text":"workflow for release pre-steps submit a PR , which should modify version in 'VERSION' and 'charts/spiderpool/Chart.yaml' push a version tag If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"pr/release/feature-new\" to be classified to \"New Features\" label \"pr/release/feature-changed\" to be classified to \"Changed Features\" label \"pr/release/feature-bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml post Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"workflow for release"},{"location":"develop/release/#workflow-for-release","text":"","title":"workflow for release"},{"location":"develop/release/#pre-steps","text":"submit a PR , which should modify version in 'VERSION' and 'charts/spiderpool/Chart.yaml'","title":"pre-steps"},{"location":"develop/release/#push-a-version-tag","text":"If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"pr/release/feature-new\" to be classified to \"New Features\" label \"pr/release/feature-changed\" to be classified to \"Changed Features\" label \"pr/release/feature-bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml","title":"push a version tag"},{"location":"develop/release/#post","text":"Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"post"},{"location":"develop/swagger_openapi/","text":"SWAGGER OPENAPI Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes. Features Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs. Usages There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR . validate spec Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec generate source codes with the given spec To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen verify the spec with current source codes to make sure whether the current source codes is out of date To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify clean the generated source codes To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code Use swagger-ui To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web. Steps For Developers Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#swagger-openapi","text":"Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#features","text":"Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs.","title":"Features"},{"location":"develop/swagger_openapi/#usages","text":"There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR .","title":"Usages"},{"location":"develop/swagger_openapi/#validate-spec","text":"Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec","title":"validate spec"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","text":"To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen","title":"generate source codes with the given spec"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","text":"To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify","title":"verify the spec with current source codes to make sure whether the current source codes is out of date"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","text":"To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code","title":"clean the generated source codes"},{"location":"develop/swagger_openapi/#use-swagger-ui","text":"To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.","title":"Use swagger-ui"},{"location":"develop/swagger_openapi/#steps-for-developers","text":"Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Steps For Developers"},{"location":"develop/test/","text":"test you could follow the below steps to test: check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init -e E2E_IP_FAMILY=ipv6 run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test # run all e2e test on ipv4-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv6 # Run all e2e tests on an enableSpiderSubnet=false cluster $ make e2e_test -e E2E_SPIDERPOOL_ENABLE_SUBNET=false # run smoke test $ make e2e_test -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} # run all e2e test $ make e2e_test 5 finally, you could visit \"http://HostIp:4040\" the in the browser of your desktop, and get flamegraph","title":"test"},{"location":"develop/test/#test","text":"you could follow the below steps to test: check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init -e E2E_IP_FAMILY=ipv6 run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test # run all e2e test on ipv4-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv6 # Run all e2e tests on an enableSpiderSubnet=false cluster $ make e2e_test -e E2E_SPIDERPOOL_ENABLE_SUBNET=false # run smoke test $ make e2e_test -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} # run all e2e test $ make e2e_test 5 finally, you could visit \"http://HostIp:4040\" the in the browser of your desktop, and get flamegraph","title":"test"},{"location":"develop/upgrade/","text":"Upgrading Spiderpool Versions This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool. Upgrade to 0.3.6 from (<=0.3.5) Description There's a design flaw for SpiderSubnet feature in auto-created IPPool label. The previous label ipam.spidernet.io/owner-application corresponding value uses '-' as separative sign. For example, we have deployment ns398-174835790/deploy398-82311862 and the corresponding label value is Deployment-ns398-174835790-deploy398-82311862 . It's very hard to unpack it to trace back what the application namespace and name is. Now, we use '_' rather than '-' as slash for SpiderSubnet feature label ipam.spidernet.io/owner-application , and the upper case will be like Deployment_ns398-174835790_deploy398-82311862 . Reference PR: #1162 In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool. The key is ipam.spidernet.io/interface , and the value is the corresponding interface name. Operation steps Find all auto-created IPPools, their name format is auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid} such as auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41 . Replace their label, just like this: shell kubectl patch sp ${auto-pool} --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}' Add one more label shell kubectl patch sp ${auto-pool} --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Update your Spiderpool components version and restart them all. Upgrade to 0.4.0 from (<0.4.0) Description Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod . Operation steps Find all SpiderEndpoint objects that their Status OwnerControllerType is None Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from None to Pod","title":"Upgrading Spiderpool Versions"},{"location":"develop/upgrade/#upgrading-spiderpool-versions","text":"This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool.","title":"Upgrading Spiderpool Versions"},{"location":"develop/upgrade/#upgrade-to-036-from-035","text":"","title":"Upgrade to 0.3.6 from (&lt;=0.3.5)"},{"location":"develop/upgrade/#description","text":"There's a design flaw for SpiderSubnet feature in auto-created IPPool label. The previous label ipam.spidernet.io/owner-application corresponding value uses '-' as separative sign. For example, we have deployment ns398-174835790/deploy398-82311862 and the corresponding label value is Deployment-ns398-174835790-deploy398-82311862 . It's very hard to unpack it to trace back what the application namespace and name is. Now, we use '_' rather than '-' as slash for SpiderSubnet feature label ipam.spidernet.io/owner-application , and the upper case will be like Deployment_ns398-174835790_deploy398-82311862 . Reference PR: #1162 In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool. The key is ipam.spidernet.io/interface , and the value is the corresponding interface name.","title":"Description"},{"location":"develop/upgrade/#operation-steps","text":"Find all auto-created IPPools, their name format is auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid} such as auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41 . Replace their label, just like this: shell kubectl patch sp ${auto-pool} --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}' Add one more label shell kubectl patch sp ${auto-pool} --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Update your Spiderpool components version and restart them all.","title":"Operation steps"},{"location":"develop/upgrade/#upgrade-to-040-from-040","text":"","title":"Upgrade to 0.4.0 from (&lt;0.4.0)"},{"location":"develop/upgrade/#description_1","text":"Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod .","title":"Description"},{"location":"develop/upgrade/#operation-steps_1","text":"Find all SpiderEndpoint objects that their Status OwnerControllerType is None Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from None to Pod","title":"Operation steps"},{"location":"usage/basic/","text":"Quick Start Let's start some Pods with Spiderpool in approximately 5 minutes. Install Spiderpool Set up the Helm repository. helm repo add spiderpool https://spidernet-io.github.io/spiderpool Set up the environment variables of the default IPv4 IPPool for the cluster. export IPV4_SUBNET_YOU_EXPECT=\"172.18.40.0/24\" export IPV4_IPRANGES_YOU_EXPECT=\"172.18.40.40-172.18.40.200\" The default IPPool usually serves some components that need IP addresses when the Kubernetes cluster is initialized, such as CoreDNS. Deploy Spiderpool with the following command. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=auto \\ --set feature.enableIPv4=true \\ --set feature.enableIPv6=false \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${IPV4_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv4IPRanges={${IPV4_IPRANGES_YOU_EXPECT}} During the deployment, the necessary TLS certificates and custom resources will be automatically created. See more details about installation . IPv6 is disabled in this case. If you want Spiderpool to work under dual stacks, set feature.enableIPv6=true . If you need to create a default IPv6 IPPool, set clusterDefaultPool.installIPv6IPPool=true and fill in the desired default IPv6 IPPool information in the clusterDefaultPool.ipv6Subnet and clusterDefaultPool.ipv6IPRanges parameters at the same time. See more details about Spiderpool helm charts parameters . Update CNI network configuration After the installation, you should update the CNI network configuration under the default path /etc/cni/net.d on the Node where you want to use Spiderpool, so that the Main CNI can use Spiderpool IPAM CNI to allocate IP addresses. Replace the ipam object with the following to update it: \"ipam\":{ \"type\":\"spiderpool\" } The following is an example configuration of macvlan CNI : { \"cniVersion\":\"0.4.0\", \"type\":\"macvlan\", \"mode\":\"bridge\", \"master\":\"eth0\", \"name\":\"macvlan\", \"ipam\":{ \"type\":\"spiderpool\" } } Create an IPPool Next, let's try to create a custom IPPool: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml The YAML file looks like: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: custom-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.50 You can replace spec.subnet and spec.ips as needed. See more details about SpiderIPPool CRD . Confirm the IPPool is working To confirm the custom IPPool is working as expected, let's create a Deployment with 3 replicas: kubectl create deployment my-dep --image=busybox --replicas=3 -- sleep infinity Pods controlled by this Deployment will be assigned with IP addresses from the cluster's default IPPool and run as expected. Check IPs of these pods with the following command: kubectl get po -l app=my-dep -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-dep-864946ffd8-h5z27 1/1 Running 0 3m10s 172.18.40.42 spider-worker <none> <none> my-dep-864946ffd8-kdl86 1/1 Running 0 3m10s 172.18.40.200 spider-worker <none> <none> my-dep-864946ffd8-vhnsj 1/1 Running 0 3m10s 172.18.40.38 spider-worker <none> <none> Allocate IP addresses from the custom IPPool In addition to the cluster's default IPPool, Spiderpool also supports allocating IP addresses from custom IPPools. You can use the following command and YAML file to specify the custom IPPool just created in the section Create an IPPool to allocate IP addresses through Pod annotation ipam.spidernet.io/ippool . See more details about pool selection rules . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ippool-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: custom-ippool-deploy spec: replicas: 3 selector: matchLabels: app: custom-ippool-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels: app: custom-ippool-deploy spec: containers: - name: custom-ippool-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As expected, Pods of Deployment custom-ippool-deploy will be assigned with IP addresses from IPPool custom-ipv4-ippool .","title":"Quick Start"},{"location":"usage/basic/#quick-start","text":"Let's start some Pods with Spiderpool in approximately 5 minutes.","title":"Quick Start"},{"location":"usage/basic/#install-spiderpool","text":"Set up the Helm repository. helm repo add spiderpool https://spidernet-io.github.io/spiderpool Set up the environment variables of the default IPv4 IPPool for the cluster. export IPV4_SUBNET_YOU_EXPECT=\"172.18.40.0/24\" export IPV4_IPRANGES_YOU_EXPECT=\"172.18.40.40-172.18.40.200\" The default IPPool usually serves some components that need IP addresses when the Kubernetes cluster is initialized, such as CoreDNS. Deploy Spiderpool with the following command. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=auto \\ --set feature.enableIPv4=true \\ --set feature.enableIPv6=false \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${IPV4_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv4IPRanges={${IPV4_IPRANGES_YOU_EXPECT}} During the deployment, the necessary TLS certificates and custom resources will be automatically created. See more details about installation . IPv6 is disabled in this case. If you want Spiderpool to work under dual stacks, set feature.enableIPv6=true . If you need to create a default IPv6 IPPool, set clusterDefaultPool.installIPv6IPPool=true and fill in the desired default IPv6 IPPool information in the clusterDefaultPool.ipv6Subnet and clusterDefaultPool.ipv6IPRanges parameters at the same time. See more details about Spiderpool helm charts parameters .","title":"Install Spiderpool"},{"location":"usage/basic/#update-cni-network-configuration","text":"After the installation, you should update the CNI network configuration under the default path /etc/cni/net.d on the Node where you want to use Spiderpool, so that the Main CNI can use Spiderpool IPAM CNI to allocate IP addresses. Replace the ipam object with the following to update it: \"ipam\":{ \"type\":\"spiderpool\" } The following is an example configuration of macvlan CNI : { \"cniVersion\":\"0.4.0\", \"type\":\"macvlan\", \"mode\":\"bridge\", \"master\":\"eth0\", \"name\":\"macvlan\", \"ipam\":{ \"type\":\"spiderpool\" } }","title":"Update CNI network configuration"},{"location":"usage/basic/#create-an-ippool","text":"Next, let's try to create a custom IPPool: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml The YAML file looks like: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: custom-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.50 You can replace spec.subnet and spec.ips as needed. See more details about SpiderIPPool CRD .","title":"Create an IPPool"},{"location":"usage/basic/#confirm-the-ippool-is-working","text":"To confirm the custom IPPool is working as expected, let's create a Deployment with 3 replicas: kubectl create deployment my-dep --image=busybox --replicas=3 -- sleep infinity Pods controlled by this Deployment will be assigned with IP addresses from the cluster's default IPPool and run as expected. Check IPs of these pods with the following command: kubectl get po -l app=my-dep -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-dep-864946ffd8-h5z27 1/1 Running 0 3m10s 172.18.40.42 spider-worker <none> <none> my-dep-864946ffd8-kdl86 1/1 Running 0 3m10s 172.18.40.200 spider-worker <none> <none> my-dep-864946ffd8-vhnsj 1/1 Running 0 3m10s 172.18.40.38 spider-worker <none> <none>","title":"Confirm the IPPool is working"},{"location":"usage/basic/#allocate-ip-addresses-from-the-custom-ippool","text":"In addition to the cluster's default IPPool, Spiderpool also supports allocating IP addresses from custom IPPools. You can use the following command and YAML file to specify the custom IPPool just created in the section Create an IPPool to allocate IP addresses through Pod annotation ipam.spidernet.io/ippool . See more details about pool selection rules . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ippool-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: custom-ippool-deploy spec: replicas: 3 selector: matchLabels: app: custom-ippool-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels: app: custom-ippool-deploy spec: containers: - name: custom-ippool-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As expected, Pods of Deployment custom-ippool-deploy will be assigned with IP addresses from IPPool custom-ipv4-ippool .","title":"Allocate IP addresses from the custom IPPool"},{"location":"usage/cli/","text":"Command line tool (spiderpoolctl) TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/cli/#command-line-tool-spiderpoolctl","text":"TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/debug/","text":"Q&A TODO.","title":"Q&A"},{"location":"usage/debug/#qa","text":"TODO.","title":"Q&amp;A"},{"location":"usage/install/","text":"Installation This guide shows how to install Spiderpool using Helm 3 . Generic Set up the Helm repository. helm repo add spiderpool https://spidernet-io.github.io/spiderpool Deploy Spiderpool using the default configuration options via Helm: helm install spiderpool spiderpool/spiderpool --namespace kube-system More details about Spiderpool charts parameters . It should be noted that the Pods of the spiderpool-controller have to run in the hostnetwork mode, because there may be no other IPAM CNI in the Kubernetes cluster that can allocate some IP addresses to them. In this regard, we used podAntiAffinity to ensure that different replicas of spiderpool-controller will not run on the same Node to avoid host port conflicts. The replicas of spiderpool-controller can be adjusted by setting parameter spiderpoolController.replicas , but please ensure that you have enough Nodes to run them. IP version Spiderpool can work in IPv4 only, IPv6 only or dual-stack case. For example, you can deploy Spiderpool in this way to enable dual stack: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set feature.enableIPv4=true \\ --set feature.enableIPv6=true By default, feature.enableIPv4 is enabled and feature.enableIPv6 is not. Certificates Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways. Auto Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=auto Note that the default value of parameter spiderpoolController.tls.method is auto . Provided If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n'` SERVER_CERT=`cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n'` SERVER_KEY=`cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n'` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=provided \\ --set spiderpoolController.tls.provided.tlsCa=${CA} \\ --set spiderpoolController.tls.provided.tlsCert=${SERVER_CERT} \\ --set spiderpoolController.tls.provided.tlsKey=${SERVER_KEY} Cert-manager It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=certmanager \\ --set spiderpoolController.tls.certmanager.issuerName=${CERT_MANAGER_ISSUER_NAME} Cluster default IPPool The cluster default IPPool usually serves some components that need IP addresses when the Kubernetes cluster is initialized, such as CoreDNS. Prepare the IP address ranges you need to use next: # \"CIDR\" IPV4_SUBNET_YOU_EXPECT=\"172.18.40.0/24\" # \"IP\" or \"IP-IP\" IPV4_IPRANGES_YOU_EXPECT=\"172.18.40.40-172.20.40.200\" Let's create an IPv4 IPPool while deploying Spiderpool, and configure it as the cluster default IPPool in the global Configmap configuration . helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set feature.enableIPv4=true \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${IPV4_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv4IPRanges={${IPV4_IPRANGES_YOU_EXPECT}} IPv6 IPPool is similar: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set feature.enableIPv6=true \\ --set clusterDefaultPool.installIPv6IPPool=true \\ --set clusterDefaultPool.ipv6Subnet=${IPV6_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv6IPRanges={${IPV6_IPRANGES_YOU_EXPECT}} Full example Here is a general deployment example, which satisfies the following conditions: Dual stack. Generate TLS certificates in auto mode. Create and configure the cluster default IPv4/6 IPPool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=auto \\ --set feature.enableIPv4=true \\ --set feature.enableIPv6=true \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.installIPv6IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${IPV4_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv4IPRanges={${IPV4_IPRANGES_YOU_EXPECT}} \\ --set clusterDefaultPool.ipv6Subnet=${IPV6_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv6IPRanges={${IPV6_IPRANGES_YOU_EXPECT}} Update CNI network configuration Finally, you should edit the CNI network configuration with the default path /etc/cni/net.d on the related Nodes so that the Main CNI can use Spiderpool IPAM CNI to allocate IP addresses. Replace the ipam object with the following: \"ipam\":{ \"type\":\"spiderpool\" } The following is an example for macvlan CNI : { \"cniVersion\":\"0.4.0\", \"type\":\"macvlan\", \"mode\":\"bridge\", \"master\":\"eth0\", \"name\":\"macvlan\", \"ipam\":{ \"type\":\"spiderpool\" } } Uninstall Generally, you can uninstall Spiderpool release in this way: helm uninstall spiderpool -n kube-system However, there are finalizers in some CRs of Spiderpool, the helm uninstall cmd may not clean up all relevant CRs. Get this cleanup script and execute it to ensure that unexpected errors will not occur when deploying Spiderpool next time. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Installation"},{"location":"usage/install/#installation","text":"This guide shows how to install Spiderpool using Helm 3 .","title":"Installation"},{"location":"usage/install/#generic","text":"Set up the Helm repository. helm repo add spiderpool https://spidernet-io.github.io/spiderpool Deploy Spiderpool using the default configuration options via Helm: helm install spiderpool spiderpool/spiderpool --namespace kube-system More details about Spiderpool charts parameters . It should be noted that the Pods of the spiderpool-controller have to run in the hostnetwork mode, because there may be no other IPAM CNI in the Kubernetes cluster that can allocate some IP addresses to them. In this regard, we used podAntiAffinity to ensure that different replicas of spiderpool-controller will not run on the same Node to avoid host port conflicts. The replicas of spiderpool-controller can be adjusted by setting parameter spiderpoolController.replicas , but please ensure that you have enough Nodes to run them.","title":"Generic"},{"location":"usage/install/#ip-version","text":"Spiderpool can work in IPv4 only, IPv6 only or dual-stack case. For example, you can deploy Spiderpool in this way to enable dual stack: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set feature.enableIPv4=true \\ --set feature.enableIPv6=true By default, feature.enableIPv4 is enabled and feature.enableIPv6 is not.","title":"IP version"},{"location":"usage/install/#certificates","text":"Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways.","title":"Certificates"},{"location":"usage/install/#auto","text":"Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=auto Note that the default value of parameter spiderpoolController.tls.method is auto .","title":"Auto"},{"location":"usage/install/#provided","text":"If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n'` SERVER_CERT=`cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n'` SERVER_KEY=`cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n'` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=provided \\ --set spiderpoolController.tls.provided.tlsCa=${CA} \\ --set spiderpoolController.tls.provided.tlsCert=${SERVER_CERT} \\ --set spiderpoolController.tls.provided.tlsKey=${SERVER_KEY}","title":"Provided"},{"location":"usage/install/#cert-manager","text":"It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=certmanager \\ --set spiderpoolController.tls.certmanager.issuerName=${CERT_MANAGER_ISSUER_NAME}","title":"Cert-manager"},{"location":"usage/install/#cluster-default-ippool","text":"The cluster default IPPool usually serves some components that need IP addresses when the Kubernetes cluster is initialized, such as CoreDNS. Prepare the IP address ranges you need to use next: # \"CIDR\" IPV4_SUBNET_YOU_EXPECT=\"172.18.40.0/24\" # \"IP\" or \"IP-IP\" IPV4_IPRANGES_YOU_EXPECT=\"172.18.40.40-172.20.40.200\" Let's create an IPv4 IPPool while deploying Spiderpool, and configure it as the cluster default IPPool in the global Configmap configuration . helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set feature.enableIPv4=true \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${IPV4_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv4IPRanges={${IPV4_IPRANGES_YOU_EXPECT}} IPv6 IPPool is similar: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set feature.enableIPv6=true \\ --set clusterDefaultPool.installIPv6IPPool=true \\ --set clusterDefaultPool.ipv6Subnet=${IPV6_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv6IPRanges={${IPV6_IPRANGES_YOU_EXPECT}}","title":"Cluster default IPPool"},{"location":"usage/install/#full-example","text":"Here is a general deployment example, which satisfies the following conditions: Dual stack. Generate TLS certificates in auto mode. Create and configure the cluster default IPv4/6 IPPool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=auto \\ --set feature.enableIPv4=true \\ --set feature.enableIPv6=true \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.installIPv6IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${IPV4_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv4IPRanges={${IPV4_IPRANGES_YOU_EXPECT}} \\ --set clusterDefaultPool.ipv6Subnet=${IPV6_SUBNET_YOU_EXPECT} \\ --set clusterDefaultPool.ipv6IPRanges={${IPV6_IPRANGES_YOU_EXPECT}}","title":"Full example"},{"location":"usage/install/#update-cni-network-configuration","text":"Finally, you should edit the CNI network configuration with the default path /etc/cni/net.d on the related Nodes so that the Main CNI can use Spiderpool IPAM CNI to allocate IP addresses. Replace the ipam object with the following: \"ipam\":{ \"type\":\"spiderpool\" } The following is an example for macvlan CNI : { \"cniVersion\":\"0.4.0\", \"type\":\"macvlan\", \"mode\":\"bridge\", \"master\":\"eth0\", \"name\":\"macvlan\", \"ipam\":{ \"type\":\"spiderpool\" } }","title":"Update CNI network configuration"},{"location":"usage/install/#uninstall","text":"Generally, you can uninstall Spiderpool release in this way: helm uninstall spiderpool -n kube-system However, there are finalizers in some CRs of Spiderpool, the helm uninstall cmd may not clean up all relevant CRs. Get this cleanup script and execute it to ensure that unexpected errors will not occur when deploying Spiderpool next time. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Uninstall"},{"location":"usage/ippool-affinity-namespace/","text":"Namespace affinity of IPPool Spiderpool supports affinity between IP pools and Namespaces. It means only Pods running under these Namespaces can use the IP pools that have an affinity to these Namespaces. Namespace affinity should be regarded as a filtering mechanism rather than a pool selection rule . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started First, create a new Namespace test-ns . kubectl create namespace test-ns Create an IPPool that will be bound to it. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.41 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns For convenience, this example uses a native Namespace label kubernetes.io/metadata.name as the matching condition of IPPool affinity. You can replace them with desired labels to match the corresponding Namespaces. Next, create two Deployments under test-ns and default Namespaces respectively, and configure the Pods therein to get IP addresses from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml You will find that the Deployment under Namespace test-ns is running. kubectl get deploy -n test-ns NAME READY UP-TO-DATE AVAILABLE AGE test-ns-deploy 1/1 1 1 35s And its Pod has been assigned with an IP address from that IPPool. kubectl get se -n test-ns NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-ns-deploy-74c6784f9-dlkmx eth0 test-ns-ipv4-ippool 172.18.41.41/24 spider-worker 46s However, the Deployment under Namespace default cannot work properly. You can troubleshoot with the Events of its Pod: kubectl describe po default-ns-deploy-5587c7bd47-xbmj2 -n default ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/default-ns-deploy-5587c7bd47-xbmj2 to spider-worker Warning FailedCreatePodSandBox 17s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"97f18ae3ee315f58347f8936f819dd20b29c2d0a3d457fc6f0022282bf513e91\": [default/default-ns-deploy-5587c7bd47-xbmj2:macvlan-cni-default]: error adding container to network \"macvlan-cni-default\": spiderpool IP allocation error: [POST /ipam/ip][500] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [test-ns-ipv4-ippool] of eth0 filtered out: unmatched Namespace affinity of IPPool test-ns-ipv4-ippool Obviously, this Pod has no permission to get IP addresses from IPPool test-ns-ipv4-ippool . You can specify a default IP pool for a Namespace and set the corresponding namespaceAffinity for the IPPool to achieve the effect of \"a Namespace static IP pool\". Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete ns test-ns kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml \\ --ignore-not-found=true","title":"Namespace affinity of IPPool"},{"location":"usage/ippool-affinity-namespace/#namespace-affinity-of-ippool","text":"Spiderpool supports affinity between IP pools and Namespaces. It means only Pods running under these Namespaces can use the IP pools that have an affinity to these Namespaces. Namespace affinity should be regarded as a filtering mechanism rather than a pool selection rule .","title":"Namespace affinity of IPPool"},{"location":"usage/ippool-affinity-namespace/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-affinity-namespace/#get-started","text":"First, create a new Namespace test-ns . kubectl create namespace test-ns Create an IPPool that will be bound to it. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.41 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns For convenience, this example uses a native Namespace label kubernetes.io/metadata.name as the matching condition of IPPool affinity. You can replace them with desired labels to match the corresponding Namespaces. Next, create two Deployments under test-ns and default Namespaces respectively, and configure the Pods therein to get IP addresses from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml You will find that the Deployment under Namespace test-ns is running. kubectl get deploy -n test-ns NAME READY UP-TO-DATE AVAILABLE AGE test-ns-deploy 1/1 1 1 35s And its Pod has been assigned with an IP address from that IPPool. kubectl get se -n test-ns NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-ns-deploy-74c6784f9-dlkmx eth0 test-ns-ipv4-ippool 172.18.41.41/24 spider-worker 46s However, the Deployment under Namespace default cannot work properly. You can troubleshoot with the Events of its Pod: kubectl describe po default-ns-deploy-5587c7bd47-xbmj2 -n default ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/default-ns-deploy-5587c7bd47-xbmj2 to spider-worker Warning FailedCreatePodSandBox 17s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"97f18ae3ee315f58347f8936f819dd20b29c2d0a3d457fc6f0022282bf513e91\": [default/default-ns-deploy-5587c7bd47-xbmj2:macvlan-cni-default]: error adding container to network \"macvlan-cni-default\": spiderpool IP allocation error: [POST /ipam/ip][500] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [test-ns-ipv4-ippool] of eth0 filtered out: unmatched Namespace affinity of IPPool test-ns-ipv4-ippool Obviously, this Pod has no permission to get IP addresses from IPPool test-ns-ipv4-ippool . You can specify a default IP pool for a Namespace and set the corresponding namespaceAffinity for the IPPool to achieve the effect of \"a Namespace static IP pool\".","title":"Get started"},{"location":"usage/ippool-affinity-namespace/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete ns test-ns kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml \\ --ignore-not-found=true","title":"Clean up"},{"location":"usage/ippool-affinity-node/","text":"Node affinity of IPPool Spiderpool supports affinity between IP pools and Nodes. It means only Pods running on these Nodes can use the IP pools that have an affinity to these Nodes. Node affinity should be regarded as a filtering mechanism rather than a pool selection rule . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started Since the cluster in this example has only two Nodes (1 master and 1 worker), it is required to remove the relevant taints on the master Node through kubectl taint , so that ordinary Pods can also be scheduled to it. If your cluster has two or more worker Nodes, please ignore the step above. Create two IPPools with 1 IP address each, one of which will provide IP addresses for all Pods running on the master Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40 nodeAffinity: matchExpressions: - {key: node-role.kubernetes.io/master, operator: Exists} The other provides IP addresses for the Pods on the worker Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.42.0/24 ips: - 172.18.42.40 nodeAffinity: matchExpressions: - {key: node-role.kubernetes.io/master, operator: DoesNotExist} Here, the value of the Node annotation node-role.kubernetes.io/master distinguishes two Nodes with different roles (or different node regions). If there is no annotation node-role.kubernetes.io/master on your Nodes, you can change it to another one or add some annotations you want. Then, create a Deployment with 2 replicas, and set podAntiAffinity to ensure that the two Pods which select the above IPPools according to the syntax of alternative IP pools can be scheduled to different Nodes. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-deploy spec: replicas: 2 selector: matchLabels: app: node-affinity-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"master-ipv4-ippool\", \"worker-ipv4-ippool\"] } labels: app: node-affinity-deploy spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: node-affinity-deploy topologyKey: kubernetes.io/hostname containers: - name: node-affinity-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] Finally, you will find that Pods on different Nodes will use different IPPools. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME node-affinity-deploy-66c9874465-rvdkm eth0 master-ipv4-ippool 172.18.41.40/24 spider-control-plane 35s node-affinity-deploy-66c9874465-wb8ds eth0 worker-ipv4-ippool 172.18.42.40/24 spider-worker 35s Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml \\ --ignore-not-found=true","title":"Node affinity of IPPool"},{"location":"usage/ippool-affinity-node/#node-affinity-of-ippool","text":"Spiderpool supports affinity between IP pools and Nodes. It means only Pods running on these Nodes can use the IP pools that have an affinity to these Nodes. Node affinity should be regarded as a filtering mechanism rather than a pool selection rule .","title":"Node affinity of IPPool"},{"location":"usage/ippool-affinity-node/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-affinity-node/#get-started","text":"Since the cluster in this example has only two Nodes (1 master and 1 worker), it is required to remove the relevant taints on the master Node through kubectl taint , so that ordinary Pods can also be scheduled to it. If your cluster has two or more worker Nodes, please ignore the step above. Create two IPPools with 1 IP address each, one of which will provide IP addresses for all Pods running on the master Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40 nodeAffinity: matchExpressions: - {key: node-role.kubernetes.io/master, operator: Exists} The other provides IP addresses for the Pods on the worker Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.42.0/24 ips: - 172.18.42.40 nodeAffinity: matchExpressions: - {key: node-role.kubernetes.io/master, operator: DoesNotExist} Here, the value of the Node annotation node-role.kubernetes.io/master distinguishes two Nodes with different roles (or different node regions). If there is no annotation node-role.kubernetes.io/master on your Nodes, you can change it to another one or add some annotations you want. Then, create a Deployment with 2 replicas, and set podAntiAffinity to ensure that the two Pods which select the above IPPools according to the syntax of alternative IP pools can be scheduled to different Nodes. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-deploy spec: replicas: 2 selector: matchLabels: app: node-affinity-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"master-ipv4-ippool\", \"worker-ipv4-ippool\"] } labels: app: node-affinity-deploy spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: node-affinity-deploy topologyKey: kubernetes.io/hostname containers: - name: node-affinity-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] Finally, you will find that Pods on different Nodes will use different IPPools. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME node-affinity-deploy-66c9874465-rvdkm eth0 master-ipv4-ippool 172.18.41.40/24 spider-control-plane 35s node-affinity-deploy-66c9874465-wb8ds eth0 worker-ipv4-ippool 172.18.42.40/24 spider-worker 35s","title":"Get started"},{"location":"usage/ippool-affinity-node/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml \\ --ignore-not-found=true","title":"Clean up"},{"location":"usage/ippool-affinity-pod/","text":"Pod affinity of IPPool Spiderpool supports affinity between IP pools and Pods. This feature helps you to better implement the capability of static IP for workloads (Deployment, StatefulSet, etc.). Pod affinity should be regarded as a filtering mechanism rather than a pool selection rule . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started First, create an IPPool configured with podAffinity . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: static-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.43 podAffinity: matchLabels: app: static This means that only the Pods with the label app: static can use this IPPool. Then, create a Deployment that select its Pods by using the same label and set the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ippool-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: static-ippool-deploy spec: replicas: 2 selector: matchLabels: app: static template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"static-ipv4-ippool\"] } labels: app: static spec: containers: - name: static-ippool-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] The Pods are running. kubectl get po -l app=static -o wide NAME READY STATUS RESTARTS AGE IP NODE static-ippool-deploy-7f478cc7d7-l7wm5 1/1 Running 0 20s 172.18.41.42 spider-control-plane static-ippool-deploy-7f478cc7d7-vphw9 1/1 Running 0 20s 172.18.41.40 spider-worker If necessary, you can try to create this Deployment which will also use the same IPPool static-ipv4-ippool to allocate IP addresses, but its Pods do not have the label app: static . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml As a result, these Pods cannot run successfully because they do not have permission to use the IPPool. kubectl describe po wrong-static-ippool-deploy-6c496cfb7d-wptq5 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 35s default-scheduler Successfully assigned default/wrong-static-ippool-deploy-6c496cfb7d-wptq5 to spider-worker Warning FailedCreatePodSandBox 34s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"a6f717aede91a356b552ad38c66112a26e5f7a4f7d23b7067870f33f05d350bc\": [default/wrong-static-ippool-deploy-6c496cfb7d-wptq5:macvlan-cni-default]: error adding container to network \"macvlan-cni-default\": spiderpool IP allocation error: [POST /ipam/ip][500] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [static-ipv4-ippool] of eth0 filtered out: unmatched Pod affinity of IPPool static-ipv4-ippool Summary The steps to enable static IP addresses (i.e, static IPPool) for a Deployment are as follows: Create an IPPool you expect and configure its podAffinity . Prepare a Deployment whose Pods have the label (or labels) matching the affinity. Usually, you should select the value of the field matchLabels as this label (or labels). Specify pool selection rules through the annotation ipam.spidernet.io/ippool or ipam.spidernet.io/ippools (multi-NIC) in Pod template. You can learn more about their differences here . Apply the manifest of this Deployment. An interesting question is, with the scale up of a Deployment, what happens when the IP addresses in the IPPool are insufficient? The result is that some Pods will not work because they cannot be assigned with some IP addresses and you have to manually expand the IPPool to solve this problem. However, if you enable the feature SpiderSubnet of Spiderpool, the IPPool will not only be automatically created with the creation of the Deployment (or other workloads), but also automatically scale up/down with the change of replicas . More details about SpiderSubnet . Finally, as for how StatefulSet enables static IP addresses, you can get some help here . Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ipv4-ippool.yaml \\ --ignore-not-found=true","title":"Pod affinity of IPPool"},{"location":"usage/ippool-affinity-pod/#pod-affinity-of-ippool","text":"Spiderpool supports affinity between IP pools and Pods. This feature helps you to better implement the capability of static IP for workloads (Deployment, StatefulSet, etc.). Pod affinity should be regarded as a filtering mechanism rather than a pool selection rule .","title":"Pod affinity of IPPool"},{"location":"usage/ippool-affinity-pod/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-affinity-pod/#get-started","text":"First, create an IPPool configured with podAffinity . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: static-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.43 podAffinity: matchLabels: app: static This means that only the Pods with the label app: static can use this IPPool. Then, create a Deployment that select its Pods by using the same label and set the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ippool-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: static-ippool-deploy spec: replicas: 2 selector: matchLabels: app: static template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"static-ipv4-ippool\"] } labels: app: static spec: containers: - name: static-ippool-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] The Pods are running. kubectl get po -l app=static -o wide NAME READY STATUS RESTARTS AGE IP NODE static-ippool-deploy-7f478cc7d7-l7wm5 1/1 Running 0 20s 172.18.41.42 spider-control-plane static-ippool-deploy-7f478cc7d7-vphw9 1/1 Running 0 20s 172.18.41.40 spider-worker If necessary, you can try to create this Deployment which will also use the same IPPool static-ipv4-ippool to allocate IP addresses, but its Pods do not have the label app: static . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml As a result, these Pods cannot run successfully because they do not have permission to use the IPPool. kubectl describe po wrong-static-ippool-deploy-6c496cfb7d-wptq5 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 35s default-scheduler Successfully assigned default/wrong-static-ippool-deploy-6c496cfb7d-wptq5 to spider-worker Warning FailedCreatePodSandBox 34s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"a6f717aede91a356b552ad38c66112a26e5f7a4f7d23b7067870f33f05d350bc\": [default/wrong-static-ippool-deploy-6c496cfb7d-wptq5:macvlan-cni-default]: error adding container to network \"macvlan-cni-default\": spiderpool IP allocation error: [POST /ipam/ip][500] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [static-ipv4-ippool] of eth0 filtered out: unmatched Pod affinity of IPPool static-ipv4-ippool","title":"Get started"},{"location":"usage/ippool-affinity-pod/#summary","text":"The steps to enable static IP addresses (i.e, static IPPool) for a Deployment are as follows: Create an IPPool you expect and configure its podAffinity . Prepare a Deployment whose Pods have the label (or labels) matching the affinity. Usually, you should select the value of the field matchLabels as this label (or labels). Specify pool selection rules through the annotation ipam.spidernet.io/ippool or ipam.spidernet.io/ippools (multi-NIC) in Pod template. You can learn more about their differences here . Apply the manifest of this Deployment. An interesting question is, with the scale up of a Deployment, what happens when the IP addresses in the IPPool are insufficient? The result is that some Pods will not work because they cannot be assigned with some IP addresses and you have to manually expand the IPPool to solve this problem. However, if you enable the feature SpiderSubnet of Spiderpool, the IPPool will not only be automatically created with the creation of the Deployment (or other workloads), but also automatically scale up/down with the change of replicas . More details about SpiderSubnet . Finally, as for how StatefulSet enables static IP addresses, you can get some help here .","title":"Summary"},{"location":"usage/ippool-affinity-pod/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/static-ipv4-ippool.yaml \\ --ignore-not-found=true","title":"Clean up"},{"location":"usage/ippool-multi/","text":"Multiple IPPool Spiderpool can specify multiple alternative IP pools for one IP allocation. Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get Started First, create two IPPools each containing 2 IP addresses. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml Create a Pod and allocate an IP address to it from these IPPools. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml You will find that you still have 3 available IP addresses, one in IPPool default-ipv4-ippool and two in IPPool backup-ipv4-ippool . kubectl get sp -l case=backup NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE backup-ipv4-ippool 4 172.18.42.0/24 0 2 false default-ipv4-ippool 4 172.18.41.0/24 1 2 false Then, create a Deployment with 2 replicas and allocate IP addresses to its Pods from the two IPPools above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: multi-ippool-deploy spec: replicas: 2 selector: matchLabels: app: multi-ippool-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"default-ipv4-ippool\", \"backup-ipv4-ippool\"] } labels: app: multi-ippool-deploy spec: containers: - name: multi-ippool-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] Spiderpool will successively try to allocate IP addresses in the order of the elements in the \"IP pool array\" until the first allocation succeeds or all fail. Of course, you can specify the pool selection rules (that defines alternative IP pools) in many ways, the Pod annotation ipam.spidernet.io/ippool is used here to select IP pools. Finally, when addresses in IPPool default-ipv4-ippool are used up, the IPPool backup-ipv4-ippool takes over. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME dummy eth0 default-ipv4-ippool 172.18.41.41/24 spider-worker 1m20s multi-ippool-deploy-669bf7cf79-4x88m eth0 default-ipv4-ippool 172.18.41.40/24 spider-worker 2m31s multi-ippool-deploy-669bf7cf79-k7zkk eth0 backup-ipv4-ippool 172.18.42.41/24 spider-worker 2m31s Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml \\ --ignore-not-found=true","title":"Multiple IPPool"},{"location":"usage/ippool-multi/#multiple-ippool","text":"Spiderpool can specify multiple alternative IP pools for one IP allocation.","title":"Multiple IPPool"},{"location":"usage/ippool-multi/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-multi/#get-started","text":"First, create two IPPools each containing 2 IP addresses. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml Create a Pod and allocate an IP address to it from these IPPools. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml You will find that you still have 3 available IP addresses, one in IPPool default-ipv4-ippool and two in IPPool backup-ipv4-ippool . kubectl get sp -l case=backup NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE backup-ipv4-ippool 4 172.18.42.0/24 0 2 false default-ipv4-ippool 4 172.18.41.0/24 1 2 false Then, create a Deployment with 2 replicas and allocate IP addresses to its Pods from the two IPPools above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: multi-ippool-deploy spec: replicas: 2 selector: matchLabels: app: multi-ippool-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"default-ipv4-ippool\", \"backup-ipv4-ippool\"] } labels: app: multi-ippool-deploy spec: containers: - name: multi-ippool-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] Spiderpool will successively try to allocate IP addresses in the order of the elements in the \"IP pool array\" until the first allocation succeeds or all fail. Of course, you can specify the pool selection rules (that defines alternative IP pools) in many ways, the Pod annotation ipam.spidernet.io/ippool is used here to select IP pools. Finally, when addresses in IPPool default-ipv4-ippool are used up, the IPPool backup-ipv4-ippool takes over. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME dummy eth0 default-ipv4-ippool 172.18.41.41/24 spider-worker 1m20s multi-ippool-deploy-669bf7cf79-4x88m eth0 default-ipv4-ippool 172.18.41.40/24 spider-worker 2m31s multi-ippool-deploy-669bf7cf79-k7zkk eth0 backup-ipv4-ippool 172.18.42.41/24 spider-worker 2m31s","title":"Get Started"},{"location":"usage/ippool-multi/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml \\ --ignore-not-found=true","title":"Clean up"},{"location":"usage/ippool-namespace/","text":"Namespace default IPPool Spiderpool provides default IP pools at Namespace level. A Pod not configured with a pool selection rule of higher priority will be assigned with IP addresses from the default IP pools of its Namespace. Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started Create a Namespace named as test-ns1 . bash kubectl create ns test-ns1 Create an IPPool to be bound with Namespace test-ns1 . bash kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml Check the status of this IPPool with the following command. bash kubectl get sp -l case=ns NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ns1-default-ipv4-ippool 4 172.18.41.0/24 0 4 false Specify pool selection rules for Namespace test-ns1 with the following command and annotation. bash kubectl patch ns test-ns1 --patch-file https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-ippool-selection-patch.yaml yaml metadata: annotations: ipam.spidernet.io/default-ipv4-ippool: '[\"ns1-default-ipv4-ippool\"]' Create a Deployment with 3 replicas in the Namespace test-ns1 . bash kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ippool-deploy.yaml Now, all Pods in the Namespace should have been assigned with an IP address from the specified IPPool. Verify it with the following command: kubectl get se -n test-ns1 NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME ns1-default-ippool-deploy-7cd5449c88-9xncm eth0 ns1-default-ipv4-ippool 172.18.41.41/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-dpfjs eth0 ns1-default-ipv4-ippool 172.18.41.43/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-vjtdd eth0 ns1-default-ipv4-ippool 172.18.41.42/24 spider-worker 58s The Namespace annotation ipam.spidernet.io/defaultv4ippool also supports the syntax of alternative IP pools , which means you can specify multiple default IP pools for a Namespace . In addition, one IPPool can be specified as the default IP pool for different Namespaces. If you want to bind an IPPool to a specific Namespace in an exclusive way, it means that no Namespace other than this (or a group of Namespaces) has permission to use this IPPool, please refer to SpiderIPPool namespace affinity . Clean up Clean relevant resources so that you can run this tutorial again. kubectl delete ns test-ns1 kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml --ignore-not-found=true","title":"Namespace default IPPool"},{"location":"usage/ippool-namespace/#namespace-default-ippool","text":"Spiderpool provides default IP pools at Namespace level. A Pod not configured with a pool selection rule of higher priority will be assigned with IP addresses from the default IP pools of its Namespace.","title":"Namespace default IPPool"},{"location":"usage/ippool-namespace/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-namespace/#get-started","text":"Create a Namespace named as test-ns1 . bash kubectl create ns test-ns1 Create an IPPool to be bound with Namespace test-ns1 . bash kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml Check the status of this IPPool with the following command. bash kubectl get sp -l case=ns NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ns1-default-ipv4-ippool 4 172.18.41.0/24 0 4 false Specify pool selection rules for Namespace test-ns1 with the following command and annotation. bash kubectl patch ns test-ns1 --patch-file https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-ippool-selection-patch.yaml yaml metadata: annotations: ipam.spidernet.io/default-ipv4-ippool: '[\"ns1-default-ipv4-ippool\"]' Create a Deployment with 3 replicas in the Namespace test-ns1 . bash kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ippool-deploy.yaml Now, all Pods in the Namespace should have been assigned with an IP address from the specified IPPool. Verify it with the following command: kubectl get se -n test-ns1 NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME ns1-default-ippool-deploy-7cd5449c88-9xncm eth0 ns1-default-ipv4-ippool 172.18.41.41/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-dpfjs eth0 ns1-default-ipv4-ippool 172.18.41.43/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-vjtdd eth0 ns1-default-ipv4-ippool 172.18.41.42/24 spider-worker 58s The Namespace annotation ipam.spidernet.io/defaultv4ippool also supports the syntax of alternative IP pools , which means you can specify multiple default IP pools for a Namespace . In addition, one IPPool can be specified as the default IP pool for different Namespaces. If you want to bind an IPPool to a specific Namespace in an exclusive way, it means that no Namespace other than this (or a group of Namespaces) has permission to use this IPPool, please refer to SpiderIPPool namespace affinity .","title":"Get started"},{"location":"usage/ippool-namespace/#clean-up","text":"Clean relevant resources so that you can run this tutorial again. kubectl delete ns test-ns1 kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml --ignore-not-found=true","title":"Clean up"},{"location":"usage/ipv6/","text":"Spiderpool IPv6 support Description This page describes how to configure and allocate IP addresses with Spiderpool by using dual stack, IPv4 only, or IPv6 only. Features Spiderpool supports: Dual stack (default) Each workload can get IPv4 and IPv6 addresses, and can communicate over IPv4 or IPv6. IPv4 only Each workload can acquire IPv4 addresses, and can communicate over IPv4. IPv6 only Each workload can acquire IPv6 addresses, and can communicate over IPv6. Get Started Enable IPv4 only Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer to install for details Check whether the property enableIPv4 of the configmap spiderpool-conf is already set to true and whether the property enableIPv6 is set to false . kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to update it with true , run helm upgrade spiderpool spiderpool/spiderpool --set feature.enableIPv4=true --set feature.enableIPv6=false -n kube-system . Enable dual stack Same as the above, run helm upgrade spiderpool spiderpool/spiderpool --set feature.enableIPv4=true --set feature.enableIPv6=true -n kube-system .","title":"Spiderpool IPv6 support"},{"location":"usage/ipv6/#spiderpool-ipv6-support","text":"","title":"Spiderpool IPv6 support"},{"location":"usage/ipv6/#description","text":"This page describes how to configure and allocate IP addresses with Spiderpool by using dual stack, IPv4 only, or IPv6 only.","title":"Description"},{"location":"usage/ipv6/#features","text":"Spiderpool supports: Dual stack (default) Each workload can get IPv4 and IPv6 addresses, and can communicate over IPv4 or IPv6. IPv4 only Each workload can acquire IPv4 addresses, and can communicate over IPv4. IPv6 only Each workload can acquire IPv6 addresses, and can communicate over IPv6.","title":"Features"},{"location":"usage/ipv6/#get-started","text":"","title":"Get Started"},{"location":"usage/ipv6/#enable-ipv4-only","text":"Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer to install for details Check whether the property enableIPv4 of the configmap spiderpool-conf is already set to true and whether the property enableIPv6 is set to false . kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to update it with true , run helm upgrade spiderpool spiderpool/spiderpool --set feature.enableIPv4=true --set feature.enableIPv6=false -n kube-system .","title":"Enable IPv4 only"},{"location":"usage/ipv6/#enable-dual-stack","text":"Same as the above, run helm upgrade spiderpool spiderpool/spiderpool --set feature.enableIPv4=true --set feature.enableIPv6=true -n kube-system .","title":"Enable dual stack"},{"location":"usage/multi-interfaces-annotation/","text":"Pod annotation of multi-NIC Spiderpool supports specifying the IP pools for each interface by Pod annotation in multi-NIC scenario. Creating Pods with multiple interfaces depends on Multus CNI . Before reading this guide, please ensure that you have installed Multus CNI and can use it skillfully. Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get Started First, let's take a look at the Multus CNI network configuration used in this example: cat /etc/cni/net.d/00-multus.conf { \"cniVersion\": \"0.3.1\", \"name\": \"multus-cni-network\", \"type\": \"multus\", \"confDir\": \"/etc/cni/net.d/\" , \"capabilities\": { \"portMappings\": true }, \"clusterNetwork\": \"macvlan-cni-default\", \"multusNamespace\": \"kube-system\", \"kubeconfig\": \"/etc/cni/net.d/multus.d/multus.kubeconfig\" } We configure macvlan-cni-default as the default CNI network of the Kubernetes cluster, which is a combination of macvlan CNI and Spiderpool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/macvlan-cni-default.yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: macvlan-cni-default namespace: kube-system spec: config: '{ \"cniVersion\": \"0.3.1\", \"type\": \"macvlan\", \"mode\": \"bridge\", \"master\": \"eth0\", \"name\": \"macvlan-cni-default\", \"ipam\": { \"type\": \"spiderpool\" } }' Create two IPPools to provide IP addresses for different interfaces. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE eth0-ipv4-ippool 4 172.18.41.0/24 0 2 false net1-ipv4-ippool 4 172.18.42.0/24 0 2 false Then, create a Deployment whose Pod is attached an additional interface (macvlan) through the Multus annotation k8s.v1.cni.cncf.io/networks . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multi-macvlan-interfaces-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: multi-macvlan-interfaces-deploy spec: replicas: 1 selector: matchLabels: app: multi-macvlan-interfaces-deploy template: metadata: annotations: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni-default ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4\": [\"eth0-ipv4-ippool\"] },{ \"interface\": \"net1\", \"ipv4\": [\"net1-ipv4-ippool\"] }] labels: app: multi-macvlan-interfaces-deploy spec: containers: - name: multi-macvlan-interfaces-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] The Pod annotation ipam.spidernet.io/ippools specifies the pool selection rules for each interfaces in the form of an array, which means that executing the CNI ADD command with the environment parameter CNI_IFNAME as eth0 will get an IP allocation result from IPPool eth0-ipv4-ippool . The interface net1 works in a similar way. As for the reason why the two interfaces are named eth0 and net1 respectively, it is because that is the convention of Multus CNI. Generally, the first interface (default interface) of a Pod will be named eth0 , and the additional interfaces attached will be named net1 , net2 ... Finally, you can check the details of the IP allocation result. kubectl get se multi-macvlan-interfaces-deploy-b99b55bd7-gvvqt -o jsonpath='{.status.current}' | jq { \"containerID\": \"57e7a0a713bc16bfeb2390969a43daef99d1625c8bebc841646a90fa854900f3\", \"creationTime\": \"2022-11-24T05:22:19Z\", \"ips\": [ { \"interface\": \"eth0\", \"ipv4\": \"172.18.41.41/24\", \"ipv4Pool\": \"eth0-ipv4-ippool\", \"vlan\": 0 }, { \"interface\": \"net1\", \"ipv4\": \"172.18.42.40/24\", \"ipv4Pool\": \"net1-ipv4-ippool\", \"vlan\": 0 } ], \"node\": \"spider-worker\" } Inspect the container. kubectl exec -it multi-macvlan-interfaces-deploy-b99b55bd7-gvvqt -- ip a ... 4: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 46:34:cc:2e:70:2c brd ff:ff:ff:ff:ff:ff inet 172.18.41.41/24 brd 172.18.41.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::4434:ccff:fe2e:702c/64 scope link valid_lft forever preferred_lft forever 5: net1@if13: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether aa:e3:32:27:75:01 brd ff:ff:ff:ff:ff:ff inet 172.18.42.40/24 brd 172.18.42.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::a8e3:32ff:fe27:7501/64 scope link valid_lft forever preferred_lft forever Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/macvlan-cni-default.yaml \\ --ignore-not-found=true","title":"Pod annotation of multi-NIC"},{"location":"usage/multi-interfaces-annotation/#pod-annotation-of-multi-nic","text":"Spiderpool supports specifying the IP pools for each interface by Pod annotation in multi-NIC scenario. Creating Pods with multiple interfaces depends on Multus CNI . Before reading this guide, please ensure that you have installed Multus CNI and can use it skillfully.","title":"Pod annotation of multi-NIC"},{"location":"usage/multi-interfaces-annotation/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/multi-interfaces-annotation/#get-started","text":"First, let's take a look at the Multus CNI network configuration used in this example: cat /etc/cni/net.d/00-multus.conf { \"cniVersion\": \"0.3.1\", \"name\": \"multus-cni-network\", \"type\": \"multus\", \"confDir\": \"/etc/cni/net.d/\" , \"capabilities\": { \"portMappings\": true }, \"clusterNetwork\": \"macvlan-cni-default\", \"multusNamespace\": \"kube-system\", \"kubeconfig\": \"/etc/cni/net.d/multus.d/multus.kubeconfig\" } We configure macvlan-cni-default as the default CNI network of the Kubernetes cluster, which is a combination of macvlan CNI and Spiderpool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/macvlan-cni-default.yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: macvlan-cni-default namespace: kube-system spec: config: '{ \"cniVersion\": \"0.3.1\", \"type\": \"macvlan\", \"mode\": \"bridge\", \"master\": \"eth0\", \"name\": \"macvlan-cni-default\", \"ipam\": { \"type\": \"spiderpool\" } }' Create two IPPools to provide IP addresses for different interfaces. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE eth0-ipv4-ippool 4 172.18.41.0/24 0 2 false net1-ipv4-ippool 4 172.18.42.0/24 0 2 false Then, create a Deployment whose Pod is attached an additional interface (macvlan) through the Multus annotation k8s.v1.cni.cncf.io/networks . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multi-macvlan-interfaces-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: multi-macvlan-interfaces-deploy spec: replicas: 1 selector: matchLabels: app: multi-macvlan-interfaces-deploy template: metadata: annotations: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni-default ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4\": [\"eth0-ipv4-ippool\"] },{ \"interface\": \"net1\", \"ipv4\": [\"net1-ipv4-ippool\"] }] labels: app: multi-macvlan-interfaces-deploy spec: containers: - name: multi-macvlan-interfaces-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] The Pod annotation ipam.spidernet.io/ippools specifies the pool selection rules for each interfaces in the form of an array, which means that executing the CNI ADD command with the environment parameter CNI_IFNAME as eth0 will get an IP allocation result from IPPool eth0-ipv4-ippool . The interface net1 works in a similar way. As for the reason why the two interfaces are named eth0 and net1 respectively, it is because that is the convention of Multus CNI. Generally, the first interface (default interface) of a Pod will be named eth0 , and the additional interfaces attached will be named net1 , net2 ... Finally, you can check the details of the IP allocation result. kubectl get se multi-macvlan-interfaces-deploy-b99b55bd7-gvvqt -o jsonpath='{.status.current}' | jq { \"containerID\": \"57e7a0a713bc16bfeb2390969a43daef99d1625c8bebc841646a90fa854900f3\", \"creationTime\": \"2022-11-24T05:22:19Z\", \"ips\": [ { \"interface\": \"eth0\", \"ipv4\": \"172.18.41.41/24\", \"ipv4Pool\": \"eth0-ipv4-ippool\", \"vlan\": 0 }, { \"interface\": \"net1\", \"ipv4\": \"172.18.42.40/24\", \"ipv4Pool\": \"net1-ipv4-ippool\", \"vlan\": 0 } ], \"node\": \"spider-worker\" } Inspect the container. kubectl exec -it multi-macvlan-interfaces-deploy-b99b55bd7-gvvqt -- ip a ... 4: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 46:34:cc:2e:70:2c brd ff:ff:ff:ff:ff:ff inet 172.18.41.41/24 brd 172.18.41.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::4434:ccff:fe2e:702c/64 scope link valid_lft forever preferred_lft forever 5: net1@if13: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether aa:e3:32:27:75:01 brd ff:ff:ff:ff:ff:ff inet 172.18.42.40/24 brd 172.18.42.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::a8e3:32ff:fe27:7501/64 scope link valid_lft forever preferred_lft forever","title":"Get Started"},{"location":"usage/multi-interfaces-annotation/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/macvlan-cni-default.yaml \\ --ignore-not-found=true","title":"Clean up"},{"location":"usage/multi-interfaces-cni/","text":"Multi-Interfaces-CNI You can create multiple interfaces with Multus CNI , and use the CNI configuration file to implement them. Set up Spiderpool If you have not set up Spiderpool yet, follow the guide Quick Installation for instructions on how to install and simply configure Spiderpool. Set up Multus If you have not set up Multus yet, follow the guide Quick Installation for instructions on how to install and simply configure Multus. Get Started Following the Multus CNI Configuration steps to implement the CNI configuration file, and create pod with multiple interfaces.","title":"Multi-Interfaces-CNI"},{"location":"usage/multi-interfaces-cni/#multi-interfaces-cni","text":"You can create multiple interfaces with Multus CNI , and use the CNI configuration file to implement them.","title":"Multi-Interfaces-CNI"},{"location":"usage/multi-interfaces-cni/#set-up-spiderpool","text":"If you have not set up Spiderpool yet, follow the guide Quick Installation for instructions on how to install and simply configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/multi-interfaces-cni/#set-up-multus","text":"If you have not set up Multus yet, follow the guide Quick Installation for instructions on how to install and simply configure Multus.","title":"Set up Multus"},{"location":"usage/multi-interfaces-cni/#get-started","text":"Following the Multus CNI Configuration steps to implement the CNI configuration file, and create pod with multiple interfaces.","title":"Get Started"},{"location":"usage/reserved-ip/","text":"Reserved IP Spiderpool reserve some IP addresses for the whole Kubernetes cluster, which will not be used by any IPAM allocation results. Typically, these IP addresses are external IP addresses or cannot be used for network communication (e.g. broadcast address). IPPool excludeIPs You may have observed that there is a field excludeIPs in SpiderIPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main function is not like this. Field excludeIPs is more of a syntax sugar , so that users can more flexibly define the IP address ranges of the IPPool. For example, create an IPPool without using excludeIPs , which contains two IP ranges: 172.18.41.40-172.18.41.44 and 172.18.41.46-172.18.41.50 , you should define the ips as follows: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: not-use-excludeips spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 But in fact, this semantics can be more succinctly described through excludeIPs : apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: use-excludeips spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.50 excludeIPs: - 172.18.41.45 Field excludeIPs will make sure that any Pod that allocates IP addresses from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with excludeIPs defined. Use SpiderReservedIP Unlike configuring field excluedIPs in SpiderIPPool CR, creating a SpiderReservedIP CR is really a way to define the global reserved IP address rules of a Kubernetes cluster. The IP addresses defined in ReservedIP cannot be used by any Pod in the cluster, regardless of whether some IPPools have inadvertently defined them. More details refer to definition of SpiderReservedIP . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get Started To understand how it works, let's do such a test. First, create an ReservedIP which reserves 9 IP addresses from 172.18.42.41 to 172.18.42.49 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-ipv4-reservedip spec: ipVersion: 4 ips: - 172.18.42.41-172.18.42.49 At the same time, create an IPPool with 10 IP addresses from 172.18.42.41 to 172.18.42.50 . Yes, we deliberately make it hold one more IP address than the ReservedIP above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.42.0/24 ips: - 172.18.42.41-172.18.42.50 Then, create a Deployment with 3 replicas and allocate IP addresses to its Pods from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: reservedip-deploy spec: replicas: 3 selector: matchLabels: app: reservedip-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ipv4-ippool\"] } labels: app: reservedip-deploy spec: containers: - name: reservedip-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] After a while, only one of these Pods using IP 172.18.42.50 can run successfully because \"all IP used out\". kubectl get po -l app=reservedip-deploy -o wide NAME READY STATUS RESTARTS AGE IP NODE reservedip-deploy-6cf9858886-cm7bp 0/1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-lb7cr 0/1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-pkcfl 1/1 Running 0 35s 172.18.42.50 spider-worker But when you delete this ReservedIP, everything will return to normal. kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle its IP addresses, but ReservedIP will still ensure that when the Pod is terminated, no other Pods can continue to use the reserved IP address. Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work. Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml \\ --ignore-not-found=true A Trap So, can you use IPPool's field excludeIPs to achieve the same effect as ReservedIP? The answer is NO ! Look at such a case, now you want to reserve an IP 172.18.43.31 for an external application of the Kubernetes cluster, which may be a Redis node. To achieve this, you created such an IPPool: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: already-in-use spec: ipVersion: 4 subnet: 172.18.43.0/24 ips: - 172.18.43.1-172.18.43.31 excludeIPs: - 172.18.43.31 I believe that if there is only one IPPool under the subnet 172.18.43.0/24 network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: created-by-someone-else spec: ipVersion: 4 subnet: 172.18.43.0/24 ips: - 172.18.43.31-172.18.43.50 Different IPPools allow to define the same field subnet , more details refer to validation of IPPool . After a period of time, a Pod may be allocated with IP 172.18.43.31 from the IPPool created-by-someone-else , and then it holds the same IP address as your Redis node. After that, the Redis may not work as well. So, if you really want to reserve an IP address instead of excluding an IP address, SpiderReservedIP makes life better.","title":"Reserved IP"},{"location":"usage/reserved-ip/#reserved-ip","text":"Spiderpool reserve some IP addresses for the whole Kubernetes cluster, which will not be used by any IPAM allocation results. Typically, these IP addresses are external IP addresses or cannot be used for network communication (e.g. broadcast address).","title":"Reserved IP"},{"location":"usage/reserved-ip/#ippool-excludeips","text":"You may have observed that there is a field excludeIPs in SpiderIPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main function is not like this. Field excludeIPs is more of a syntax sugar , so that users can more flexibly define the IP address ranges of the IPPool. For example, create an IPPool without using excludeIPs , which contains two IP ranges: 172.18.41.40-172.18.41.44 and 172.18.41.46-172.18.41.50 , you should define the ips as follows: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: not-use-excludeips spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 But in fact, this semantics can be more succinctly described through excludeIPs : apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: use-excludeips spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.50 excludeIPs: - 172.18.41.45 Field excludeIPs will make sure that any Pod that allocates IP addresses from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with excludeIPs defined.","title":"IPPool excludeIPs"},{"location":"usage/reserved-ip/#use-spiderreservedip","text":"Unlike configuring field excluedIPs in SpiderIPPool CR, creating a SpiderReservedIP CR is really a way to define the global reserved IP address rules of a Kubernetes cluster. The IP addresses defined in ReservedIP cannot be used by any Pod in the cluster, regardless of whether some IPPools have inadvertently defined them. More details refer to definition of SpiderReservedIP .","title":"Use SpiderReservedIP"},{"location":"usage/reserved-ip/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/reserved-ip/#get-started","text":"To understand how it works, let's do such a test. First, create an ReservedIP which reserves 9 IP addresses from 172.18.42.41 to 172.18.42.49 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-ipv4-reservedip spec: ipVersion: 4 ips: - 172.18.42.41-172.18.42.49 At the same time, create an IPPool with 10 IP addresses from 172.18.42.41 to 172.18.42.50 . Yes, we deliberately make it hold one more IP address than the ReservedIP above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.42.0/24 ips: - 172.18.42.41-172.18.42.50 Then, create a Deployment with 3 replicas and allocate IP addresses to its Pods from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: reservedip-deploy spec: replicas: 3 selector: matchLabels: app: reservedip-deploy template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ipv4-ippool\"] } labels: app: reservedip-deploy spec: containers: - name: reservedip-deploy image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] After a while, only one of these Pods using IP 172.18.42.50 can run successfully because \"all IP used out\". kubectl get po -l app=reservedip-deploy -o wide NAME READY STATUS RESTARTS AGE IP NODE reservedip-deploy-6cf9858886-cm7bp 0/1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-lb7cr 0/1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-pkcfl 1/1 Running 0 35s 172.18.42.50 spider-worker But when you delete this ReservedIP, everything will return to normal. kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle its IP addresses, but ReservedIP will still ensure that when the Pod is terminated, no other Pods can continue to use the reserved IP address. Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work.","title":"Get Started"},{"location":"usage/reserved-ip/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml \\ --ignore-not-found=true","title":"Clean up"},{"location":"usage/reserved-ip/#a-trap","text":"So, can you use IPPool's field excludeIPs to achieve the same effect as ReservedIP? The answer is NO ! Look at such a case, now you want to reserve an IP 172.18.43.31 for an external application of the Kubernetes cluster, which may be a Redis node. To achieve this, you created such an IPPool: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: already-in-use spec: ipVersion: 4 subnet: 172.18.43.0/24 ips: - 172.18.43.1-172.18.43.31 excludeIPs: - 172.18.43.31 I believe that if there is only one IPPool under the subnet 172.18.43.0/24 network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool: apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: created-by-someone-else spec: ipVersion: 4 subnet: 172.18.43.0/24 ips: - 172.18.43.31-172.18.43.50 Different IPPools allow to define the same field subnet , more details refer to validation of IPPool . After a period of time, a Pod may be allocated with IP 172.18.43.31 from the IPPool created-by-someone-else , and then it holds the same IP address as your Redis node. After that, the Redis may not work as well. So, if you really want to reserve an IP address instead of excluding an IP address, SpiderReservedIP makes life better.","title":"A Trap"},{"location":"usage/spider-subnet/","text":"SpiderSubnet The Spiderpool owns a CRD SpiderSubnet, which can help applications (such as Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet) to create a corresponding SpiderIPPool. Here are some annotations that you should write down on the application template pod annotation: Annotation Description Example ipam.spidernet.io/subnet Choose one SpiderSubnet V4 and V6 CR to use {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/subnets Choose multiple SpiderSubnet V4 and V6 CR to use (the current version only supports to use the first one) [{\"interface\":\"eth0\", \"ipv4\":[\"v4-subnet1\"],\"ipv6\":[\"v6-subnet1\"]}] ipam.spidernet.io/ippool-ip-number The IP numbers of the corresponding SpiderIPPool (fixed and flexible mode) +2 ipam.spidernet.io/ippool-reclaim Specify the corresponding SpiderIPPool to delete or not once the application was deleted (default true) true Notice The annotation ipam.spidernet.io/subnets has higher priority over ipam.spidernet.io/subnet . If you specify both of them two, it only uses ipam.spidernet.io/subnets mode. For annotation ipam.spidernet.io/ippool-ip-number , you can use '2' for fixed IP number or '+2' for flexible mode. The value '+2' means the SpiderSubnet auto-created IPPool will add 2 more IPs based on your application replicas. If you choose to use flexible mode, the auto-created IPPool IPs will expand or shrink dynamically by your application replicas. The current version only supports to use one SpiderSubnet V4/V6 CR, you shouldn't specify 2 or more SpiderSubnet V4 CRs and the spiderpool-controller will choose the first one to use. Get Started Enable SpiderSubnet feature Firstly, please ensure you have installed the Spiderpool and configure the CNI file, refer install for details. Check configmap spiderpool-conf property enableSpiderSubnet whether is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , just execute helm upgrade spiderpool spiderpool/spiderpool --set feature.enableSpiderSubnet=true -n kube-system . Set cluster Subnet DefaultFlexibleIPNumber In the upper operation you can also find property clusterSubnetDefaultFlexibleIPNumber in configmap spiderpool-conf , this is the cluster default flexible IP number. For example, if you set it 1 and it will be the same with ipam.spidernet.io/ippool-ip-number: +1 . Furthermore, you could do not specify the annotation because it will use the clusterSubnetDefaultFlexibleIPNumber by default. If you want to change it, just execute helm upgrade spiderpool spiderpool/spiderpool --set clusterDefaultPool.subnetFlexibleIPNumber=2 -n kube-system Create a SpiderSubnet Install a SpiderSubnet example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/subnet-demo.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/deploy-use-subnet.yaml Check the related CR data Here's the SpiderSubnet, SpiderIPPool, and Pod information. NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 3 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 3 200 ------------------------------------------------------------------------------------------ $ kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e 4 172.16.0.0/16 1 3 false auto-deployment-default-demo-deploy-subnet-v6-eth0-6b26cd19032e 6 fc00:f853:ccd:e790::/64 1 3 false ------------------------------------------------------------------------------------------ $ kubectl get sp auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: creationTimestamp: \"2022-12-26T06:16:04Z\" finalizers: - spiderpool.spidernet.io generation: 2 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/ippool-version: IPv4 ipam.spidernet.io/owner-application: Deployment_default_demo-deploy-subnet ipam.spidernet.io/owner-application-uid: 9608de8b-fe9b-4a20-9d7c-6b26cd19032e ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: ac4ab322-ddde-4a6f-9290-bc795b60ca3f resourceVersion: \"1690\" uid: e09ae97c-9693-4d78-b49f-412573e1da95 spec: disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.3 podAffinity: matchLabels: app: demo-deploy-subnet subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 1 allocatedIPs: 172.16.41.2: containerID: 9e5ccb7900f32c6dc76ed3fbd309724ca6eb0235d2a18b2e850c7c91fa28f91d interface: eth0 namespace: default node: spider-worker ownerControllerName: demo-deploy-subnet ownerControllerType: Deployment pod: demo-deploy-subnet-b454f5b69-vsc8d autoDesiredIPCount: 3 totalIPCount: 3 ------------------------------------------------------------------------------------------ $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-b454f5b69-vsc8d 1/1 Running 0 67s 172.16.41.2 spider-worker <none> <none> Try to scale the deployment replicas and check the SpiderIPPool. $ kubectl patch deploy demo-deploy-subnet --patch '{\"spec\": {\"replicas\": 2}}' deployment.apps/demo-deploy-subnet patched ------------------------------------------------------------------------------------------ $ kubectl get ss NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 4 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 4 200 ------------------------------------------------------------------------------------------ $ kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e 4 172.16.0.0/16 2 4 false auto-deployment-default-demo-deploy-subnet-v6-eth0-6b26cd19032e 6 fc00:f853:ccd:e790::/64 2 4 false ------------------------------------------------------------------------------------------ $ kubectl get sp auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: creationTimestamp: \"2022-12-26T06:16:04Z\" finalizers: - spiderpool.spidernet.io generation: 3 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/ippool-version: IPv4 ipam.spidernet.io/owner-application: Deployment_default_demo-deploy-subnet ipam.spidernet.io/owner-application-uid: 9608de8b-fe9b-4a20-9d7c-6b26cd19032e ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: ac4ab322-ddde-4a6f-9290-bc795b60ca3f resourceVersion: \"2009\" uid: e09ae97c-9693-4d78-b49f-412573e1da95 spec: disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.3 - 172.16.41.200 podAffinity: matchLabels: app: demo-deploy-subnet subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 2 allocatedIPs: 172.16.41.2: containerID: 9e5ccb7900f32c6dc76ed3fbd309724ca6eb0235d2a18b2e850c7c91fa28f91d interface: eth0 namespace: default node: spider-worker ownerControllerName: demo-deploy-subnet ownerControllerType: Deployment pod: demo-deploy-subnet-b454f5b69-vsc8d 172.16.41.3: containerID: 3eea370feb7557749098f002a72e95377a369e9af53cc1af6dc9f21cbebec82c interface: eth0 namespace: default node: spider-control-plane ownerControllerName: demo-deploy-subnet ownerControllerType: Deployment pod: demo-deploy-subnet-b454f5b69-687w4 autoDesiredIPCount: 4 totalIPCount: 4 ------------------------------------------------------------------------------------------ $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-b454f5b69-687w4 1/1 Running 0 2m35s 172.16.41.3 spider-control-plane <none> <none> demo-deploy-subnet-b454f5b69-vsc8d 1/1 Running 0 4m38s 172.16.41.2 spider-worker <none> <none> As you can see, the SpiderSubnet object subnet-demo-v4 allocates another IP to SpiderIPPool auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e and SpiderSubnet object subnet-demo-v6 allocates another IP to SpiderIPPool auto-deployment-default-demo-deploy-subnet-v6-eth0-6b26cd19032e . Cluster Default SpiderSubnet In order to simplify SpiderSubnet usage, we add ClusterDefaultSubnet support. Once we enable SpiderSubnet feature and have Cluster default subnet, we can create the application directly without any other annotations. Check ClusterDefaultSubnet Firstly, let's check the configmap clusterDefaultIPv4Subnet and clusterDefaultIPv6Subnet properties. If there are no values, we can set it by ourselves. kubectl -n kube-system get configmap spiderpool-conf -o yaml Create Application Just create the deployment without any other subnet annotations. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/cluster-default-subnet-deployment.yaml Notice it will create the auto-create IPPool with the configmap clusterSubnetDefaultFlexibleIPNumber property. The ClusterDefaultSubnet function only supports single interface SpiderSubnet with multiple interfaces Make sure the interface name and use ipam.spidernet.io/subnets annotation just like this: annotations: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni2 ipam.spidernet.io/subnets: |- [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}] Install the example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/multiple-interfaces.yaml","title":"SpiderSubnet"},{"location":"usage/spider-subnet/#spidersubnet","text":"The Spiderpool owns a CRD SpiderSubnet, which can help applications (such as Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet) to create a corresponding SpiderIPPool. Here are some annotations that you should write down on the application template pod annotation: Annotation Description Example ipam.spidernet.io/subnet Choose one SpiderSubnet V4 and V6 CR to use {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/subnets Choose multiple SpiderSubnet V4 and V6 CR to use (the current version only supports to use the first one) [{\"interface\":\"eth0\", \"ipv4\":[\"v4-subnet1\"],\"ipv6\":[\"v6-subnet1\"]}] ipam.spidernet.io/ippool-ip-number The IP numbers of the corresponding SpiderIPPool (fixed and flexible mode) +2 ipam.spidernet.io/ippool-reclaim Specify the corresponding SpiderIPPool to delete or not once the application was deleted (default true) true","title":"SpiderSubnet"},{"location":"usage/spider-subnet/#notice","text":"The annotation ipam.spidernet.io/subnets has higher priority over ipam.spidernet.io/subnet . If you specify both of them two, it only uses ipam.spidernet.io/subnets mode. For annotation ipam.spidernet.io/ippool-ip-number , you can use '2' for fixed IP number or '+2' for flexible mode. The value '+2' means the SpiderSubnet auto-created IPPool will add 2 more IPs based on your application replicas. If you choose to use flexible mode, the auto-created IPPool IPs will expand or shrink dynamically by your application replicas. The current version only supports to use one SpiderSubnet V4/V6 CR, you shouldn't specify 2 or more SpiderSubnet V4 CRs and the spiderpool-controller will choose the first one to use.","title":"Notice"},{"location":"usage/spider-subnet/#get-started","text":"","title":"Get Started"},{"location":"usage/spider-subnet/#enable-spidersubnet-feature","text":"Firstly, please ensure you have installed the Spiderpool and configure the CNI file, refer install for details. Check configmap spiderpool-conf property enableSpiderSubnet whether is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , just execute helm upgrade spiderpool spiderpool/spiderpool --set feature.enableSpiderSubnet=true -n kube-system .","title":"Enable SpiderSubnet feature"},{"location":"usage/spider-subnet/#set-cluster-subnet-defaultflexibleipnumber","text":"In the upper operation you can also find property clusterSubnetDefaultFlexibleIPNumber in configmap spiderpool-conf , this is the cluster default flexible IP number. For example, if you set it 1 and it will be the same with ipam.spidernet.io/ippool-ip-number: +1 . Furthermore, you could do not specify the annotation because it will use the clusterSubnetDefaultFlexibleIPNumber by default. If you want to change it, just execute helm upgrade spiderpool spiderpool/spiderpool --set clusterDefaultPool.subnetFlexibleIPNumber=2 -n kube-system","title":"Set cluster Subnet DefaultFlexibleIPNumber"},{"location":"usage/spider-subnet/#create-a-spidersubnet","text":"Install a SpiderSubnet example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/subnet-demo.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/deploy-use-subnet.yaml","title":"Create a SpiderSubnet"},{"location":"usage/spider-subnet/#check-the-related-cr-data","text":"Here's the SpiderSubnet, SpiderIPPool, and Pod information. NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 3 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 3 200 ------------------------------------------------------------------------------------------ $ kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e 4 172.16.0.0/16 1 3 false auto-deployment-default-demo-deploy-subnet-v6-eth0-6b26cd19032e 6 fc00:f853:ccd:e790::/64 1 3 false ------------------------------------------------------------------------------------------ $ kubectl get sp auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: creationTimestamp: \"2022-12-26T06:16:04Z\" finalizers: - spiderpool.spidernet.io generation: 2 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/ippool-version: IPv4 ipam.spidernet.io/owner-application: Deployment_default_demo-deploy-subnet ipam.spidernet.io/owner-application-uid: 9608de8b-fe9b-4a20-9d7c-6b26cd19032e ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: ac4ab322-ddde-4a6f-9290-bc795b60ca3f resourceVersion: \"1690\" uid: e09ae97c-9693-4d78-b49f-412573e1da95 spec: disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.3 podAffinity: matchLabels: app: demo-deploy-subnet subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 1 allocatedIPs: 172.16.41.2: containerID: 9e5ccb7900f32c6dc76ed3fbd309724ca6eb0235d2a18b2e850c7c91fa28f91d interface: eth0 namespace: default node: spider-worker ownerControllerName: demo-deploy-subnet ownerControllerType: Deployment pod: demo-deploy-subnet-b454f5b69-vsc8d autoDesiredIPCount: 3 totalIPCount: 3 ------------------------------------------------------------------------------------------ $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-b454f5b69-vsc8d 1/1 Running 0 67s 172.16.41.2 spider-worker <none> <none> Try to scale the deployment replicas and check the SpiderIPPool. $ kubectl patch deploy demo-deploy-subnet --patch '{\"spec\": {\"replicas\": 2}}' deployment.apps/demo-deploy-subnet patched ------------------------------------------------------------------------------------------ $ kubectl get ss NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 4 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 4 200 ------------------------------------------------------------------------------------------ $ kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e 4 172.16.0.0/16 2 4 false auto-deployment-default-demo-deploy-subnet-v6-eth0-6b26cd19032e 6 fc00:f853:ccd:e790::/64 2 4 false ------------------------------------------------------------------------------------------ $ kubectl get sp auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: creationTimestamp: \"2022-12-26T06:16:04Z\" finalizers: - spiderpool.spidernet.io generation: 3 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/ippool-version: IPv4 ipam.spidernet.io/owner-application: Deployment_default_demo-deploy-subnet ipam.spidernet.io/owner-application-uid: 9608de8b-fe9b-4a20-9d7c-6b26cd19032e ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: ac4ab322-ddde-4a6f-9290-bc795b60ca3f resourceVersion: \"2009\" uid: e09ae97c-9693-4d78-b49f-412573e1da95 spec: disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.3 - 172.16.41.200 podAffinity: matchLabels: app: demo-deploy-subnet subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 2 allocatedIPs: 172.16.41.2: containerID: 9e5ccb7900f32c6dc76ed3fbd309724ca6eb0235d2a18b2e850c7c91fa28f91d interface: eth0 namespace: default node: spider-worker ownerControllerName: demo-deploy-subnet ownerControllerType: Deployment pod: demo-deploy-subnet-b454f5b69-vsc8d 172.16.41.3: containerID: 3eea370feb7557749098f002a72e95377a369e9af53cc1af6dc9f21cbebec82c interface: eth0 namespace: default node: spider-control-plane ownerControllerName: demo-deploy-subnet ownerControllerType: Deployment pod: demo-deploy-subnet-b454f5b69-687w4 autoDesiredIPCount: 4 totalIPCount: 4 ------------------------------------------------------------------------------------------ $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-b454f5b69-687w4 1/1 Running 0 2m35s 172.16.41.3 spider-control-plane <none> <none> demo-deploy-subnet-b454f5b69-vsc8d 1/1 Running 0 4m38s 172.16.41.2 spider-worker <none> <none> As you can see, the SpiderSubnet object subnet-demo-v4 allocates another IP to SpiderIPPool auto-deployment-default-demo-deploy-subnet-v4-eth0-6b26cd19032e and SpiderSubnet object subnet-demo-v6 allocates another IP to SpiderIPPool auto-deployment-default-demo-deploy-subnet-v6-eth0-6b26cd19032e .","title":"Check the related CR data"},{"location":"usage/spider-subnet/#cluster-default-spidersubnet","text":"In order to simplify SpiderSubnet usage, we add ClusterDefaultSubnet support. Once we enable SpiderSubnet feature and have Cluster default subnet, we can create the application directly without any other annotations.","title":"Cluster Default SpiderSubnet"},{"location":"usage/spider-subnet/#check-clusterdefaultsubnet","text":"Firstly, let's check the configmap clusterDefaultIPv4Subnet and clusterDefaultIPv6Subnet properties. If there are no values, we can set it by ourselves. kubectl -n kube-system get configmap spiderpool-conf -o yaml","title":"Check ClusterDefaultSubnet"},{"location":"usage/spider-subnet/#create-application","text":"Just create the deployment without any other subnet annotations. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/cluster-default-subnet-deployment.yaml","title":"Create Application"},{"location":"usage/spider-subnet/#notice_1","text":"it will create the auto-create IPPool with the configmap clusterSubnetDefaultFlexibleIPNumber property. The ClusterDefaultSubnet function only supports single interface","title":"Notice"},{"location":"usage/spider-subnet/#spidersubnet-with-multiple-interfaces","text":"Make sure the interface name and use ipam.spidernet.io/subnets annotation just like this: annotations: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni2 ipam.spidernet.io/subnets: |- [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}] Install the example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/multiple-interfaces.yaml","title":"SpiderSubnet with multiple interfaces"},{"location":"usage/statefulset/","text":"StatefulSet Description The spiderpool supports IP assignment for StatefulSet. When the number of statefulset replicas is not scaled up or down, all pods could hold same IP address even when the pod is restarting or rebuilding. Pod restarts Once a pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID. In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property. Pod is deleted and re-created After deleting a StatefulSet pod, kubernetes will re-create a pod with the same name. In this case, Spiderpool will also keep the previous IP and update the ContainerID. Notice Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its pods are running. When the statefulset is scaled down and then scaled up, the scaled-up pod is not guaranteed to get the IP of scaled-down pod event they have the same name The RIPOGT feature (reclaim IP for the pod of graceful-period timeout) does work for statefulset pod. Get Started Enable StatefulSet support Firstly, please ensure you have installed the spiderpool and configure the CNI file. Refer to install for details. Check whether the property enableStatefulSet of the configmap spiderpool-conf is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , run helm upgrade spiderpool spiderpool/spiderpool --set feature.enableStatefulSet=true -n kube-system . Create a StatefulSet This is an example to install a StatefulSet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset/statefulset-demo.yaml Validate the Spiderpool related CR data Here's the created Pod, spiderippool, and spiderendpoint CR information: ```text $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 82s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ... ``` Try to delete pod demo-sts-0 and check whether the rebuilding pod keeps the previous IP or not. ```text $ kubectl delete po demo-sts-0 pod \"demo-sts-0\" deleted $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 20s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker history: - containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker - containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ownerControllerType: StatefulSet ... ``` And you can see, the re-created Pod still holds the previous IP, spiderippool, and spiderendpoint updated containerID property. clean up Delete the StatefulSet object: demo-sts . $ kubectl delete sts demo-sts statefulset.apps \"demo-sts\" deleted --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0 --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml Error from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found The related data is cleaned up now.","title":"StatefulSet"},{"location":"usage/statefulset/#statefulset","text":"","title":"StatefulSet"},{"location":"usage/statefulset/#description","text":"The spiderpool supports IP assignment for StatefulSet. When the number of statefulset replicas is not scaled up or down, all pods could hold same IP address even when the pod is restarting or rebuilding. Pod restarts Once a pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID. In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property. Pod is deleted and re-created After deleting a StatefulSet pod, kubernetes will re-create a pod with the same name. In this case, Spiderpool will also keep the previous IP and update the ContainerID.","title":"Description"},{"location":"usage/statefulset/#notice","text":"Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its pods are running. When the statefulset is scaled down and then scaled up, the scaled-up pod is not guaranteed to get the IP of scaled-down pod event they have the same name The RIPOGT feature (reclaim IP for the pod of graceful-period timeout) does work for statefulset pod.","title":"Notice"},{"location":"usage/statefulset/#get-started","text":"","title":"Get Started"},{"location":"usage/statefulset/#enable-statefulset-support","text":"Firstly, please ensure you have installed the spiderpool and configure the CNI file. Refer to install for details. Check whether the property enableStatefulSet of the configmap spiderpool-conf is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , run helm upgrade spiderpool spiderpool/spiderpool --set feature.enableStatefulSet=true -n kube-system .","title":"Enable StatefulSet support"},{"location":"usage/statefulset/#create-a-statefulset","text":"This is an example to install a StatefulSet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset/statefulset-demo.yaml","title":"Create a StatefulSet"},{"location":"usage/statefulset/#validate-the-spiderpool-related-cr-data","text":"Here's the created Pod, spiderippool, and spiderendpoint CR information: ```text $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 82s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ... ``` Try to delete pod demo-sts-0 and check whether the rebuilding pod keeps the previous IP or not. ```text $ kubectl delete po demo-sts-0 pod \"demo-sts-0\" deleted $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 20s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker history: - containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker - containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ownerControllerType: StatefulSet ... ``` And you can see, the re-created Pod still holds the previous IP, spiderippool, and spiderendpoint updated containerID property.","title":"Validate the Spiderpool related CR data"},{"location":"usage/statefulset/#clean-up","text":"Delete the StatefulSet object: demo-sts . $ kubectl delete sts demo-sts statefulset.apps \"demo-sts\" deleted --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0 --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml Error from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found The related data is cleaned up now.","title":"clean up"},{"location":"usage/third-party-controller/","text":"Spiderpool supports third-party controllers Description This page will use OpenKruise to demonstrate how Spiderpool supports third-party controllers. As for how to install OpenKruise with Helm , please refer to OpenKruise Notice In the current version, we do not support extended StatefulSet to keep fixed IP address because we can't get the extended StatefulSet object yaml to fetch its replicas for scaling the IPPool IPs counts. For kubernetes StatefulSet, we use the StatefulSet object replicas property and stable and serial pod name to choose whether to keep its IP address for next schedule. If the third party controller controllers StatefulSet , it surely runs normally and keeps the StatefulSet characteristic. Set up Spiderpool If you have not deployed Spiderpool yet, see installation for instructions on how to deploy and configure Spiderpool. Create an IPPool Next, let's create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: custom-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.50 Run Finally, create an OpenKruise CloneSet that has 3 replicas. apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: custom-kruise-cloneset spec: replicas: 3 selector: matchLabels: app: custom-kruise-cloneset template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels: app: custom-kruise-cloneset spec: containers: - name: custom-kruise-cloneset image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1/1 Running 0 96s 172.18.41.44 spider-worker <none> 2/2 custom-kruise-cloneset-c4z9f 1/1 Running 0 96s 172.18.41.50 spider-worker <none> 2/2 custom-kruise-cloneset-w9kfm 1/1 Running 0 96s 172.18.41.46 spider-worker <none> 2/2 Third-party controller with SpiderSubnet feature We also support third-party controllers to use SpiderSubnet feature, it is used as the normal kubernetes application controllers. Refer SpiderSubnet for details. Notice You must specify a fixed IP number for auto-created IPPool if you want to use SpiderSubnet feature. Here's an example ipam.spidernet.io/ippool-ip-number: \"5\" We don't support reclaim IPPool for third-party controller currently. So, setting annotation ipam.spidernet.io/ippool-reclaim: \"true\" does not take effect. And you need to delete the corresponding auto-created IPPool by yourself once you clean up the third-party controller application. Run We assume you have already enabled SpiderSubnet feature and created cluster default subnet. The following two yaml will lead to the same effect. apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: custom-kruise-cloneset spec: replicas: 3 selector: matchLabels: app: custom-kruise-cloneset template: metadata: annotations: ipam.spidernet.io/ippool-ip-number: \"5\" labels: app: custom-kruise-cloneset spec: containers: - name: custom-kruise-cloneset image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: custom-kruise-cloneset spec: replicas: 3 selector: matchLabels: app: custom-kruise-cloneset template: metadata: annotations: ipam.spidernet.io/subnet: |- {\"ipv4\": [\"default-v4-subnet\"], \"ipv6\": [\"default-v6-subnet\"]} ipam.spidernet.io/ippool-ip-number: \"5\" labels: app: custom-kruise-cloneset spec: containers: - name: custom-kruise-cloneset image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As expected, Spiderpool will create auto-created IPPool from default-v4-subnet and default-v6-subnet objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE auto-unknown-default-custom-kruise-cloneset-v4-eth0-1f5d4dcf3251 4 172.18.0.0/16 3 5 false auto-unknown-default-custom-kruise-cloneset-v6-eth0-1f5d4dcf3251 6 fc00:f853:ccd:e793::/64 3 5 false ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-98q8r 1/1 Running 0 3m27s 172.18.40.8 spider-worker <none> 2/2 custom-kruise-cloneset-gql4j 1/1 Running 0 3m27s 172.18.40.9 spider-worker <none> 2/2 custom-kruise-cloneset-kzt5q 1/1 Running 0 3m27s 172.18.40.6 spider-worker <none> 2/2","title":"Spiderpool supports third-party controllers"},{"location":"usage/third-party-controller/#spiderpool-supports-third-party-controllers","text":"","title":"Spiderpool supports third-party controllers"},{"location":"usage/third-party-controller/#description","text":"This page will use OpenKruise to demonstrate how Spiderpool supports third-party controllers. As for how to install OpenKruise with Helm , please refer to OpenKruise","title":"Description"},{"location":"usage/third-party-controller/#notice","text":"In the current version, we do not support extended StatefulSet to keep fixed IP address because we can't get the extended StatefulSet object yaml to fetch its replicas for scaling the IPPool IPs counts. For kubernetes StatefulSet, we use the StatefulSet object replicas property and stable and serial pod name to choose whether to keep its IP address for next schedule. If the third party controller controllers StatefulSet , it surely runs normally and keeps the StatefulSet characteristic.","title":"Notice"},{"location":"usage/third-party-controller/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, see installation for instructions on how to deploy and configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/third-party-controller/#create-an-ippool","text":"Next, let's create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: custom-ipv4-ippool spec: ipVersion: 4 subnet: 172.18.41.0/24 ips: - 172.18.41.40-172.18.41.50","title":"Create an IPPool"},{"location":"usage/third-party-controller/#run","text":"Finally, create an OpenKruise CloneSet that has 3 replicas. apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: custom-kruise-cloneset spec: replicas: 3 selector: matchLabels: app: custom-kruise-cloneset template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels: app: custom-kruise-cloneset spec: containers: - name: custom-kruise-cloneset image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1/1 Running 0 96s 172.18.41.44 spider-worker <none> 2/2 custom-kruise-cloneset-c4z9f 1/1 Running 0 96s 172.18.41.50 spider-worker <none> 2/2 custom-kruise-cloneset-w9kfm 1/1 Running 0 96s 172.18.41.46 spider-worker <none> 2/2","title":"Run"},{"location":"usage/third-party-controller/#third-party-controller-with-spidersubnet-feature","text":"We also support third-party controllers to use SpiderSubnet feature, it is used as the normal kubernetes application controllers. Refer SpiderSubnet for details.","title":"Third-party controller with SpiderSubnet feature"},{"location":"usage/third-party-controller/#notice_1","text":"You must specify a fixed IP number for auto-created IPPool if you want to use SpiderSubnet feature. Here's an example ipam.spidernet.io/ippool-ip-number: \"5\" We don't support reclaim IPPool for third-party controller currently. So, setting annotation ipam.spidernet.io/ippool-reclaim: \"true\" does not take effect. And you need to delete the corresponding auto-created IPPool by yourself once you clean up the third-party controller application.","title":"Notice"},{"location":"usage/third-party-controller/#run_1","text":"We assume you have already enabled SpiderSubnet feature and created cluster default subnet. The following two yaml will lead to the same effect. apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: custom-kruise-cloneset spec: replicas: 3 selector: matchLabels: app: custom-kruise-cloneset template: metadata: annotations: ipam.spidernet.io/ippool-ip-number: \"5\" labels: app: custom-kruise-cloneset spec: containers: - name: custom-kruise-cloneset image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata: name: custom-kruise-cloneset spec: replicas: 3 selector: matchLabels: app: custom-kruise-cloneset template: metadata: annotations: ipam.spidernet.io/subnet: |- {\"ipv4\": [\"default-v4-subnet\"], \"ipv6\": [\"default-v6-subnet\"]} ipam.spidernet.io/ippool-ip-number: \"5\" labels: app: custom-kruise-cloneset spec: containers: - name: custom-kruise-cloneset image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As expected, Spiderpool will create auto-created IPPool from default-v4-subnet and default-v6-subnet objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE auto-unknown-default-custom-kruise-cloneset-v4-eth0-1f5d4dcf3251 4 172.18.0.0/16 3 5 false auto-unknown-default-custom-kruise-cloneset-v6-eth0-1f5d4dcf3251 6 fc00:f853:ccd:e793::/64 3 5 false ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-98q8r 1/1 Running 0 3m27s 172.18.40.8 spider-worker <none> 2/2 custom-kruise-cloneset-gql4j 1/1 Running 0 3m27s 172.18.40.9 spider-worker <none> 2/2 custom-kruise-cloneset-kzt5q 1/1 Running 0 3m27s 172.18.40.6 spider-worker <none> 2/2","title":"Run"}]}
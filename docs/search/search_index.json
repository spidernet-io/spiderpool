{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spiderpool English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is a CNCF Landscape Level Project . Introduction Spiderpool is a Kubernetes underlay network solution. It provides rich IPAM features and CNI integration capabilities, powering CNI projects in the open source community, allowing multiple CNIs to collaborate effectively. It enables underlay CNI to run perfectly in environments such as bare metal, virtual machines, and any public cloud . Why developing Spiderpool? Currently, the open source community does not provide comprehensive, friendly, and intelligent underlay network solutions, so Spiderpool aims to provide many innovative features: Rich IPAM feature. Shared and dedicated IP pools, assigning fixed IP address, automatic operation of dedicated IP pools for creating, scaling, and reclaiming. It could match kinds of underlay network requirements. Underlay CNI and overlay CNI cooperation, multiple CNI interfaces for pod. Spiderpool helps assign IP address to multiple underlay interfaces, coordinate policy route between interfaces to ensure consistence data path of request and reply packets. Multiple CNIs cooperate to reduce hardware requirements for deploying the cluster. Enhance underlay CNI like Macvlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI to connect Pod and host to access clusterIP and check pod health, and to detect IP conflict and gateway accessibility. Not only limited to bare metal environments in data centers, but also providing a unified underlay CNI solution for openstack, vmware, and various public cloud scenarios. underlay CNI There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. The article provides a brief comparison of IPAM and network performance between the two technologies, which offers better insights into the unique features and use cases of Spiderpool. Why underlay network solutions? The following requirements necessitate underlay network solutions: Network performance. Network performance advantages, like low latency, high throughput, low forwarding overhead of node\u2019s CPU. It fits to applications like financial and AI application. Transformation cost. The traditional host application has characteristic, like exposing service with host fixed IP, separating communication with different subnets. At the beginning of migrating to the kubernetes, the underlay network solution spend low transformation cost of netowrk, application can directly use Pod IP for cluster east-west and north-south communication. Network security. In the data center, it may be used to enforce network security to Pod, like firewall and isolating communication with VLAN. The underlay network solution could expose the Pod packet without tunnel encapsulation, and meet requirements. Bandwitdth. It can help Pod to customize the exit network interface of the node, thereby ensuring bandwidth isolation of the underlying subnet. It can help components such as kubevirt , storage, and logging, to transmit massive amounts of data. Multi clusters. For multi clusters, all Pods of multi clusters are connected to the underlay network, so they could communicate smoothly without the need to deploy additional components for cluster connectivity. Architecture Spiderpool consists of the following components: Spiderpool controller: deployment that manage CRD validation, status updates, IP recovery, and automated IP pools Spiderpool agent: daemonset that help Spiderpool plugins by performing IP allocation and coordinator plugin for information synchronization. IPAM plugin: a binary plugin on each host that CNI can utilize to implement IP allocation. coordinator plugin: a binary plugin on each host that CNI can use for multi-NIC route coordination, IP conflict detection, and host connectivity. ifacer plugin: A binary plugin on each host that helps CNIs such as macvlan and ipvlan dynamically create bond and vlan interfaces On top of its own components, Spiderpool relies on open-source underlay CNIs to allocate network interfaces to Pods. You can use Multus CNI to manage multiple NICs and CNI configurations. Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool, such as: Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI Use case: underlay CNIs access layer2 In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI , SR-IOV CNI , ipvlan CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding). Use case: adding an auxiliary underlay CNI NIC for overlay CNI In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes. Use case: underlay CNI on public cloud and VM It is hard to implement underlay CNI in public cloud, openstack, vmvare. It requires the vendor underlay CNI on specific environments, as these environments typically have the following limitations: The IAAS network infrastructure implements MAC restrictions for packets. On the one hand, security checks are conducted on the source MAC to ensure that the source MAC address is the same as the MAC address of VM network interface. On the other hand, restrictions have been placed on the destination MAC, which only supports packet forwarding by the MAC address of VM network interfaces. The MAC address of the Pod in the common CNI plugin is newly generated, which leads to Pod communication failure. The IAAS network infrastructure implements IP restrictions on packets. Only when the destination and source IP of the packet are assigned to VM, packet could be forwarded rightly. The common CNI plugin assigns IP addresses to Pods that do not comply with IAAS settings, which leads to Pod communication failure. Spiderpool provides IP pool based on node topology, aligning with IP allocation settings of VMs. In conjunction with ipvlan CNI, it provides underlay CNI solutions for various public cloud environments. Quick start Refer to Quick start , set up a cluster quickly. Major features For applications requiring static IP addresses, it could be supported by IP pools owning limited IP adddress set and pod affinity. See example for more details. For applications not requiring static IP addresses, they can share an IP pool. See example for more details. For stateful applications, IP addresses can be automatically fixed for each Pod, and the overall IP scaling range can be fixed as well. See example for more details. Subnet feature, on the one hand, could help to separate the responsibility from the infrastructure administrator and the application administrator. On the other hand, it supports to automatically create and dynamically scale the fixed IP ippools to each applcation requiring static IPs. which could help reduce operation burden of IP pools burden. See example for more details. In additional to kubernetes-native controller, subnet feature also supports third-party pod controllers based on operator. See example for details. For Pods of an application run across different network zones, it could assign IP addresses of different subnets. See example for details. Support to assign IP address from different subnets to multiple NICs of a Pod, and help coordinate policy route between interfaces to ensure consistent data path of request and reply packets. For scenarios involving multiple Underlay NICs, please refer to the example . For scenarios involving one Overlay NIC and multiple Underlay NICs, please refer to the example . It supports to set default IP pools for the cluster or for the namespace. Besides, A IP pool could be shared by the whole cluster or bound to a specified namespace. See example for details. Strengthen CNI like Macvlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , to access clusterIP and pod healthy check ( example ), to detect IP conflict and gateway reachability ( example ). Node-based IP pool feature meets the complex subnet design of each node. refer to example On vmware vsphere platform, Spiderpool underlay network solution does not require \"hybrid forwarding\" mode of the Vswitch , which ensures the network performance of vsphere platform. refer to example Spiderpool underlay network solution could run on public cloud of any vendors, and openstack platform, which could meet needs of multi cloud and hybrid cloud with unified CNI stack\u3002refer to alibabaCloud When starting the Pod, it could help dynamically build the bond interface and vlan interface for the master interface of Macvlan CNI , ipvlan CNI . See example for details. It could specify customized routes by IP pool and pod annotation. See example for details. Easy generation of Multus NetworkAttachmentDefinition custom resources with best-practice CNI configuration, also ensure well formatted JSON to improve experience. See example for details. Multiple IP pools can be set for the application for prevent IP address from running out. See example for details. Set reserved IPs that will not be assigned to Pods, it can avoid misusing IP addresses already taken by hosts out of the cluster. See example for details. Outstanding performance for assigning and releasing Pod IPs, showcased in the test report . Well-designed IP reclaim mechanism could help assign IP address in time and quickly recover from the breakdown for the cluster or application. See example for details. All above features can work in ipv4-only, ipv6-only, and dual-stack scenarios. See example for details. Support AMD64 and ARM64. Metrics Blogs Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 Cloud-Native Spiderpool: IP Allocation Across Network Zones Spiderpool: a new solution to fixed application IPs for Calico \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6 Roadmap roadmap","title":"Spiderpool"},{"location":"#spiderpool","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is a CNCF Landscape Level Project .","title":"Spiderpool"},{"location":"#introduction","text":"Spiderpool is a Kubernetes underlay network solution. It provides rich IPAM features and CNI integration capabilities, powering CNI projects in the open source community, allowing multiple CNIs to collaborate effectively. It enables underlay CNI to run perfectly in environments such as bare metal, virtual machines, and any public cloud . Why developing Spiderpool? Currently, the open source community does not provide comprehensive, friendly, and intelligent underlay network solutions, so Spiderpool aims to provide many innovative features: Rich IPAM feature. Shared and dedicated IP pools, assigning fixed IP address, automatic operation of dedicated IP pools for creating, scaling, and reclaiming. It could match kinds of underlay network requirements. Underlay CNI and overlay CNI cooperation, multiple CNI interfaces for pod. Spiderpool helps assign IP address to multiple underlay interfaces, coordinate policy route between interfaces to ensure consistence data path of request and reply packets. Multiple CNIs cooperate to reduce hardware requirements for deploying the cluster. Enhance underlay CNI like Macvlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI to connect Pod and host to access clusterIP and check pod health, and to detect IP conflict and gateway accessibility. Not only limited to bare metal environments in data centers, but also providing a unified underlay CNI solution for openstack, vmware, and various public cloud scenarios.","title":"Introduction"},{"location":"#underlay-cni","text":"There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. The article provides a brief comparison of IPAM and network performance between the two technologies, which offers better insights into the unique features and use cases of Spiderpool. Why underlay network solutions? The following requirements necessitate underlay network solutions: Network performance. Network performance advantages, like low latency, high throughput, low forwarding overhead of node\u2019s CPU. It fits to applications like financial and AI application. Transformation cost. The traditional host application has characteristic, like exposing service with host fixed IP, separating communication with different subnets. At the beginning of migrating to the kubernetes, the underlay network solution spend low transformation cost of netowrk, application can directly use Pod IP for cluster east-west and north-south communication. Network security. In the data center, it may be used to enforce network security to Pod, like firewall and isolating communication with VLAN. The underlay network solution could expose the Pod packet without tunnel encapsulation, and meet requirements. Bandwitdth. It can help Pod to customize the exit network interface of the node, thereby ensuring bandwidth isolation of the underlying subnet. It can help components such as kubevirt , storage, and logging, to transmit massive amounts of data. Multi clusters. For multi clusters, all Pods of multi clusters are connected to the underlay network, so they could communicate smoothly without the need to deploy additional components for cluster connectivity.","title":"underlay CNI"},{"location":"#architecture","text":"Spiderpool consists of the following components: Spiderpool controller: deployment that manage CRD validation, status updates, IP recovery, and automated IP pools Spiderpool agent: daemonset that help Spiderpool plugins by performing IP allocation and coordinator plugin for information synchronization. IPAM plugin: a binary plugin on each host that CNI can utilize to implement IP allocation. coordinator plugin: a binary plugin on each host that CNI can use for multi-NIC route coordination, IP conflict detection, and host connectivity. ifacer plugin: A binary plugin on each host that helps CNIs such as macvlan and ipvlan dynamically create bond and vlan interfaces On top of its own components, Spiderpool relies on open-source underlay CNIs to allocate network interfaces to Pods. You can use Multus CNI to manage multiple NICs and CNI configurations. Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool, such as: Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI","title":"Architecture"},{"location":"#use-case-underlay-cnis-access-layer2","text":"In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI , SR-IOV CNI , ipvlan CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding).","title":"Use case: underlay CNIs access layer2"},{"location":"#use-case-adding-an-auxiliary-underlay-cni-nic-for-overlay-cni","text":"In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes.","title":"Use case: adding an auxiliary underlay CNI NIC for overlay CNI"},{"location":"#use-case-underlay-cni-on-public-cloud-and-vm","text":"It is hard to implement underlay CNI in public cloud, openstack, vmvare. It requires the vendor underlay CNI on specific environments, as these environments typically have the following limitations: The IAAS network infrastructure implements MAC restrictions for packets. On the one hand, security checks are conducted on the source MAC to ensure that the source MAC address is the same as the MAC address of VM network interface. On the other hand, restrictions have been placed on the destination MAC, which only supports packet forwarding by the MAC address of VM network interfaces. The MAC address of the Pod in the common CNI plugin is newly generated, which leads to Pod communication failure. The IAAS network infrastructure implements IP restrictions on packets. Only when the destination and source IP of the packet are assigned to VM, packet could be forwarded rightly. The common CNI plugin assigns IP addresses to Pods that do not comply with IAAS settings, which leads to Pod communication failure. Spiderpool provides IP pool based on node topology, aligning with IP allocation settings of VMs. In conjunction with ipvlan CNI, it provides underlay CNI solutions for various public cloud environments.","title":"Use case: underlay CNI on public cloud and VM"},{"location":"#quick-start","text":"Refer to Quick start , set up a cluster quickly.","title":"Quick start"},{"location":"#major-features","text":"For applications requiring static IP addresses, it could be supported by IP pools owning limited IP adddress set and pod affinity. See example for more details. For applications not requiring static IP addresses, they can share an IP pool. See example for more details. For stateful applications, IP addresses can be automatically fixed for each Pod, and the overall IP scaling range can be fixed as well. See example for more details. Subnet feature, on the one hand, could help to separate the responsibility from the infrastructure administrator and the application administrator. On the other hand, it supports to automatically create and dynamically scale the fixed IP ippools to each applcation requiring static IPs. which could help reduce operation burden of IP pools burden. See example for more details. In additional to kubernetes-native controller, subnet feature also supports third-party pod controllers based on operator. See example for details. For Pods of an application run across different network zones, it could assign IP addresses of different subnets. See example for details. Support to assign IP address from different subnets to multiple NICs of a Pod, and help coordinate policy route between interfaces to ensure consistent data path of request and reply packets. For scenarios involving multiple Underlay NICs, please refer to the example . For scenarios involving one Overlay NIC and multiple Underlay NICs, please refer to the example . It supports to set default IP pools for the cluster or for the namespace. Besides, A IP pool could be shared by the whole cluster or bound to a specified namespace. See example for details. Strengthen CNI like Macvlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , to access clusterIP and pod healthy check ( example ), to detect IP conflict and gateway reachability ( example ). Node-based IP pool feature meets the complex subnet design of each node. refer to example On vmware vsphere platform, Spiderpool underlay network solution does not require \"hybrid forwarding\" mode of the Vswitch , which ensures the network performance of vsphere platform. refer to example Spiderpool underlay network solution could run on public cloud of any vendors, and openstack platform, which could meet needs of multi cloud and hybrid cloud with unified CNI stack\u3002refer to alibabaCloud When starting the Pod, it could help dynamically build the bond interface and vlan interface for the master interface of Macvlan CNI , ipvlan CNI . See example for details. It could specify customized routes by IP pool and pod annotation. See example for details. Easy generation of Multus NetworkAttachmentDefinition custom resources with best-practice CNI configuration, also ensure well formatted JSON to improve experience. See example for details. Multiple IP pools can be set for the application for prevent IP address from running out. See example for details. Set reserved IPs that will not be assigned to Pods, it can avoid misusing IP addresses already taken by hosts out of the cluster. See example for details. Outstanding performance for assigning and releasing Pod IPs, showcased in the test report . Well-designed IP reclaim mechanism could help assign IP address in time and quickly recover from the breakdown for the cluster or application. See example for details. All above features can work in ipv4-only, ipv6-only, and dual-stack scenarios. See example for details. Support AMD64 and ARM64. Metrics","title":"Major features"},{"location":"#blogs","text":"Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 Cloud-Native Spiderpool: IP Allocation Across Network Zones Spiderpool: a new solution to fixed application IPs for Calico \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6","title":"Blogs"},{"location":"#roadmap","text":"roadmap","title":"Roadmap"},{"location":"README-zh_CN/","text":"Spiderpool English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u662f CNCF Landscape \u9879\u76ee \u3002 Spiderpool \u4ecb\u7ecd Spiderpool \u662f\u4e00\u4e2a kubernetes \u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684 IPAM \u548c CNI \u6574\u5408\u80fd\u529b\uff0c \u5f3a\u5927\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684\u73b0\u6709 CNI \u9879\u76ee\uff0c\u8ba9\u591a CNI \u534f\u540c\u5de5\u4f5c\u53ef\u771f\u6b63\u843d\u5730\uff0c\u5b83\u4f7f\u5f97 underlay CNI \u80fd\u591f\u5b8c\u7f8e\u5730\u8fd0\u884c\u5728 \u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u3001\u4efb\u610f\u516c\u6709\u4e91\u7b49\u73af\u5883\u4e0b \u3002 \u4e3a\u4ec0\u4e48\u5e0c\u671b\u7814\u53d1 Spiderpool? \u5f53\u524d\u5f00\u6e90\u793e\u533a\u4e2d\u5e76\u672a\u63d0\u4f9b\u5168\u9762\u3001\u53cb\u597d\u3001\u667a\u80fd\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0cSpiderpool \u56e0\u6b64\u63d0\u4f9b\u4e86\u5f88\u591a\u521b\u65b0\u7684\u529f\u80fd\uff1a \u4e30\u5bcc\u7684 IPAM \u80fd\u529b\u3002\u63d0\u4f9b\u5171\u4eab\u3001\u72ec\u4eab\u7684 IP \u6c60\uff0c\u652f\u6301\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u81ea\u52a8\u5316\u7ba1\u7406\u72ec\u4eab\u7684 IP \u6c60\uff0c\u5b9e\u73b0\u56fa\u5b9a IP \u5730\u5740\u7684\u52a8\u6001\u521b\u5efa\u3001\u6269\u5bb9\u3001\u7f29\u5bb9\u548c\u56de\u6536\u7b49\u3002 overlay CNI \u548c underlay CNI \u534f\u540c\uff0cPod \u5177\u5907\u591a\u79cd CNI \u7f51\u5361\u3002Spiderpool \u80fd\u591f\u5b9a\u5236\u591a\u4e2a underlay CNI \u7f51\u5361\u7684 IP \u5730\u5740\uff0c\u8c03\u534f\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\u800c\u907f\u514d\u4e22\u5305\u3002 \u591a CNI \u534f\u540c\u80fd\u6709\u6548\u964d\u4f4e\u96c6\u7fa4\u8282\u70b9\u7684\u786c\u4ef6\u4e00\u81f4\u8981\u6c42\u3002 \u589e\u5f3a\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684 underlay CNI\uff0c\u5982 Macvlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\u7b49\uff0c \u6253\u901a Pod \u548c\u5bbf\u4e3b\u673a\u7684\u8fde\u901a\u6027\uff0c\u4f7f\u5f97 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49\u901a\u4fe1\u6210\u529f\uff0c\u5e76\u4e14\u652f\u6301 Pod \u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u7b49\u3002 \u4e0d\u4ec5\u9650\u4e8e\u5e94\u7528\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u88f8\u91d1\u5c5e\u73af\u5883\uff0c\u540c\u65f6\u4e5f\u4e3a openstack\u3001vmware \u548c\u5404\u79cd\u516c\u6709\u4e91\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684 underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 underlay CNI \u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff0c\"overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"underlay \u7f51\u7edc\u65b9\u6848\"\uff0c \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 \u6587\u7ae0 \u5bf9\u4e24\u79cd\u65b9\u6848\u7684 IPAM \u548c\u7f51\u7edc\u6027\u80fd\u505a\u4e86\u7b80\u5355\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 \u4e3a\u4ec0\u4e48\u9700\u8981 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1f\u5b58\u5728\u5f88\u591a\u5e94\u7528\u573a\u666f\uff1a \u7f51\u7edc\u6027\u80fd\u3002underlay \u7f51\u7edc\u65b9\u6848\u80fd\u591f\u63d0\u4f9b\u4f4e\u5ef6\u65f6\u3001\u9ad8\u541e\u5410\u91cf\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u964d\u4f4e\u5bbf\u4e3b\u673a\u7684\u7f51\u7edc\u8f6c\u53d1\u7684 CPU \u5f00\u9500\uff0c\u80fd\u591f\u6ee1\u8db3\u7f51\u7edc\u6027\u80fd\u8981\u6c42\u9ad8\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u6025\u901f\u91d1\u878d\u4ea4\u6613\u3001AI \u8bad\u7ec3\u7b49\u5e94\u7528\u3002 \u4e0a\u4e91\u6210\u672c\u3002\u4f20\u7edf\u4e3b\u673a\u5e94\u7528\uff0c\u57fa\u4e8e\u4e3b\u673a IP \u8fdb\u884c\u670d\u52a1\u66b4\u9732\uff0c\u4f7f\u7528\u591a\u5b50\u7f51\u5bf9\u63a5\u4e0d\u540c\u4e1a\u52a1\u7b49\u7279\u70b9\u3002\u5728\u5e94\u7528\u8fc1\u79fb\u4e0a\u4e91\u7684\u521d\u671f\uff0cunderlay \u7f51\u7edc\u65b9\u6848\u80fd\u591f\u7ed9\u5e94\u7528\u63d0\u51fa\u66f4\u4f4e\u7684\u7f51\u7edc\u6539\u9020\u6210\u672c\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528 POD IP \u8fdb\u884c\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 \u7f51\u7edc\u5b89\u5168\u3002\u6570\u636e\u4e2d\u5fc3\u6709\u81ea\u8eab\u7684\u7f51\u7edc\u5b89\u5168\u7ba1\u7406\u9700\u6c42\uff0c\u4f8b\u5982\u4f7f\u7528\u9632\u706b\u5899\u6765\u7ba1\u63a7\u7f51\u7edc\u901a\u4fe1\u3001\u57fa\u4e8evlan \u9694\u79bb\u901a\u4fe1\u3001\u6cbf\u7528\u4f20\u7edf\u7684\u7f51\u7edc\u76d1\u63a7\u65b9\u6848\u3002underlay \u7f51\u7edc\u65b9\u6848\u4f7f\u5f97 POD \u901a\u4fe1\u6570\u636e\u5305\u76f4\u63a5\u66b4\u9732\u5728\u5e95\u5c42\u7f51\u7edc\u4e2d\uff0c\u65e0\u9700\u6253\u96a7\u9053\u5c01\u88c5\uff0c\u53ef\u6ee1\u8db3\u9700\u6c42\u3002 \u5e26\u5bbd\u72ec\u7acb\u3002underlay \u7f51\u7edc\u65b9\u6848\u53ef\u4e3a\u5bb9\u5668\u5b9a\u5236\u5bbf\u4e3b\u673a\u7684\u51fa\u53e3\u7f51\u5361\uff0c\u4ece\u800c\u4fdd\u969c\u5e95\u5c42\u5b50\u7f51\u7684\u5e26\u5bbd\u9694\u79bb\u3002\u53ef\u6ee1\u8db3 kubevirt \u3001\u5b58\u50a8\u3001\u65e5\u5fd7\u7b49\u7ec4\u4ef6\uff0c\u4f20\u8f93\u6d77\u91cf\u7684\u6570\u636e\uff0c\u907f\u514d\u5f71\u54cd\u5176\u5b83\u6b63\u5e38\u7684\u4e1a\u52a1\u901a\u4fe1\u3002 \u591a\u4e91\u8fde\u63a5\u3002\u591a\u96c6\u7fa4\u573a\u666f\u4e0b\uff0c\u5bb9\u5668\u90fd\u5bf9\u63a5 underlay \u7f51\u7edc\uff0c\u4f7f\u5f97\u591a\u96c6\u7fa4\u95f4\u7684 \u5bb9\u5668\u7f51\u7edc\u5929\u7136\u8054\u901a\uff0c\u65e0\u9700\u90e8\u7f72\u989d\u5916\u7684\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u7ec4\u4ef6\u3002 \u67b6\u6784 Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool controller: \u662f\u4e00\u7ec4 deployment\uff0c\u5b9e\u65bd\u4e86\u5bf9\u5404\u79cd CRD \u6821\u9a8c\u3001\u72b6\u6001\u66f4\u65b0\u3001IP \u56de\u6536\u3001\u81ea\u52a8 IP \u6c60\u7684\u7ba1\u7406\u7b49 Spiderpool agent\uff1a\u662f\u4e00\u7ec4 daemonset\uff0c\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u4fe1\u606f\u540c\u6b65 Spiderpool plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d coordinator plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u534f\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u7b49 ifacer plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u5e2e\u52a9 macvlan \u3001ipvlan \u7b49 CNI \u52a8\u6001\u521b\u5efa bond \u548c vlan \u5b50\u63a5\u53e3 \u9664\u4e86\u4ee5\u4e0a Spiderpool \u81ea\u8eab\u7684\u7ec4\u4ef6\u4ee5\u5916\uff0c\u8fd8\u9700\u8981\u914d\u5408\u67d0\u4e2a\u5f00\u6e90\u7684 underlay CNI \u6765\u7ed9 Pod \u5206\u914d\u7f51\u5361\uff0c \u53ef\u914d\u5408 Multus CNI \u6765\u5b9e\u65bd\u591a\u7f51\u5361\u548c CNI \u914d\u7f6e\u7ba1\u7406\u3002 \u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool\uff0c\u4f8b\u5982\uff1a Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI \u5e94\u7528\u573a\u666f\uff1a\u63a5\u5165 L2 \u7f51\u7edc\u7684 underlay CNI \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI\uff0c \u4f8b\u5982 Macvlan CNI , SR-IOV CNI , ipvlan CNI \u5b9e\u73b0\uff1a \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b\uff0c\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u534f\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c \u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c \u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SR-IOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 Pod\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SR-IOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SR-IOV \u9ad8\u901f\u7f51\u5361\uff0c \u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462\uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c \u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c Macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002 \u5e94\u7528\u573a\u666f\uff1aoverlay CNI \u7684 Pod \u52a0\u5165 underlay CNI \u8f85\u52a9\u7f51\u5361 \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a Pod \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361 \uff08\u4f8b\u5982 Calico , Cilium \uff09 \u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 Macvlan CNI , SR-IOV CNI \uff09\uff0c\u53ef\u5b9e\u73b0\uff1a \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u534f\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u534f\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001 \u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002 \u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c\u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 Pod \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c \u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 Pod \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002\u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c \u5728\u4fdd\u969c\u96c6\u7fa4\u5185 Pod \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c\u3002 \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90\u3002 \u5e94\u7528\u573a\u666f \uff1aunderlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a \u5728\u516c\u6709\u4e91\u3001openstack\u3001vmvare \u7b49\u73af\u5883\u4e0b\u5b9e\u65bd underlay CNI\uff0c\u901a\u5e38\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u7684\u5382\u5546 CNI \u63d2\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u73af\u5883\u901a\u5e38\u6709\u5982\u4e0b\u9650\u5236\uff1a IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e8c\u5c42\u62a5\u5934\u4e2d\u7684 MAC \u9650\u5236\uff0c\u4e00\u65b9\u9762\uff0c\u5bf9\u6e90 MAC \u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c \u4ee5\u786e\u4fdd\u6e90 MAC \u5730\u5740\u4e0e\u865a\u62df\u673a\u7f51\u5361 MAC \u76f8\u540c\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u76ee\u7684 MAC \u505a\u4e86\u9650\u5236\uff0c\u53ea\u652f\u6301\u8f6c\u53d1 IAAS \u4e2d\u6240\u6709\u865a\u62df\u673a\u7f51\u5361\u7684 MAC\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0cPod \u5206\u914d\u7684\u7f51\u5361\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e09\u5c42\u62a5\u5934\u7684 IP \u9650\u5236\uff0c\u53ea\u6709\u6570\u636e\u5305\u7684\u76ee\u7684\u548c\u6e90 IP \u662f\u5728 IAAS \u4e2d\u5206\u914d\u7ed9\u4e86\u865a\u62df\u673a\u7f51\u5361\u65f6\uff0c\u6570\u636e\u5305\u624d\u80fd\u5f97\u5230\u8f6c\u53d1\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0c\u7ed9 Pod \u5206\u914d\u7684 IP \u5730\u5740\u4e0d\u7b26\u5408 IAAS \u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u4e0e\u865a\u62df\u673a\u7684\u76f8\u540c IP \u5206\u914d\u8bbe\u7f6e\u5bf9\u9f50\uff0c\u518d\u914d\u5408 ipvlan CNI\uff0c \u4ece\u800c\u80fd\u591f\u4e3a\u5404\u79cd\u516c\u6709\u4e91\u73af\u5883\u63d0\u4f9b underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u5feb\u901f\u5f00\u59cb \u5feb\u901f\u642d\u5efa Spiderpool\uff0c\u542f\u52a8\u4e00\u4e2a\u5e94\u7528\uff0c\u53ef\u53c2\u8003 \u5feb\u901f\u642d\u5efa \u3002 \u529f\u80fd \u5bf9\u4e8e\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e IP \u6c60\u6709\u9650\u7684\u7684 IP \u5730\u5740\u96c6\u5408\u548c\u5e94\u7528\u4eb2\u548c\u6027\u6765\u652f\u6301\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u6ca1\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\uff0c\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u5171\u4eab\u4e00\u4e2a IP \u6c60\uff0c \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e StatefulSet \u5e94\u7528\uff0cSpiderpool \u652f\u6301\u81ea\u52a8\u4e3a\u6bcf\u4e00\u4e2a Pod \u56fa\u5b9a IP \u5730\u5740\uff0c\u4e5f\u53ef\u63a7\u5236\u5e94\u7528\u6240\u6709 Pod \u6240\u4f7f\u7528\u7684 IP \u8303\u56f4\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 Subnet \u529f\u80fd\uff0c\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7ba1\u7406\u5458\u7684\u804c\u8d23\u5206\u79bb\u3002 \u53e6\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u4e3a\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\u81ea\u52a8\u7ba1\u7406 IP \u6c60\uff0c\u5305\u62ec\u81ea\u52a8\u521b\u5efa\u3001\u6269\u7f29\u5bb9 IP\u3001\u5220\u9664 \u56fa\u5b9a IP \u6c60\uff0c \u8fd9\u80fd\u591f\u51cf\u5c11\u5927\u91cf\u7684\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8be5\u529f\u80fd\u9664\u4e86\u652f\u6301 K8S \u539f\u751f\u7684\u5e94\u7528\u63a7\u5236\u5668\uff0c\u540c\u65f6\u652f\u6301\u57fa\u4e8e operator \u5b9e\u73b0\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u63a7\u5236\u5668\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u4e00\u4e2a\u8de8\u5b50\u7f51\u90e8\u7f72\u7684\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u5176\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301\u4e3a Pod \u591a\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff0c\u5e76\u5e2e\u52a9\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u534f\u8c03\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u4e22\u5305\u3002 \u5bf9\u4e8e Pod \u5177\u5907\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e Pod \u5177\u5907\u4e00\u4e2a overlay \u7f51\u5361\u548c\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u8bbe\u7f6e\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\uff0c\u4e5f\u53ef\u79df\u6237\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\u3002\u540c\u65f6\uff0cIP \u6c60\u65e2\u53ef\u4ee5\u88ab\u6574\u4e2a\u96c6\u7fa4\u5171\u4eab\uff0c \u4e5f\u53ef\u88ab\u9650\u5b9a\u4e3a\u88ab\u4e00\u4e2a\u79df\u6237\u4f7f\u7528\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u5f00\u6e90\u7684 Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\uff0c \u53ef\u5e2e\u52a9\u89e3\u51b3 ClusterIP \u8bbf\u95ee\u3001Pod \u5bbf\u4e3b\u673a\u5065\u5eb7\u68c0\u67e5\u7b49\u95ee\u9898\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e76\u4e14\uff0c\u80fd\u591f\u5e2e\u52a9\u5b9e\u65bd IP \u5730\u5740\u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u8bc1 Pod \u901a\u4fe1\u6b63\u5e38\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u57fa\u4e8e\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u6ee1\u8db3\u6bcf\u4e2a\u8282\u70b9\u7cbe\u7ec6\u5316\u7684\u5b50\u7f51\u89c4\u5212\u9700\u6c42\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u5728 vmware vsphere \u5e73\u53f0\u4e0a\uff0c\u65e0\u9700\u6253\u5f00 vswitch \u7684 \"\u6df7\u6742\"\u8f6c\u53d1\u6a21\u5f0f \uff0c\u5373\u53ef\u8fd0\u884c underlay CNI \u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u786e\u4fdd vsphere \u5e73\u53f0\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53c2\u8003 \u4f8b\u5b50 spiderpool \u80fd\u5728\u4efb\u610f\u5382\u5546\u7684\u516c\u6709\u4e91\u5e73\u53f0\u3001openstack \u4e0a\u8fd0\u884c\u5bb9\u5668 underlay \u7f51\u7edc\uff0c\u4ece\u800c\u7528\u7edf\u4e00\u7684\u6280\u672f\u6808\u6ee1\u8db3\u591a\u4e91\u3001\u6df7\u5408\u4e91\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002\u5177\u4f53\u53ef\u53c2\u8003 \u963f\u91cc\u4e91\u4f8b\u5b50 \u5728 Pod \u542f\u52a8\u65f6\uff0c\u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u52a8\u6001\u521b\u5efa BOND \u63a5\u53e3\u548c VLAN \u5b50\u63a5\u53e3\uff0c\u4ee5\u5e2e\u52a9 Macvlan CNI \u548c ipvlan CNI \u51c6\u5907 master \u63a5\u53e3\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u901a\u8fc7 IP \u6c60\u548c Pod annotaiton \u7b49\u591a\u79cd\u65b9\u5f0f\u5b9a\u5236\u81ea\u5b9a\u4e49\u8def\u7531\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u4ee5\u6700\u4f73\u5b9e\u8df5\u7684 CNI \u914d\u7f6e\u6765\u4fbf\u6377\u5730\u751f\u6210 Multus NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5e76\u4e14\u4fdd\u8bc1\u5176\u6b63\u786e\u7684 JSON \u683c\u5f0f\u6765\u63d0\u9ad8\u4f7f\u7528\u4f53\u9a8c\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e94\u7528\u53ef\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\uff0c\u5b9e\u73b0 IP \u8d44\u6e90\u7684\u5907\u7528\u6548\u679c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8bbe\u7f6e\u5168\u5c40\u7684\u9884\u7559 IP\uff0c\u8ba9 IPAM \u4e0d\u5206\u914d\u51fa\u8fd9\u4e9b IP \u5730\u5740\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u5df2\u7528 IP \u51b2\u7a81\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u62a5\u544a \u3002 \u5408\u7406\u7684 IP \u56de\u6536\u673a\u5236\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6216\u5e94\u7528\u5728\u6545\u969c\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u53ca\u65f6\u5206\u914d\u5230 IP \u5730\u5740\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u6240\u6709\u7684\u529f\u80fd\u90fd\u80fd\u591f\u5728 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u4e0b\u5de5\u4f5c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301 AMD64 \u548c ARM64 \u6307\u6807 Blogs Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 \u4e91\u539f\u751f Spiderpool\uff1a\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d SpiderPool\uff1aCalico \u56fa\u5b9a\u5e94\u7528 IP \u7684\u4e00\u79cd\u65b0\u9009\u62e9 \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6 Roadmap roadmap","title":"Spiderpool"},{"location":"README-zh_CN/#spiderpool","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u662f CNCF Landscape \u9879\u76ee \u3002","title":"Spiderpool"},{"location":"README-zh_CN/#spiderpool_1","text":"Spiderpool \u662f\u4e00\u4e2a kubernetes \u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684 IPAM \u548c CNI \u6574\u5408\u80fd\u529b\uff0c \u5f3a\u5927\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684\u73b0\u6709 CNI \u9879\u76ee\uff0c\u8ba9\u591a CNI \u534f\u540c\u5de5\u4f5c\u53ef\u771f\u6b63\u843d\u5730\uff0c\u5b83\u4f7f\u5f97 underlay CNI \u80fd\u591f\u5b8c\u7f8e\u5730\u8fd0\u884c\u5728 \u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u3001\u4efb\u610f\u516c\u6709\u4e91\u7b49\u73af\u5883\u4e0b \u3002 \u4e3a\u4ec0\u4e48\u5e0c\u671b\u7814\u53d1 Spiderpool? \u5f53\u524d\u5f00\u6e90\u793e\u533a\u4e2d\u5e76\u672a\u63d0\u4f9b\u5168\u9762\u3001\u53cb\u597d\u3001\u667a\u80fd\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0cSpiderpool \u56e0\u6b64\u63d0\u4f9b\u4e86\u5f88\u591a\u521b\u65b0\u7684\u529f\u80fd\uff1a \u4e30\u5bcc\u7684 IPAM \u80fd\u529b\u3002\u63d0\u4f9b\u5171\u4eab\u3001\u72ec\u4eab\u7684 IP \u6c60\uff0c\u652f\u6301\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u81ea\u52a8\u5316\u7ba1\u7406\u72ec\u4eab\u7684 IP \u6c60\uff0c\u5b9e\u73b0\u56fa\u5b9a IP \u5730\u5740\u7684\u52a8\u6001\u521b\u5efa\u3001\u6269\u5bb9\u3001\u7f29\u5bb9\u548c\u56de\u6536\u7b49\u3002 overlay CNI \u548c underlay CNI \u534f\u540c\uff0cPod \u5177\u5907\u591a\u79cd CNI \u7f51\u5361\u3002Spiderpool \u80fd\u591f\u5b9a\u5236\u591a\u4e2a underlay CNI \u7f51\u5361\u7684 IP \u5730\u5740\uff0c\u8c03\u534f\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\u800c\u907f\u514d\u4e22\u5305\u3002 \u591a CNI \u534f\u540c\u80fd\u6709\u6548\u964d\u4f4e\u96c6\u7fa4\u8282\u70b9\u7684\u786c\u4ef6\u4e00\u81f4\u8981\u6c42\u3002 \u589e\u5f3a\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684 underlay CNI\uff0c\u5982 Macvlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\u7b49\uff0c \u6253\u901a Pod \u548c\u5bbf\u4e3b\u673a\u7684\u8fde\u901a\u6027\uff0c\u4f7f\u5f97 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49\u901a\u4fe1\u6210\u529f\uff0c\u5e76\u4e14\u652f\u6301 Pod \u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u7b49\u3002 \u4e0d\u4ec5\u9650\u4e8e\u5e94\u7528\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u88f8\u91d1\u5c5e\u73af\u5883\uff0c\u540c\u65f6\u4e5f\u4e3a openstack\u3001vmware \u548c\u5404\u79cd\u516c\u6709\u4e91\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684 underlay CNI \u89e3\u51b3\u65b9\u6848\u3002","title":"Spiderpool \u4ecb\u7ecd"},{"location":"README-zh_CN/#underlay-cni","text":"\u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff0c\"overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"underlay \u7f51\u7edc\u65b9\u6848\"\uff0c \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 \u6587\u7ae0 \u5bf9\u4e24\u79cd\u65b9\u6848\u7684 IPAM \u548c\u7f51\u7edc\u6027\u80fd\u505a\u4e86\u7b80\u5355\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 \u4e3a\u4ec0\u4e48\u9700\u8981 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1f\u5b58\u5728\u5f88\u591a\u5e94\u7528\u573a\u666f\uff1a \u7f51\u7edc\u6027\u80fd\u3002underlay \u7f51\u7edc\u65b9\u6848\u80fd\u591f\u63d0\u4f9b\u4f4e\u5ef6\u65f6\u3001\u9ad8\u541e\u5410\u91cf\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u964d\u4f4e\u5bbf\u4e3b\u673a\u7684\u7f51\u7edc\u8f6c\u53d1\u7684 CPU \u5f00\u9500\uff0c\u80fd\u591f\u6ee1\u8db3\u7f51\u7edc\u6027\u80fd\u8981\u6c42\u9ad8\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u6025\u901f\u91d1\u878d\u4ea4\u6613\u3001AI \u8bad\u7ec3\u7b49\u5e94\u7528\u3002 \u4e0a\u4e91\u6210\u672c\u3002\u4f20\u7edf\u4e3b\u673a\u5e94\u7528\uff0c\u57fa\u4e8e\u4e3b\u673a IP \u8fdb\u884c\u670d\u52a1\u66b4\u9732\uff0c\u4f7f\u7528\u591a\u5b50\u7f51\u5bf9\u63a5\u4e0d\u540c\u4e1a\u52a1\u7b49\u7279\u70b9\u3002\u5728\u5e94\u7528\u8fc1\u79fb\u4e0a\u4e91\u7684\u521d\u671f\uff0cunderlay \u7f51\u7edc\u65b9\u6848\u80fd\u591f\u7ed9\u5e94\u7528\u63d0\u51fa\u66f4\u4f4e\u7684\u7f51\u7edc\u6539\u9020\u6210\u672c\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528 POD IP \u8fdb\u884c\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 \u7f51\u7edc\u5b89\u5168\u3002\u6570\u636e\u4e2d\u5fc3\u6709\u81ea\u8eab\u7684\u7f51\u7edc\u5b89\u5168\u7ba1\u7406\u9700\u6c42\uff0c\u4f8b\u5982\u4f7f\u7528\u9632\u706b\u5899\u6765\u7ba1\u63a7\u7f51\u7edc\u901a\u4fe1\u3001\u57fa\u4e8evlan \u9694\u79bb\u901a\u4fe1\u3001\u6cbf\u7528\u4f20\u7edf\u7684\u7f51\u7edc\u76d1\u63a7\u65b9\u6848\u3002underlay \u7f51\u7edc\u65b9\u6848\u4f7f\u5f97 POD \u901a\u4fe1\u6570\u636e\u5305\u76f4\u63a5\u66b4\u9732\u5728\u5e95\u5c42\u7f51\u7edc\u4e2d\uff0c\u65e0\u9700\u6253\u96a7\u9053\u5c01\u88c5\uff0c\u53ef\u6ee1\u8db3\u9700\u6c42\u3002 \u5e26\u5bbd\u72ec\u7acb\u3002underlay \u7f51\u7edc\u65b9\u6848\u53ef\u4e3a\u5bb9\u5668\u5b9a\u5236\u5bbf\u4e3b\u673a\u7684\u51fa\u53e3\u7f51\u5361\uff0c\u4ece\u800c\u4fdd\u969c\u5e95\u5c42\u5b50\u7f51\u7684\u5e26\u5bbd\u9694\u79bb\u3002\u53ef\u6ee1\u8db3 kubevirt \u3001\u5b58\u50a8\u3001\u65e5\u5fd7\u7b49\u7ec4\u4ef6\uff0c\u4f20\u8f93\u6d77\u91cf\u7684\u6570\u636e\uff0c\u907f\u514d\u5f71\u54cd\u5176\u5b83\u6b63\u5e38\u7684\u4e1a\u52a1\u901a\u4fe1\u3002 \u591a\u4e91\u8fde\u63a5\u3002\u591a\u96c6\u7fa4\u573a\u666f\u4e0b\uff0c\u5bb9\u5668\u90fd\u5bf9\u63a5 underlay \u7f51\u7edc\uff0c\u4f7f\u5f97\u591a\u96c6\u7fa4\u95f4\u7684 \u5bb9\u5668\u7f51\u7edc\u5929\u7136\u8054\u901a\uff0c\u65e0\u9700\u90e8\u7f72\u989d\u5916\u7684\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u7ec4\u4ef6\u3002","title":"underlay CNI"},{"location":"README-zh_CN/#_1","text":"Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool controller: \u662f\u4e00\u7ec4 deployment\uff0c\u5b9e\u65bd\u4e86\u5bf9\u5404\u79cd CRD \u6821\u9a8c\u3001\u72b6\u6001\u66f4\u65b0\u3001IP \u56de\u6536\u3001\u81ea\u52a8 IP \u6c60\u7684\u7ba1\u7406\u7b49 Spiderpool agent\uff1a\u662f\u4e00\u7ec4 daemonset\uff0c\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u4fe1\u606f\u540c\u6b65 Spiderpool plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d coordinator plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u534f\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u7b49 ifacer plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u5e2e\u52a9 macvlan \u3001ipvlan \u7b49 CNI \u52a8\u6001\u521b\u5efa bond \u548c vlan \u5b50\u63a5\u53e3 \u9664\u4e86\u4ee5\u4e0a Spiderpool \u81ea\u8eab\u7684\u7ec4\u4ef6\u4ee5\u5916\uff0c\u8fd8\u9700\u8981\u914d\u5408\u67d0\u4e2a\u5f00\u6e90\u7684 underlay CNI \u6765\u7ed9 Pod \u5206\u914d\u7f51\u5361\uff0c \u53ef\u914d\u5408 Multus CNI \u6765\u5b9e\u65bd\u591a\u7f51\u5361\u548c CNI \u914d\u7f6e\u7ba1\u7406\u3002 \u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool\uff0c\u4f8b\u5982\uff1a Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI","title":"\u67b6\u6784"},{"location":"README-zh_CN/#l2-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI\uff0c \u4f8b\u5982 Macvlan CNI , SR-IOV CNI , ipvlan CNI \u5b9e\u73b0\uff1a \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b\uff0c\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u534f\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c \u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c \u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SR-IOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 Pod\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SR-IOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SR-IOV \u9ad8\u901f\u7f51\u5361\uff0c \u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462\uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c \u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c Macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002","title":"\u5e94\u7528\u573a\u666f\uff1a\u63a5\u5165 L2 \u7f51\u7edc\u7684 underlay CNI"},{"location":"README-zh_CN/#overlay-cni-pod-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a Pod \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361 \uff08\u4f8b\u5982 Calico , Cilium \uff09 \u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 Macvlan CNI , SR-IOV CNI \uff09\uff0c\u53ef\u5b9e\u73b0\uff1a \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u534f\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u534f\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001 \u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002 \u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c\u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 Pod \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c \u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 Pod \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002\u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c \u5728\u4fdd\u969c\u96c6\u7fa4\u5185 Pod \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c\u3002 \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90\u3002","title":"\u5e94\u7528\u573a\u666f\uff1aoverlay CNI \u7684 Pod \u52a0\u5165 underlay CNI \u8f85\u52a9\u7f51\u5361"},{"location":"README-zh_CN/#underlay-cni_1","text":"\u5728\u516c\u6709\u4e91\u3001openstack\u3001vmvare \u7b49\u73af\u5883\u4e0b\u5b9e\u65bd underlay CNI\uff0c\u901a\u5e38\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u7684\u5382\u5546 CNI \u63d2\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u73af\u5883\u901a\u5e38\u6709\u5982\u4e0b\u9650\u5236\uff1a IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e8c\u5c42\u62a5\u5934\u4e2d\u7684 MAC \u9650\u5236\uff0c\u4e00\u65b9\u9762\uff0c\u5bf9\u6e90 MAC \u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c \u4ee5\u786e\u4fdd\u6e90 MAC \u5730\u5740\u4e0e\u865a\u62df\u673a\u7f51\u5361 MAC \u76f8\u540c\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u76ee\u7684 MAC \u505a\u4e86\u9650\u5236\uff0c\u53ea\u652f\u6301\u8f6c\u53d1 IAAS \u4e2d\u6240\u6709\u865a\u62df\u673a\u7f51\u5361\u7684 MAC\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0cPod \u5206\u914d\u7684\u7f51\u5361\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e09\u5c42\u62a5\u5934\u7684 IP \u9650\u5236\uff0c\u53ea\u6709\u6570\u636e\u5305\u7684\u76ee\u7684\u548c\u6e90 IP \u662f\u5728 IAAS \u4e2d\u5206\u914d\u7ed9\u4e86\u865a\u62df\u673a\u7f51\u5361\u65f6\uff0c\u6570\u636e\u5305\u624d\u80fd\u5f97\u5230\u8f6c\u53d1\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0c\u7ed9 Pod \u5206\u914d\u7684 IP \u5730\u5740\u4e0d\u7b26\u5408 IAAS \u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u4e0e\u865a\u62df\u673a\u7684\u76f8\u540c IP \u5206\u914d\u8bbe\u7f6e\u5bf9\u9f50\uff0c\u518d\u914d\u5408 ipvlan CNI\uff0c \u4ece\u800c\u80fd\u591f\u4e3a\u5404\u79cd\u516c\u6709\u4e91\u73af\u5883\u63d0\u4f9b underlay CNI \u89e3\u51b3\u65b9\u6848\u3002","title":"\u5e94\u7528\u573a\u666f \uff1aunderlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a"},{"location":"README-zh_CN/#_2","text":"\u5feb\u901f\u642d\u5efa Spiderpool\uff0c\u542f\u52a8\u4e00\u4e2a\u5e94\u7528\uff0c\u53ef\u53c2\u8003 \u5feb\u901f\u642d\u5efa \u3002","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"README-zh_CN/#_3","text":"\u5bf9\u4e8e\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e IP \u6c60\u6709\u9650\u7684\u7684 IP \u5730\u5740\u96c6\u5408\u548c\u5e94\u7528\u4eb2\u548c\u6027\u6765\u652f\u6301\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u6ca1\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\uff0c\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u5171\u4eab\u4e00\u4e2a IP \u6c60\uff0c \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e StatefulSet \u5e94\u7528\uff0cSpiderpool \u652f\u6301\u81ea\u52a8\u4e3a\u6bcf\u4e00\u4e2a Pod \u56fa\u5b9a IP \u5730\u5740\uff0c\u4e5f\u53ef\u63a7\u5236\u5e94\u7528\u6240\u6709 Pod \u6240\u4f7f\u7528\u7684 IP \u8303\u56f4\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 Subnet \u529f\u80fd\uff0c\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7ba1\u7406\u5458\u7684\u804c\u8d23\u5206\u79bb\u3002 \u53e6\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u4e3a\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\u81ea\u52a8\u7ba1\u7406 IP \u6c60\uff0c\u5305\u62ec\u81ea\u52a8\u521b\u5efa\u3001\u6269\u7f29\u5bb9 IP\u3001\u5220\u9664 \u56fa\u5b9a IP \u6c60\uff0c \u8fd9\u80fd\u591f\u51cf\u5c11\u5927\u91cf\u7684\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8be5\u529f\u80fd\u9664\u4e86\u652f\u6301 K8S \u539f\u751f\u7684\u5e94\u7528\u63a7\u5236\u5668\uff0c\u540c\u65f6\u652f\u6301\u57fa\u4e8e operator \u5b9e\u73b0\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u63a7\u5236\u5668\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u4e00\u4e2a\u8de8\u5b50\u7f51\u90e8\u7f72\u7684\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u5176\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301\u4e3a Pod \u591a\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff0c\u5e76\u5e2e\u52a9\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u534f\u8c03\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u4e22\u5305\u3002 \u5bf9\u4e8e Pod \u5177\u5907\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e Pod \u5177\u5907\u4e00\u4e2a overlay \u7f51\u5361\u548c\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u8bbe\u7f6e\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\uff0c\u4e5f\u53ef\u79df\u6237\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\u3002\u540c\u65f6\uff0cIP \u6c60\u65e2\u53ef\u4ee5\u88ab\u6574\u4e2a\u96c6\u7fa4\u5171\u4eab\uff0c \u4e5f\u53ef\u88ab\u9650\u5b9a\u4e3a\u88ab\u4e00\u4e2a\u79df\u6237\u4f7f\u7528\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u5f00\u6e90\u7684 Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\uff0c \u53ef\u5e2e\u52a9\u89e3\u51b3 ClusterIP \u8bbf\u95ee\u3001Pod \u5bbf\u4e3b\u673a\u5065\u5eb7\u68c0\u67e5\u7b49\u95ee\u9898\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e76\u4e14\uff0c\u80fd\u591f\u5e2e\u52a9\u5b9e\u65bd IP \u5730\u5740\u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u8bc1 Pod \u901a\u4fe1\u6b63\u5e38\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u57fa\u4e8e\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u6ee1\u8db3\u6bcf\u4e2a\u8282\u70b9\u7cbe\u7ec6\u5316\u7684\u5b50\u7f51\u89c4\u5212\u9700\u6c42\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u5728 vmware vsphere \u5e73\u53f0\u4e0a\uff0c\u65e0\u9700\u6253\u5f00 vswitch \u7684 \"\u6df7\u6742\"\u8f6c\u53d1\u6a21\u5f0f \uff0c\u5373\u53ef\u8fd0\u884c underlay CNI \u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u786e\u4fdd vsphere \u5e73\u53f0\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53c2\u8003 \u4f8b\u5b50 spiderpool \u80fd\u5728\u4efb\u610f\u5382\u5546\u7684\u516c\u6709\u4e91\u5e73\u53f0\u3001openstack \u4e0a\u8fd0\u884c\u5bb9\u5668 underlay \u7f51\u7edc\uff0c\u4ece\u800c\u7528\u7edf\u4e00\u7684\u6280\u672f\u6808\u6ee1\u8db3\u591a\u4e91\u3001\u6df7\u5408\u4e91\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002\u5177\u4f53\u53ef\u53c2\u8003 \u963f\u91cc\u4e91\u4f8b\u5b50 \u5728 Pod \u542f\u52a8\u65f6\uff0c\u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u52a8\u6001\u521b\u5efa BOND \u63a5\u53e3\u548c VLAN \u5b50\u63a5\u53e3\uff0c\u4ee5\u5e2e\u52a9 Macvlan CNI \u548c ipvlan CNI \u51c6\u5907 master \u63a5\u53e3\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u901a\u8fc7 IP \u6c60\u548c Pod annotaiton \u7b49\u591a\u79cd\u65b9\u5f0f\u5b9a\u5236\u81ea\u5b9a\u4e49\u8def\u7531\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u4ee5\u6700\u4f73\u5b9e\u8df5\u7684 CNI \u914d\u7f6e\u6765\u4fbf\u6377\u5730\u751f\u6210 Multus NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5e76\u4e14\u4fdd\u8bc1\u5176\u6b63\u786e\u7684 JSON \u683c\u5f0f\u6765\u63d0\u9ad8\u4f7f\u7528\u4f53\u9a8c\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e94\u7528\u53ef\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\uff0c\u5b9e\u73b0 IP \u8d44\u6e90\u7684\u5907\u7528\u6548\u679c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8bbe\u7f6e\u5168\u5c40\u7684\u9884\u7559 IP\uff0c\u8ba9 IPAM \u4e0d\u5206\u914d\u51fa\u8fd9\u4e9b IP \u5730\u5740\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u5df2\u7528 IP \u51b2\u7a81\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u62a5\u544a \u3002 \u5408\u7406\u7684 IP \u56de\u6536\u673a\u5236\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6216\u5e94\u7528\u5728\u6545\u969c\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u53ca\u65f6\u5206\u914d\u5230 IP \u5730\u5740\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u6240\u6709\u7684\u529f\u80fd\u90fd\u80fd\u591f\u5728 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u4e0b\u5de5\u4f5c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301 AMD64 \u548c ARM64 \u6307\u6807","title":"\u529f\u80fd"},{"location":"README-zh_CN/#blogs","text":"Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 \u4e91\u539f\u751f Spiderpool\uff1a\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d SpiderPool\uff1aCalico \u56fa\u5b9a\u5e94\u7528 IP \u7684\u4e00\u79cd\u65b0\u9009\u62e9 \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6","title":"Blogs"},{"location":"README-zh_CN/#roadmap","text":"roadmap","title":"Roadmap"},{"location":"USERS/","text":"Adopter orgnization description a bank in ShangHai, China a cluster with a size of 2000+ pods, production environment a bank in SiChuan, China about 80 clusters , not more thant 100 nodes for each cluster a broker in ShangHai, China a cluster with a size of about 50 nodes a smart phone vendor, china a smart phone vendor, china","title":"Adopter"},{"location":"USERS/#adopter","text":"orgnization description a bank in ShangHai, China a cluster with a size of 2000+ pods, production environment a bank in SiChuan, China about 80 clusters , not more thant 100 nodes for each cluster a broker in ShangHai, China a cluster with a size of about 50 nodes a smart phone vendor, china a smart phone vendor, china","title":"Adopter"},{"location":"concepts/allocation/","text":"IP Allocation When a pod is creating, it will follow steps below to get IP allocations. Get all IPPool candidates. For which IPPool is used by a pod, the following rules are listed from high to low priority which means the previous rule would override the latter rule. SpiderSubnet annotation. ipam.spidernet.io/subnets and ipam.spidernet.io/subnet will choose to use auto-created ippool if the SpiderSubnet feature is enabled. See SpiderSubnet for details. Honor pod annotation. ipam.spidernet.io/ippools\" and \"ipam.spidernet.io/ippool could be used to specify an ippool. See Pod Annotation for details. Namespace annotation. ipam.spidernet.io/defaultv4ippool and ipam.spidernet.io/defaultv6ippool could be used to specify an ippool. See namespace annotation for details. CNI configuration file. It can be set to default_ipv4_ippool and default_ipv6_ippool in the CNI configuration file. See configuration for details. Cluster default IPPool. We can set SpiderIPPool CR object with default property, in which we'll regard it as a default pool in cluster. See configuration for details. Filter valid IPPool candidates. After getting IPv4 and IPv6 IPPool candidates, it looks into each IPPool and figures out whether it meets following rules, and learns which candidate IPPool is available. Filter terminating IPPools. The disable field of the IPPool is false . This property means the IPPool is not available to be used. Check IPPool.Spec.NodeName and IPPool.Spec.NodeAffinity properties whether match the scheduled node of the pod or not. If not match, this IPPool would be filtered. ( NodeName has higher priority than NodeAffinity ) Check IPPool.Spec.NamespaceName and IPPool.Spec.NamespaceAffinity properties whether match the namespace of the pod or not. If not match, this IPPool would be filtered. ( NamespaceName has higher priority than NamespaceAffinity ) The IPPool.Spec.PodAffinity field of the IPPool must meet the pod Check IPPool.Spec.MultusName properties whether match the pod current NIC Multus configuration or not. If not match, this IPPool would be filtered. The available IP resource of the IPPool is not exhausted Sort IPPool candidates. We'll sort these IPPool candidates with our custom priority rules, then the IPAM prefers allocating IP addresses from the candidates in sequence. The IPPool resource with IPPool.Spec.PodAffinity property has the highest priority. The IPPool resource with IPPool.Spec.NodeName or IPPool.Spec.NodeAffinity property has the second-highest priority. The IPPool resource with IPPool.Spec.NamespaceName or IPPool.Spec.NamespaceAffinity property has the second-highest priority. The IPPool resource with IPPool.Spec.MultusName property has the lowest priority. Notice: here are some simple instance to describe this rule. IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with single affinity property IPPool.Spec.PodAffinity . IPPoolA with single property IPPool.Spec.PodAffinity has higher priority than IPPoolB with properties IPPool.Spec.NodeName and IPPool.Spec.NamespaceName . IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with properties IPPool.Spec.PodAffinity , IPPool.Spec.NamespaceName and IPPool.Spec.MultusName . Assign IP from valid IPPool candidates. When trying to assign IP from the IPPool candidates, it follows rules as below. The IP is not reserved by the \"exclude_ips\" field of the IPPool and all ReservedIP instances Notice: If the pod belongs to StatefulSet, it would be assigned IP addresses with the upper rules firstly. And it will try to reuse the last allocated IP addresses once the pod 'restarts'.","title":"IP Allocation"},{"location":"concepts/allocation/#ip-allocation","text":"When a pod is creating, it will follow steps below to get IP allocations. Get all IPPool candidates. For which IPPool is used by a pod, the following rules are listed from high to low priority which means the previous rule would override the latter rule. SpiderSubnet annotation. ipam.spidernet.io/subnets and ipam.spidernet.io/subnet will choose to use auto-created ippool if the SpiderSubnet feature is enabled. See SpiderSubnet for details. Honor pod annotation. ipam.spidernet.io/ippools\" and \"ipam.spidernet.io/ippool could be used to specify an ippool. See Pod Annotation for details. Namespace annotation. ipam.spidernet.io/defaultv4ippool and ipam.spidernet.io/defaultv6ippool could be used to specify an ippool. See namespace annotation for details. CNI configuration file. It can be set to default_ipv4_ippool and default_ipv6_ippool in the CNI configuration file. See configuration for details. Cluster default IPPool. We can set SpiderIPPool CR object with default property, in which we'll regard it as a default pool in cluster. See configuration for details. Filter valid IPPool candidates. After getting IPv4 and IPv6 IPPool candidates, it looks into each IPPool and figures out whether it meets following rules, and learns which candidate IPPool is available. Filter terminating IPPools. The disable field of the IPPool is false . This property means the IPPool is not available to be used. Check IPPool.Spec.NodeName and IPPool.Spec.NodeAffinity properties whether match the scheduled node of the pod or not. If not match, this IPPool would be filtered. ( NodeName has higher priority than NodeAffinity ) Check IPPool.Spec.NamespaceName and IPPool.Spec.NamespaceAffinity properties whether match the namespace of the pod or not. If not match, this IPPool would be filtered. ( NamespaceName has higher priority than NamespaceAffinity ) The IPPool.Spec.PodAffinity field of the IPPool must meet the pod Check IPPool.Spec.MultusName properties whether match the pod current NIC Multus configuration or not. If not match, this IPPool would be filtered. The available IP resource of the IPPool is not exhausted Sort IPPool candidates. We'll sort these IPPool candidates with our custom priority rules, then the IPAM prefers allocating IP addresses from the candidates in sequence. The IPPool resource with IPPool.Spec.PodAffinity property has the highest priority. The IPPool resource with IPPool.Spec.NodeName or IPPool.Spec.NodeAffinity property has the second-highest priority. The IPPool resource with IPPool.Spec.NamespaceName or IPPool.Spec.NamespaceAffinity property has the second-highest priority. The IPPool resource with IPPool.Spec.MultusName property has the lowest priority. Notice: here are some simple instance to describe this rule. IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with single affinity property IPPool.Spec.PodAffinity . IPPoolA with single property IPPool.Spec.PodAffinity has higher priority than IPPoolB with properties IPPool.Spec.NodeName and IPPool.Spec.NamespaceName . IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with properties IPPool.Spec.PodAffinity , IPPool.Spec.NamespaceName and IPPool.Spec.MultusName . Assign IP from valid IPPool candidates. When trying to assign IP from the IPPool candidates, it follows rules as below. The IP is not reserved by the \"exclude_ips\" field of the IPPool and all ReservedIP instances Notice: If the pod belongs to StatefulSet, it would be assigned IP addresses with the upper rules firstly. And it will try to reuse the last allocated IP addresses once the pod 'restarts'.","title":"IP Allocation"},{"location":"concepts/arch-zh_CN/","text":"\u67b6\u6784 underlay \u7f51\u7edc \u548c overlay \u7f51\u7edc\u573a\u666f\u7684\u6bd4\u8f83 \u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff0c\u201coverlay \u7f51\u7edc\u65b9\u6848\u201d \u548c \u201cunderlay \u7f51\u7edc\u65b9\u6848\u201d\uff0c \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 \u6587\u7ae0 \u5bf9\u4e24\u79cd\u65b9\u6848\u7684 IPAM \u548c\u7f51\u7edc\u6027\u80fd\u505a\u4e86\u7b80\u5355\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 \u4e3a\u4ec0\u4e48\u9700\u8981 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1f\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u573a\u666f\u4e0b\uff0c\u5b58\u5728\u5f88\u591a\u5e94\u7528\u573a\u666f\uff1a \u4f4e\u5ef6\u65f6\u5e94\u7528\u7684\u9700\u6c42\uff0cunderlay \u7f51\u7edc\u65b9\u6848\u7684\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u4f1a\u4f18\u4e8e overlay \u7f51\u7edc\u65b9\u6848 \u4f20\u7edf\u4e3b\u673a\u5e94\u7528\u4e0a\u4e91\u521d\u671f\uff0c\u8fd8\u5ef6\u4f20\u7edf\u7f51\u7edc\u7684\u5bf9\u63a5\u65b9\u5f0f\uff0c\u4f8b\u5982\u670d\u52a1\u66b4\u9732\u548c\u53d1\u73b0\u3001\u591a\u5b50\u7f51\u5bf9\u63a5\u7b49 \u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u7ba1\u7406\u7684\u9700\u6c42\uff0c\u5e0c\u671b\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u5bf9\u5e94\u7528\u5b9e\u65bd\u5b89\u5168\u7ba1\u63a7\uff0c\u5e0c\u671b\u4f7f\u7528\u4f20\u7edf\u7684\u7f51\u7edc\u89c2\u6d4b\u624b\u6bb5\u5b9e\u65bd\u96c6\u7fa4\u7f51\u7edc\u76d1\u63a7 \u67b6\u6784 Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool controller: \u662f\u4e00\u7ec4 deployment\uff0c\u5b9e\u65bd\u4e86\u5bf9\u5404\u79cd CRD \u6821\u9a8c\u3001\u72b6\u6001\u66f4\u65b0\u3001IP \u56de\u6536\u3001\u81ea\u52a8 IP \u6c60\u7684\u7ba1\u7406\u7b49 Spiderpool agent\uff1a\u662f\u4e00\u7ec4 daemonset\uff0c\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u4fe1\u606f\u540c\u6b65 Spiderpool plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d coordinator plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u8c10\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u7b49 \u9664\u4e86\u4ee5\u4e0a Spiderpool \u81ea\u8eab\u7684\u7ec4\u4ef6\u4ee5\u5916\uff0c\u8fd8\u9700\u8981\u914d\u5408\u67d0\u4e2a\u5f00\u6e90\u7684 underlay CNI \u6765\u7ed9 POD \u5206\u914d\u7f51\u5361\uff0c\u53ef\u914d\u5408 multus CNI \u6765\u5b9e\u65bd\u591a\u7f51\u5361\u548c CNI \u914d\u7f6e\u7ba1\u7406\u3002 \u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI calico CNI , weave CNI \u5e94\u7528\u573a\u666f\uff1a\u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI \uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a POD \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u8c10\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c\u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 POD \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c\u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c\u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002 \u5e94\u7528\u573a\u666f\uff1aoverlay CNI \u548c underlay CNI \u534f\u540c \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a\u4e3a POD \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361\uff08\u4f8b\u5982 calico , cilium \uff09\u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\uff0c\u53ef\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a POD \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u8c10\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u8c10\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 POD \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c\u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 POD \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002 \u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c\u5728\u4fdd\u969c\u96c6\u7fa4\u5185 POD \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#_1","text":"","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#underlay-overlay","text":"\u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff0c\u201coverlay \u7f51\u7edc\u65b9\u6848\u201d \u548c \u201cunderlay \u7f51\u7edc\u65b9\u6848\u201d\uff0c \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 \u6587\u7ae0 \u5bf9\u4e24\u79cd\u65b9\u6848\u7684 IPAM \u548c\u7f51\u7edc\u6027\u80fd\u505a\u4e86\u7b80\u5355\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 \u4e3a\u4ec0\u4e48\u9700\u8981 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1f\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u573a\u666f\u4e0b\uff0c\u5b58\u5728\u5f88\u591a\u5e94\u7528\u573a\u666f\uff1a \u4f4e\u5ef6\u65f6\u5e94\u7528\u7684\u9700\u6c42\uff0cunderlay \u7f51\u7edc\u65b9\u6848\u7684\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u4f1a\u4f18\u4e8e overlay \u7f51\u7edc\u65b9\u6848 \u4f20\u7edf\u4e3b\u673a\u5e94\u7528\u4e0a\u4e91\u521d\u671f\uff0c\u8fd8\u5ef6\u4f20\u7edf\u7f51\u7edc\u7684\u5bf9\u63a5\u65b9\u5f0f\uff0c\u4f8b\u5982\u670d\u52a1\u66b4\u9732\u548c\u53d1\u73b0\u3001\u591a\u5b50\u7f51\u5bf9\u63a5\u7b49 \u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u7ba1\u7406\u7684\u9700\u6c42\uff0c\u5e0c\u671b\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u5bf9\u5e94\u7528\u5b9e\u65bd\u5b89\u5168\u7ba1\u63a7\uff0c\u5e0c\u671b\u4f7f\u7528\u4f20\u7edf\u7684\u7f51\u7edc\u89c2\u6d4b\u624b\u6bb5\u5b9e\u65bd\u96c6\u7fa4\u7f51\u7edc\u76d1\u63a7","title":"underlay \u7f51\u7edc \u548c overlay \u7f51\u7edc\u573a\u666f\u7684\u6bd4\u8f83"},{"location":"concepts/arch-zh_CN/#_2","text":"Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool controller: \u662f\u4e00\u7ec4 deployment\uff0c\u5b9e\u65bd\u4e86\u5bf9\u5404\u79cd CRD \u6821\u9a8c\u3001\u72b6\u6001\u66f4\u65b0\u3001IP \u56de\u6536\u3001\u81ea\u52a8 IP \u6c60\u7684\u7ba1\u7406\u7b49 Spiderpool agent\uff1a\u662f\u4e00\u7ec4 daemonset\uff0c\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u4fe1\u606f\u540c\u6b65 Spiderpool plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d coordinator plugin\uff1a\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u4e8c\u8fdb\u5236\u63d2\u4ef6\uff0c\u4f9b CNI \u8c03\u7528\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u8c10\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u7b49 \u9664\u4e86\u4ee5\u4e0a Spiderpool \u81ea\u8eab\u7684\u7ec4\u4ef6\u4ee5\u5916\uff0c\u8fd8\u9700\u8981\u914d\u5408\u67d0\u4e2a\u5f00\u6e90\u7684 underlay CNI \u6765\u7ed9 POD \u5206\u914d\u7f51\u5361\uff0c\u53ef\u914d\u5408 multus CNI \u6765\u5b9e\u65bd\u591a\u7f51\u5361\u548c CNI \u914d\u7f6e\u7ba1\u7406\u3002 \u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI calico CNI , weave CNI","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI \uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a POD \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u8c10\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c\u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 POD \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c\u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c\u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002","title":"\u5e94\u7528\u573a\u666f\uff1a\u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c"},{"location":"concepts/arch-zh_CN/#overlay-cni-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a\u4e3a POD \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361\uff08\u4f8b\u5982 calico , cilium \uff09\u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\uff0c\u53ef\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a POD \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u8c10\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u8c10\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 POD \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c\u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 POD \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002 \u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c\u5728\u4fdd\u969c\u96c6\u7fa4\u5185 POD \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90","title":"\u5e94\u7528\u573a\u666f\uff1aoverlay CNI \u548c underlay CNI \u534f\u540c"},{"location":"concepts/arch/","text":"Spiderpool architecture Comparison of underlay and overlay network scenarios There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. The article provides a brief comparison of IPAM and network performance between the two technologies, which offers better insights into the unique features and use cases of Spiderpool. Why underlay network solutions? In data center scenarios, the following requirements necessitate underlay network solutions: Low-latency applications need optimized network latency and throughput provided by underlay networks Initial migration of traditional host applications to the cloud use traditional network methods such as service exposure and discovery and multi subnets Network management in the data center desires security controls such as firewalls and traditional network observation techniques to implement cluster network monitoring. Architecture Spiderpool consists of the following components: Spiderpool controller: a set of deployments that manage CRD validation, status updates, IP recovery, and automated IP pools Spiderpool agent: a set of daemonsets that help Spiderpool plugin by performing IP allocation and coordinator plugin for information synchronization. Spiderpool plugin: a binary plugin on each host that CNI can utilize to implement IP allocation. coordinator plugin: a binary plugin on each host that CNI can use for multi-NIC route coordination, IP conflict detection, and host connectivity. On top of its own components, Spiderpool relies on open-source underlay CNIs to allocate network interfaces to Pods. You can use Multus CNI to manage multiple NICs and CNI configurations. Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool, such as: Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI Use case: collaborate with one or more underlay CNIs In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI and SR-IOV CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding). Use case: collaborate with overlay and underlay CNIs In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes.","title":"Architecture"},{"location":"concepts/arch/#spiderpool-architecture","text":"","title":"Spiderpool architecture"},{"location":"concepts/arch/#comparison-of-underlay-and-overlay-network-scenarios","text":"There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. The article provides a brief comparison of IPAM and network performance between the two technologies, which offers better insights into the unique features and use cases of Spiderpool. Why underlay network solutions? In data center scenarios, the following requirements necessitate underlay network solutions: Low-latency applications need optimized network latency and throughput provided by underlay networks Initial migration of traditional host applications to the cloud use traditional network methods such as service exposure and discovery and multi subnets Network management in the data center desires security controls such as firewalls and traditional network observation techniques to implement cluster network monitoring.","title":"Comparison of underlay and overlay network scenarios"},{"location":"concepts/arch/#architecture","text":"Spiderpool consists of the following components: Spiderpool controller: a set of deployments that manage CRD validation, status updates, IP recovery, and automated IP pools Spiderpool agent: a set of daemonsets that help Spiderpool plugin by performing IP allocation and coordinator plugin for information synchronization. Spiderpool plugin: a binary plugin on each host that CNI can utilize to implement IP allocation. coordinator plugin: a binary plugin on each host that CNI can use for multi-NIC route coordination, IP conflict detection, and host connectivity. On top of its own components, Spiderpool relies on open-source underlay CNIs to allocate network interfaces to Pods. You can use Multus CNI to manage multiple NICs and CNI configurations. Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool, such as: Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI","title":"Architecture"},{"location":"concepts/arch/#use-case-collaborate-with-one-or-more-underlay-cnis","text":"In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI and SR-IOV CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding).","title":"Use case: collaborate with one or more underlay CNIs"},{"location":"concepts/arch/#use-case-collaborate-with-overlay-and-underlay-cnis","text":"In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes.","title":"Use case: collaborate with overlay and underlay CNIs"},{"location":"concepts/gc/","text":"Resource Reclaim IP garbage collection Context When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod. when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call CNI plugin. A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: the new pod may fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue. Solution For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time. The spiderpool controller takes charge of this responsibility. For more details, please refer to IP GC . SpiderIPPool garbage collection To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted. SpiderEndpoint garbage collection Once a pod is created and gets IPs from SpiderIPPool , Spiderpool will create a corresponding SpiderEndpoint object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to OwnerReference with the pod. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then spiderpool controller will remove the Current data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up)","title":"Recycle IP"},{"location":"concepts/gc/#resource-reclaim","text":"","title":"Resource Reclaim"},{"location":"concepts/gc/#ip-garbage-collection","text":"","title":"IP garbage collection"},{"location":"concepts/gc/#context","text":"When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod. when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call CNI plugin. A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: the new pod may fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue.","title":"Context"},{"location":"concepts/gc/#solution","text":"For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time. The spiderpool controller takes charge of this responsibility. For more details, please refer to IP GC .","title":"Solution"},{"location":"concepts/gc/#spiderippool-garbage-collection","text":"To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted.","title":"SpiderIPPool garbage collection"},{"location":"concepts/gc/#spiderendpoint-garbage-collection","text":"Once a pod is created and gets IPs from SpiderIPPool , Spiderpool will create a corresponding SpiderEndpoint object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to OwnerReference with the pod. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then spiderpool controller will remove the Current data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up)","title":"SpiderEndpoint garbage collection"},{"location":"concepts/metrics/","text":"Metric Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller. spiderpool controller The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 spiderpool agent The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711 Get Started Enable Metric support Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer install for details Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true . Metric reference Spiderpool Agent Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric) Spiderpool Controller Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric)","title":"Metrics"},{"location":"concepts/metrics/#metric","text":"Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller.","title":"Metric"},{"location":"concepts/metrics/#spiderpool-controller","text":"The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721","title":"spiderpool controller"},{"location":"concepts/metrics/#spiderpool-agent","text":"The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711","title":"spiderpool agent"},{"location":"concepts/metrics/#get-started","text":"","title":"Get Started"},{"location":"concepts/metrics/#enable-metric-support","text":"Firstly, please ensure you have installed the spiderpool and configured the CNI file, refer install for details Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true .","title":"Enable Metric support"},{"location":"concepts/metrics/#metric-reference","text":"","title":"Metric reference"},{"location":"concepts/metrics/#spiderpool-agent_1","text":"Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric)","title":"Spiderpool Agent"},{"location":"concepts/metrics/#spiderpool-controller_1","text":"Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric)","title":"Spiderpool Controller"},{"location":"concepts/mulltusconfig-zh_CN/","text":"","title":"mulltusconfig zh CN"},{"location":"concepts/multusconfig/","text":"","title":"Multus"},{"location":"concepts/performance-zh_CN/","text":"Spiderpool \u6027\u80fd\u6d4b\u8bd5 \u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u662f\u4e00\u4e2a\u9002\u7528\u4e8e underlay \u7f51\u7edc\u7684\u9ad8\u6027\u80fd IPAM CNI \u63d2\u4ef6\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684 underlay IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts \uff0c Kube-OVN \uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u5728 \u201d1000 Pod\u201c \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002 \u80cc\u666f \u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7684\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002 \u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002 underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u7684\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u7684\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002 \u73af\u5883 Kubernetes: v1.25.4 container runtime: containerd 1.6.12 OS: CentOS Linux 8 kernel: 4.18.0-348.7.1.el8_5.x86_64 Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi \u6d4b\u8bd5\u5bf9\u8c61 \u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e 0.3.1 \u7248\u672c\u7684 CNI Specification \uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5bf9\u63a5 underlay \u7f51\u7edc\u7684\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a Main CNI Main CNI \u7248\u672c IPAM CNI IPAM CNI \u7248\u672c \u7279\u70b9 macvlan v1.1.1 Spiderpool v0.4.1 \u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u4e2a IP \u6c60\uff0c\u6bcf\u4e2a\u6c60\u4e2d\u7684 IP \u5730\u5740\u90fd\u53ef\u4ee5\u88ab\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\u7684 Pod \u6240\u4f7f\u7528\uff0c\u5f53\u96c6\u7fa4\u4e2d\u7684\u591a\u4e2a Pod \u5e76\u53d1\u7684\u4ece\u540c\u4e00\u4e2a\u6c60\u4e2d\u5206\u914d IP \u5730\u5740\u65f6\uff0c\u5b58\u5728\u7ade\u4e89\u3002\u652f\u6301\u6258\u7ba1 IP \u6c60\u7684\u5168\u751f\u547d\u6d41\u7a0b\uff0c\u4f7f\u5176\u540c\u6b65\u7684\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u521b\u5efa\u3001\u6269\u7f29\u5bb9\u3001\u5220\u9664\uff0c\u5f31\u5316\u4e86\u8fc7\u5927\u7684\u5171\u4eab\u6c60\u6240\u5e26\u6765\u7684\u5e76\u53d1\u6216\u5b58\u50a8\u95ee\u9898\u3002 macvlan v1.1.1 Whereabouts (CRD backend) v0.6.1 \u5404\u8282\u70b9\u53ef\u4ee5\u5b9a\u4e49\u5404\u81ea\u53ef\u7528\u7684 IP \u6c60\u8303\u56f4\uff0c\u82e5\u8282\u70b9\u95f4\u5b58\u5728\u91cd\u590d\u5b9a\u4e49\u7684 IP \u5730\u5740\uff0c\u90a3\u4e48\u8fd9\u4e9b IP \u5730\u5740\u4e0a\u5347\u4e3a\u4e00\u79cd\u5171\u4eab\u8d44\u6e90\u3002 Kube-OVN (underlay) v1.11.3 Kube-OVN v1.11.3 \u4ee5\u5b50\u7f51\u6765\u7ec4\u7ec7 IP \u5730\u5740\uff0c\u6bcf\u4e2a Namespace \u53ef\u4ee5\u5f52\u5c5e\u4e8e\u7279\u5b9a\u7684\u5b50\u7f51\uff0c Namespace \u4e0b\u7684 Pod \u4f1a\u81ea\u52a8\u4ece\u6240\u5c5e\u7684\u5b50\u7f51\u4e2d\u83b7\u53d6 IP \u5730\u5740\u3002\u5b50\u7f51\u4e5f\u662f\u4e00\u79cd\u96c6\u7fa4\u8d44\u6e90\uff0c\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u5730\u5740\u53ef\u4ee5\u5206\u5e03\u5728\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\u3002 Calico v3.23.3 calico-ipam (CRD backend) v3.23.3 \u6bcf\u4e2a\u8282\u70b9\u72ec\u4eab\u4e00\u4e2a\u6216\u591a\u4e2a IP block\uff0c\u5404\u8282\u70b9\u4e0a\u7684 Pod \u4ec5\u4f7f\u7528\u672c\u5730 IP block \u4e2d\u7684 IP \u5730\u5740\uff0c\u8282\u70b9\u95f4\u65e0\u4efb\u4f55\u7ade\u4e89\u4e0e\u51b2\u7a81\uff0c\u5206\u914d\u7684\u6548\u7387\u975e\u5e38\u9ad8\u3002 \u65b9\u6848 \u6d4b\u8bd5\u671f\u95f4\uff0c\u6211\u4eec\u4f1a\u9075\u5faa\u5982\u4e0b\u7ea6\u5b9a\uff1a \u6d4b\u8bd5 IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002 \u6d4b\u8bd5 underlay IPAM CNI \u63d2\u4ef6\u65f6\uff0c\u5c3d\u6700\u5927\u53ef\u80fd\u7684\u786e\u4fdd\u53ef\u7528\u7684 IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u91cf\u4e3a 1:1 \u3002\u4f8b\u5982\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u8ba1\u5212\u521b\u5efa 1000 \u4e2a Pod\uff0c\u90a3\u4e48\u5e94\u5f53\u9650\u5236\u53ef\u7528\u7684 IPv4/IPv6 \u5730\u5740\u6570\u91cf\u5747\u4e3a 1000 \u4e2a\u3002 \u5177\u4f53\u7684\uff0c\u6211\u4eec\u4f1a\u5c1d\u8bd5\u4ee5\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5728\u4e0a\u8ff0 Kubenetes \u96c6\u7fa4\u4e0a\u6765\u542f\u52a8\u603b\u8ba1 1000 \u4e2a Pod\uff0c\u5e76\u8bb0\u5f55\u6240\u6709 Pod \u5747\u8fbe\u5230 Running \u7684\u8017\u65f6\uff1a \u4ec5\u521b\u5efa\u4e00\u4e2a Deployment\uff0c\u5176\u526f\u672c\u6570\u4e3a 1000\u3002 \u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002 \u7136\u540e\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e00\u6b21\u6027\u7684\u5220\u9664\u8fd9 1000 \u4e2a Pod\uff0c\u8bb0\u5f55\u88ab\u91cd\u5efa\u7684 1000 \u4e2a Pod \u5168\u90e8 Running \u7684\u8017\u65f6\uff1a kubectl get pod | grep \"prefix\" | awk '{print $1}' | xargs kubectl delete pod \u63a5\u4e0b\u6765\uff0c\u5c06\u6240\u6709\u8282\u70b9\u4e0b\u7535\u540e\u518d\u4e0a\u7535\uff0c\u6a21\u62df\u6545\u969c\u6062\u590d\uff0c\u8bb0\u5f55 1000 \u4e2a Pod \u518d\u6b21\u8fbe\u5230 Running \u7684\u8017\u65f6\u3002 \u6700\u540e\uff0c\u6211\u4eec\u5220\u9664\u6240\u6709\u7684 Deployment\uff0c\u8bb0\u5f55\u6240\u6709 Pod \u5b8c\u5168\u6d88\u5931\u7684\u8017\u65f6\u3002 \u7ed3\u679c IPv4/IPv6 \u53cc\u6808 \u5355\u4e2a 1000 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 2m35s 9m50s 3m4s 1m50s macvlan + Whereabouts 25m18s \u5931\u8d25 \u5931\u8d25 3m5s Kube-OVN 3m55s 7m20s 11m6s 2m13s Calico + calico-ipam 1m56s 4m6s 3m42s 1m36s \u5728\u6d4b\u8bd5 macvlan + Whereabouts \u8fd9\u4e2a\u7ec4\u5408\u671f\u95f4\uff0c\u521b\u5efa\u7684\u573a\u666f\u4e2d 922 \u4e2a Pod \u5728 14m25s \u5185\u4ee5\u8f83\u4e3a\u5747\u5300\u7684\u901f\u7387\u8fbe\u5230 Running \u72b6\u6001\uff0c\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u6700\u7ec8 1000 \u4e2a Pod \u82b1\u4e86 25m18s \u8fbe\u5230 Running \u72b6\u6001\u3002\u81f3\u4e8e\u91cd\u5efa\u7684\u573a\u666f\uff0c\u5728 55 \u4e2a Pod \u8fbe\u5230 Running \u72b6\u6001\u540e\uff0cWhereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002 100 \u4e2a 10 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 1m37s 3m27s 3m3s 1m22s macvlan + Whereabouts 21m49s \u5931\u8d25 \u5931\u8d25 2m9s Kube-OVN 4m6s 7m46s 10m22s 2m8s Calico + calico-ipam 1m57s 3m58s 4m16s 1m35s IPv4 \u5355\u6808 \u5355\u4e2a 1000 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 2m18s 6m41s 3m1s 1m37s macvlan + Whereabouts 8m16s \u5931\u8d25 \u5931\u8d25 2m7s Kube-OVN 3m32s 7m7s 9m41s 1m47s Calico + calico-ipam 1m41s 3m33s 3m42s 1m27s 100 \u4e2a 10 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 1m4s 3m23s 3m3s 1m23s macvlan + Whereabouts 8m13s \u5931\u8d25 \u5931\u8d25 2m7s Kube-OVN 3m36s 7m14s 8m52s 1m41s Calico + calico-ipam 1m39s 3m25s 4m24s 1m27s \u5c0f\u7ed3 \u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e underlay \u7f51\u7edc\u7684 IPAM CNI \u63d2\u4ef6\uff0c\u5176\u76f8\u8f83\u4e8e\u4e3b\u6d41\u7684 overlay IPAM CNI \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u4ea6\u4e0d\u900a\u8272\u4e8e\u540e\u8005\u3002","title":"Spiderpool \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/performance-zh_CN/#spiderpool","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u662f\u4e00\u4e2a\u9002\u7528\u4e8e underlay \u7f51\u7edc\u7684\u9ad8\u6027\u80fd IPAM CNI \u63d2\u4ef6\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684 underlay IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts \uff0c Kube-OVN \uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u5728 \u201d1000 Pod\u201c \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002","title":"Spiderpool \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/performance-zh_CN/#_1","text":"\u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7684\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002 \u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002 underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u7684\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u7684\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002","title":"\u80cc\u666f"},{"location":"concepts/performance-zh_CN/#_2","text":"Kubernetes: v1.25.4 container runtime: containerd 1.6.12 OS: CentOS Linux 8 kernel: 4.18.0-348.7.1.el8_5.x86_64 Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi","title":"\u73af\u5883"},{"location":"concepts/performance-zh_CN/#_3","text":"\u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e 0.3.1 \u7248\u672c\u7684 CNI Specification \uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5bf9\u63a5 underlay \u7f51\u7edc\u7684\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a Main CNI Main CNI \u7248\u672c IPAM CNI IPAM CNI \u7248\u672c \u7279\u70b9 macvlan v1.1.1 Spiderpool v0.4.1 \u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u4e2a IP \u6c60\uff0c\u6bcf\u4e2a\u6c60\u4e2d\u7684 IP \u5730\u5740\u90fd\u53ef\u4ee5\u88ab\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\u7684 Pod \u6240\u4f7f\u7528\uff0c\u5f53\u96c6\u7fa4\u4e2d\u7684\u591a\u4e2a Pod \u5e76\u53d1\u7684\u4ece\u540c\u4e00\u4e2a\u6c60\u4e2d\u5206\u914d IP \u5730\u5740\u65f6\uff0c\u5b58\u5728\u7ade\u4e89\u3002\u652f\u6301\u6258\u7ba1 IP \u6c60\u7684\u5168\u751f\u547d\u6d41\u7a0b\uff0c\u4f7f\u5176\u540c\u6b65\u7684\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u521b\u5efa\u3001\u6269\u7f29\u5bb9\u3001\u5220\u9664\uff0c\u5f31\u5316\u4e86\u8fc7\u5927\u7684\u5171\u4eab\u6c60\u6240\u5e26\u6765\u7684\u5e76\u53d1\u6216\u5b58\u50a8\u95ee\u9898\u3002 macvlan v1.1.1 Whereabouts (CRD backend) v0.6.1 \u5404\u8282\u70b9\u53ef\u4ee5\u5b9a\u4e49\u5404\u81ea\u53ef\u7528\u7684 IP \u6c60\u8303\u56f4\uff0c\u82e5\u8282\u70b9\u95f4\u5b58\u5728\u91cd\u590d\u5b9a\u4e49\u7684 IP \u5730\u5740\uff0c\u90a3\u4e48\u8fd9\u4e9b IP \u5730\u5740\u4e0a\u5347\u4e3a\u4e00\u79cd\u5171\u4eab\u8d44\u6e90\u3002 Kube-OVN (underlay) v1.11.3 Kube-OVN v1.11.3 \u4ee5\u5b50\u7f51\u6765\u7ec4\u7ec7 IP \u5730\u5740\uff0c\u6bcf\u4e2a Namespace \u53ef\u4ee5\u5f52\u5c5e\u4e8e\u7279\u5b9a\u7684\u5b50\u7f51\uff0c Namespace \u4e0b\u7684 Pod \u4f1a\u81ea\u52a8\u4ece\u6240\u5c5e\u7684\u5b50\u7f51\u4e2d\u83b7\u53d6 IP \u5730\u5740\u3002\u5b50\u7f51\u4e5f\u662f\u4e00\u79cd\u96c6\u7fa4\u8d44\u6e90\uff0c\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u5730\u5740\u53ef\u4ee5\u5206\u5e03\u5728\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\u3002 Calico v3.23.3 calico-ipam (CRD backend) v3.23.3 \u6bcf\u4e2a\u8282\u70b9\u72ec\u4eab\u4e00\u4e2a\u6216\u591a\u4e2a IP block\uff0c\u5404\u8282\u70b9\u4e0a\u7684 Pod \u4ec5\u4f7f\u7528\u672c\u5730 IP block \u4e2d\u7684 IP \u5730\u5740\uff0c\u8282\u70b9\u95f4\u65e0\u4efb\u4f55\u7ade\u4e89\u4e0e\u51b2\u7a81\uff0c\u5206\u914d\u7684\u6548\u7387\u975e\u5e38\u9ad8\u3002","title":"\u6d4b\u8bd5\u5bf9\u8c61"},{"location":"concepts/performance-zh_CN/#_4","text":"\u6d4b\u8bd5\u671f\u95f4\uff0c\u6211\u4eec\u4f1a\u9075\u5faa\u5982\u4e0b\u7ea6\u5b9a\uff1a \u6d4b\u8bd5 IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002 \u6d4b\u8bd5 underlay IPAM CNI \u63d2\u4ef6\u65f6\uff0c\u5c3d\u6700\u5927\u53ef\u80fd\u7684\u786e\u4fdd\u53ef\u7528\u7684 IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u91cf\u4e3a 1:1 \u3002\u4f8b\u5982\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u8ba1\u5212\u521b\u5efa 1000 \u4e2a Pod\uff0c\u90a3\u4e48\u5e94\u5f53\u9650\u5236\u53ef\u7528\u7684 IPv4/IPv6 \u5730\u5740\u6570\u91cf\u5747\u4e3a 1000 \u4e2a\u3002 \u5177\u4f53\u7684\uff0c\u6211\u4eec\u4f1a\u5c1d\u8bd5\u4ee5\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5728\u4e0a\u8ff0 Kubenetes \u96c6\u7fa4\u4e0a\u6765\u542f\u52a8\u603b\u8ba1 1000 \u4e2a Pod\uff0c\u5e76\u8bb0\u5f55\u6240\u6709 Pod \u5747\u8fbe\u5230 Running \u7684\u8017\u65f6\uff1a \u4ec5\u521b\u5efa\u4e00\u4e2a Deployment\uff0c\u5176\u526f\u672c\u6570\u4e3a 1000\u3002 \u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002 \u7136\u540e\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e00\u6b21\u6027\u7684\u5220\u9664\u8fd9 1000 \u4e2a Pod\uff0c\u8bb0\u5f55\u88ab\u91cd\u5efa\u7684 1000 \u4e2a Pod \u5168\u90e8 Running \u7684\u8017\u65f6\uff1a kubectl get pod | grep \"prefix\" | awk '{print $1}' | xargs kubectl delete pod \u63a5\u4e0b\u6765\uff0c\u5c06\u6240\u6709\u8282\u70b9\u4e0b\u7535\u540e\u518d\u4e0a\u7535\uff0c\u6a21\u62df\u6545\u969c\u6062\u590d\uff0c\u8bb0\u5f55 1000 \u4e2a Pod \u518d\u6b21\u8fbe\u5230 Running \u7684\u8017\u65f6\u3002 \u6700\u540e\uff0c\u6211\u4eec\u5220\u9664\u6240\u6709\u7684 Deployment\uff0c\u8bb0\u5f55\u6240\u6709 Pod \u5b8c\u5168\u6d88\u5931\u7684\u8017\u65f6\u3002","title":"\u65b9\u6848"},{"location":"concepts/performance-zh_CN/#_5","text":"","title":"\u7ed3\u679c"},{"location":"concepts/performance-zh_CN/#ipv4ipv6","text":"\u5355\u4e2a 1000 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 2m35s 9m50s 3m4s 1m50s macvlan + Whereabouts 25m18s \u5931\u8d25 \u5931\u8d25 3m5s Kube-OVN 3m55s 7m20s 11m6s 2m13s Calico + calico-ipam 1m56s 4m6s 3m42s 1m36s \u5728\u6d4b\u8bd5 macvlan + Whereabouts \u8fd9\u4e2a\u7ec4\u5408\u671f\u95f4\uff0c\u521b\u5efa\u7684\u573a\u666f\u4e2d 922 \u4e2a Pod \u5728 14m25s \u5185\u4ee5\u8f83\u4e3a\u5747\u5300\u7684\u901f\u7387\u8fbe\u5230 Running \u72b6\u6001\uff0c\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u6700\u7ec8 1000 \u4e2a Pod \u82b1\u4e86 25m18s \u8fbe\u5230 Running \u72b6\u6001\u3002\u81f3\u4e8e\u91cd\u5efa\u7684\u573a\u666f\uff0c\u5728 55 \u4e2a Pod \u8fbe\u5230 Running \u72b6\u6001\u540e\uff0cWhereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002 100 \u4e2a 10 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 1m37s 3m27s 3m3s 1m22s macvlan + Whereabouts 21m49s \u5931\u8d25 \u5931\u8d25 2m9s Kube-OVN 4m6s 7m46s 10m22s 2m8s Calico + calico-ipam 1m57s 3m58s 4m16s 1m35s","title":"IPv4/IPv6 \u53cc\u6808"},{"location":"concepts/performance-zh_CN/#ipv4","text":"\u5355\u4e2a 1000 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 2m18s 6m41s 3m1s 1m37s macvlan + Whereabouts 8m16s \u5931\u8d25 \u5931\u8d25 2m7s Kube-OVN 3m32s 7m7s 9m41s 1m47s Calico + calico-ipam 1m41s 3m33s 3m42s 1m27s 100 \u4e2a 10 \u526f\u672c\u7684 Deployment\uff1a CNI \u521b\u5efa \u91cd\u5efa \u6545\u969c\u6062\u590d \u5220\u9664 macvlan + Spiderpool 1m4s 3m23s 3m3s 1m23s macvlan + Whereabouts 8m13s \u5931\u8d25 \u5931\u8d25 2m7s Kube-OVN 3m36s 7m14s 8m52s 1m41s Calico + calico-ipam 1m39s 3m25s 4m24s 1m27s","title":"IPv4 \u5355\u6808"},{"location":"concepts/performance-zh_CN/#_6","text":"\u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e underlay \u7f51\u7edc\u7684 IPAM CNI \u63d2\u4ef6\uff0c\u5176\u76f8\u8f83\u4e8e\u4e3b\u6d41\u7684 overlay IPAM CNI \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u4ea6\u4e0d\u900a\u8272\u4e8e\u540e\u8005\u3002","title":"\u5c0f\u7ed3"},{"location":"concepts/performance/","text":"Spiderpool Performance Testing English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is a high-performance IPAM CNI plugin for underlay networks. This report will compare its performance with the mainstream underlay IPAM CNI plugins (such as Whereabouts , Kube-OVN ) and the widely used overlay IPAM CNI plugin calico-ipam under the \"1000 Pod\" scenario. Background Why do we need to do performance testing on the underlay IPAM CNI plugin? The speed at which IPAM allocates IP addresses largely determines the speed of application publishing. Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures. Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources. ENV Kubernetes: v1.25.4 container runtime: containerd 1.6.12 OS: CentOS Linux 8 kernel: 4.18.0-348.7.1.el8_5.x86_64 Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi Objects This test is based on the CNI Specification 0.3.1 , using macvlan with Spiderpool, and selecting several other underlay network solutions in the open source community for comparison: Main CNI Main CNI Version IPAM CNI IPAM CNI Version Features macvlan v1.1.1 Spiderpool v0.4.1 There are multiple IP pools in a cluster, and the IP addresses in each pool can be used by Pods on any Node in the cluster. Competition occurs when multiple Pods in a cluster allocate IP addresses from the same pool. Support hosting the entire lifecycle of an IP pool, synchronizing it with workload creation, scaling, deletion, and reducing concurrency or storage issues caused by overly large shared pools. macvlan v1.1.1 Whereabouts (CRD backend) v0.6.1 Each Node can define its own available IP pool ranges. If there are duplicate IP addresses defined between Nodes, these IP addresses are promoted as a shared resource. Kube-OVN (underlay) v1.11.3 Kube-OVN v1.11.3 IP addresses are organized by Subnet. Each Namespace can belong to a specific Subnet. Pods under the Namespace will automatically obtain IP addresses from the Subnet they belong to. Subnets are also a cluster resource, and the IP addresses of the same Subnet can be distributed on any Node. Calico v3.23.3 calico-ipam (CRD backend) v3.23.3 Each Node has one or more IP blocks exclusively, and the Pods on each Node only use the IP addresses in the local IP block. There is no competition or conflict between Nodes, and the allocation efficiency is very high. Plan During the testing, we will follow the following agreement: Testing under IPv4 stack and IPv4/IPv6 dual-stack. When testing the underlay IPAM CNI plugins, ensure that the number of available IP addresses is 1:1 to the number of Pods as much as possible. For example, if we plan to create 1000 Pods in the next step, we should limit the number of available IPv4/IPv6 addresses to 1000. Specifically, we will attempt to create a total of 1000 Pods on the Kubernetes cluster in the following two ways, and record the time taken for all Pods to become Running : Create only one Deployment with 1000 replicas. Create 100 Deployments with 10 replicas per Deployment. Then, we will use the following command to delete these 1000 Pods at once, and record the time it took for all Pods to become Running : kubectl get pod | grep \"prefix\" | awk '{print $1}' | xargs kubectl delete pod Next, power down all nodes and then power up, simulate fault recovery, and record the time it took for all Pods to become Running again. Finally, we delete all Deployments and record the time taken for all Pods to completely disappear. Result IPv4/IPv6 dual-stack 1 Deployment with 1000 replicas: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 2m35s 9m50s 3m4s 1m50s macvlan + Whereabouts 25m18s failure failure 3m5s Kube-OVN 3m55s 7m20s 11m6s 2m13s Calico + calico-ipam 1m56s 4m6s 3m42s 1m36s During the testing of macvlan + Whereabouts, in the creation scenario, 922 Pods became Running at a relatively uniform rate within 14m25s. After that, the growth rate of Pods significantly decreased, and ultimately it took 25m18s for 1000 Pods to become Running . As for the re-creation scenario, after 55 Pods became Running , Whereabouts basically stopped working, and the time consumption was close to infinity. 100 Deployments with 10 replicas: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 1m37s 3m27s 3m3s 1m22s macvlan + Whereabouts 21m49s failure failure 2m9s Kube-OVN 4m6s 7m46s 10m22s 2m8s Calico + calico-ipam 1m57s 3m58s 4m16s 1m35s IPv4 stack 1 Deployment with 1000 replicas: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 2m18s 6m41s 3m1s 1m37s macvlan + Whereabouts 8m16s failure failure 2m7s Kube-OVN 3m32s 7m7s 9m41s 1m47s Calico + calico-ipam 1m41s 3m33s 3m42s 1m27s 100 Deployments with 10 replicas per Deployment: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 1m4s 3m23s 3m3s 1m23s macvlan + Whereabouts 8m13s failure failure 2m7s Kube-OVN 3m36s 7m14s 8m52s 1m41s Calico + calico-ipam 1m39s 3m25s 4m24s 1m27s Summary Although Spiderpool is an IPAM CNI plugin suitable for underlay networks, it faces more complex IP address preemption and conflict issues than mainstream overlay IPAM CNI plugins, but its performance in most scenarios is not inferior to the latter.","title":"IPAM Performance"},{"location":"concepts/performance/#spiderpool-performance-testing","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is a high-performance IPAM CNI plugin for underlay networks. This report will compare its performance with the mainstream underlay IPAM CNI plugins (such as Whereabouts , Kube-OVN ) and the widely used overlay IPAM CNI plugin calico-ipam under the \"1000 Pod\" scenario.","title":"Spiderpool Performance Testing"},{"location":"concepts/performance/#background","text":"Why do we need to do performance testing on the underlay IPAM CNI plugin? The speed at which IPAM allocates IP addresses largely determines the speed of application publishing. Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures. Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources.","title":"Background"},{"location":"concepts/performance/#env","text":"Kubernetes: v1.25.4 container runtime: containerd 1.6.12 OS: CentOS Linux 8 kernel: 4.18.0-348.7.1.el8_5.x86_64 Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi","title":"ENV"},{"location":"concepts/performance/#objects","text":"This test is based on the CNI Specification 0.3.1 , using macvlan with Spiderpool, and selecting several other underlay network solutions in the open source community for comparison: Main CNI Main CNI Version IPAM CNI IPAM CNI Version Features macvlan v1.1.1 Spiderpool v0.4.1 There are multiple IP pools in a cluster, and the IP addresses in each pool can be used by Pods on any Node in the cluster. Competition occurs when multiple Pods in a cluster allocate IP addresses from the same pool. Support hosting the entire lifecycle of an IP pool, synchronizing it with workload creation, scaling, deletion, and reducing concurrency or storage issues caused by overly large shared pools. macvlan v1.1.1 Whereabouts (CRD backend) v0.6.1 Each Node can define its own available IP pool ranges. If there are duplicate IP addresses defined between Nodes, these IP addresses are promoted as a shared resource. Kube-OVN (underlay) v1.11.3 Kube-OVN v1.11.3 IP addresses are organized by Subnet. Each Namespace can belong to a specific Subnet. Pods under the Namespace will automatically obtain IP addresses from the Subnet they belong to. Subnets are also a cluster resource, and the IP addresses of the same Subnet can be distributed on any Node. Calico v3.23.3 calico-ipam (CRD backend) v3.23.3 Each Node has one or more IP blocks exclusively, and the Pods on each Node only use the IP addresses in the local IP block. There is no competition or conflict between Nodes, and the allocation efficiency is very high.","title":"Objects"},{"location":"concepts/performance/#plan","text":"During the testing, we will follow the following agreement: Testing under IPv4 stack and IPv4/IPv6 dual-stack. When testing the underlay IPAM CNI plugins, ensure that the number of available IP addresses is 1:1 to the number of Pods as much as possible. For example, if we plan to create 1000 Pods in the next step, we should limit the number of available IPv4/IPv6 addresses to 1000. Specifically, we will attempt to create a total of 1000 Pods on the Kubernetes cluster in the following two ways, and record the time taken for all Pods to become Running : Create only one Deployment with 1000 replicas. Create 100 Deployments with 10 replicas per Deployment. Then, we will use the following command to delete these 1000 Pods at once, and record the time it took for all Pods to become Running : kubectl get pod | grep \"prefix\" | awk '{print $1}' | xargs kubectl delete pod Next, power down all nodes and then power up, simulate fault recovery, and record the time it took for all Pods to become Running again. Finally, we delete all Deployments and record the time taken for all Pods to completely disappear.","title":"Plan"},{"location":"concepts/performance/#result","text":"","title":"Result"},{"location":"concepts/performance/#ipv4ipv6-dual-stack","text":"1 Deployment with 1000 replicas: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 2m35s 9m50s 3m4s 1m50s macvlan + Whereabouts 25m18s failure failure 3m5s Kube-OVN 3m55s 7m20s 11m6s 2m13s Calico + calico-ipam 1m56s 4m6s 3m42s 1m36s During the testing of macvlan + Whereabouts, in the creation scenario, 922 Pods became Running at a relatively uniform rate within 14m25s. After that, the growth rate of Pods significantly decreased, and ultimately it took 25m18s for 1000 Pods to become Running . As for the re-creation scenario, after 55 Pods became Running , Whereabouts basically stopped working, and the time consumption was close to infinity. 100 Deployments with 10 replicas: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 1m37s 3m27s 3m3s 1m22s macvlan + Whereabouts 21m49s failure failure 2m9s Kube-OVN 4m6s 7m46s 10m22s 2m8s Calico + calico-ipam 1m57s 3m58s 4m16s 1m35s","title":"IPv4/IPv6 dual-stack"},{"location":"concepts/performance/#ipv4-stack","text":"1 Deployment with 1000 replicas: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 2m18s 6m41s 3m1s 1m37s macvlan + Whereabouts 8m16s failure failure 2m7s Kube-OVN 3m32s 7m7s 9m41s 1m47s Calico + calico-ipam 1m41s 3m33s 3m42s 1m27s 100 Deployments with 10 replicas per Deployment: CNI Creation Re-creation Recovery Deletion macvlan + Spiderpool 1m4s 3m23s 3m3s 1m23s macvlan + Whereabouts 8m13s failure failure 2m7s Kube-OVN 3m36s 7m14s 8m52s 1m41s Calico + calico-ipam 1m39s 3m25s 4m24s 1m27s","title":"IPv4 stack"},{"location":"concepts/performance/#summary","text":"Although Spiderpool is an IPAM CNI plugin suitable for underlay networks, it faces more complex IP address preemption and conflict issues than mainstream overlay IPAM CNI plugins, but its performance in most scenarios is not inferior to the latter.","title":"Summary"},{"location":"concepts/solution-zh_CN/","text":"Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u65b9\u6848 \u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff1a\"Overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"Underlay \u7f51\u7edc\u65b9\u6848\"\u3002 \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 Spiderpool \u662f\u4e3a Underlay \u7f51\u7edc\u7279\u70b9\u800c\u8bbe\u8ba1\uff0c\u4ee5\u4e0b\u5bf9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 Overlay \u7f51\u7edc\u65b9\u6848 IPAM \u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u7f51\u7edc\u540c\u5bbf\u4e3b\u673a\u7f51\u7edc\u7684\u89e3\u8026\uff0c\u4f8b\u5982 Calico \u3001 Cilium \u7b49 CNI \u63d2\u4ef6\uff0c \u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u8fd9\u7c7b\u6280\u672f\u6d41\u6d3e\u7684 IPAM \u5206\u914d\u7279\u70b9\u662f\uff1a Pod \u5b50\u7f51\u4e2d\u7684 IP \u5730\u5740\u6309\u7167\u8282\u70b9\u8fdb\u884c\u4e86\u5206\u5272 \u4ee5\u4e00\u4e2a\u66f4\u5c0f\u5b50\u7f51\u63a9\u7801\u957f\u5ea6\u4e3a\u5355\u4f4d\uff0c\u628a Pod subnet \u5206\u5272\u51fa\u66f4\u5c0f\u7684 IP block \u96c6\u5408\uff0c\u4f9d\u636e IP \u4f7f\u7528\u7684\u7528\u91cf\u60c5\u51b5\uff0c\u6bcf\u4e2a node \u90fd\u4f1a\u83b7\u53d6\u5230\u4e00\u4e2a\u6216\u8005\u591a\u4e2a IP block\u3002 \u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u7279\u70b9\uff1a\u7b2c\u4e00\uff0c\u6bcf\u4e2a node \u4e0a\u7684 IPAM \u63d2\u4ef6\u53ea\u9700\u8981\u5728\u672c\u5730\u7684 IP block \u4e2d\u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u65f6\uff0c\u4e0e\u5176\u5b83 node \u4e0a\u7684 IPAM \u65e0 IP \u5206\u914d\u51b2\u7a81\uff0cIPAM \u5206\u914d\u6548\u7387\u66f4\u9ad8\u3002 \u7b2c\u4e8c\uff0c\u67d0\u4e2a\u5177\u4f53\u7684 IP \u5730\u5740\u8ddf\u968f IP block \u96c6\u5408\uff0c\u4f1a\u76f8\u5bf9\u56fa\u5b9a\u7684\u4e00\u76f4\u5728\u67d0\u4e2a node \u4e0a\u88ab\u5206\u914d\uff0c\u6ca1\u6cd5\u968f\u540c Pod \u4e00\u8d77\u88ab\u8c03\u5ea6\u6f02\u79fb\u3002 IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b \u53ea\u8981 Pod \u5b50\u7f51\u4e0d\u4e0e\u76f8\u5173\u7f51\u7edc\u91cd\u53e0\uff0c\u518d\u80fd\u591f\u5408\u7406\u5229\u7528 NAT \u6280\u672f\uff0cKubernetes \u5355\u4e2a\u96c6\u7fa4\u53ef\u4ee5\u62e5\u6709\u5145\u6c9b\u7684 IP \u5730\u5740\u8d44\u6e90\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0d\u4f1a\u56e0\u4e3a IP \u4e0d\u591f\u800c\u542f\u52a8\u5931\u8d25\uff0cIPAM \u7ec4\u4ef6\u9762\u4e34\u7684\u5f02\u5e38 IP \u56de\u6536\u538b\u529b\u8f83\u5c0f\u3002 \u6ca1\u6709\u5e94\u7528 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42 \u5bf9\u4e8e\u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6709\u65e0\u72b6\u6001\u5e94\u7528\u548c\u6709\u72b6\u6001\u5e94\u7528\u7684\u533a\u522b\uff1a\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c \u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\" \u7684\u9700\u6c42\uff0c\u53ea\u80fd\u8ba9\u6240\u6709 Pod \u526f\u672c\u56fa\u5b9a\u5728\u4e00\u4e2a IP \u5730\u5740\u7684\u96c6\u5408\u5185\uff1b\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod name \u7b49\u4fe1\u606f\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u6709\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\uff0c\u8981\u5b9e\u73b0\u5355\u4e2a Pod \u548c\u5177\u4f53 IP \u5730\u5740\u7684\u5f3a\u7ed1\u5b9a\u3002 \u5728 \"Overlay \u7f51\u7edc\u65b9\u6848\"\u65b9\u6848\u4e0b\uff0c\u591a\u662f\u501f\u52a9\u4e86 NAT \u6280\u672f\u5411\u96c6\u7fa4\u5916\u90e8\u66b4\u9732\u670d\u52a1\u7684\u5165\u53e3\u548c\u6e90\u5730\u5740\uff0c\u501f\u52a9 DNS\u3001clusterIP \u7b49\u6280\u672f\u6765\u5b9e\u73b0\u96c6\u7fa4\u4e1c\u897f\u5411\u901a\u4fe1\u3002 \u5176\u6b21\uff0cIPAM \u7684 IP block \u65b9\u5f0f\u628a IP \u76f8\u5bf9\u56fa\u5b9a\u5230\u67d0\u4e2a\u8282\u70b9\u4e0a\uff0c\u800c\u4e0d\u80fd\u4fdd\u8bc1\u5e94\u7528\u526f\u672c\u7684\u8ddf\u968f\u8c03\u5ea6\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u7684 \"IP \u5730\u5740\u56fa\u5b9a\"\u80fd\u529b\u65e0\u7528\u6b66\u4e4b\u5730\uff0c\u5f53\u524d\u793e\u533a\u7684\u4e3b\u6d41 CNI \u591a\u6570\u4e0d\u652f\u6301 \"IP \u5730\u5740\u56fa\u5b9a\"\uff0c\u6216\u8005\u652f\u6301\u65b9\u6cd5\u8f83\u4e3a\u7b80\u964b\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u70b9\u662f\uff0c\u65e0\u8bba\u96c6\u7fa4\u90e8\u7f72\u5728\u4ec0\u4e48\u6837\u7684\u5e95\u5c42\u7f51\u7edc\u73af\u5883\u4e0a\uff0cCNI \u63d2\u4ef6\u7684\u517c\u5bb9\u6027\u90fd\u975e\u5e38\u597d\uff0c\u4e14\u90fd\u80fd\u591f\u4e3a Pod \u63d0\u4f9b\u5b50\u7f51\u72ec\u7acb\u3001IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b\u7684\u7f51\u7edc\u3002 Underlay \u7f51\u7edc\u65b9\u6848 IPAM \u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u5171\u4eab\u5bbf\u4e3b\u673a\u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5373 Pod \u76f4\u63a5\u83b7\u53d6\u5bbf\u4e3b\u673a\u7f51\u7edc\u4e2d\u7684 IP \u5730\u5740\u3002\u8fd9\u6837\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 Underlay \u7f51\u7edc\u65b9\u6848\u7684\u5b9e\u65bd\uff0c\u6709\u4e24\u79cd\u5178\u578b\u7684\u573a\u666f\uff1a\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5b9e\u65bd\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u4e0a\uff1b\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5728 IAAS \u73af\u5883\u4e0a\uff0c\u4f8b\u5982\u516c\u6709\u4e91\u3002\u4ee5\u4e0b\u603b\u7ed3\u4e86\"\u4f20\u7edf\u7f51\u7edc\u573a\u666f\"\u7684 IPAM \u7279\u70b9\uff1a \u5355\u4e2a IP \u5730\u5740\u5e94\u8be5\u80fd\u591f\u5728\u4efb\u4e00\u8282\u70b9\u4e0a\u88ab\u5206\u914d \u8fd9\u4e2a\u9700\u6c42\u6709\u591a\u65b9\u9762\u7684\u539f\u56e0\uff1a\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u8bbe\u5907\u589e\u52a0\u3001\u591a\u96c6\u7fa4\u6280\u672f\u7684\u53d1\u5c55\uff0cIPv4 \u5730\u5740\u8d44\u6e90\u7a00\u7f3a\uff0c\u8981\u6c42 IPAM \u63d0\u9ad8 IP \u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\uff1b \u5bf9\u4e8e\u6709 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5176 Pod \u526f\u672c\u53ef\u80fd\u4f1a\u8c03\u5ea6\u5230\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u8fd8\u4f1a\u53d1\u751f\u8282\u70b9\u95f4\u7684\u6f02\u79fb\uff0c\u8981\u6c42 IP \u5730\u5740\u4e00\u8d77\u6f02\u79fb\u3002 \u56e0\u6b64\uff0c\u5728\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4e00\u4e2a IP \u5730\u5740\u5e94\u8be5\u5177\u5907\u80fd\u591f\u88ab\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684\u53ef\u80fd\u3002 \u540c\u4e00\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\uff0c\u80fd\u5b9e\u73b0\u8de8\u5b50\u7f51\u83b7\u53d6 IP \u5730\u5740 \u4f8b\u5982\uff0c\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u5bbf\u4e3b\u673a1\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.1.0/24\uff0c\u800c\u5bbf\u4e3b\u673a2\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.2.0/24\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0c \u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\u3002 \u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a \u5f88\u591a\u4f20\u7edf\u5e94\u7528\u5728\u4e91\u5316\u6539\u9020\u524d\uff0c\u662f\u90e8\u7f72\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u7684\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u7f51\u7edc\u672a\u5f15\u5165 NAT \u5730\u5740\u8f6c\u6362\uff0c\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u9700\u8981\u611f\u77e5\u5bf9\u65b9\u7684\u6e90 IP \u6216\u76ee\u7684 IP\uff0c \u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u4e5f\u4e60\u60ef\u4e86\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u6765\u7cbe\u7ec6\u7ba1\u63a7\u7f51\u7edc\u5b89\u5168\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0a\u4e91\u540e\uff0c\u65e0\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u8303\u56f4\u7684\u56fa\u5b9a\uff0c\u6709\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u5730\u5740\u7684\u552f\u4e00\u5bf9\u5e94\uff0c\u8fd9\u6837\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u6539\u9020\u5de5\u4f5c\u3002 \u4e00\u4e2a Pod \u7684\u591a\u7f51\u5361\u83b7\u53d6\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740 \u65e2\u7136\u662f\u5bf9\u63a5 Underlay \u7f51\u7edc\uff0cPod \u5c31\u4f1a\u6709\u591a\u7f51\u5361\u9700\u6c42\uff0c\u4ee5\u4f7f\u5176\u901a\u8fbe\u4e0d\u540c\u7684 Underlay \u5b50\u7f51\uff0c\u8fd9\u8981\u6c42 IPAM \u80fd\u591f\u7ed9\u5e94\u7528\u7684\u4e0d\u540c\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u4e0b\u7684 IP \u5730\u5740\u3002 IP \u5730\u5740\u51b2\u7a81 \u5728 Underlay \u7f51\u7edc\u4e2d\uff0c\u66f4\u52a0\u5bb9\u6613\u51fa\u73b0 IP \u51b2\u7a81\uff0c\u4f8b\u5982\uff0cPod \u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u4e3b\u673a IP \u53d1\u751f\u4e86\u51b2\u7a81\uff0c\u4e0e\u5176\u5b83\u5bf9\u63a5\u4e86\u76f8\u540c\u5b50\u7f51\u7684\u96c6\u7fa4\u51b2\u7a81\uff0c \u800c IPAM \u7ec4\u4ef6\u5f88\u96be\u611f\u77e5\u5916\u90e8\u8fd9\u4e9b\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u591a\u9700\u8981\u501f\u52a9 CNI \u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3002 \u5df2\u7528 IP \u5730\u5740\u7684\u91ca\u653e\u56de\u6536 \u56e0\u4e3a Underlay \u7f51\u7edc IP \u5730\u5740\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5e94\u7528\u6709 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6240\u4ee5\uff0c\"\u5e94\u5f53\"\u88ab\u91ca\u653e\u7684 IP \u5730\u5740\u82e5\u672a\u88ab IPAM \u7ec4\u4ef6\u56de\u6536\uff0c\u65b0\u542f\u52a8\u7684 Pod \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7f3a\u5c11 IP \u5730\u5740\u800c\u5931\u8d25\u3002 \u8fd9\u5c31\u8981\u6c42 IPAM \u7ec4\u4ef6\u62e5\u6709\u66f4\u52a0\u7cbe\u51c6\u3001\u9ad8\u6548\u3001\u53ca\u65f6\u7684 IP \u56de\u6536\u673a\u5236\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u52bf\u6709\uff1a\u65e0\u9700\u7f51\u7edc NAT \u6620\u5c04\u7684\u5f15\u5165\uff0c\u5bf9\u5e94\u7528\u7684\u4e91\u5316\u7f51\u7edc\u6539\u9020\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u7684\u4fbf\u5229\uff1b\u5e95\u5c42\u7f51\u7edc\u7684\u706b\u5899\u7b49\u8bbe\u5907\uff0c\u53ef\u5bf9 Pod \u901a\u4fe1\u5b9e\u73b0\u76f8\u5bf9\u8f83\u4e3a\u7cbe\u7ec6\u7684\u7ba1\u63a7\uff1b\u65e0\u9700\u96a7\u9053\u6280\u672f\uff0c \u7f51\u7edc\u901a\u4fe1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u65f6\u6027\u80fd\u4e5f\u76f8\u5bf9\u7684\u63d0\u9ad8\u4e86\u3002 Overlay CNI \u548c Underlay CNI \u7684\u6027\u80fd Overlay CNI \u9700\u8981\u501f\u52a9\u96a7\u9053\u5c01\u88c5\u548c\u5bbf\u4e3b\u673a\u8f6c\u53d1\u5b9e\u73b0\u8de8\u4e3b\u673a\u7684 Pod \u901a\u4fe1\uff0c\u800c Underlay CNI \u53ef\u5b9e\u73b0\u76f4\u63a5\u5bf9\u63a5\u5bbf\u4e3b\u673a\u7f51\u7edc\uff0c\u56e0\u6b64\u5728\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u8868\u73b0\u4e0a\u6709\u5dee\u5f02\u3002 \u5728\u4e00\u4e2a\u5177\u5907 2 \u4e2a\u88f8\u91d1\u5c5e\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5b89\u88c5\u4e00\u4e2a 25G mellanox CX5 \u7f51\u5361\uff0c\u5206\u522b\u6d4b\u8bd5\u5982\u4e0b\uff1a 2 \u4e2a\u88f8\u91d1\u5c5e\u8282\u70b9\u95f4\u7684\u7f51\u5361\u7684\u6027\u80fd \u57fa\u4e8e macvlan \u7684\u8de8\u8282\u70b9 Pod \u7f51\u7edc\u6027\u80fd \u57fa\u4e8e SR-IOV \u7684\u8de8\u8282\u70b9 Pod \u7f51\u7edc\u6027\u80fd \u57fa\u4e8e calico(v3.23) vxlan \u7684\u8de8\u8282\u70b9 Pod \u7684\u7f51\u7edc\u6027\u80fd \u57fa\u4e8e cilium(v1.13) vxlan \u7684\u8de8\u8282\u70b9 Pod \u7684\u7f51\u7edc\u6027\u80fd \u6700\u7ec8\u7684\u6d4b\u8bd5\u62a5\u544a\u5982\u4e0a\u6240\u793a\uff0c\u8bf8\u5982 macvlan \u548c SRIOV \u7684 Underlay CNI \u6027\u80fd\u8f83\u597d\u3002","title":"Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u65b9\u6848"},{"location":"concepts/solution-zh_CN/#underlay-overlay","text":"\u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff1a\"Overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"Underlay \u7f51\u7edc\u65b9\u6848\"\u3002 \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 Spiderpool \u662f\u4e3a Underlay \u7f51\u7edc\u7279\u70b9\u800c\u8bbe\u8ba1\uff0c\u4ee5\u4e0b\u5bf9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002","title":"Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u65b9\u6848"},{"location":"concepts/solution-zh_CN/#overlay-ipam","text":"\u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u7f51\u7edc\u540c\u5bbf\u4e3b\u673a\u7f51\u7edc\u7684\u89e3\u8026\uff0c\u4f8b\u5982 Calico \u3001 Cilium \u7b49 CNI \u63d2\u4ef6\uff0c \u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u8fd9\u7c7b\u6280\u672f\u6d41\u6d3e\u7684 IPAM \u5206\u914d\u7279\u70b9\u662f\uff1a Pod \u5b50\u7f51\u4e2d\u7684 IP \u5730\u5740\u6309\u7167\u8282\u70b9\u8fdb\u884c\u4e86\u5206\u5272 \u4ee5\u4e00\u4e2a\u66f4\u5c0f\u5b50\u7f51\u63a9\u7801\u957f\u5ea6\u4e3a\u5355\u4f4d\uff0c\u628a Pod subnet \u5206\u5272\u51fa\u66f4\u5c0f\u7684 IP block \u96c6\u5408\uff0c\u4f9d\u636e IP \u4f7f\u7528\u7684\u7528\u91cf\u60c5\u51b5\uff0c\u6bcf\u4e2a node \u90fd\u4f1a\u83b7\u53d6\u5230\u4e00\u4e2a\u6216\u8005\u591a\u4e2a IP block\u3002 \u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u7279\u70b9\uff1a\u7b2c\u4e00\uff0c\u6bcf\u4e2a node \u4e0a\u7684 IPAM \u63d2\u4ef6\u53ea\u9700\u8981\u5728\u672c\u5730\u7684 IP block \u4e2d\u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u65f6\uff0c\u4e0e\u5176\u5b83 node \u4e0a\u7684 IPAM \u65e0 IP \u5206\u914d\u51b2\u7a81\uff0cIPAM \u5206\u914d\u6548\u7387\u66f4\u9ad8\u3002 \u7b2c\u4e8c\uff0c\u67d0\u4e2a\u5177\u4f53\u7684 IP \u5730\u5740\u8ddf\u968f IP block \u96c6\u5408\uff0c\u4f1a\u76f8\u5bf9\u56fa\u5b9a\u7684\u4e00\u76f4\u5728\u67d0\u4e2a node \u4e0a\u88ab\u5206\u914d\uff0c\u6ca1\u6cd5\u968f\u540c Pod \u4e00\u8d77\u88ab\u8c03\u5ea6\u6f02\u79fb\u3002 IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b \u53ea\u8981 Pod \u5b50\u7f51\u4e0d\u4e0e\u76f8\u5173\u7f51\u7edc\u91cd\u53e0\uff0c\u518d\u80fd\u591f\u5408\u7406\u5229\u7528 NAT \u6280\u672f\uff0cKubernetes \u5355\u4e2a\u96c6\u7fa4\u53ef\u4ee5\u62e5\u6709\u5145\u6c9b\u7684 IP \u5730\u5740\u8d44\u6e90\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0d\u4f1a\u56e0\u4e3a IP \u4e0d\u591f\u800c\u542f\u52a8\u5931\u8d25\uff0cIPAM \u7ec4\u4ef6\u9762\u4e34\u7684\u5f02\u5e38 IP \u56de\u6536\u538b\u529b\u8f83\u5c0f\u3002 \u6ca1\u6709\u5e94\u7528 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42 \u5bf9\u4e8e\u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6709\u65e0\u72b6\u6001\u5e94\u7528\u548c\u6709\u72b6\u6001\u5e94\u7528\u7684\u533a\u522b\uff1a\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c \u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\" \u7684\u9700\u6c42\uff0c\u53ea\u80fd\u8ba9\u6240\u6709 Pod \u526f\u672c\u56fa\u5b9a\u5728\u4e00\u4e2a IP \u5730\u5740\u7684\u96c6\u5408\u5185\uff1b\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod name \u7b49\u4fe1\u606f\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u6709\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\uff0c\u8981\u5b9e\u73b0\u5355\u4e2a Pod \u548c\u5177\u4f53 IP \u5730\u5740\u7684\u5f3a\u7ed1\u5b9a\u3002 \u5728 \"Overlay \u7f51\u7edc\u65b9\u6848\"\u65b9\u6848\u4e0b\uff0c\u591a\u662f\u501f\u52a9\u4e86 NAT \u6280\u672f\u5411\u96c6\u7fa4\u5916\u90e8\u66b4\u9732\u670d\u52a1\u7684\u5165\u53e3\u548c\u6e90\u5730\u5740\uff0c\u501f\u52a9 DNS\u3001clusterIP \u7b49\u6280\u672f\u6765\u5b9e\u73b0\u96c6\u7fa4\u4e1c\u897f\u5411\u901a\u4fe1\u3002 \u5176\u6b21\uff0cIPAM \u7684 IP block \u65b9\u5f0f\u628a IP \u76f8\u5bf9\u56fa\u5b9a\u5230\u67d0\u4e2a\u8282\u70b9\u4e0a\uff0c\u800c\u4e0d\u80fd\u4fdd\u8bc1\u5e94\u7528\u526f\u672c\u7684\u8ddf\u968f\u8c03\u5ea6\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u7684 \"IP \u5730\u5740\u56fa\u5b9a\"\u80fd\u529b\u65e0\u7528\u6b66\u4e4b\u5730\uff0c\u5f53\u524d\u793e\u533a\u7684\u4e3b\u6d41 CNI \u591a\u6570\u4e0d\u652f\u6301 \"IP \u5730\u5740\u56fa\u5b9a\"\uff0c\u6216\u8005\u652f\u6301\u65b9\u6cd5\u8f83\u4e3a\u7b80\u964b\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u70b9\u662f\uff0c\u65e0\u8bba\u96c6\u7fa4\u90e8\u7f72\u5728\u4ec0\u4e48\u6837\u7684\u5e95\u5c42\u7f51\u7edc\u73af\u5883\u4e0a\uff0cCNI \u63d2\u4ef6\u7684\u517c\u5bb9\u6027\u90fd\u975e\u5e38\u597d\uff0c\u4e14\u90fd\u80fd\u591f\u4e3a Pod \u63d0\u4f9b\u5b50\u7f51\u72ec\u7acb\u3001IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b\u7684\u7f51\u7edc\u3002","title":"Overlay \u7f51\u7edc\u65b9\u6848 IPAM"},{"location":"concepts/solution-zh_CN/#underlay-ipam","text":"\u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u5171\u4eab\u5bbf\u4e3b\u673a\u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5373 Pod \u76f4\u63a5\u83b7\u53d6\u5bbf\u4e3b\u673a\u7f51\u7edc\u4e2d\u7684 IP \u5730\u5740\u3002\u8fd9\u6837\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 Underlay \u7f51\u7edc\u65b9\u6848\u7684\u5b9e\u65bd\uff0c\u6709\u4e24\u79cd\u5178\u578b\u7684\u573a\u666f\uff1a\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5b9e\u65bd\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u4e0a\uff1b\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5728 IAAS \u73af\u5883\u4e0a\uff0c\u4f8b\u5982\u516c\u6709\u4e91\u3002\u4ee5\u4e0b\u603b\u7ed3\u4e86\"\u4f20\u7edf\u7f51\u7edc\u573a\u666f\"\u7684 IPAM \u7279\u70b9\uff1a \u5355\u4e2a IP \u5730\u5740\u5e94\u8be5\u80fd\u591f\u5728\u4efb\u4e00\u8282\u70b9\u4e0a\u88ab\u5206\u914d \u8fd9\u4e2a\u9700\u6c42\u6709\u591a\u65b9\u9762\u7684\u539f\u56e0\uff1a\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u8bbe\u5907\u589e\u52a0\u3001\u591a\u96c6\u7fa4\u6280\u672f\u7684\u53d1\u5c55\uff0cIPv4 \u5730\u5740\u8d44\u6e90\u7a00\u7f3a\uff0c\u8981\u6c42 IPAM \u63d0\u9ad8 IP \u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\uff1b \u5bf9\u4e8e\u6709 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5176 Pod \u526f\u672c\u53ef\u80fd\u4f1a\u8c03\u5ea6\u5230\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u8fd8\u4f1a\u53d1\u751f\u8282\u70b9\u95f4\u7684\u6f02\u79fb\uff0c\u8981\u6c42 IP \u5730\u5740\u4e00\u8d77\u6f02\u79fb\u3002 \u56e0\u6b64\uff0c\u5728\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4e00\u4e2a IP \u5730\u5740\u5e94\u8be5\u5177\u5907\u80fd\u591f\u88ab\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684\u53ef\u80fd\u3002 \u540c\u4e00\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\uff0c\u80fd\u5b9e\u73b0\u8de8\u5b50\u7f51\u83b7\u53d6 IP \u5730\u5740 \u4f8b\u5982\uff0c\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u5bbf\u4e3b\u673a1\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.1.0/24\uff0c\u800c\u5bbf\u4e3b\u673a2\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.2.0/24\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0c \u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\u3002 \u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a \u5f88\u591a\u4f20\u7edf\u5e94\u7528\u5728\u4e91\u5316\u6539\u9020\u524d\uff0c\u662f\u90e8\u7f72\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u7684\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u7f51\u7edc\u672a\u5f15\u5165 NAT \u5730\u5740\u8f6c\u6362\uff0c\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u9700\u8981\u611f\u77e5\u5bf9\u65b9\u7684\u6e90 IP \u6216\u76ee\u7684 IP\uff0c \u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u4e5f\u4e60\u60ef\u4e86\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u6765\u7cbe\u7ec6\u7ba1\u63a7\u7f51\u7edc\u5b89\u5168\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0a\u4e91\u540e\uff0c\u65e0\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u8303\u56f4\u7684\u56fa\u5b9a\uff0c\u6709\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u5730\u5740\u7684\u552f\u4e00\u5bf9\u5e94\uff0c\u8fd9\u6837\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u6539\u9020\u5de5\u4f5c\u3002 \u4e00\u4e2a Pod \u7684\u591a\u7f51\u5361\u83b7\u53d6\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740 \u65e2\u7136\u662f\u5bf9\u63a5 Underlay \u7f51\u7edc\uff0cPod \u5c31\u4f1a\u6709\u591a\u7f51\u5361\u9700\u6c42\uff0c\u4ee5\u4f7f\u5176\u901a\u8fbe\u4e0d\u540c\u7684 Underlay \u5b50\u7f51\uff0c\u8fd9\u8981\u6c42 IPAM \u80fd\u591f\u7ed9\u5e94\u7528\u7684\u4e0d\u540c\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u4e0b\u7684 IP \u5730\u5740\u3002 IP \u5730\u5740\u51b2\u7a81 \u5728 Underlay \u7f51\u7edc\u4e2d\uff0c\u66f4\u52a0\u5bb9\u6613\u51fa\u73b0 IP \u51b2\u7a81\uff0c\u4f8b\u5982\uff0cPod \u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u4e3b\u673a IP \u53d1\u751f\u4e86\u51b2\u7a81\uff0c\u4e0e\u5176\u5b83\u5bf9\u63a5\u4e86\u76f8\u540c\u5b50\u7f51\u7684\u96c6\u7fa4\u51b2\u7a81\uff0c \u800c IPAM \u7ec4\u4ef6\u5f88\u96be\u611f\u77e5\u5916\u90e8\u8fd9\u4e9b\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u591a\u9700\u8981\u501f\u52a9 CNI \u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3002 \u5df2\u7528 IP \u5730\u5740\u7684\u91ca\u653e\u56de\u6536 \u56e0\u4e3a Underlay \u7f51\u7edc IP \u5730\u5740\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5e94\u7528\u6709 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6240\u4ee5\uff0c\"\u5e94\u5f53\"\u88ab\u91ca\u653e\u7684 IP \u5730\u5740\u82e5\u672a\u88ab IPAM \u7ec4\u4ef6\u56de\u6536\uff0c\u65b0\u542f\u52a8\u7684 Pod \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7f3a\u5c11 IP \u5730\u5740\u800c\u5931\u8d25\u3002 \u8fd9\u5c31\u8981\u6c42 IPAM \u7ec4\u4ef6\u62e5\u6709\u66f4\u52a0\u7cbe\u51c6\u3001\u9ad8\u6548\u3001\u53ca\u65f6\u7684 IP \u56de\u6536\u673a\u5236\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u52bf\u6709\uff1a\u65e0\u9700\u7f51\u7edc NAT \u6620\u5c04\u7684\u5f15\u5165\uff0c\u5bf9\u5e94\u7528\u7684\u4e91\u5316\u7f51\u7edc\u6539\u9020\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u7684\u4fbf\u5229\uff1b\u5e95\u5c42\u7f51\u7edc\u7684\u706b\u5899\u7b49\u8bbe\u5907\uff0c\u53ef\u5bf9 Pod \u901a\u4fe1\u5b9e\u73b0\u76f8\u5bf9\u8f83\u4e3a\u7cbe\u7ec6\u7684\u7ba1\u63a7\uff1b\u65e0\u9700\u96a7\u9053\u6280\u672f\uff0c \u7f51\u7edc\u901a\u4fe1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u65f6\u6027\u80fd\u4e5f\u76f8\u5bf9\u7684\u63d0\u9ad8\u4e86\u3002","title":"Underlay \u7f51\u7edc\u65b9\u6848 IPAM"},{"location":"concepts/solution-zh_CN/#overlay-cni-underlay-cni","text":"Overlay CNI \u9700\u8981\u501f\u52a9\u96a7\u9053\u5c01\u88c5\u548c\u5bbf\u4e3b\u673a\u8f6c\u53d1\u5b9e\u73b0\u8de8\u4e3b\u673a\u7684 Pod \u901a\u4fe1\uff0c\u800c Underlay CNI \u53ef\u5b9e\u73b0\u76f4\u63a5\u5bf9\u63a5\u5bbf\u4e3b\u673a\u7f51\u7edc\uff0c\u56e0\u6b64\u5728\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u8868\u73b0\u4e0a\u6709\u5dee\u5f02\u3002 \u5728\u4e00\u4e2a\u5177\u5907 2 \u4e2a\u88f8\u91d1\u5c5e\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5b89\u88c5\u4e00\u4e2a 25G mellanox CX5 \u7f51\u5361\uff0c\u5206\u522b\u6d4b\u8bd5\u5982\u4e0b\uff1a 2 \u4e2a\u88f8\u91d1\u5c5e\u8282\u70b9\u95f4\u7684\u7f51\u5361\u7684\u6027\u80fd \u57fa\u4e8e macvlan \u7684\u8de8\u8282\u70b9 Pod \u7f51\u7edc\u6027\u80fd \u57fa\u4e8e SR-IOV \u7684\u8de8\u8282\u70b9 Pod \u7f51\u7edc\u6027\u80fd \u57fa\u4e8e calico(v3.23) vxlan \u7684\u8de8\u8282\u70b9 Pod \u7684\u7f51\u7edc\u6027\u80fd \u57fa\u4e8e cilium(v1.13) vxlan \u7684\u8de8\u8282\u70b9 Pod \u7684\u7f51\u7edc\u6027\u80fd \u6700\u7ec8\u7684\u6d4b\u8bd5\u62a5\u544a\u5982\u4e0a\u6240\u793a\uff0c\u8bf8\u5982 macvlan \u548c SRIOV \u7684 Underlay CNI \u6027\u80fd\u8f83\u597d\u3002","title":"Overlay CNI \u548c Underlay CNI \u7684\u6027\u80fd"},{"location":"concepts/solution/","text":"Underlay and overlay network solutions There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool. IPAM for overlay networks These solutions implement the decoupling of the Pod network and host network, such as Calico , Cilium and other CNI plugins. Typically, they use tunnel technology such as vxlan to build an overlay network plane, and use NAT technology for north-south traffic. These IPAM solutions have the following characteristics: Divide Pod subnets into node-based IP blocks In terms of a smaller subnet mask, the Pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account. First, since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, achieving more efficient allocation. Second, a specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound Pod. Sufficient IP address resources Subnets not overlapping with any CIDR, could be used by the cluster, so the cluster has enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP addresses. No requirement for static IP addresses For the static IP address requirement, there is a difference between a stateless application and a stateful application. Regarding stateless application like deployment, the Pod's name will change when the Pod restarts, and the business logic of the application itself is stateless. Thus static IP addresses means that all the Pod replicas are fixed in a set of IP addresses; for stateful applications such as statefulset, considering both the fixed information including Pod's names and stateful business logic, the strong binding of one Pod and one specific IP address needs to be implemented for static IP addresses. The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology, and realizes the east-west communication through DNS, clusterIP and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas follow the scheduling. Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addressed\", or support it in a rough way. The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment, and can provide independent subnets with sufficient IP addresses for Pods. These solutions share the node's network for Pods, which means Pods can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications. There are two typical scenarios for underlay network solutions: clusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\": An IP address able to be assigned to any node As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the Pod replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together. Therefore, an IP address should be able to be allocated to a Pod on any node. Different replicas within one application could obtain IP addresses across subnets Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need to be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes. Static IP addresses For some traditional applications, the source IPs or destination IPs need to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means. Therefore, in order to reduce the transformation chores after the applications move to the Kubernetes, applications need static IP addresses. Pods with Multiple NICs need IP addresses of different underlay subnets Since the Pod is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets. IP conflict Underlay networks are more prone to IP conflicts. For instance, Pods conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection. Release and recover IP addresses Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched Pod may fail due to the lack of IP addresses owing to some IP addresses not being released by abnormal Pods. This requires IPAMs to have a more accurate, efficient and timely IP recovery mechanism. The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of Pod communication; no tunneling technology contributes to improved throughput and latency performance of network communications. Performance comparison between overlay CNIs and underlay CNIs Overlay CNIs rely on tunnel encapsulation and host forwarding to achieve Pod communication across nodes, while underlay CNIs can directly access the host network without tunneling overhead. This results in differences in network latency and throughput performance. To compare the performance of different CNI solutions, tests were conducted on a cluster with two bare-metal nodes equipped with 25G Mellanox CX5 NICs: Network interface performance between the two bare-metal nodes Cross-node Pod network performance based on Macvlan Cross-node Pod network performance based on SR-IOV Cross-node Pod network performance based on Calico(v3.23) vxlan Cross-node Pod network performance based on Cilium(v1.13) vxlan As shown in the above figure, the test results demonstrate that underlay CNIs like Macvlan and SR-IOV outperform overlay solutions.","title":"Underlay and overlay solutions"},{"location":"concepts/solution/#underlay-and-overlay-network-solutions","text":"There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool.","title":"Underlay and overlay network solutions"},{"location":"concepts/solution/#ipam-for-overlay-networks","text":"These solutions implement the decoupling of the Pod network and host network, such as Calico , Cilium and other CNI plugins. Typically, they use tunnel technology such as vxlan to build an overlay network plane, and use NAT technology for north-south traffic. These IPAM solutions have the following characteristics: Divide Pod subnets into node-based IP blocks In terms of a smaller subnet mask, the Pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account. First, since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, achieving more efficient allocation. Second, a specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound Pod. Sufficient IP address resources Subnets not overlapping with any CIDR, could be used by the cluster, so the cluster has enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP addresses. No requirement for static IP addresses For the static IP address requirement, there is a difference between a stateless application and a stateful application. Regarding stateless application like deployment, the Pod's name will change when the Pod restarts, and the business logic of the application itself is stateless. Thus static IP addresses means that all the Pod replicas are fixed in a set of IP addresses; for stateful applications such as statefulset, considering both the fixed information including Pod's names and stateful business logic, the strong binding of one Pod and one specific IP address needs to be implemented for static IP addresses. The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology, and realizes the east-west communication through DNS, clusterIP and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas follow the scheduling. Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addressed\", or support it in a rough way. The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment, and can provide independent subnets with sufficient IP addresses for Pods. These solutions share the node's network for Pods, which means Pods can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications. There are two typical scenarios for underlay network solutions: clusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\": An IP address able to be assigned to any node As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the Pod replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together. Therefore, an IP address should be able to be allocated to a Pod on any node. Different replicas within one application could obtain IP addresses across subnets Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need to be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes. Static IP addresses For some traditional applications, the source IPs or destination IPs need to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means. Therefore, in order to reduce the transformation chores after the applications move to the Kubernetes, applications need static IP addresses. Pods with Multiple NICs need IP addresses of different underlay subnets Since the Pod is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets. IP conflict Underlay networks are more prone to IP conflicts. For instance, Pods conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection. Release and recover IP addresses Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched Pod may fail due to the lack of IP addresses owing to some IP addresses not being released by abnormal Pods. This requires IPAMs to have a more accurate, efficient and timely IP recovery mechanism. The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of Pod communication; no tunneling technology contributes to improved throughput and latency performance of network communications.","title":"IPAM for overlay networks"},{"location":"concepts/solution/#performance-comparison-between-overlay-cnis-and-underlay-cnis","text":"Overlay CNIs rely on tunnel encapsulation and host forwarding to achieve Pod communication across nodes, while underlay CNIs can directly access the host network without tunneling overhead. This results in differences in network latency and throughput performance. To compare the performance of different CNI solutions, tests were conducted on a cluster with two bare-metal nodes equipped with 25G Mellanox CX5 NICs: Network interface performance between the two bare-metal nodes Cross-node Pod network performance based on Macvlan Cross-node Pod network performance based on SR-IOV Cross-node Pod network performance based on Calico(v3.23) vxlan Cross-node Pod network performance based on Cilium(v1.13) vxlan As shown in the above figure, the test results demonstrate that underlay CNIs like Macvlan and SR-IOV outperform overlay solutions.","title":"Performance comparison between overlay CNIs and underlay CNIs"},{"location":"develop/CODE-OF-CONDUCT/","text":"Code of Conduct This project follows the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"develop/CODE-OF-CONDUCT/#code-of-conduct","text":"This project follows the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"develop/contributing/","text":"contributing unitest run the following command to check unitest make unitest-tests setup cluster and run test check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow before start the test, you shoud know there are test scenes as following kind setup cluster test test underlay CNI make e2e_init_underlay make e2e_test_underlay test overlay CNI for calico make e2e_init_overlay_calico make e2e_test_overlay_calico test overlay CNI for cilium make e2e_init_overlay_cilium make e2e_test_overlay_cilium if you are in China, it could add -e E2E_CHINA_IMAGE_REGISTRY=true to pull images from china image registry, add -e HTTP_PROXY=http://${ADDR} to get chart build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init_underlay ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv6 # for china developer not able access ghcr.io # it pulls images from another image registry and just use http proxy to pull chart $ make e2e_init_underlay -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with calico cni $ make e2e_init_calico -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with cilium cni $ make e2e_init_cilium -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test_underlay # run all e2e test on ipv4-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test_underlay -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test_underlay -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test_underlay -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_calico # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_cilium $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${MULTUS_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init_underlay -e E2E_SPIDERPOOL_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} \\ -e E2E_MULTUS_IMAGE_NAME=${MULTUS_IMAGE_NAME} # run all e2e test $ make e2e_test finally, you could visit \" http://HostIp:4040 \" the in the browser of your desktop, and get flamegraph clean make clean_e2e Submit Pull Request A pull request will be checked by following workflow, which is required for merging. Action: your PR should be signed off When you commit your modification, add -s in your commit command git commit -s Action: check yaml files If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml . Action: check golang source code It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label Action: check licenses Any golang or shell file should be licensed correctly. Action: check markdown file Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling . Action: lint yaml file If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml . Action: lint chart Action: lint openapi.yaml Action: check code spell Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case. Changelog How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Contribution Guide"},{"location":"develop/contributing/#contributing","text":"","title":"contributing"},{"location":"develop/contributing/#unitest","text":"run the following command to check unitest make unitest-tests","title":"unitest"},{"location":"develop/contributing/#setup-cluster-and-run-test","text":"check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow before start the test, you shoud know there are test scenes as following kind setup cluster test test underlay CNI make e2e_init_underlay make e2e_test_underlay test overlay CNI for calico make e2e_init_overlay_calico make e2e_test_overlay_calico test overlay CNI for cilium make e2e_init_overlay_cilium make e2e_test_overlay_cilium if you are in China, it could add -e E2E_CHINA_IMAGE_REGISTRY=true to pull images from china image registry, add -e HTTP_PROXY=http://${ADDR} to get chart build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init_underlay ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv6 # for china developer not able access ghcr.io # it pulls images from another image registry and just use http proxy to pull chart $ make e2e_init_underlay -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with calico cni $ make e2e_init_calico -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with cilium cni $ make e2e_init_cilium -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test_underlay # run all e2e test on ipv4-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test_underlay -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test_underlay -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test_underlay -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_calico # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_cilium $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${MULTUS_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init_underlay -e E2E_SPIDERPOOL_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} \\ -e E2E_MULTUS_IMAGE_NAME=${MULTUS_IMAGE_NAME} # run all e2e test $ make e2e_test finally, you could visit \" http://HostIp:4040 \" the in the browser of your desktop, and get flamegraph clean make clean_e2e","title":"setup cluster and run test"},{"location":"develop/contributing/#submit-pull-request","text":"A pull request will be checked by following workflow, which is required for merging.","title":"Submit Pull Request"},{"location":"develop/contributing/#action-your-pr-should-be-signed-off","text":"When you commit your modification, add -s in your commit command git commit -s","title":"Action: your PR should be signed off"},{"location":"develop/contributing/#action-check-yaml-files","text":"If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml .","title":"Action: check yaml files"},{"location":"develop/contributing/#action-check-golang-source-code","text":"It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label","title":"Action: check golang source code"},{"location":"develop/contributing/#action-check-licenses","text":"Any golang or shell file should be licensed correctly.","title":"Action: check licenses"},{"location":"develop/contributing/#action-check-markdown-file","text":"Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling .","title":"Action: check markdown file"},{"location":"develop/contributing/#action-lint-yaml-file","text":"If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml .","title":"Action: lint yaml file"},{"location":"develop/contributing/#action-lint-chart","text":"","title":"Action: lint chart"},{"location":"develop/contributing/#action-lint-openapiyaml","text":"","title":"Action: lint openapi.yaml"},{"location":"develop/contributing/#action-check-code-spell","text":"Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Action: check code spell"},{"location":"develop/contributing/#changelog","text":"How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Changelog"},{"location":"develop/release/","text":"workflow for release pre-steps update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml' update version in '/VERSION' a version tag should be set on right branch. The version should go with v0.1.0-rc0 v0.1.0-rc1 v0.1.0 v0.1.1 v0.1.2 v0.2.0-rc0 v0.2.0 update roadmap push a version tag If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"pr/release/feature-new\" to be classified to \"New Features\" label \"pr/release/feature-changed\" to be classified to \"Changed Features\" label \"pr/release/feature-bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml post Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"Release workflow"},{"location":"develop/release/#workflow-for-release","text":"","title":"workflow for release"},{"location":"develop/release/#pre-steps","text":"update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml' update version in '/VERSION' a version tag should be set on right branch. The version should go with v0.1.0-rc0 v0.1.0-rc1 v0.1.0 v0.1.1 v0.1.2 v0.2.0-rc0 v0.2.0 update roadmap","title":"pre-steps"},{"location":"develop/release/#push-a-version-tag","text":"If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"pr/release/feature-new\" to be classified to \"New Features\" label \"pr/release/feature-changed\" to be classified to \"Changed Features\" label \"pr/release/feature-bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml","title":"push a version tag"},{"location":"develop/release/#post","text":"Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"post"},{"location":"develop/roadmap/","text":"roadmap feature description status ippool ip settings done in v0.2.0 namesapce affinity done in v0.4.0 application affinity done in v0.4.0 multiple default ippool done in v0.6.0 multusname done in v0.6.0 nodename done in v0.6.0 default cluster ippool done in v0.2.0 default namespace ippool done in v0.4.0 default CNI ippool done in v0.4.0 annotation ippool done in v0.2.0 annotation route done in v0.2.0 subnet automatically create ippool done in v0.4.0 automatically scaling and deletion ip according to application done in v0.4.0 automatically delete ippool done in v0.5.0 support annotation for multiple interface done in v0.4.0 keep ippool after deleting application done in v0.5.0 support deployment, statefulset, job, replicaset done in v0.4.0 support operator controller done in v0.4.0 flexible ip number done in v0.5.0 ippool inherit route and gateway attribute from its subnet done in v0.6.0 reservedIP reservedIP done in v0.4.0 fixed ip fixed ip for each pod of statefulset done in v0.5.0 fixed ip ranges for statefulset, deployment, replicaset done in v0.4.0 spidermultusconfig support macvlan ipvlan sriov custom done in v0.6.0 ipam plugin cni v1.0.0 done in v0.4.0 ifacer plugin bond interface done in v0.6.0 vlan interface done in v0.6.0 coordinator plugin support underlay and overlay mode done in v0.6.0 CRD spidercoordinators for configuration done in v0.6.0 tune policy route for overlay and underlay mode done in v0.6.0 detect ip conflict and gateway done in v0.6.0 specify the MAC of pod done in v0.6.0 specify the default route of pod interface done in v0.6.0 ovs/macvlan/sriov/ipvlan visit clusterIP done in v0.6.0 visit local node to guarantee the pod health check done in v0.6.0 visit nodePort with spec.externalTrafficPolicy=local or spec.externalTrafficPolicy=cluster done in v0.6.0 calico/weave fixed ip done in v0.5.0 recycle IP recycle IP taken by deleted pod done in v0.4.0 recycle IP taken by deleting pod done in v0.4.0 dual-stack dual-stack done in v0.2.0 CLI debug and operate. check which pod an IP is taken by, check IP usage , trigger GC in plan multi-cluster (1) spiderpool could synchronize ippool resource within a same subnet from other cluster, so it could help avoid IP conflict (2)leader cluster could synchronize all Spiderpool resource from member clusters, which help manager all underlay IP address in plan cilium cooperate with cilium in plan RDMA RDMA in plan egressGateway egressGateway in plan","title":"Roadmap"},{"location":"develop/roadmap/#roadmap","text":"feature description status ippool ip settings done in v0.2.0 namesapce affinity done in v0.4.0 application affinity done in v0.4.0 multiple default ippool done in v0.6.0 multusname done in v0.6.0 nodename done in v0.6.0 default cluster ippool done in v0.2.0 default namespace ippool done in v0.4.0 default CNI ippool done in v0.4.0 annotation ippool done in v0.2.0 annotation route done in v0.2.0 subnet automatically create ippool done in v0.4.0 automatically scaling and deletion ip according to application done in v0.4.0 automatically delete ippool done in v0.5.0 support annotation for multiple interface done in v0.4.0 keep ippool after deleting application done in v0.5.0 support deployment, statefulset, job, replicaset done in v0.4.0 support operator controller done in v0.4.0 flexible ip number done in v0.5.0 ippool inherit route and gateway attribute from its subnet done in v0.6.0 reservedIP reservedIP done in v0.4.0 fixed ip fixed ip for each pod of statefulset done in v0.5.0 fixed ip ranges for statefulset, deployment, replicaset done in v0.4.0 spidermultusconfig support macvlan ipvlan sriov custom done in v0.6.0 ipam plugin cni v1.0.0 done in v0.4.0 ifacer plugin bond interface done in v0.6.0 vlan interface done in v0.6.0 coordinator plugin support underlay and overlay mode done in v0.6.0 CRD spidercoordinators for configuration done in v0.6.0 tune policy route for overlay and underlay mode done in v0.6.0 detect ip conflict and gateway done in v0.6.0 specify the MAC of pod done in v0.6.0 specify the default route of pod interface done in v0.6.0 ovs/macvlan/sriov/ipvlan visit clusterIP done in v0.6.0 visit local node to guarantee the pod health check done in v0.6.0 visit nodePort with spec.externalTrafficPolicy=local or spec.externalTrafficPolicy=cluster done in v0.6.0 calico/weave fixed ip done in v0.5.0 recycle IP recycle IP taken by deleted pod done in v0.4.0 recycle IP taken by deleting pod done in v0.4.0 dual-stack dual-stack done in v0.2.0 CLI debug and operate. check which pod an IP is taken by, check IP usage , trigger GC in plan multi-cluster (1) spiderpool could synchronize ippool resource within a same subnet from other cluster, so it could help avoid IP conflict (2)leader cluster could synchronize all Spiderpool resource from member clusters, which help manager all underlay IP address in plan cilium cooperate with cilium in plan RDMA RDMA in plan egressGateway egressGateway in plan","title":"roadmap"},{"location":"develop/swagger_openapi/","text":"SWAGGER OPENAPI Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes. Features Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs. Usages There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR . validate spec Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec generate source codes with the given spec To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen verify the spec with current source codes to make sure whether the current source codes is out of date To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify clean the generated source codes To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code Use swagger-ui To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web. Steps For Developers Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Swagger OpenAPI"},{"location":"develop/swagger_openapi/#swagger-openapi","text":"Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#features","text":"Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs.","title":"Features"},{"location":"develop/swagger_openapi/#usages","text":"There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR .","title":"Usages"},{"location":"develop/swagger_openapi/#validate-spec","text":"Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec","title":"validate spec"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","text":"To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen","title":"generate source codes with the given spec"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","text":"To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify","title":"verify the spec with current source codes to make sure whether the current source codes is out of date"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","text":"To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code","title":"clean the generated source codes"},{"location":"develop/swagger_openapi/#use-swagger-ui","text":"To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.","title":"Use swagger-ui"},{"location":"develop/swagger_openapi/#steps-for-developers","text":"Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Steps For Developers"},{"location":"reference/_example/","text":"CRD \u57fa\u672c\u63cf\u8ff0 \u672cCRD \u662f\u505a\u4ec0\u4e48\u7684 \u914d\u7f6e\u8bf4\u660e \u8868\u683c\uff08\u5b57\u6bb5\u3001\u63cf\u8ff0\u3001\u7f3a\u7701\u503c\uff09\uff0c\u5305\u62ec\u4e86 status \u7684\u4fe1\u606f\u8bf4\u660e \u4f7f\u7528\u4f8b\u5b50 \u7ed9\u51fa\u4e00\u4e9b\u573a\u666f\u573a\u666f\u4e0b\u7684 CR yaml","title":"CRD"},{"location":"reference/_example/#crd","text":"","title":"CRD"},{"location":"reference/_example/#_1","text":"\u672cCRD \u662f\u505a\u4ec0\u4e48\u7684","title":"\u57fa\u672c\u63cf\u8ff0"},{"location":"reference/_example/#_2","text":"\u8868\u683c\uff08\u5b57\u6bb5\u3001\u63cf\u8ff0\u3001\u7f3a\u7701\u503c\uff09\uff0c\u5305\u62ec\u4e86 status \u7684\u4fe1\u606f\u8bf4\u660e","title":"\u914d\u7f6e\u8bf4\u660e"},{"location":"reference/_example/#_3","text":"\u7ed9\u51fa\u4e00\u4e9b\u573a\u666f\u573a\u666f\u4e0b\u7684 CR yaml","title":"\u4f7f\u7528\u4f8b\u5b50"},{"location":"reference/annotation/","text":"Annotations Spiderpool provides annotations for configuring custom IPPools and routes. Pod annotations After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations. ipam.spidernet.io/subnet Specify the Subnets used to generate IPPools and allocate IP addresses. ipam.spidernet.io/subnet : |- { \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] } ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/subnets ipam.spidernet.io/subnets : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-subnet2\"], \"ipv6\": [\"demo-v6-subnet2\"] }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/ippool-ip-number This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1'). ipam.spidernet.io/ippool-ip-number : +1 ipam.spidernet.io/ippool-reclaim This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true'). ipam.spidernet.io/ippool-reclaim : true ipam.spidernet.io/ippool Specify the IPPools used to allocate IP addresses. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"] } ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/ippools It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. cleangateway (bool, optional): If set to true, the IPAM plugin will not return the default route (generated by spec.gateway ) recorded in the IPPool. ipam.spidernet.io/routes You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address. Namespace annotations A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace. ipam.spidernet.io/default-ipv4-ippool ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'","title":"Annotations"},{"location":"reference/annotation/#annotations","text":"Spiderpool provides annotations for configuring custom IPPools and routes.","title":"Annotations"},{"location":"reference/annotation/#pod-annotations","text":"After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations.","title":"Pod annotations"},{"location":"reference/annotation/#ipamspidernetiosubnet","text":"Specify the Subnets used to generate IPPools and allocate IP addresses. ipam.spidernet.io/subnet : |- { \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] } ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/subnet"},{"location":"reference/annotation/#ipamspidernetiosubnets","text":"ipam.spidernet.io/subnets : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-subnet2\"], \"ipv6\": [\"demo-v6-subnet2\"] }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/subnets"},{"location":"reference/annotation/#ipamspidernetioippool-ip-number","text":"This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1'). ipam.spidernet.io/ippool-ip-number : +1","title":"ipam.spidernet.io/ippool-ip-number"},{"location":"reference/annotation/#ipamspidernetioippool-reclaim","text":"This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true'). ipam.spidernet.io/ippool-reclaim : true","title":"ipam.spidernet.io/ippool-reclaim"},{"location":"reference/annotation/#ipamspidernetioippool","text":"Specify the IPPools used to allocate IP addresses. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"] } ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/ippool"},{"location":"reference/annotation/#ipamspidernetioippools","text":"It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. cleangateway (bool, optional): If set to true, the IPAM plugin will not return the default route (generated by spec.gateway ) recorded in the IPPool.","title":"ipam.spidernet.io/ippools"},{"location":"reference/annotation/#ipamspidernetioroutes","text":"You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address.","title":"ipam.spidernet.io/routes"},{"location":"reference/annotation/#namespace-annotations","text":"A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace.","title":"Namespace annotations"},{"location":"reference/annotation/#ipamspidernetiodefault-ipv4-ippool","text":"ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]'","title":"ipam.spidernet.io/default-ipv4-ippool"},{"location":"reference/annotation/#ipamspidernetiodefault-ipv6-ippool","text":"ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'","title":"ipam.spidernet.io/default-ipv6-ippool"},{"location":"reference/configmap/","text":"Configuration Instructions for global configuration and environment arguments of Spiderpool. Configmap Configuration Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : kube-system data : conf.yml : | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock enableIPv4: true enableIPv6: true enableStatefulSet: true enableSpiderSubnet: true clusterSubnetDefaultFlexibleIPNumber: 1 ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet capability of Spiderpool. false : Disable StatefulSet capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster.","title":"Configmap"},{"location":"reference/configmap/#configuration","text":"Instructions for global configuration and environment arguments of Spiderpool.","title":"Configuration"},{"location":"reference/configmap/#configmap-configuration","text":"Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : kube-system data : conf.yml : | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock enableIPv4: true enableIPv6: true enableStatefulSet: true enableSpiderSubnet: true clusterSubnetDefaultFlexibleIPNumber: 1 ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet capability of Spiderpool. false : Disable StatefulSet capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster.","title":"Configmap Configuration"},{"location":"reference/crd-spidercoordinator/","text":"Spidercoordinator A Spidercoordinator resource represents the global default configuration of the cni meta-plugin: coordinator. There is only one instance of this resource, which is automatically generated while you install Spiderpool and does not need to be created manually. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderCoordinator metadata : name : default spec : detectGateway : false detectIPConflict : false hostRPFilter : 0 hostRuleTable : 500 mode : underlay podCIDRType : cluster podDefaultRouteNIC : eth0 podMACPrefix : \"\" tunePodRoutes : true status : overlayPodCIDR : - 10.233.64.0/18 - fd85:ee78:d8a6:8607::1:0000/112 phase : Synced serviceCIDR : - 10.233.0.0/18 - fd85:ee78:d8a6:8607::1000/116 Spidercoordinators definition Metadata Field Description Schema Validation name The name of this Spidercoordinators resource string required Spec This is the Spidercoordinators spec for users to configure. Field Description Schema Validation Values Default mode The mode in which the coordinator. auto: automatically determine if it's overlay or underlay. underlay: coordinator creates veth devices to solve the problem that CNIs such as macvlan cannot communicate with clusterIP. overlay: fix the problem that CNIs such as Macvlan cannot access ClusterIP through the Calico network card attached to the pod,coordinate policy route between interfaces to ensure consistence data path of request and reply packets string require auto,underlay,overlay auto podCIDRType The ways to fetch the CIDR of the cluster string require cluster,calico,cilium,none cluster tunePodRoutes tune pod's route while the pod is attached to multiple NICs bool optional true,false true podDefaultRouteNIC The NIC where the pod's default route resides string optional \"\",eth0,net1... underlay: eth0,overlay: net1 detectGateway enable detect gateway while launching pod, If the gateway is unreachable, pod will be failed to created; Note: We use ARP probes to detect if the gateway is reachable, and some gateway routers may warn about this boolean optional true,false false detectIPConflict enable the pod's ip if is conflicting while launching pod. If an IP conflict of the pod is detected, pod will be failed to created boolean optional true,false false podMACPrefix fix the pod's mac address with this prefix + 4 bytes IP string optional a invalid mac address prefix \"\" hostRPFilter sysctls: rp_filter in host int required 0,1,2;suggest to be 0 0 hostRuleTable The directly routing table of the host accessing the pod's underlay IP will be placed in this policy routing table int required int 500 Status (subresource) The Spidercoordinators status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation overlayPodCIDR the cluster pod cidr []string required serviceCIDR the cluster service cidr []string required phase Represents the status of synchronization string required","title":"CRD Spidercoordinator"},{"location":"reference/crd-spidercoordinator/#spidercoordinator","text":"A Spidercoordinator resource represents the global default configuration of the cni meta-plugin: coordinator. There is only one instance of this resource, which is automatically generated while you install Spiderpool and does not need to be created manually.","title":"Spidercoordinator"},{"location":"reference/crd-spidercoordinator/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderCoordinator metadata : name : default spec : detectGateway : false detectIPConflict : false hostRPFilter : 0 hostRuleTable : 500 mode : underlay podCIDRType : cluster podDefaultRouteNIC : eth0 podMACPrefix : \"\" tunePodRoutes : true status : overlayPodCIDR : - 10.233.64.0/18 - fd85:ee78:d8a6:8607::1:0000/112 phase : Synced serviceCIDR : - 10.233.0.0/18 - fd85:ee78:d8a6:8607::1000/116","title":"Sample YAML"},{"location":"reference/crd-spidercoordinator/#spidercoordinators-definition","text":"","title":"Spidercoordinators definition"},{"location":"reference/crd-spidercoordinator/#metadata","text":"Field Description Schema Validation name The name of this Spidercoordinators resource string required","title":"Metadata"},{"location":"reference/crd-spidercoordinator/#spec","text":"This is the Spidercoordinators spec for users to configure. Field Description Schema Validation Values Default mode The mode in which the coordinator. auto: automatically determine if it's overlay or underlay. underlay: coordinator creates veth devices to solve the problem that CNIs such as macvlan cannot communicate with clusterIP. overlay: fix the problem that CNIs such as Macvlan cannot access ClusterIP through the Calico network card attached to the pod,coordinate policy route between interfaces to ensure consistence data path of request and reply packets string require auto,underlay,overlay auto podCIDRType The ways to fetch the CIDR of the cluster string require cluster,calico,cilium,none cluster tunePodRoutes tune pod's route while the pod is attached to multiple NICs bool optional true,false true podDefaultRouteNIC The NIC where the pod's default route resides string optional \"\",eth0,net1... underlay: eth0,overlay: net1 detectGateway enable detect gateway while launching pod, If the gateway is unreachable, pod will be failed to created; Note: We use ARP probes to detect if the gateway is reachable, and some gateway routers may warn about this boolean optional true,false false detectIPConflict enable the pod's ip if is conflicting while launching pod. If an IP conflict of the pod is detected, pod will be failed to created boolean optional true,false false podMACPrefix fix the pod's mac address with this prefix + 4 bytes IP string optional a invalid mac address prefix \"\" hostRPFilter sysctls: rp_filter in host int required 0,1,2;suggest to be 0 0 hostRuleTable The directly routing table of the host accessing the pod's underlay IP will be placed in this policy routing table int required int 500","title":"Spec"},{"location":"reference/crd-spidercoordinator/#status-subresource","text":"The Spidercoordinators status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation overlayPodCIDR the cluster pod cidr []string required serviceCIDR the cluster service cidr []string required phase Represents the status of synchronization string required","title":"Status (subresource)"},{"location":"reference/crd-spiderendpoint/","text":"SpiderEndpoint A SpiderEndpoint resource represents IP address allocation details for the corresponding pod. This resource one to one pod, and it will inherit the pod name and pod namespace. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderEndpoint metadata : name : test-app-1-9dc78fb9-rs99d status : current : ips : - cleanGateway : false interface : eth0 ipv4 : 172.31.199.193/20 ipv4Gateway : 172.31.207.253 ipv4Pool : worker-172 vlan : 0 node : dc-test02 uid : e7b50a38-25c2-41d0-b332-7f619c69194e ownerControllerName : test-app-1 ownerControllerType : Deployment SpiderEndpoint definition Metadata Field Description Schema Validation name the name of this SpiderEndpoint resource string required namespace the namespace of this SpiderEndpoint resource string required Status (subresource) The IPPool status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation current the IP allocation details of the corresponding pod PodIPAllocation required ownerControllerType the corresponding pod top owner controller type string required ownerControllerName the corresponding pod top owner controller name string required PodIPAllocation This property describes the SpiderEndpoint corresponding pod details. Field Description Schema Validation uid corresponding pod uid string required node total IP counts of this pool to use string required ips current allocated IP counts list of IPAllocationDetail required IPAllocationDetail This property describes single Interface allocation details. Field Description Schema Validation Default interface single interface name string required ipv4 single IPv4 allocated IP address string optional ipv6 single IPv6 allocated IP address string optional ipv4Pool the IPv4 allocated IP address corresponding pool string optional ipv6Pool the IPv6 allocated IP address corresponding pool string optional vlan vlan ID int optional 0 ipv4Gateway the IPv4 gateway IP address string optional ipv6Gateway the IPv6 gateway IP address string optional cleanGateway a flag to choose whether need default route by the gateway boolean optional routes the allocation routes list if Route optional","title":"CRD SpiderEndpoint"},{"location":"reference/crd-spiderendpoint/#spiderendpoint","text":"A SpiderEndpoint resource represents IP address allocation details for the corresponding pod. This resource one to one pod, and it will inherit the pod name and pod namespace.","title":"SpiderEndpoint"},{"location":"reference/crd-spiderendpoint/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderEndpoint metadata : name : test-app-1-9dc78fb9-rs99d status : current : ips : - cleanGateway : false interface : eth0 ipv4 : 172.31.199.193/20 ipv4Gateway : 172.31.207.253 ipv4Pool : worker-172 vlan : 0 node : dc-test02 uid : e7b50a38-25c2-41d0-b332-7f619c69194e ownerControllerName : test-app-1 ownerControllerType : Deployment","title":"Sample YAML"},{"location":"reference/crd-spiderendpoint/#spiderendpoint-definition","text":"","title":"SpiderEndpoint definition"},{"location":"reference/crd-spiderendpoint/#metadata","text":"Field Description Schema Validation name the name of this SpiderEndpoint resource string required namespace the namespace of this SpiderEndpoint resource string required","title":"Metadata"},{"location":"reference/crd-spiderendpoint/#status-subresource","text":"The IPPool status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation current the IP allocation details of the corresponding pod PodIPAllocation required ownerControllerType the corresponding pod top owner controller type string required ownerControllerName the corresponding pod top owner controller name string required","title":"Status (subresource)"},{"location":"reference/crd-spiderendpoint/#podipallocation","text":"This property describes the SpiderEndpoint corresponding pod details. Field Description Schema Validation uid corresponding pod uid string required node total IP counts of this pool to use string required ips current allocated IP counts list of IPAllocationDetail required","title":"PodIPAllocation"},{"location":"reference/crd-spiderendpoint/#ipallocationdetail","text":"This property describes single Interface allocation details. Field Description Schema Validation Default interface single interface name string required ipv4 single IPv4 allocated IP address string optional ipv6 single IPv6 allocated IP address string optional ipv4Pool the IPv4 allocated IP address corresponding pool string optional ipv6Pool the IPv6 allocated IP address corresponding pool string optional vlan vlan ID int optional 0 ipv4Gateway the IPv4 gateway IP address string optional ipv6Gateway the IPv6 gateway IP address string optional cleanGateway a flag to choose whether need default route by the gateway boolean optional routes the allocation routes list if Route optional","title":"IPAllocationDetail"},{"location":"reference/crd-spiderippool/","text":"SpiderIPPool A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : ipVersion : 4 subnet : 172.31.192.0/20 ips : - 172.31.199.180-172.31.199.189 - 172.31.199.205-172.31.199.209 excludeIPs : - 172.31.199.186-172.31.199.188 - 172.31.199.207 gateway : 172.31.207.253 default : true disable : false SpiderIPPool definition Metadata Field Description Schema Validation name the name of this SpiderIPPool resource string required Spec This is the IPPool spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this pool int optional 4,6 subnet subnet of this pool string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this pool to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this pool to filter list of strings optional array of IP ranges and single IP address gateway gateway for this pool string optional an IP address vlan vlan ID int optional [0,4094] 0 routes custom routes in this pool (please don't set default route 0.0.0.0/0 if property gateway exists) list of route optional podAffinity specify which pods can use this pool labelSelector optional kubernetes LabelSelector namespaceAffinity specify which namespaces pods can use this pool labelSelector optional kubernetes LabelSelector namespaceName specify which namespaces pods can use this pool (The priority is higher than property namespaceAffinity ) list of strings optional nodeAffinity specify which nodes pods can use this pool labelSelector optional kubernetes LabelSelector nodeName specify which nodes pods can use this pool (The priority is higher than property nodeAffinity ) list of strings optional multusName specify which multus net-attach-def objects can use this pool list of strings optional default configure this resource as a default pool for pods boolean optional true,false false disable configure whether the pool is usable boolean optional true,false false Status (subresource) The IPPool status is a subresource that processed automatically by the system to summarize the current state Field Description Schema allocatedIPs current IP allocations in this pool string totalIPCount total IP counts of this pool to use int allocatedIPCount current allocated IP counts int Route Field Description Schema Validation dst destination of this route string required gw gateway of this route string required Pod Affinity For details on configuring SpiderIPPool podAffinity, please read the Pod Affinity of IPPool . Namespace Affinity For details on configuring SpiderIPPool namespaceAffinity or namespaceName, please read the Namespace Affinity of IPPool . Notice: namespaceName has higher priority than namespaceAffinity . Node Affinity For details on configuring SpiderIPPool nodeAffinity or nodeName, please read the Node Affinity of IPPool and Network topology allocation . Notice: nodeName has higher priority than nodeAffinity .","title":"CRD SpiderIPPool"},{"location":"reference/crd-spiderippool/#spiderippool","text":"A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned.","title":"SpiderIPPool"},{"location":"reference/crd-spiderippool/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : ipVersion : 4 subnet : 172.31.192.0/20 ips : - 172.31.199.180-172.31.199.189 - 172.31.199.205-172.31.199.209 excludeIPs : - 172.31.199.186-172.31.199.188 - 172.31.199.207 gateway : 172.31.207.253 default : true disable : false","title":"Sample YAML"},{"location":"reference/crd-spiderippool/#spiderippool-definition","text":"","title":"SpiderIPPool definition"},{"location":"reference/crd-spiderippool/#metadata","text":"Field Description Schema Validation name the name of this SpiderIPPool resource string required","title":"Metadata"},{"location":"reference/crd-spiderippool/#spec","text":"This is the IPPool spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this pool int optional 4,6 subnet subnet of this pool string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this pool to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this pool to filter list of strings optional array of IP ranges and single IP address gateway gateway for this pool string optional an IP address vlan vlan ID int optional [0,4094] 0 routes custom routes in this pool (please don't set default route 0.0.0.0/0 if property gateway exists) list of route optional podAffinity specify which pods can use this pool labelSelector optional kubernetes LabelSelector namespaceAffinity specify which namespaces pods can use this pool labelSelector optional kubernetes LabelSelector namespaceName specify which namespaces pods can use this pool (The priority is higher than property namespaceAffinity ) list of strings optional nodeAffinity specify which nodes pods can use this pool labelSelector optional kubernetes LabelSelector nodeName specify which nodes pods can use this pool (The priority is higher than property nodeAffinity ) list of strings optional multusName specify which multus net-attach-def objects can use this pool list of strings optional default configure this resource as a default pool for pods boolean optional true,false false disable configure whether the pool is usable boolean optional true,false false","title":"Spec"},{"location":"reference/crd-spiderippool/#status-subresource","text":"The IPPool status is a subresource that processed automatically by the system to summarize the current state Field Description Schema allocatedIPs current IP allocations in this pool string totalIPCount total IP counts of this pool to use int allocatedIPCount current allocated IP counts int","title":"Status (subresource)"},{"location":"reference/crd-spiderippool/#route","text":"Field Description Schema Validation dst destination of this route string required gw gateway of this route string required","title":"Route"},{"location":"reference/crd-spiderippool/#pod-affinity","text":"For details on configuring SpiderIPPool podAffinity, please read the Pod Affinity of IPPool .","title":"Pod Affinity"},{"location":"reference/crd-spiderippool/#namespace-affinity","text":"For details on configuring SpiderIPPool namespaceAffinity or namespaceName, please read the Namespace Affinity of IPPool . Notice: namespaceName has higher priority than namespaceAffinity .","title":"Namespace Affinity"},{"location":"reference/crd-spiderippool/#node-affinity","text":"For details on configuring SpiderIPPool nodeAffinity or nodeName, please read the Node Affinity of IPPool and Network topology allocation . Notice: nodeName has higher priority than nodeAffinity .","title":"Node Affinity"},{"location":"reference/crd-spidermultusconfig/","text":"SpiderMultusConfig A SpiderMultusConfig resource represents a best practice to generate a multus net-attach-def CR object for spiderpool to use. For details on using this CRD, please read the SpiderMultusConfig guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : demo namespace : default annotations : multus.spidernet.io/cr-name : \"macvlan-100\" multus.spidernet.io/cni-version : 0.4.0 spec : cniType : macvlan macvlan : master : [ \"eth0\" ] vlanID : 100 ippools : ipv4 : [ \"default-pool-v4\" ] ipv6 : [ \"default-pool-v6\" ] SpiderMultusConfig definition Metadata Field Description Schema Validation name The name of this SpiderMultusConfig resource string required namespace The namespace of this SpiderMultusConfig resource string required annotations The annotations of this SpiderMultusConfig resource map optional Metadata.annotations You can also set annotations for this SpiderMultusConfig resource, then the corresponding Multus net-attach-def resource will inherit these annotations too. And you can also use special annotation multus.spidernet.io/cr-name and multus.spidernet.io/cni-version to customize the corresponding Multus net-attach-def resource name and CNI version. Field Description Schema Validation Default multus.spidernet.io/cr-name The customized Multus net-attach-def resource name string optional multus.spidernet.io/cni-version The customized Multus net-attach-def resource CNI version string optional 0.3.1 Spec This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values Default cniType expected main CNI type string require macvlan,ipvlan,sriov,ovs,custom macvlan macvlan CNI configuration SpiderMacvlanCniConfig optional ipvlan ipvlan CNI configuration SpiderIPvlanCniConfig optional sriov sriov CNI configuration SpiderSRIOVCniConfig optional ovs ovs CNI configuration SpiderOvsCniConfig optional enableCoordinator enable coordinator or not boolean optional true,false true coordinator coordinator CNI configuration CoordinatorSpec optional customCNI a string that represents custom CNI configuration string optional SpiderMacvlanCniConfig Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderIPvlanCniConfig Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderSRIOVCniConfig Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with SRIOV string required vlanID vlan ID int optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderOvsCniConfig Field Description Schema Validation bridge name of the bridge to use string required vlan vlan ID of attached port. Trunk port if not specified int optional trunk List of VLAN ID's and/or ranges of accepted VLAN ID's Trunk optional deviceID PCI address of a VF in valid sysfs format string optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional BondConfig Field Description Schema Validation Values Name the expected bond interface name string required Mode bond interface mode int required [0,6] Options expected bond Interface configurations string optional Trunk Field Description Schema Validation Values minID the min value of vlan ID int optional [0,4094] maxID the max value of vlan ID int optional [0,4094] id the value of vlan ID int optional [0,4094] SpiderpoolPools Field Description Schema Validation ipv4 the default IPv4 IPPools in your CNI configurations list of strings optional ipv6 the default IPv6 IPPools in your CNI configurations list of strings optional","title":"CRD Spidermultusconfig"},{"location":"reference/crd-spidermultusconfig/#spidermultusconfig","text":"A SpiderMultusConfig resource represents a best practice to generate a multus net-attach-def CR object for spiderpool to use. For details on using this CRD, please read the SpiderMultusConfig guide .","title":"SpiderMultusConfig"},{"location":"reference/crd-spidermultusconfig/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : demo namespace : default annotations : multus.spidernet.io/cr-name : \"macvlan-100\" multus.spidernet.io/cni-version : 0.4.0 spec : cniType : macvlan macvlan : master : [ \"eth0\" ] vlanID : 100 ippools : ipv4 : [ \"default-pool-v4\" ] ipv6 : [ \"default-pool-v6\" ]","title":"Sample YAML"},{"location":"reference/crd-spidermultusconfig/#spidermultusconfig-definition","text":"","title":"SpiderMultusConfig definition"},{"location":"reference/crd-spidermultusconfig/#metadata","text":"Field Description Schema Validation name The name of this SpiderMultusConfig resource string required namespace The namespace of this SpiderMultusConfig resource string required annotations The annotations of this SpiderMultusConfig resource map optional","title":"Metadata"},{"location":"reference/crd-spidermultusconfig/#metadataannotations","text":"You can also set annotations for this SpiderMultusConfig resource, then the corresponding Multus net-attach-def resource will inherit these annotations too. And you can also use special annotation multus.spidernet.io/cr-name and multus.spidernet.io/cni-version to customize the corresponding Multus net-attach-def resource name and CNI version. Field Description Schema Validation Default multus.spidernet.io/cr-name The customized Multus net-attach-def resource name string optional multus.spidernet.io/cni-version The customized Multus net-attach-def resource CNI version string optional 0.3.1","title":"Metadata.annotations"},{"location":"reference/crd-spidermultusconfig/#spec","text":"This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values Default cniType expected main CNI type string require macvlan,ipvlan,sriov,ovs,custom macvlan macvlan CNI configuration SpiderMacvlanCniConfig optional ipvlan ipvlan CNI configuration SpiderIPvlanCniConfig optional sriov sriov CNI configuration SpiderSRIOVCniConfig optional ovs ovs CNI configuration SpiderOvsCniConfig optional enableCoordinator enable coordinator or not boolean optional true,false true coordinator coordinator CNI configuration CoordinatorSpec optional customCNI a string that represents custom CNI configuration string optional","title":"Spec"},{"location":"reference/crd-spidermultusconfig/#spidermacvlancniconfig","text":"Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderMacvlanCniConfig"},{"location":"reference/crd-spidermultusconfig/#spideripvlancniconfig","text":"Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderIPvlanCniConfig"},{"location":"reference/crd-spidermultusconfig/#spidersriovcniconfig","text":"Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with SRIOV string required vlanID vlan ID int optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderSRIOVCniConfig"},{"location":"reference/crd-spidermultusconfig/#spiderovscniconfig","text":"Field Description Schema Validation bridge name of the bridge to use string required vlan vlan ID of attached port. Trunk port if not specified int optional trunk List of VLAN ID's and/or ranges of accepted VLAN ID's Trunk optional deviceID PCI address of a VF in valid sysfs format string optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderOvsCniConfig"},{"location":"reference/crd-spidermultusconfig/#bondconfig","text":"Field Description Schema Validation Values Name the expected bond interface name string required Mode bond interface mode int required [0,6] Options expected bond Interface configurations string optional","title":"BondConfig"},{"location":"reference/crd-spidermultusconfig/#trunk","text":"Field Description Schema Validation Values minID the min value of vlan ID int optional [0,4094] maxID the max value of vlan ID int optional [0,4094] id the value of vlan ID int optional [0,4094]","title":"Trunk"},{"location":"reference/crd-spidermultusconfig/#spiderpoolpools","text":"Field Description Schema Validation ipv4 the default IPv4 IPPools in your CNI configurations list of strings optional ipv6 the default IPv6 IPPools in your CNI configurations list of strings optional","title":"SpiderpoolPools"},{"location":"reference/crd-spiderreservedip/","text":"SpiderReservedIP A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. For details on using this CRD, please read the SpiderReservedIP guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : exclude-ips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 SpiderReservedIP definition Metadata Field Description Schema Validation name the name of this SpiderReservedIP resource string required Spec This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values ipVersion IP version of this resource int optional 4,6 ips IP ranges for this resource that we expect not to use list of strings optional array of IP ranges and single IP address","title":"CRD SpiderReservedIP"},{"location":"reference/crd-spiderreservedip/#spiderreservedip","text":"A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. For details on using this CRD, please read the SpiderReservedIP guide .","title":"SpiderReservedIP"},{"location":"reference/crd-spiderreservedip/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : exclude-ips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50","title":"Sample YAML"},{"location":"reference/crd-spiderreservedip/#spiderreservedip-definition","text":"","title":"SpiderReservedIP definition"},{"location":"reference/crd-spiderreservedip/#metadata","text":"Field Description Schema Validation name the name of this SpiderReservedIP resource string required","title":"Metadata"},{"location":"reference/crd-spiderreservedip/#spec","text":"This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values ipVersion IP version of this resource int optional 4,6 ips IP ranges for this resource that we expect not to use list of strings optional array of IP ranges and single IP address","title":"Spec"},{"location":"reference/crd-spidersubnet/","text":"SpiderSubnet A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. For details on using this CRD, please read the SpiderSubnet guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : default-v4-subnet spec : ipVersion : 4 ips : - 172.22.40.2-172.22.40.254 subnet : 172.22.0.0/16 excludeIPs : - 172.22.40.10-172.22.40.20 gateway : 172.22.40.1 SpiderSubnet definition Metadata Field Description Schema Validation name the name of this SpiderSubnet resource string required Spec This is the SpiderSubnet spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this subnet int optional 4,6 subnet subnet of this resource string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this resource to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this resource to filter list of strings optional array of IP ranges and single IP address gateway gateway for this resource string optional an IP address vlan vlan ID int optional [0,4094] 0 routes custom routes in this resource list of Route optional Status (subresource) The Subnet status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema controlledIPPools current IP allocations in this subnet resource string totalIPCount total IP addresses counts of this subnet resource to use int allocatedIPCount current allocated IP addresses counts int","title":"CRD SpiderSubnet"},{"location":"reference/crd-spidersubnet/#spidersubnet","text":"A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. For details on using this CRD, please read the SpiderSubnet guide .","title":"SpiderSubnet"},{"location":"reference/crd-spidersubnet/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : default-v4-subnet spec : ipVersion : 4 ips : - 172.22.40.2-172.22.40.254 subnet : 172.22.0.0/16 excludeIPs : - 172.22.40.10-172.22.40.20 gateway : 172.22.40.1","title":"Sample YAML"},{"location":"reference/crd-spidersubnet/#spidersubnet-definition","text":"","title":"SpiderSubnet definition"},{"location":"reference/crd-spidersubnet/#metadata","text":"Field Description Schema Validation name the name of this SpiderSubnet resource string required","title":"Metadata"},{"location":"reference/crd-spidersubnet/#spec","text":"This is the SpiderSubnet spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this subnet int optional 4,6 subnet subnet of this resource string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this resource to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this resource to filter list of strings optional array of IP ranges and single IP address gateway gateway for this resource string optional an IP address vlan vlan ID int optional [0,4094] 0 routes custom routes in this resource list of Route optional","title":"Spec"},{"location":"reference/crd-spidersubnet/#status-subresource","text":"The Subnet status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema controlledIPPools current IP allocations in this subnet resource string totalIPCount total IP addresses counts of this subnet resource to use int allocatedIPCount current allocated IP addresses counts int","title":"Status (subresource)"},{"location":"reference/plugin-coordinator/","text":"Meta plugin: Coordinator Configuration Spiderpool provides a CNI meta-plugin called 'coordinator', which works after the main CNI is called, and provides the following main features: Fixed an issue where Underlay pods could not access ClusterIP Tune the pod's routing to ensure that packets are routed consistently while a pod is in multi-NIC Supports detecting if the IP of a pod is in conflict Supports detecting if the gateway of a pod is reachable Support for fixing MAC address prefixes for pods CNI fields description This is a complete manifest file for the coordinator's multus network-attachment-definition: 'Spidercoordinators default CR' is the global default configuration (all fields) for the 'coordinator' plugin, which has a lower priority than the configuration in NetworkAttachmentDefinition CR. If NetworkAttachmentDefinition CR is not configured, 'Spidercoordinators CR' is used as the default. For more details, see: Spidercoordinator Field Description Schema Validation Default type The name of this Spidercoordinators resource string required coordinator mode the mode in which the coordinator run. \"auto\": Automatically determine if it's overlay or underlay; \"underlay\": All NICs for pods are underlay NICs, and in this case the coordinator will create veth-pairs device to solve the problem of underlay pods accessing services; \"overlay\": The coordinator does not create veth-pair devices, but the first NIC of the pod cannot be an underlay NIC, which is created by overlay CNI (e.g. calico, cilium). Solve the problem of pod access to service through the first NIC; \"disable\": The coordinator does nothing and exits directly string optional auto tunePodRoutes Tune the pod's routing tables while a pod is in multi-NIC mode bool optional true podDefaultRouteNic Configure the default routed NIC for the pod while a pod is in multi-NIC mode string optional \"\" podDefaultCniNic The name of the pod's first NIC defaults to eth0 in kubernetes bool optional eth0 detectGateway Enable gateway detection while creating pods, which prevent pod creation if the gateway is unreachable bool optional false detectIPConflict Enable IP conflicting checking for pods, which prevent pod creation if the pod's ip is conflicting bool optional false podMACPrefix Enable fixing MAC address prefixes for pods. empty value is mean to disable string optional \"\" overlayPodCIDR The default cluster CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} serviceCIDR The default service CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} hijackCIDR The CIDR that need to be forwarded via the host network, For example, the address of nodelocaldns(169.254.20.10/32 by default) []stirng optional []string{} hostRuleTable The routes on the host that communicates with the pod's underlay IPs will belong to this routing table number int optional 500 hostRPFilter Set the rp_filter sysctl parameter on the host, which is recommended to be set to 0 int optional 0 detectOptions The advanced configuration of detectGateway and detectIPConflict, including retry numbers(default is 3), interval(default is 1s) and timeout(default is 1s) obejct optional nil logOptions The configuration of logging, including logLevel(default is debug) and logFile(default is /var/log/spidernet/coordinator.log) obejct optional nil Configure Examples Supports detecting if the IP of a pod is in conflict apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"auto\", \"detectIPConflict\": false } ] } Supports detecting if the IP of a pod is in conflict apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"auto\", \"detectIPConflict\": true } ] } Supports detecting if the gateway of a pod is reachable apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"detectGateway\": true } ] } Support for fixing MAC address prefixes for pods apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"podMACPrefix\": \"0a:1b\" } ] } Setting pod's default route NIC while pod in Multi-NIC mode apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"podDefaultRouteNic\": \"eth0\" } ] } You can also set it by ipam.spidernet.io/default-route-nic: eth0 in the pod's annotations. Configure the subnets that need to be forwarded via the host network apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"hijackCIDR\": [\"169.254.20.10/32\"] } ] } 169.254.20.10/32 is default ip address of nodelocaldns.","title":"Coordinatorr plugin"},{"location":"reference/plugin-coordinator/#meta-plugin-coordinator-configuration","text":"Spiderpool provides a CNI meta-plugin called 'coordinator', which works after the main CNI is called, and provides the following main features: Fixed an issue where Underlay pods could not access ClusterIP Tune the pod's routing to ensure that packets are routed consistently while a pod is in multi-NIC Supports detecting if the IP of a pod is in conflict Supports detecting if the gateway of a pod is reachable Support for fixing MAC address prefixes for pods","title":"Meta plugin: Coordinator Configuration"},{"location":"reference/plugin-coordinator/#cni-fields-description","text":"This is a complete manifest file for the coordinator's multus network-attachment-definition: 'Spidercoordinators default CR' is the global default configuration (all fields) for the 'coordinator' plugin, which has a lower priority than the configuration in NetworkAttachmentDefinition CR. If NetworkAttachmentDefinition CR is not configured, 'Spidercoordinators CR' is used as the default. For more details, see: Spidercoordinator Field Description Schema Validation Default type The name of this Spidercoordinators resource string required coordinator mode the mode in which the coordinator run. \"auto\": Automatically determine if it's overlay or underlay; \"underlay\": All NICs for pods are underlay NICs, and in this case the coordinator will create veth-pairs device to solve the problem of underlay pods accessing services; \"overlay\": The coordinator does not create veth-pair devices, but the first NIC of the pod cannot be an underlay NIC, which is created by overlay CNI (e.g. calico, cilium). Solve the problem of pod access to service through the first NIC; \"disable\": The coordinator does nothing and exits directly string optional auto tunePodRoutes Tune the pod's routing tables while a pod is in multi-NIC mode bool optional true podDefaultRouteNic Configure the default routed NIC for the pod while a pod is in multi-NIC mode string optional \"\" podDefaultCniNic The name of the pod's first NIC defaults to eth0 in kubernetes bool optional eth0 detectGateway Enable gateway detection while creating pods, which prevent pod creation if the gateway is unreachable bool optional false detectIPConflict Enable IP conflicting checking for pods, which prevent pod creation if the pod's ip is conflicting bool optional false podMACPrefix Enable fixing MAC address prefixes for pods. empty value is mean to disable string optional \"\" overlayPodCIDR The default cluster CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} serviceCIDR The default service CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} hijackCIDR The CIDR that need to be forwarded via the host network, For example, the address of nodelocaldns(169.254.20.10/32 by default) []stirng optional []string{} hostRuleTable The routes on the host that communicates with the pod's underlay IPs will belong to this routing table number int optional 500 hostRPFilter Set the rp_filter sysctl parameter on the host, which is recommended to be set to 0 int optional 0 detectOptions The advanced configuration of detectGateway and detectIPConflict, including retry numbers(default is 3), interval(default is 1s) and timeout(default is 1s) obejct optional nil logOptions The configuration of logging, including logLevel(default is debug) and logFile(default is /var/log/spidernet/coordinator.log) obejct optional nil","title":"CNI fields description"},{"location":"reference/plugin-coordinator/#configure-examples","text":"Supports detecting if the IP of a pod is in conflict apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"auto\", \"detectIPConflict\": false } ] } Supports detecting if the IP of a pod is in conflict apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"auto\", \"detectIPConflict\": true } ] } Supports detecting if the gateway of a pod is reachable apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"detectGateway\": true } ] } Support for fixing MAC address prefixes for pods apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"podMACPrefix\": \"0a:1b\" } ] } Setting pod's default route NIC while pod in Multi-NIC mode apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"podDefaultRouteNic\": \"eth0\" } ] } You can also set it by ipam.spidernet.io/default-route-nic: eth0 in the pod's annotations. Configure the subnets that need to be forwarded via the host network apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : coordinator-demo namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"coordinator\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"hijackCIDR\": [\"169.254.20.10/32\"] } ] } 169.254.20.10/32 is default ip address of nodelocaldns.","title":"Configure Examples"},{"location":"reference/plugin-ifacer/","text":"Meta plugin: Ifacer Configuration The ifacer plugin helps users create VLAN sub-interfaces or bond devices based on the provided CNI configuration file. Here are some examples to show how to configure it: Create a vlan sub-interface base on master interface The CR of multus's net-atta-def is configured like this: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : ifacer-vlan120-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-vlan120\", \"plugins\": [ { \"type\": \"ifacer\", \"interfaces\": [\"ens160\"], \"vlanID\": 120 }, { \"type\": \"macvlan\", \"master\": \"ens160.120\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"tune_mode\": \"underlay\" } ] } Fields description: plugins[0].type(string, required): ifacer, the name of plugin. plugins[0].interfaces([]string,required): ifacer create VLAN sub-interfaces based on this master interface. Note: When creating a VLAN sub-interface, the elements of the array must have only one master interface. And the interface exists on the host. plugins[0].vlanID: The VLAN tag of the VLAN sub-interface. Note: the value must be in range: [0,4094]. and \"0\" indicates that no VLAN sub-interfaces will be created. Note: The name of the created vlan sub-interface is spliced from the master interface and vlanId. The format is: \" . \". If a VLAN sub-interface with the same name already exists on the node, ifacer checks if the interface is in the UP state. if not, sets to UP and exits. Create a bond device The CR of multus's net-atta-def is configured like this: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : ifacer-bond0-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-bond0\", \"plugins\": [ { \"type\": \"ifacer\", \"interfaces\": [\"ens160\",\"ens192\"], \"vlanID\": 120, \"bond\": { \"name\": \"bond0\", \"mode\": 0, \"options\": \"primary=ens160\" } }, { \"type\": \"macvlan\", \"master\": \"ens160.120\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"tune_mode\": \"underlay\" } ] } Fields description: plugins[0].type(string, required): ifacer, the name of plugin. plugins[0].interfaces([]string,required): ifacer create VLAN sub-interfaces based on this master interface. Note: When creating a VLAN sub-interface, the elements of the array must have only one master interface. And the interface exists on the host. plugins[0].vlanID(int,optional): The VLAN tag of the VLAN sub-interface created based on the bond device. Note: the value must be in range: [0,4094]. and \"0\" indicates that no VLAN sub-interfaces will be created. plugins[0].bond.name(string,optional): the name of bond device, If not specified, the default is sp_bond0. plugins[0].bond.mode(string,optional): bond mode, the value must be in range: [0,6]. plugins[0].bond.options(string,optional), bond options for the bonding driver are supplied as parameters to the bonding module at load time, or are specified via sysfs. Multiple parameters separated by \";\"\uff0cinput-formatted: \"primary=ens160;arp_interval=1\". More details see https://www.kernel.org/doc/Documentation/networking/bonding.txt . Note: If a bond device with the same name already exists on the node, ifacer checks if the interface is bond type and in the UP state. if not bond type, return error; if not in UP state,sets to UP and exits. If a VLAN sub-interface created base on the bond device with the same name already exists on the node, ifacer checks if the interface is in the UP state. if not, sets to UP and exits.","title":"Ifacer plugin"},{"location":"reference/plugin-ifacer/#meta-plugin-ifacer-configuration","text":"The ifacer plugin helps users create VLAN sub-interfaces or bond devices based on the provided CNI configuration file. Here are some examples to show how to configure it:","title":"Meta plugin: Ifacer Configuration"},{"location":"reference/plugin-ifacer/#create-a-vlan-sub-interface-base-on-master-interface","text":"The CR of multus's net-atta-def is configured like this: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : ifacer-vlan120-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-vlan120\", \"plugins\": [ { \"type\": \"ifacer\", \"interfaces\": [\"ens160\"], \"vlanID\": 120 }, { \"type\": \"macvlan\", \"master\": \"ens160.120\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"tune_mode\": \"underlay\" } ] } Fields description: plugins[0].type(string, required): ifacer, the name of plugin. plugins[0].interfaces([]string,required): ifacer create VLAN sub-interfaces based on this master interface. Note: When creating a VLAN sub-interface, the elements of the array must have only one master interface. And the interface exists on the host. plugins[0].vlanID: The VLAN tag of the VLAN sub-interface. Note: the value must be in range: [0,4094]. and \"0\" indicates that no VLAN sub-interfaces will be created. Note: The name of the created vlan sub-interface is spliced from the master interface and vlanId. The format is: \" . \". If a VLAN sub-interface with the same name already exists on the node, ifacer checks if the interface is in the UP state. if not, sets to UP and exits.","title":"Create a vlan sub-interface base on master interface"},{"location":"reference/plugin-ifacer/#create-a-bond-device","text":"The CR of multus's net-atta-def is configured like this: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : ifacer-bond0-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-bond0\", \"plugins\": [ { \"type\": \"ifacer\", \"interfaces\": [\"ens160\",\"ens192\"], \"vlanID\": 120, \"bond\": { \"name\": \"bond0\", \"mode\": 0, \"options\": \"primary=ens160\" } }, { \"type\": \"macvlan\", \"master\": \"ens160.120\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"tune_mode\": \"underlay\" } ] } Fields description: plugins[0].type(string, required): ifacer, the name of plugin. plugins[0].interfaces([]string,required): ifacer create VLAN sub-interfaces based on this master interface. Note: When creating a VLAN sub-interface, the elements of the array must have only one master interface. And the interface exists on the host. plugins[0].vlanID(int,optional): The VLAN tag of the VLAN sub-interface created based on the bond device. Note: the value must be in range: [0,4094]. and \"0\" indicates that no VLAN sub-interfaces will be created. plugins[0].bond.name(string,optional): the name of bond device, If not specified, the default is sp_bond0. plugins[0].bond.mode(string,optional): bond mode, the value must be in range: [0,6]. plugins[0].bond.options(string,optional), bond options for the bonding driver are supplied as parameters to the bonding module at load time, or are specified via sysfs. Multiple parameters separated by \";\"\uff0cinput-formatted: \"primary=ens160;arp_interval=1\". More details see https://www.kernel.org/doc/Documentation/networking/bonding.txt . Note: If a bond device with the same name already exists on the node, ifacer checks if the interface is bond type and in the UP state. if not bond type, return error; if not in UP state,sets to UP and exits. If a VLAN sub-interface created base on the bond device with the same name already exists on the node, ifacer checks if the interface is in the UP state. if not, sets to UP and exits.","title":"Create a bond device"},{"location":"reference/plugin-ipam/","text":"IPAM Plugin Configuration Here is an example of IPAM configuration. { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-pod-network\" , \"plugins\" :[ { \"name\" : \"macvlan-pod-network\" , \"type\" : \"macvlan\" , \"master\" : \"ens256\" , \"mode\" : \"bridge\" , \"mtu\" : 1500 , \"ipam\" :{ \"type\" : \"spiderpool\" , \"log_file_path\" : \"/var/log/spidernet/spiderpool.log\" , \"log_file_max_size\" : \"100\" , \"log_file_max_age\" : \"30\" , \"log_file_max_count\" : 7 , \"log_level\" : \"INFO\" , \"default_ipv4_ippool\" : [ \"default-ipv4-pool1\" , \"default-ipv4-pool2\" ], \"default_ipv6_ippool\" : [ \"default-ipv6-pool1\" , \"default-ipv6-pool2\" ] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM plugin"},{"location":"reference/plugin-ipam/#ipam-plugin-configuration","text":"Here is an example of IPAM configuration. { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-pod-network\" , \"plugins\" :[ { \"name\" : \"macvlan-pod-network\" , \"type\" : \"macvlan\" , \"master\" : \"ens256\" , \"mode\" : \"bridge\" , \"mtu\" : 1500 , \"ipam\" :{ \"type\" : \"spiderpool\" , \"log_file_path\" : \"/var/log/spidernet/spiderpool.log\" , \"log_file_max_size\" : \"100\" , \"log_file_max_age\" : \"30\" , \"log_file_max_count\" : 7 , \"log_level\" : \"INFO\" , \"default_ipv4_ippool\" : [ \"default-ipv4-pool1\" , \"default-ipv4-pool2\" ], \"default_ipv6_ippool\" : [ \"default-ipv6-pool1\" , \"default-ipv6-pool2\" ] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM Plugin Configuration"},{"location":"reference/spiderpool-agent/","text":"spiderpool-agent This page describes CLI options and ENV of spiderpool-agent. spiderpool-agent daemon Run the spiderpool agent daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin ENV env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide. spiderpool-agent shutdown Notify of stopping the spiderpool-agent daemon. spiderpool-agent metric Get local metrics. Options --port string http server port of local metric (default to 5711)","title":"spiderpool-agent"},{"location":"reference/spiderpool-agent/#spiderpool-agent","text":"This page describes CLI options and ENV of spiderpool-agent.","title":"spiderpool-agent"},{"location":"reference/spiderpool-agent/#spiderpool-agent-daemon","text":"Run the spiderpool agent daemon.","title":"spiderpool-agent daemon"},{"location":"reference/spiderpool-agent/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin","title":"Options"},{"location":"reference/spiderpool-agent/#env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide.","title":"ENV"},{"location":"reference/spiderpool-agent/#spiderpool-agent-shutdown","text":"Notify of stopping the spiderpool-agent daemon.","title":"spiderpool-agent shutdown"},{"location":"reference/spiderpool-agent/#spiderpool-agent-metric","text":"Get local metrics.","title":"spiderpool-agent metric"},{"location":"reference/spiderpool-agent/#options_1","text":"--port string http server port of local metric (default to 5711)","title":"Options"},{"location":"reference/spiderpool-controller/","text":"spiderpool-controller This page describes CLI options and ENV of spiderpool-controller. spiderpool-controller daemon Run the spiderpool controller daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) ENV env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listening on. Disabled if empty. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED true Enable/disable IP GC for Terminating pod. spiderpool-controller shutdown Notify of stopping spiderpool-controller daemon. spiderpool-controller metric Get local metrics. Options --port string http server port of local metric (default to 5721) spiderpool-controller status Show status: Whether local is controller leader ... Options --port string http server port of local metric (default to 5720)","title":"spiderpool-controller"},{"location":"reference/spiderpool-controller/#spiderpool-controller","text":"This page describes CLI options and ENV of spiderpool-controller.","title":"spiderpool-controller"},{"location":"reference/spiderpool-controller/#spiderpool-controller-daemon","text":"Run the spiderpool controller daemon.","title":"spiderpool-controller daemon"},{"location":"reference/spiderpool-controller/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map)","title":"Options"},{"location":"reference/spiderpool-controller/#env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listening on. Disabled if empty. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED true Enable/disable IP GC for Terminating pod.","title":"ENV"},{"location":"reference/spiderpool-controller/#spiderpool-controller-shutdown","text":"Notify of stopping spiderpool-controller daemon.","title":"spiderpool-controller shutdown"},{"location":"reference/spiderpool-controller/#spiderpool-controller-metric","text":"Get local metrics.","title":"spiderpool-controller metric"},{"location":"reference/spiderpool-controller/#options_1","text":"--port string http server port of local metric (default to 5721)","title":"Options"},{"location":"reference/spiderpool-controller/#spiderpool-controller-status","text":"Show status: Whether local is controller leader ...","title":"spiderpool-controller status"},{"location":"reference/spiderpool-controller/#options_2","text":"--port string http server port of local metric (default to 5720)","title":"Options"},{"location":"reference/spiderpoolctl/","text":"spiderpoolctl This page describes CLI usage of spiderpoolctl for debug. spiderpoolctl gc Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address) spiderpoolctl ip show Show a pod that is taking this IP. Options --ip string [required] ip spiderpoolctl ip release Try to release an IP. Options --ip string [optional] ip --force [optional] force release ip spiderpoolctl ip set Set IP to be taken by a pod. This will update ippool and workload endpoint resource. Options --ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"spiderpoolctl"},{"location":"reference/spiderpoolctl/#spiderpoolctl","text":"This page describes CLI usage of spiderpoolctl for debug.","title":"spiderpoolctl"},{"location":"reference/spiderpoolctl/#spiderpoolctl-gc","text":"Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address)","title":"spiderpoolctl gc"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-show","text":"Show a pod that is taking this IP.","title":"spiderpoolctl ip show"},{"location":"reference/spiderpoolctl/#options","text":"--ip string [required] ip","title":"Options"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-release","text":"Try to release an IP.","title":"spiderpoolctl ip release"},{"location":"reference/spiderpoolctl/#options_1","text":"--ip string [optional] ip --force [optional] force release ip","title":"Options"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-set","text":"Set IP to be taken by a pod. This will update ippool and workload endpoint resource.","title":"spiderpoolctl ip set"},{"location":"reference/spiderpoolctl/#options_2","text":"--ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"Options"},{"location":"usage/_feature_example_zh/","text":"\u67d0\u529f\u80fd \u4ecb\u7ecd \u672c\u6587\u4e3a\u4e86\u6f14\u793a\u4ec0\u4e48\uff0c\u5b83\u7684\u5e94\u7528\u573a\u666f \u9879\u76ee\u529f\u80fd \u672c\u9879\u76ee\u6709\u4ec0\u4e48\u529f\u80fd\uff0c\u5b83\u4e3a\u4ec0\u4e48\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u529f\u80fd\u7684\u5e94\u7528\u573a\u666f\uff0c\u529f\u80fd\u5b9e\u65bd\u7684\u9650\u5236\u6709\u54ea\u4e9b \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49\uff0c\u672c\u9879\u76ee\u5b89\u88c5\u65f6\u54ea\u4e9b\u9009\u578b\u8981\u6253\u5f00\u6216\u5173\u95ed \u6b65\u9aa4 step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b(\u7528\u4e8e\u6392\u969c)\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6","title":"\u67d0\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_1","text":"","title":"\u67d0\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_2","text":"\u672c\u6587\u4e3a\u4e86\u6f14\u793a\u4ec0\u4e48\uff0c\u5b83\u7684\u5e94\u7528\u573a\u666f","title":"\u4ecb\u7ecd"},{"location":"usage/_feature_example_zh/#_3","text":"\u672c\u9879\u76ee\u6709\u4ec0\u4e48\u529f\u80fd\uff0c\u5b83\u4e3a\u4ec0\u4e48\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u529f\u80fd\u7684\u5e94\u7528\u573a\u666f\uff0c\u529f\u80fd\u5b9e\u65bd\u7684\u9650\u5236\u6709\u54ea\u4e9b","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_4","text":"\u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49\uff0c\u672c\u9879\u76ee\u5b89\u88c5\u65f6\u54ea\u4e9b\u9009\u578b\u8981\u6253\u5f00\u6216\u5173\u95ed","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/_feature_example_zh/#_5","text":"step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b(\u7528\u4e8e\u6392\u969c)\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6","title":"\u6b65\u9aa4"},{"location":"usage/_install_example_zh/","text":"\u5b89\u88c5\u6587\u6863 \u4ecb\u7ecd \u672c\u6587\u8bf4\u660e\u4e3a\u4e86\u5b89\u88c5\u51fa\u4ec0\u4e48\u6837\u7684\u4e00\u5957\u96c6\u7fa4\uff0c\u5b83\u7684\u4ef7\u503c\u662f\u4ec0\u4e48 \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49 \u6b65\u9aa4 step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6 \u53ea\u8c08\u5b89\u88c5\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u72b6\u6001","title":"\u5b89\u88c5\u6587\u6863"},{"location":"usage/_install_example_zh/#_1","text":"","title":"\u5b89\u88c5\u6587\u6863"},{"location":"usage/_install_example_zh/#_2","text":"\u672c\u6587\u8bf4\u660e\u4e3a\u4e86\u5b89\u88c5\u51fa\u4ec0\u4e48\u6837\u7684\u4e00\u5957\u96c6\u7fa4\uff0c\u5b83\u7684\u4ef7\u503c\u662f\u4ec0\u4e48","title":"\u4ecb\u7ecd"},{"location":"usage/_install_example_zh/#_3","text":"\u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/_install_example_zh/#_4","text":"step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6 \u53ea\u8c08\u5b89\u88c5\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u72b6\u6001","title":"\u6b65\u9aa4"},{"location":"usage/cli/","text":"Command line tool (spiderpoolctl) TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/cli/#command-line-tool-spiderpoolctl","text":"TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/coordinator-zh_CN/","text":"Coordinator Spiderpool \u5185\u7f6e\u4e00\u4e2a\u53eb coordinator \u7684 CNI meta-plugin, \u5b83\u5728 Main CNI\u88ab\u8c03\u7528\u4e4b\u540e\u518d\u5de5\u4f5c\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u529f\u80fd: \u89e3\u51b3 Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u5728 Pod \u591a\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u8def\u7531\uff0c\u786e\u4fdd\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e00\u81f4 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecd coordinator \u5982\u4f55\u89e3\u51b3\u6216\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u3002 \u6ce8\u610f: \u5982\u679c\u60a8\u901a\u8fc7 SpinderMultusConfig CR \u5e2e\u52a9\u521b\u5efa NetworkAttachmentDefinition CR\uff0c\u60a8\u53ef\u4ee5\u5728 SpinderMultusConfig \u4e2d\u914d\u7f6e coordinator (\u6240\u6709\u5b57\u6bb5)\u3002\u53c2\u8003: SpinderMultusConfig \u3002 Spidercoordinators CR \u4f5c\u4e3a coordinator \u63d2\u4ef6\u7684\u5168\u5c40\u7f3a\u7701\u914d\u7f6e(\u6240\u6709\u5b57\u6bb5)\uff0c\u5176\u4f18\u5148\u7ea7\u4f4e\u4e8e NetworkAttachmentDefinition CR \u4e2d\u7684\u914d\u7f6e\u3002 \u5982\u679c\u5728 NetworkAttachmentDefinition CR \u672a\u914d\u7f6e, \u5c06\u4f7f\u7528 Spidercoordinators CR \u4f5c\u4e3a\u7f3a\u7701\u503c\u3002\u66f4\u591a\u8be6\u60c5\u53c2\u8003: Spidercoordinator \u3002 \u89e3\u51b3 Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u6211\u4eec\u5728\u4f7f\u7528\u4e00\u4e9b\u5982 Macvlan\u3001IPvlan\u3001Sriov \u7b49 Underlay CNI\u65f6\uff0c\u4f1a\u9047\u5230\u5176 Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8fd9\u5e38\u5e38\u662f\u56e0\u4e3a underlay pod \u8bbf\u95ee CLusterIP \u9700\u8981\u7ecf\u8fc7\u5728\u4ea4\u6362\u673a\u7684\u7f51\u5173\uff0c\u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 ClusterIP \u7684\u8def\u7531\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bbf\u95ee\u3002 \u914d\u7f6e coordinator \u8fd0\u884c\u5728 underlay \u6a21\u5f0f \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u7f51\u5361\u662f\u5426\u662f eth0 , \u5982\u679c\u662f\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a Underlay \u6a21\u5f0f\u3002 \u5982\u679c\u5f53\u524d\u7f51\u5361\u4e0d\u662f eth0 \uff0c\u90a3\u4e48 coordinator \u5c06\u68c0\u6d4b Pod \u4e2d\u662f\u5426\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5982\u679c\u662f\uff0c\u5219\u5224\u65ad\u4e3a Underlay \u6a21\u5f0f\u3002 \u5f53\u60a8\u7684\u4e1a\u52a1\u90e8\u7f72\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u6216\u8005 IAAS \u73af\u5883\u4e0a\u65f6\uff0c\u4e1a\u52a1 Pod \u7684 IP \u5730\u5740\u53ef\u80fd\u76f4\u63a5\u4ece\u5bbf\u4e3b\u673a\u7684 IP \u5b50\u7f51\u5206\u914d\u3002\u5e94\u7528 Pod \u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u3002 \u8be5\u6a21\u5f0f\u7684\u4f18\u70b9\u6709: \u6ca1\u6709 NAT \u6620\u5c04\u7684\u5e72\u6270\uff0cPod \u6700\u5927\u7a0b\u5ea6\u4fdd\u7559\u6e90 IP \u80fd\u591f\u5229\u7528\u5e95\u5c42\u7f51\u7edc\u8bbe\u5907\u5bf9 Pod \u505a\u8bbf\u95ee\u63a7\u5236 \u4e0d\u4f9d\u8d56\u96a7\u9053\u6280\u672f\uff0cPod \u7f51\u7edc\u901a\u4fe1\u7684\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u9ad8 \u4f7f\u7528\u6ce8\u610f: Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf(ClusterIP)\u65f6\uff0c\u6d41\u91cf\u4f1a\u5148\u8df3\u7ed9\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u672c\u5730\u5bbf\u4e3b\u673a\u4f7f\u7528\u81ea\u5df1\u7684IP\u53bb\u8bbf\u95ee\u76ee\u7684pod\uff0c\u56e0\u6b64\uff0c\u6d41\u91cf\u7684\u8f6c\u53d1\u53ef\u80fd\u501f\u52a9\u5916\u90e8\u8def\u7531\u5668\u8fdb\u884c\u4e09\u5c42\u8df3\u8f6c\uff0c\u56e0\u6b64\uff0c\u8bf7\u786e\u4fdd\u76f8\u5173\u4e09\u5c42\u7f51\u7edc\u7684\u901a\u8fbe\u3002 \u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e coordinator \u8fd0\u884c\u5728 underlay \u6a21\u5f0f\u89e3\u51b3\u3002\u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u63d2\u4ef6\u5c06\u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u5c06\u4e00\u7aef\u653e\u7f6e\u4e8e\u4e3b\u673a\uff0c\u53e6\u4e00\u7aef\u653e\u7f6e\u4e0e Pod \u7684 network namespace \u4e2d\uff0c\u7136\u540e\u5728 Pod \u91cc\u9762\u8bbe\u7f6e\u4e00\u4e9b\u8def\u7531\u89c4\u5219\uff0c \u4f7f Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth \u8bbe\u5907\u8f6c\u53d1\u3002Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u76ee\u6807\u65f6\u4ece eth0 \u6216 net1 \u8f6c\u53d1\u3002 \u4e0b\u9762\u5c06\u901a\u8fc7\u4e00\u4e9b\u4f8b\u5b50\u4ecb\u7ecd: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\" } ] } mode: \u6307\u5b9a coordinator \u8fd0\u884c\u5728 underlay \u6a21\u5f0f\u3002\u6216\u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: v1.multus-cni.io/default-network: kube-system/macvlan-underlay , coordinator \u5c06\u4f1a\u81ea\u52a8\u5224\u65ad mode \u4e3a underlay\u3002 \u5f53\u4ee5 macvlan-underlay \u521b\u5efa Pod\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 \u662f\u8282\u70b9\u7684 IP,\u786e\u4fdd Pod \u8bbf\u95ee\u672c\u8282\u70b9\u65f6\u4ece veth0 \u8f6c\u53d1\u3002 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 \u662f\u96c6\u7fa4 Service \u7684 CIDR, \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth0 \u8f6c\u53d1\u3002 \u8fd9\u4e2a\u65b9\u6848\u5f3a\u70c8\u4f9d\u8d56\u4e0e kube-proxy \u7684 MASQUERADE \u3002 \u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e kube-proxy \u7684 masqueradeAll \u4e3a true\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u4e0d\u540c\uff0c \u65e0\u9700\u5f00\u542f masqueradeAll , \u5b83\u4eec\u4e4b\u95f4\u7684\u8bbf\u95ee\u5c06\u4f1a\u88ab SNAT\u3002\u4f46\u5982\u679c Pod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u76f8\u540c\uff0c\u90a3\u6211\u4eec\u5fc5\u987b\u8981\u8bbe\u7f6e masqueradeAll \u4e3a true. \u914d\u7f6e coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f \u4e0e Underlay \u6a21\u5f0f\u76f8\u5bf9\u5e94\uff0c\u6211\u4eec\u6709\u65f6\u5019\u5e76\u4e0d\u5173\u5fc3\u96c6\u7fa4\u90e8\u7f72\u73af\u5883\u7684\u5e95\u5c42\u7f51\u7edc\u662f\u4ec0\u4e48\uff0c\u6211\u4eec\u5e0c\u671b\u96c6\u7fa4\u80fd\u591f\u8fd0\u884c\u5728\u5927\u591a\u6570\u7684\u5e95\u5c42\u7f51\u7edc\u3002\u5e38\u5e38\u4f1a\u7528\u5230\u5982 Calico \u548c Cilium \u7b49CNI, \u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u8c03\u7528\u7f51\u5361\u662f\u5426\u4e0d\u662f eth0 \u3002\u5982\u679c\u4e0d\u662f\uff0c\u786e\u8ba4 Pod \u4e2d\u4e0d\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a overlay \u6a21\u5f0f\u3002 \u6b64\u6a21\u5f0f\u7684\u4f18\u70b9\u6709: IP \u5730\u5740\u5145\u6c9b\uff0c\u51e0\u4e4e\u4e0d\u5b58\u5728\u5730\u5740\u77ed\u7f3a\u7684\u95ee\u9898 \u517c\u5bb9\u6027\u5f3a\uff0c\u80fd\u591f\u8fd0\u884c\u5728\u5927\u591a\u6570\u5e95\u5c42\u7f51\u7edc\u4e0b \u4f46\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u5b58\u5728\u4ee5\u4e0b\u7684\u95ee\u9898: \u56e0\u4e3a calico \u4f1a\u7528\u5230\u4e00\u4e9b\u5c01\u88c5\u6280\u672f\uff0c\u6027\u80fd\u4f1a\u5f97\u5230\u5f71\u54cd \u5927\u591a\u90fd\u4e0d\u652f\u6301 Pod \u7684 IP \u5730\u5740\u56fa\u5b9a \u5982\u679c\u901a\u8fc7 Multus \u4e3a Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\uff0cPod \u901a\u4fe1\u65f6\u5e38\u5e38\u9047\u5230\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4 Pod \u65e0\u6cd5\u6b63\u5e38\u901a\u4fe1 \u5f53 Pod \u9644\u52a0\u4e86\u591a\u5f20\u7f51\u5361\u65f6\uff0c\u6781\u5927\u53ef\u80fd\u51fa\u73b0 Pod \u901a\u4fe1\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u5982\u679c\u6570\u636e\u94fe\u8def\u4e0a\u5b58\u5728\u4e00\u4e9b\u5b89\u5168\u8bbe\u5907\uff0c\u7531\u4e8e\u6570\u636e\u5305\u7684\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\uff0c\u6d41\u91cf\u53ef\u80fd\u88ab\u5b89\u5168\u8bbe\u5907\u8ba4\u4e3a\u662f \"\u534a\u8fde\u63a5\"(\u6ca1\u6709 TCP SYN \u62a5\u6587\u7684\u8bb0\u5f55\uff0c\u4f46\u6536\u5230 TCP ACK \u62a5\u6587 )\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b89\u5168\u8bbe\u5907\u4f1a\u963b\u65ad\u6389\u8be5\u8fde\u63a5\uff0c\u9020\u6210 Pod \u901a\u4fe1\u5f02\u5e38\u3002 \u4e0a\u8ff0\u95ee\u9898\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f\u89e3\u51b3\u3002 \u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u4e0d\u4f1a\u521b\u5efa veth \u8bbe\u5907\uff0c\u800c\u662f\u8bbe\u7f6e\u4e00\u4e9b\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece eth0(\u901a\u5e38\u7531 Calico\u3001Cilium\u7b49CNI\u521b\u5efa) \u8f6c\u53d1\uff0cPod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 (\u901a\u5e38\u7531 Macvlan\u3001IPvlan \u7b49CNI\u521b\u5efa) \u8f6c\u53d1\u3002 \u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u4f8b\u5b50\u6765\u4ecb\u7ecd\u8fd9\u4e2a\u6a21\u5f0f: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-overlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-overlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"overlay\" } ] } mode: \u6307\u5b9a coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f\u3002\u6216\u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-overlay \uff0ccoordinator \u5c06\u4f1a\u81ea\u52a8\u5224\u65ad mode \u4e3a overlay\u3002 \u5f53\u4ee5 macvlan-overlay \u521b\u5efa Pod\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip rule 0 : from all lookup local 32759 : from 10 .233.105.154 lookup 100 32761 : from all to 169 .254.1.1 lookup 100 32762 : from all to 10 .233.64.0/18 lookup 100 32763 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.102 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 # ip r show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link \u8fd9\u4e9b\u7b56\u7565\u8def\u7531: 32759: from 10.233.105.154 lookup 100 : \u786e\u4fdd\u4ece eth0(calico \u7f51\u5361)\u53d1\u51fa\u7684\u6570\u636e\u5305\u8d70table 100 32762: from all to 10.233.64.0/18 lookup 100 : \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u8d70 table 100\uff0c\u4ece eth0 \u8f6c\u53d1\u51fa\u53bb\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cnet1 \u7684\u6240\u6709\u5b50\u7f51\u8def\u7531\u4fdd\u7559\u5728 Main \u8868; eth0 \u7684\u5b50\u7f51\u8def\u7531\u4fdd\u7559\u5728 Table 100\u3002 \u5728 Overlay \u6a21\u5f0f\u4e0b, Pod \u8bbf\u95ee ClusterIP \u5b8c\u5168\u501f\u52a9\u4e8e Overlay CNI \u7684\u7f51\u5361(eth0)\uff0c\u65e0\u9700\u5176\u4ed6\u7279\u6b8a\u914d\u7f6e\u3002 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff0c\u652f\u6301\u68c0\u6d4b IPv4 \u548c IPv6 \u5730\u5740\u3002\u6211\u4eec\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\uff0c\u5982\u679c\u53d1\u73b0\u56de\u590d\u62a5\u6587\u7684 Mac \u5730\u5740\u4e0d\u662f Pod \u672c\u8eab\uff0c\u90a3\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e2a IP \u662f\u51b2\u7a81\u7684\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u914d\u7f6e: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\", \"detectGateway\": false, \"detectIPConflict\": true } ] } \u5982\u679c\u53d1\u73b0 Pod \u7684 IP \u662f\u51b2\u7a81\uff0c\u90a3\u4e48 Pod \u5c06\u4f1a\u521b\u5efa\u5931\u8d25\u3002\u5728 Pod \u7684 Event \u4e8b\u4ef6\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u51b2\u7a81 IP \u7684\u4e3b\u673a\u7684\u7269\u7406\u5730\u5740\u3002 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe\uff0c\u652f\u6301\u68c0\u6d4b IPv4 \u548c IPv6 \u7684\u7f51\u5173\u5730\u5740\u3002\u6211\u4eec\u901a\u8fc7\u53d1\u9001 ICMP \u62a5\u6587\uff0c\u63a2\u6d4b\u7f51\u5173\u5730\u5740\u662f\u5426\u53ef\u8fbe\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u914d\u7f6e: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\", \"detectGateway\": true } ] } \u5982\u679c\u53d1\u73b0 Pod \u7684\u7f51\u5173\u4e0d\u53ef\u8fbe\uff0c\u90a3\u4e48 Pod \u5c06\u4f1a\u521b\u5efa\u5931\u8d25\u3002\u5728 Pod \u7684 Event \u4e8b\u4ef6\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709 Pod \u7684\u7f51\u5173\u4e0d\u53ef\u8fbe\u7684\u7c7b\u4f3c\u9519\u8bef\u3002 \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 coordinator \u63d2\u4ef6\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00\uff0cPod \u7684 Mac \u5730\u5740\u5c06\u7531\u914d\u7f6e\u7684 Mac \u5730\u5740\u524d\u7f00 + Pod's IP \u7ec4\u6210\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u914d\u7f6e: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\", \"podMACPrefix\": \"0a:1b\" } ] } \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u6211\u4eec\u53ef\u4ee5\u68c0\u6d4b Pod \u7684 Mac \u5730\u5740\u7684\u524d\u7f00\u662f\u5426\u662f \"0a:1b\" \u5df2\u77e5\u95ee\u9898 Underlay \u6a21\u5f0f\u4e0b\uff0cunderlay Pod \u4e0e Overlay Pod(calico or cilium) \u8fdb\u884c TCP \u901a\u4fe1\u5931\u8d25 \u6b64\u95ee\u9898\u662f\u56e0\u4e3a\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\uff0c\u53d1\u51fa\u7684\u8bf7\u6c42\u62a5\u6587\u5339\u914d\u6e90Pod \u4fa7\u7684\u8def\u7531\uff0c\u4f1a\u901a\u8fc7 veth0 \u8f6c\u53d1\u5230\u4e3b\u673a\u4fa7\uff0c\u518d\u7531\u4e3b\u673a\u4fa7\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u76ee\u6807 Pod \u770b\u89c1\u6570\u636e\u5305\u7684\u6e90 IP \u4e3a \u6e90 Pod \u7684 Underlay IP\uff0c\u76f4\u63a5\u8d70 Underlay \u7f51\u7edc\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u6e90 Pod \u6240\u5728\u4e3b\u673a\u3002 \u5728\u8be5\u4e3b\u673a\u770b\u6765\u8fd9\u662f\u4e00\u4e2a\u975e\u6cd5\u7684\u6570\u636e\u5305(\u610f\u5916\u7684\u6536\u5230 TCP \u7684\u7b2c\u4e8c\u6b21\u63e1\u624b\u62a5\u6587\uff0c\u8ba4\u4e3a\u662f conntrack table invalid), \u6240\u4ee5\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u663e\u5f0f\u7684 drop \u3002 \u76ee\u524d\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 kube-proxy \u7684\u6a21\u5f0f\u4e3a ipvs \u89c4\u907f\u3002 Overlay \u6a21\u5f0f\u4e0b, \u5f53 Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\u3002\u5982\u679c\u96c6\u7fa4\u7684\u7f3a\u7701CNI \u4e3a Cilium, Pod \u7684 underlay \u7f51\u5361 \u65e0\u6cd5\u4e0e\u8282\u70b9\u901a\u4fe1\u3002 \u6211\u4eec\u501f\u52a9\u7f3a\u7701CNI\u521b\u5efa Veth \u8bbe\u5907\uff0c\u5b9e\u73b0 Pod \u7684 underlay IP \u4e0e\u8282\u70b9\u901a\u4fe1(\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0cmacvlan \u5728 bridge \u6a21\u5f0f\u4e0b\uff0c \u5176\u7236\u5b50\u63a5\u53e3\u65e0\u6cd5\u76f4\u63a5)\uff0c\u4f46 Cilium \u4e0d\u5141\u8bb8\u975e Cilium \u5b50\u7f51\u7684 IP \u4ece Veth \u8bbe\u5907\u8f6c\u53d1\u3002","title":"Coordinator"},{"location":"usage/coordinator-zh_CN/#coordinator","text":"Spiderpool \u5185\u7f6e\u4e00\u4e2a\u53eb coordinator \u7684 CNI meta-plugin, \u5b83\u5728 Main CNI\u88ab\u8c03\u7528\u4e4b\u540e\u518d\u5de5\u4f5c\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u529f\u80fd: \u89e3\u51b3 Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u5728 Pod \u591a\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u8def\u7531\uff0c\u786e\u4fdd\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e00\u81f4 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecd coordinator \u5982\u4f55\u89e3\u51b3\u6216\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u3002 \u6ce8\u610f: \u5982\u679c\u60a8\u901a\u8fc7 SpinderMultusConfig CR \u5e2e\u52a9\u521b\u5efa NetworkAttachmentDefinition CR\uff0c\u60a8\u53ef\u4ee5\u5728 SpinderMultusConfig \u4e2d\u914d\u7f6e coordinator (\u6240\u6709\u5b57\u6bb5)\u3002\u53c2\u8003: SpinderMultusConfig \u3002 Spidercoordinators CR \u4f5c\u4e3a coordinator \u63d2\u4ef6\u7684\u5168\u5c40\u7f3a\u7701\u914d\u7f6e(\u6240\u6709\u5b57\u6bb5)\uff0c\u5176\u4f18\u5148\u7ea7\u4f4e\u4e8e NetworkAttachmentDefinition CR \u4e2d\u7684\u914d\u7f6e\u3002 \u5982\u679c\u5728 NetworkAttachmentDefinition CR \u672a\u914d\u7f6e, \u5c06\u4f7f\u7528 Spidercoordinators CR \u4f5c\u4e3a\u7f3a\u7701\u503c\u3002\u66f4\u591a\u8be6\u60c5\u53c2\u8003: Spidercoordinator \u3002","title":"Coordinator"},{"location":"usage/coordinator-zh_CN/#underlay-pod-clusterip","text":"\u6211\u4eec\u5728\u4f7f\u7528\u4e00\u4e9b\u5982 Macvlan\u3001IPvlan\u3001Sriov \u7b49 Underlay CNI\u65f6\uff0c\u4f1a\u9047\u5230\u5176 Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8fd9\u5e38\u5e38\u662f\u56e0\u4e3a underlay pod \u8bbf\u95ee CLusterIP \u9700\u8981\u7ecf\u8fc7\u5728\u4ea4\u6362\u673a\u7684\u7f51\u5173\uff0c\u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 ClusterIP \u7684\u8def\u7531\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bbf\u95ee\u3002","title":"\u89e3\u51b3 Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898"},{"location":"usage/coordinator-zh_CN/#coordinator-underlay","text":"\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u7f51\u5361\u662f\u5426\u662f eth0 , \u5982\u679c\u662f\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a Underlay \u6a21\u5f0f\u3002 \u5982\u679c\u5f53\u524d\u7f51\u5361\u4e0d\u662f eth0 \uff0c\u90a3\u4e48 coordinator \u5c06\u68c0\u6d4b Pod \u4e2d\u662f\u5426\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5982\u679c\u662f\uff0c\u5219\u5224\u65ad\u4e3a Underlay \u6a21\u5f0f\u3002 \u5f53\u60a8\u7684\u4e1a\u52a1\u90e8\u7f72\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u6216\u8005 IAAS \u73af\u5883\u4e0a\u65f6\uff0c\u4e1a\u52a1 Pod \u7684 IP \u5730\u5740\u53ef\u80fd\u76f4\u63a5\u4ece\u5bbf\u4e3b\u673a\u7684 IP \u5b50\u7f51\u5206\u914d\u3002\u5e94\u7528 Pod \u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u3002 \u8be5\u6a21\u5f0f\u7684\u4f18\u70b9\u6709: \u6ca1\u6709 NAT \u6620\u5c04\u7684\u5e72\u6270\uff0cPod \u6700\u5927\u7a0b\u5ea6\u4fdd\u7559\u6e90 IP \u80fd\u591f\u5229\u7528\u5e95\u5c42\u7f51\u7edc\u8bbe\u5907\u5bf9 Pod \u505a\u8bbf\u95ee\u63a7\u5236 \u4e0d\u4f9d\u8d56\u96a7\u9053\u6280\u672f\uff0cPod \u7f51\u7edc\u901a\u4fe1\u7684\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u9ad8 \u4f7f\u7528\u6ce8\u610f: Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf(ClusterIP)\u65f6\uff0c\u6d41\u91cf\u4f1a\u5148\u8df3\u7ed9\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u672c\u5730\u5bbf\u4e3b\u673a\u4f7f\u7528\u81ea\u5df1\u7684IP\u53bb\u8bbf\u95ee\u76ee\u7684pod\uff0c\u56e0\u6b64\uff0c\u6d41\u91cf\u7684\u8f6c\u53d1\u53ef\u80fd\u501f\u52a9\u5916\u90e8\u8def\u7531\u5668\u8fdb\u884c\u4e09\u5c42\u8df3\u8f6c\uff0c\u56e0\u6b64\uff0c\u8bf7\u786e\u4fdd\u76f8\u5173\u4e09\u5c42\u7f51\u7edc\u7684\u901a\u8fbe\u3002 \u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e coordinator \u8fd0\u884c\u5728 underlay \u6a21\u5f0f\u89e3\u51b3\u3002\u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u63d2\u4ef6\u5c06\u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u5c06\u4e00\u7aef\u653e\u7f6e\u4e8e\u4e3b\u673a\uff0c\u53e6\u4e00\u7aef\u653e\u7f6e\u4e0e Pod \u7684 network namespace \u4e2d\uff0c\u7136\u540e\u5728 Pod \u91cc\u9762\u8bbe\u7f6e\u4e00\u4e9b\u8def\u7531\u89c4\u5219\uff0c \u4f7f Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth \u8bbe\u5907\u8f6c\u53d1\u3002Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u76ee\u6807\u65f6\u4ece eth0 \u6216 net1 \u8f6c\u53d1\u3002 \u4e0b\u9762\u5c06\u901a\u8fc7\u4e00\u4e9b\u4f8b\u5b50\u4ecb\u7ecd: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\" } ] } mode: \u6307\u5b9a coordinator \u8fd0\u884c\u5728 underlay \u6a21\u5f0f\u3002\u6216\u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: v1.multus-cni.io/default-network: kube-system/macvlan-underlay , coordinator \u5c06\u4f1a\u81ea\u52a8\u5224\u65ad mode \u4e3a underlay\u3002 \u5f53\u4ee5 macvlan-underlay \u521b\u5efa Pod\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 \u662f\u8282\u70b9\u7684 IP,\u786e\u4fdd Pod \u8bbf\u95ee\u672c\u8282\u70b9\u65f6\u4ece veth0 \u8f6c\u53d1\u3002 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 \u662f\u96c6\u7fa4 Service \u7684 CIDR, \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth0 \u8f6c\u53d1\u3002 \u8fd9\u4e2a\u65b9\u6848\u5f3a\u70c8\u4f9d\u8d56\u4e0e kube-proxy \u7684 MASQUERADE \u3002 \u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e kube-proxy \u7684 masqueradeAll \u4e3a true\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u4e0d\u540c\uff0c \u65e0\u9700\u5f00\u542f masqueradeAll , \u5b83\u4eec\u4e4b\u95f4\u7684\u8bbf\u95ee\u5c06\u4f1a\u88ab SNAT\u3002\u4f46\u5982\u679c Pod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u76f8\u540c\uff0c\u90a3\u6211\u4eec\u5fc5\u987b\u8981\u8bbe\u7f6e masqueradeAll \u4e3a true.","title":"\u914d\u7f6e coordinator \u8fd0\u884c\u5728 underlay \u6a21\u5f0f"},{"location":"usage/coordinator-zh_CN/#coordinator-overlay","text":"\u4e0e Underlay \u6a21\u5f0f\u76f8\u5bf9\u5e94\uff0c\u6211\u4eec\u6709\u65f6\u5019\u5e76\u4e0d\u5173\u5fc3\u96c6\u7fa4\u90e8\u7f72\u73af\u5883\u7684\u5e95\u5c42\u7f51\u7edc\u662f\u4ec0\u4e48\uff0c\u6211\u4eec\u5e0c\u671b\u96c6\u7fa4\u80fd\u591f\u8fd0\u884c\u5728\u5927\u591a\u6570\u7684\u5e95\u5c42\u7f51\u7edc\u3002\u5e38\u5e38\u4f1a\u7528\u5230\u5982 Calico \u548c Cilium \u7b49CNI, \u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u8c03\u7528\u7f51\u5361\u662f\u5426\u4e0d\u662f eth0 \u3002\u5982\u679c\u4e0d\u662f\uff0c\u786e\u8ba4 Pod \u4e2d\u4e0d\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a overlay \u6a21\u5f0f\u3002 \u6b64\u6a21\u5f0f\u7684\u4f18\u70b9\u6709: IP \u5730\u5740\u5145\u6c9b\uff0c\u51e0\u4e4e\u4e0d\u5b58\u5728\u5730\u5740\u77ed\u7f3a\u7684\u95ee\u9898 \u517c\u5bb9\u6027\u5f3a\uff0c\u80fd\u591f\u8fd0\u884c\u5728\u5927\u591a\u6570\u5e95\u5c42\u7f51\u7edc\u4e0b \u4f46\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u5b58\u5728\u4ee5\u4e0b\u7684\u95ee\u9898: \u56e0\u4e3a calico \u4f1a\u7528\u5230\u4e00\u4e9b\u5c01\u88c5\u6280\u672f\uff0c\u6027\u80fd\u4f1a\u5f97\u5230\u5f71\u54cd \u5927\u591a\u90fd\u4e0d\u652f\u6301 Pod \u7684 IP \u5730\u5740\u56fa\u5b9a \u5982\u679c\u901a\u8fc7 Multus \u4e3a Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\uff0cPod \u901a\u4fe1\u65f6\u5e38\u5e38\u9047\u5230\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4 Pod \u65e0\u6cd5\u6b63\u5e38\u901a\u4fe1 \u5f53 Pod \u9644\u52a0\u4e86\u591a\u5f20\u7f51\u5361\u65f6\uff0c\u6781\u5927\u53ef\u80fd\u51fa\u73b0 Pod \u901a\u4fe1\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u5982\u679c\u6570\u636e\u94fe\u8def\u4e0a\u5b58\u5728\u4e00\u4e9b\u5b89\u5168\u8bbe\u5907\uff0c\u7531\u4e8e\u6570\u636e\u5305\u7684\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\uff0c\u6d41\u91cf\u53ef\u80fd\u88ab\u5b89\u5168\u8bbe\u5907\u8ba4\u4e3a\u662f \"\u534a\u8fde\u63a5\"(\u6ca1\u6709 TCP SYN \u62a5\u6587\u7684\u8bb0\u5f55\uff0c\u4f46\u6536\u5230 TCP ACK \u62a5\u6587 )\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b89\u5168\u8bbe\u5907\u4f1a\u963b\u65ad\u6389\u8be5\u8fde\u63a5\uff0c\u9020\u6210 Pod \u901a\u4fe1\u5f02\u5e38\u3002 \u4e0a\u8ff0\u95ee\u9898\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f\u89e3\u51b3\u3002 \u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u4e0d\u4f1a\u521b\u5efa veth \u8bbe\u5907\uff0c\u800c\u662f\u8bbe\u7f6e\u4e00\u4e9b\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece eth0(\u901a\u5e38\u7531 Calico\u3001Cilium\u7b49CNI\u521b\u5efa) \u8f6c\u53d1\uff0cPod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 (\u901a\u5e38\u7531 Macvlan\u3001IPvlan \u7b49CNI\u521b\u5efa) \u8f6c\u53d1\u3002 \u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u4f8b\u5b50\u6765\u4ecb\u7ecd\u8fd9\u4e2a\u6a21\u5f0f: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-overlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-overlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"overlay\" } ] } mode: \u6307\u5b9a coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f\u3002\u6216\u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-overlay \uff0ccoordinator \u5c06\u4f1a\u81ea\u52a8\u5224\u65ad mode \u4e3a overlay\u3002 \u5f53\u4ee5 macvlan-overlay \u521b\u5efa Pod\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip rule 0 : from all lookup local 32759 : from 10 .233.105.154 lookup 100 32761 : from all to 169 .254.1.1 lookup 100 32762 : from all to 10 .233.64.0/18 lookup 100 32763 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.102 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 # ip r show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link \u8fd9\u4e9b\u7b56\u7565\u8def\u7531: 32759: from 10.233.105.154 lookup 100 : \u786e\u4fdd\u4ece eth0(calico \u7f51\u5361)\u53d1\u51fa\u7684\u6570\u636e\u5305\u8d70table 100 32762: from all to 10.233.64.0/18 lookup 100 : \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u8d70 table 100\uff0c\u4ece eth0 \u8f6c\u53d1\u51fa\u53bb\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cnet1 \u7684\u6240\u6709\u5b50\u7f51\u8def\u7531\u4fdd\u7559\u5728 Main \u8868; eth0 \u7684\u5b50\u7f51\u8def\u7531\u4fdd\u7559\u5728 Table 100\u3002 \u5728 Overlay \u6a21\u5f0f\u4e0b, Pod \u8bbf\u95ee ClusterIP \u5b8c\u5168\u501f\u52a9\u4e8e Overlay CNI \u7684\u7f51\u5361(eth0)\uff0c\u65e0\u9700\u5176\u4ed6\u7279\u6b8a\u914d\u7f6e\u3002","title":"\u914d\u7f6e coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f"},{"location":"usage/coordinator-zh_CN/#pod-ip","text":"\u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff0c\u652f\u6301\u68c0\u6d4b IPv4 \u548c IPv6 \u5730\u5740\u3002\u6211\u4eec\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\uff0c\u5982\u679c\u53d1\u73b0\u56de\u590d\u62a5\u6587\u7684 Mac \u5730\u5740\u4e0d\u662f Pod \u672c\u8eab\uff0c\u90a3\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e2a IP \u662f\u51b2\u7a81\u7684\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u914d\u7f6e: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\", \"detectGateway\": false, \"detectIPConflict\": true } ] } \u5982\u679c\u53d1\u73b0 Pod \u7684 IP \u662f\u51b2\u7a81\uff0c\u90a3\u4e48 Pod \u5c06\u4f1a\u521b\u5efa\u5931\u8d25\u3002\u5728 Pod \u7684 Event \u4e8b\u4ef6\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u51b2\u7a81 IP \u7684\u4e3b\u673a\u7684\u7269\u7406\u5730\u5740\u3002","title":"\u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81"},{"location":"usage/coordinator-zh_CN/#pod","text":"\u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe\uff0c\u652f\u6301\u68c0\u6d4b IPv4 \u548c IPv6 \u7684\u7f51\u5173\u5730\u5740\u3002\u6211\u4eec\u901a\u8fc7\u53d1\u9001 ICMP \u62a5\u6587\uff0c\u63a2\u6d4b\u7f51\u5173\u5730\u5740\u662f\u5426\u53ef\u8fbe\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u914d\u7f6e: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\", \"detectGateway\": true } ] } \u5982\u679c\u53d1\u73b0 Pod \u7684\u7f51\u5173\u4e0d\u53ef\u8fbe\uff0c\u90a3\u4e48 Pod \u5c06\u4f1a\u521b\u5efa\u5931\u8d25\u3002\u5728 Pod \u7684 Event \u4e8b\u4ef6\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709 Pod \u7684\u7f51\u5173\u4e0d\u53ef\u8fbe\u7684\u7c7b\u4f3c\u9519\u8bef\u3002","title":"\u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe"},{"location":"usage/coordinator-zh_CN/#pod-mac","text":"\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 coordinator \u63d2\u4ef6\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00\uff0cPod \u7684 Mac \u5730\u5740\u5c06\u7531\u914d\u7f6e\u7684 Mac \u5730\u5740\u524d\u7f00 + Pod's IP \u7ec4\u6210\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u914d\u7f6e: apiVersion : k8s.cni.cncf.io/v1 kind : NetworkAttachmentDefinition metadata : name : macvlan-underlay namespace : kube-system spec : config : |- { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-underlay\", \"plugins\": [ { \"type\": \"macvlan\", \"master\": \"ens160\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" } },{ \"type\": \"coordinator\", \"mode\": \"underlay\", \"podMACPrefix\": \"0a:1b\" } ] } \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u6211\u4eec\u53ef\u4ee5\u68c0\u6d4b Pod \u7684 Mac \u5730\u5740\u7684\u524d\u7f00\u662f\u5426\u662f \"0a:1b\"","title":"\u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00"},{"location":"usage/coordinator-zh_CN/#_1","text":"Underlay \u6a21\u5f0f\u4e0b\uff0cunderlay Pod \u4e0e Overlay Pod(calico or cilium) \u8fdb\u884c TCP \u901a\u4fe1\u5931\u8d25 \u6b64\u95ee\u9898\u662f\u56e0\u4e3a\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\uff0c\u53d1\u51fa\u7684\u8bf7\u6c42\u62a5\u6587\u5339\u914d\u6e90Pod \u4fa7\u7684\u8def\u7531\uff0c\u4f1a\u901a\u8fc7 veth0 \u8f6c\u53d1\u5230\u4e3b\u673a\u4fa7\uff0c\u518d\u7531\u4e3b\u673a\u4fa7\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u76ee\u6807 Pod \u770b\u89c1\u6570\u636e\u5305\u7684\u6e90 IP \u4e3a \u6e90 Pod \u7684 Underlay IP\uff0c\u76f4\u63a5\u8d70 Underlay \u7f51\u7edc\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u6e90 Pod \u6240\u5728\u4e3b\u673a\u3002 \u5728\u8be5\u4e3b\u673a\u770b\u6765\u8fd9\u662f\u4e00\u4e2a\u975e\u6cd5\u7684\u6570\u636e\u5305(\u610f\u5916\u7684\u6536\u5230 TCP \u7684\u7b2c\u4e8c\u6b21\u63e1\u624b\u62a5\u6587\uff0c\u8ba4\u4e3a\u662f conntrack table invalid), \u6240\u4ee5\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u663e\u5f0f\u7684 drop \u3002 \u76ee\u524d\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 kube-proxy \u7684\u6a21\u5f0f\u4e3a ipvs \u89c4\u907f\u3002 Overlay \u6a21\u5f0f\u4e0b, \u5f53 Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\u3002\u5982\u679c\u96c6\u7fa4\u7684\u7f3a\u7701CNI \u4e3a Cilium, Pod \u7684 underlay \u7f51\u5361 \u65e0\u6cd5\u4e0e\u8282\u70b9\u901a\u4fe1\u3002 \u6211\u4eec\u501f\u52a9\u7f3a\u7701CNI\u521b\u5efa Veth \u8bbe\u5907\uff0c\u5b9e\u73b0 Pod \u7684 underlay IP \u4e0e\u8282\u70b9\u901a\u4fe1(\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0cmacvlan \u5728 bridge \u6a21\u5f0f\u4e0b\uff0c \u5176\u7236\u5b50\u63a5\u53e3\u65e0\u6cd5\u76f4\u63a5)\uff0c\u4f46 Cilium \u4e0d\u5141\u8bb8\u975e Cilium \u5b50\u7f51\u7684 IP \u4ece Veth \u8bbe\u5907\u8f6c\u53d1\u3002","title":"\u5df2\u77e5\u95ee\u9898"},{"location":"usage/coordinator/","text":"","title":"Plugin coordinator"},{"location":"usage/debug/","text":"Q&A TODO.","title":"FAQ"},{"location":"usage/debug/#qa","text":"TODO.","title":"Q&amp;A"},{"location":"usage/gc-zh_CN/","text":"\u56de\u6536 IP English | \u7b80\u4f53\u4e2d\u6587 \u4ecb\u7ecd \u5728 Kubernetes \u4e2d\uff0c\u5783\u573e\u56de\u6536\uff08Garbage Collection\uff0c\u7b80\u79f0GC\uff09\u5bf9\u4e8e IP \u5730\u5740\u7684\u56de\u6536\u975e\u5e38\u91cd\u8981\u3002IP \u5730\u5740\u7684\u53ef\u7528\u6027\u5173\u7cfb\u5230 Pod \u662f\u5426\u80fd\u591f\u542f\u52a8\u6210\u529f\u3002GC \u673a\u5236\u53ef\u4ee5\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u4e0d\u518d\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u548c IP \u5730\u5740\u7684\u8017\u5c3d\u3002\u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u4f18\u79c0\u7684 GC \u80fd\u529b\u3002 \u9879\u76ee\u529f\u80fd \u5728 IPAM \u4e2d\u8bb0\u5f55\u4e86\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u4f46\u662f\u8fd9\u4e9b Pod \u5728 Kubernetes \u96c6\u7fa4\u4e2d\u5df2\u7ecf\u4e0d\u590d\u5b58\u5728\uff0c\u8fd9\u4e9b IP \u53ef\u79f0\u4e3a \u50f5\u5c38 IP \uff0cSpiderpool \u53ef\u9488\u5bf9 \u50f5\u5c38 IP \u8fdb\u884c\u56de\u6536\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u5728\u96c6\u7fa4\u4e2d delete Pod \u65f6\uff0c\u4f46\u7531\u4e8e \u7f51\u7edc\u5f02\u5e38 \u6216 cni \u4e8c\u8fdb\u5236 crash \u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8c03\u7528 cni delete \u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4 IP \u5730\u5740\u65e0\u6cd5\u88ab cni \u56de\u6536\u3002 \u5728 cni delete \u5931\u8d25 \u7b49\u6545\u969c\u573a\u666f\uff0c\u5982\u679c\u4e00\u4e2a\u66fe\u7ecf\u5206\u914d\u4e86 IP \u7684 Pod \u88ab\u9500\u6bc1\u540e\uff0c\u4f46\u5728 IPAM \u4e2d\u8fd8\u8bb0\u5f55\u5206\u914d\u7740IP \u5730\u5740\uff0c\u5f62\u6210\u4e86\u50f5\u5c38 IP \u7684\u73b0\u8c61\u3002Spiderpool \u9488\u5bf9\u8fd9\u79cd\u95ee\u9898\uff0c\u4f1a\u57fa\u4e8e\u5468\u671f\u548c\u4e8b\u4ef6\u626b\u63cf\u673a\u5236\uff0c\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u50f5\u5c38 IP \u5730\u5740\u3002 \u8282\u70b9\u610f\u5916\u5b95\u673a\u540e\uff0c\u96c6\u7fa4\u4e2d\u7684 Pod \u6c38\u4e45\u5904\u4e8e deleting \u72b6\u6001\uff0cPod \u5360\u7528\u7684 IP \u5730\u5740\u65e0\u6cd5\u88ab\u91ca\u653e\u3002 \u5bf9\u5904\u4e8e Terminating \u72b6\u6001\u7684 Pod\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond \u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED \u6765\u63a7\u5236\u3002\u8be5\u80fd\u529b\u80fd\u591f\u7528\u4ee5\u89e3\u51b3 \u8282\u70b9\u610f\u5916\u5b95\u673a \u7684\u6545\u969c\u573a\u666f\u3002","title":"\u56de\u6536 IP"},{"location":"usage/gc-zh_CN/#ip","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"\u56de\u6536 IP"},{"location":"usage/gc-zh_CN/#_1","text":"\u5728 Kubernetes \u4e2d\uff0c\u5783\u573e\u56de\u6536\uff08Garbage Collection\uff0c\u7b80\u79f0GC\uff09\u5bf9\u4e8e IP \u5730\u5740\u7684\u56de\u6536\u975e\u5e38\u91cd\u8981\u3002IP \u5730\u5740\u7684\u53ef\u7528\u6027\u5173\u7cfb\u5230 Pod \u662f\u5426\u80fd\u591f\u542f\u52a8\u6210\u529f\u3002GC \u673a\u5236\u53ef\u4ee5\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u4e0d\u518d\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u548c IP \u5730\u5740\u7684\u8017\u5c3d\u3002\u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u4f18\u79c0\u7684 GC \u80fd\u529b\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/gc-zh_CN/#_2","text":"\u5728 IPAM \u4e2d\u8bb0\u5f55\u4e86\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u4f46\u662f\u8fd9\u4e9b Pod \u5728 Kubernetes \u96c6\u7fa4\u4e2d\u5df2\u7ecf\u4e0d\u590d\u5b58\u5728\uff0c\u8fd9\u4e9b IP \u53ef\u79f0\u4e3a \u50f5\u5c38 IP \uff0cSpiderpool \u53ef\u9488\u5bf9 \u50f5\u5c38 IP \u8fdb\u884c\u56de\u6536\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u5728\u96c6\u7fa4\u4e2d delete Pod \u65f6\uff0c\u4f46\u7531\u4e8e \u7f51\u7edc\u5f02\u5e38 \u6216 cni \u4e8c\u8fdb\u5236 crash \u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8c03\u7528 cni delete \u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4 IP \u5730\u5740\u65e0\u6cd5\u88ab cni \u56de\u6536\u3002 \u5728 cni delete \u5931\u8d25 \u7b49\u6545\u969c\u573a\u666f\uff0c\u5982\u679c\u4e00\u4e2a\u66fe\u7ecf\u5206\u914d\u4e86 IP \u7684 Pod \u88ab\u9500\u6bc1\u540e\uff0c\u4f46\u5728 IPAM \u4e2d\u8fd8\u8bb0\u5f55\u5206\u914d\u7740IP \u5730\u5740\uff0c\u5f62\u6210\u4e86\u50f5\u5c38 IP \u7684\u73b0\u8c61\u3002Spiderpool \u9488\u5bf9\u8fd9\u79cd\u95ee\u9898\uff0c\u4f1a\u57fa\u4e8e\u5468\u671f\u548c\u4e8b\u4ef6\u626b\u63cf\u673a\u5236\uff0c\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u50f5\u5c38 IP \u5730\u5740\u3002 \u8282\u70b9\u610f\u5916\u5b95\u673a\u540e\uff0c\u96c6\u7fa4\u4e2d\u7684 Pod \u6c38\u4e45\u5904\u4e8e deleting \u72b6\u6001\uff0cPod \u5360\u7528\u7684 IP \u5730\u5740\u65e0\u6cd5\u88ab\u91ca\u653e\u3002 \u5bf9\u5904\u4e8e Terminating \u72b6\u6001\u7684 Pod\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond \u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED \u6765\u63a7\u5236\u3002\u8be5\u80fd\u529b\u80fd\u591f\u7528\u4ee5\u89e3\u51b3 \u8282\u70b9\u610f\u5916\u5b95\u673a \u7684\u6545\u969c\u573a\u666f\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/gc/","text":"Reclaim IP English | \u7b80\u4f53\u4e2d\u6587 Introduce In Kubernetes, garbage collection (Garbage Collection, GC for short) is very important for the recycling of IP addresses. The availability of IP addresses is critical to whether a Pod can start successfully. The GC mechanism can automatically reclaim these unused IP addresses, avoiding waste of resources and exhaustion of IP addresses. This article will introduce Spiderpool's excellent GC capabilities. Project Functions The IP addresses assigned to Pods are recorded in IPAM, but these Pods no longer exist in the Kubernetes cluster. These IPs can be called zombie IPs . Spiderpool can recycle zombie IPs . Its implementation principle is as follows : When deleting Pod in the cluster, but due to problems such as network exception or cni binary crash , the call to cni delete fails, resulting in the IP address not being reclaimed by cni. In failure scenarios such as cni delete failure , if a Pod that has been assigned an IP is destroyed, but the IP address is still recorded in the IPAM, a phenomenon of zombie IP is formed. For this kind of problem, Spiderpool will automatically recycle these zombie IP addresses based on the cycle and event scanning mechanism. After a node goes down unexpectedly, the Pod in the cluster is permanently in the deleting state, and the IP address occupied by the Pod cannot be released. For a Pod in Terminating state, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond . This feature can be controlled by the environment variable SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED . This capability can be used to solve the failure scenario of unexpected node downtime .","title":"Reclaim IP"},{"location":"usage/gc/#reclaim-ip","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Reclaim IP"},{"location":"usage/gc/#introduce","text":"In Kubernetes, garbage collection (Garbage Collection, GC for short) is very important for the recycling of IP addresses. The availability of IP addresses is critical to whether a Pod can start successfully. The GC mechanism can automatically reclaim these unused IP addresses, avoiding waste of resources and exhaustion of IP addresses. This article will introduce Spiderpool's excellent GC capabilities.","title":"Introduce"},{"location":"usage/gc/#project-functions","text":"The IP addresses assigned to Pods are recorded in IPAM, but these Pods no longer exist in the Kubernetes cluster. These IPs can be called zombie IPs . Spiderpool can recycle zombie IPs . Its implementation principle is as follows : When deleting Pod in the cluster, but due to problems such as network exception or cni binary crash , the call to cni delete fails, resulting in the IP address not being reclaimed by cni. In failure scenarios such as cni delete failure , if a Pod that has been assigned an IP is destroyed, but the IP address is still recorded in the IPAM, a phenomenon of zombie IP is formed. For this kind of problem, Spiderpool will automatically recycle these zombie IP addresses based on the cycle and event scanning mechanism. After a node goes down unexpectedly, the Pod in the cluster is permanently in the deleting state, and the IP address occupied by the Pod cannot be released. For a Pod in Terminating state, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond . This feature can be controlled by the environment variable SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED . This capability can be used to solve the failure scenario of unexpected node downtime .","title":"Project Functions"},{"location":"usage/ifacer-zh_CN/","text":"CNI meta-plugin: ifacer English | \u7b80\u4f53\u4e2d\u6587 \u4ecb\u7ecd \u5f53 Pod \u4f7f\u7528 Vlan \u7f51\u7edc\u65f6\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\u7f51\u7edc\u7ba1\u7406\u5458\u63d0\u524d\u5728\u8282\u70b9\u914d\u7f6e\u5404\u79cd Vlan \u6216 Bond \u63a5\u53e3, \u8fd9\u4e00\u8fc7\u7a0b\u9700\u8981\u4eba\u5de5\u53c2\u4e0e\uff0c\u914d\u7f6e\u7e41\u7410\u4e14\u6613\u51fa\u9519\u3002Spiderpool \u63d0\u4f9b\u4e00\u4e2a\u540d\u4e3a ifacer \u7684 CNI meta-plugins, \u8be5\u63d2\u4ef6\u53ef\u5728\u521b\u5efa Pod \u65f6\uff0c\u6839\u636e\u63d0\u4f9b\u7684 ifacer \u63d2\u4ef6\u914d\u7f6e, \u52a8\u6001\u7684\u5728\u8282\u70b9\u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u6216 Bond \u63a5\u53e3\uff0c\u6781\u5927\u7684\u7b80\u5316\u4e86\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u914d\u7f6e\u5de5\u4f5c\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4f1a\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e2a\u63d2\u4ef6\u3002 \u529f\u80fd \u8be5\u63d2\u4ef6\u652f\u6301: \u652f\u6301\u52a8\u6001\u7684\u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u652f\u6301\u52a8\u6001\u7684\u521b\u5efa Bond \u63a5\u53e3 \u6ce8\u610f: \u901a\u8fc7\u8be5\u63d2\u4ef6\u521b\u5efa\u7684 Vlan/Bond \u63a5\u53e3\uff0c\u5f53\u8282\u70b9\u91cd\u542f\u65f6\u4f1a\u4e22\u5931\uff0c\u4f46 Pod \u91cd\u542f\u540e\u4f1a\u81ea\u52a8\u521b\u5efa \u63d2\u4ef6\u4e0d\u652f\u6301\u5220\u9664\u5df2\u521b\u5efa\u7684 Vlan/Bond \u63a5\u53e3 \u63d2\u4ef6\u4e0d\u652f\u6301\u5728\u521b\u5efa\u65f6\u914d\u7f6e Vlan/Bond \u63a5\u53e3\u7684\u5730\u5740 \u4f7f\u7528\u8981\u6c42 \u4f7f\u7528\u8be5\u63d2\u4ef6\u65e0\u7279\u6b8a\u7684\u9650\u5236: \u5305\u62ec Kubernetes\u3001Kernel \u7248\u672c\u7b49\u3002\u5b89\u88c5 Spiderpool \u65f6\uff0c\u4f1a\u81ea\u52a8\u5b89\u88c5\u8be5\u63d2\u4ef6\u5230\u6bcf\u4e2a\u4e3b\u673a\u7684 /opt/cni/bin/ \u8def\u5f84\u4e0b, \u60a8\u53ef\u4ee5\u901a\u8fc7\u68c0\u6d4b\u6bcf\u4e2a\u4e3b\u673a\u8be5\u8def\u5f84\u4e0b\u662f\u5426\u5b58\u5728 ifacer \u4e8c\u8fdb\u5236\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u3002 \u5982\u4f55\u4f7f\u7528 \u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u4f8b\u5b50\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 ifacer \u63d2\u4ef6\u3002\u5728\u4ecb\u7ecd\u4f8b\u5b50\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e24\u4e2a IP \u6c60\u7528\u4e8e\u6d4b\u8bd5: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: vlan100 spec: gateway: 172 .100.0.1 ipVersion: 4 ips: - 172 .100.0.100-172.100.0.200 subnet: 172 .100.0.0/16 vlan: 100 EOF vlan100 \u7528\u4e8e\u52a8\u6001\u521b\u5efa Vlan \u5b50\u63a5\u53e3\u7684\u4f8b\u5b50 [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: vlan200 spec: gateway: 172 .200.0.1 ipVersion: 4 ips: - 172 .200.0.100-172.200.0.200 subnet: 172 .200.0.0/16 vlan: 200 EOF vlan200 \u7528\u4e8e\u52a8\u6001\u521b\u5efa Bond \u63a5\u53e3\u7684\u4f8b\u5b50 \u52a8\u6001\u7684\u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u6211\u4eec\u901a\u8fc7 SpiderMultusConfig \u4e3a ifacer \u751f\u6210\u4e00\u4e2a\u5bf9\u5e94\u7684 Multus network-attachment-definition \u914d\u7f6e: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay macvlan: master: - ens192 vlanID: 100 ippools: ipv4: - vlan100 EOF spidermultusconfig.spiderpool.spidernet.io/macvlan-conf created \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: [ root@controller1 ~ ] # kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o=jsonpath='{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-conf1\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"macvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" : [ \"vlan100\" ] } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u4ece\u914d\u7f6e\u53ef\u4ee5\u770b\u5230: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0cifacer \u5c06\u57fa\u4e8e ens192 \u521b\u5efa\u4e00\u4e2a vlan tag \u4e3a: 100 \u7684\u5b50\u63a5\u53e3 main cni: macvlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: ens192.100 , \u4e5f\u5c31\u662f\u901a\u8fc7 ifacer \u521b\u5efa\u7684 Vlan \u5b50\u63a5\u53e3: ens192.100 \u5982\u679c\u8282\u70b9\u4e0a\u5df2\u7ecf\u521b\u5efa\u597d Vlan \u5b50\u63a5\u53e3\uff0c\u4e0d\u9700\u8981\u4f7f\u7528 ifacer \u3002\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u914d\u7f6e master \u5b57\u6bb5\u4e3a: ens192.100 \uff0c\u5e76\u4e14\u4e0d\u914d\u7f6e vlanID \u5373\u53ef, \u5982\u4e0b: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100 \u63a5\u4e0b\u6765\u6211\u4eec\u521b\u5efa\u6d4b\u8bd5\u7684\u4e1a\u52a1 Pod: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: v1.multus-cni.io/default-network: macvlan-conf labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5f53 Pod \u6210\u529f\u7684\u88ab\u521b\u5efa\uff0c\u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u8be5\u8282\u70b9\u4e00\u4e2a\u540d\u4e3a ens192.100 \u7684 Vlan \u5b50\u63a5\u53e3\u88ab\u6210\u529f\u521b\u5efa: [ root@controller1 ~ ] # kubectl get po -o wide | grep nginx nginx-5d6dc85ff4-gdvkh 1 /1 Running 0 32s 172 .100.0.163 worker1 <none> <none> [ root@worker1 ~ ] # ip l show type vlan 135508 : ens192.100@ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 00 :50:56:b4:3c:82 brd ff:ff:ff:ff:ff:ff \u7ecf\u8fc7\u6d4b\u8bd5\uff0cPod \u5404\u79cd\u8fde\u901a\u6027\u6b63\u5e38\u3002 \u52a8\u6001\u7684\u521b\u5efa Bond \u63a5\u53e3 \u6211\u4eec\u901a\u8fc7 SpiderMultusConfig \u4e3a ifacer \u751f\u6210\u4e00\u4e2a\u5bf9\u5e94\u7684 Multus network-attachment-definition \u914d\u7f6e: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay macvlan: master: - ens192 - ens160 vlanID: 200 ippools: ipv4: - vlan200 bond: name: bond0 mode: 1 options: \"\" EOF spidermultusconfig.spiderpool.spidernet.io/macvlan-conf created \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: [ root@controller1 ~ ] # kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o jsonpath='{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens160\" ] , \"vlanID\" : 200 , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"macvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" : [ \"vlan100\" ] } , \"master\" : \"bond0.200\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u914d\u7f6e\u8bf4\u660e: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0cifacer \u5c06\u57fa\u4e8e [\"ens192\",\"ens160\"] \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 bond \u63a5\u53e3\uff0cmode \u4e3a 1(active-backup)\u3002\u7136\u540e\u518d\u57fa\u4e8e bond0 \u521b\u5efa\u540d\u4e3a bond0.200 \u7684 Vlan \u5b50\u63a5\u53e3, Vlan tag \u4e3a 200 main cni: macvlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: bond0.200 \u521b\u5efa Bond \u5982\u679c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options \u5b9e\u73b0. \u8f93\u5165\u683c\u5f0f\u4e3a: \"primary=ens160;arp_interval=1\",\u591a\u4e2a\u53c2\u6570\u7528\";\"\u8fde\u63a5 \u63a5\u4e0b\u6765\u6211\u4eec\u521b\u5efa\u6d4b\u8bd5\u7684\u4e1a\u52a1 Pod: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: v1.multus-cni.io/default-network: macvlan-conf labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5f53 Pod \u6210\u529f\u7684\u88ab\u521b\u5efa\uff0c\u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u8be5\u8282\u70b9\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 Bond \u63a5\u53e3 \u548c bond0.100 \u7684 Vlan \u5b50\u63a5\u53e3 \u88ab\u6210\u529f\u521b\u5efa: [ root@controller1 ~ ] # kubectl get po -o wide | grep nginx nginx-1c8wcv2sg5-jcwla 1 /1 Running 0 32s 172 .200.0.163 worker1 <none> <none> [ root@worker1 ~ ] # ip --details link show type bond 135510 : bond0: <BROADCAST,MULTICAST,MASTER,UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:8f:14 brd ff:ff:ff:ff:ff:ff promiscuity 1 bond mode active-backup miimon 0 updelay 0 downdelay 0 use_carrier 1 arp_interval 1 arp_validate none arp_all_targets any primary ens192 primary_reselect always fail_over_mac none xmit_hash_policy layer2 resend_igmp 1 num_grat_arp 1 all_slaves_active 0 min_links 0 lp_interval 1 packets_per_slave 1 lacp_rate slow ad_select stable tlb_dynamic_lb 1 addrgenmode eui64 numtxqueues 16 numrxqueues 16 gso_max_size 65536 gso_max_segs 65535 [ root@worker1 ~ ] # ip link show type vlan 135508 : bond0.100@ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 00 :50:56:b4:3c:82 brd ff:ff:ff:ff:ff:ff \u7ecf\u8fc7\u6d4b\u8bd5\uff0cPod \u5404\u79cd\u8fde\u901a\u6027\u6b63\u5e38\u3002","title":"CNI meta-plugin: ifacer"},{"location":"usage/ifacer-zh_CN/#cni-meta-plugin-ifacer","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"CNI meta-plugin: ifacer"},{"location":"usage/ifacer-zh_CN/#_1","text":"\u5f53 Pod \u4f7f\u7528 Vlan \u7f51\u7edc\u65f6\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\u7f51\u7edc\u7ba1\u7406\u5458\u63d0\u524d\u5728\u8282\u70b9\u914d\u7f6e\u5404\u79cd Vlan \u6216 Bond \u63a5\u53e3, \u8fd9\u4e00\u8fc7\u7a0b\u9700\u8981\u4eba\u5de5\u53c2\u4e0e\uff0c\u914d\u7f6e\u7e41\u7410\u4e14\u6613\u51fa\u9519\u3002Spiderpool \u63d0\u4f9b\u4e00\u4e2a\u540d\u4e3a ifacer \u7684 CNI meta-plugins, \u8be5\u63d2\u4ef6\u53ef\u5728\u521b\u5efa Pod \u65f6\uff0c\u6839\u636e\u63d0\u4f9b\u7684 ifacer \u63d2\u4ef6\u914d\u7f6e, \u52a8\u6001\u7684\u5728\u8282\u70b9\u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u6216 Bond \u63a5\u53e3\uff0c\u6781\u5927\u7684\u7b80\u5316\u4e86\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u914d\u7f6e\u5de5\u4f5c\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4f1a\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e2a\u63d2\u4ef6\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/ifacer-zh_CN/#_2","text":"\u8be5\u63d2\u4ef6\u652f\u6301: \u652f\u6301\u52a8\u6001\u7684\u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u652f\u6301\u52a8\u6001\u7684\u521b\u5efa Bond \u63a5\u53e3 \u6ce8\u610f: \u901a\u8fc7\u8be5\u63d2\u4ef6\u521b\u5efa\u7684 Vlan/Bond \u63a5\u53e3\uff0c\u5f53\u8282\u70b9\u91cd\u542f\u65f6\u4f1a\u4e22\u5931\uff0c\u4f46 Pod \u91cd\u542f\u540e\u4f1a\u81ea\u52a8\u521b\u5efa \u63d2\u4ef6\u4e0d\u652f\u6301\u5220\u9664\u5df2\u521b\u5efa\u7684 Vlan/Bond \u63a5\u53e3 \u63d2\u4ef6\u4e0d\u652f\u6301\u5728\u521b\u5efa\u65f6\u914d\u7f6e Vlan/Bond \u63a5\u53e3\u7684\u5730\u5740","title":"\u529f\u80fd"},{"location":"usage/ifacer-zh_CN/#_3","text":"\u4f7f\u7528\u8be5\u63d2\u4ef6\u65e0\u7279\u6b8a\u7684\u9650\u5236: \u5305\u62ec Kubernetes\u3001Kernel \u7248\u672c\u7b49\u3002\u5b89\u88c5 Spiderpool \u65f6\uff0c\u4f1a\u81ea\u52a8\u5b89\u88c5\u8be5\u63d2\u4ef6\u5230\u6bcf\u4e2a\u4e3b\u673a\u7684 /opt/cni/bin/ \u8def\u5f84\u4e0b, \u60a8\u53ef\u4ee5\u901a\u8fc7\u68c0\u6d4b\u6bcf\u4e2a\u4e3b\u673a\u8be5\u8def\u5f84\u4e0b\u662f\u5426\u5b58\u5728 ifacer \u4e8c\u8fdb\u5236\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u3002","title":"\u4f7f\u7528\u8981\u6c42"},{"location":"usage/ifacer-zh_CN/#_4","text":"\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u4f8b\u5b50\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 ifacer \u63d2\u4ef6\u3002\u5728\u4ecb\u7ecd\u4f8b\u5b50\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e24\u4e2a IP \u6c60\u7528\u4e8e\u6d4b\u8bd5: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: vlan100 spec: gateway: 172 .100.0.1 ipVersion: 4 ips: - 172 .100.0.100-172.100.0.200 subnet: 172 .100.0.0/16 vlan: 100 EOF vlan100 \u7528\u4e8e\u52a8\u6001\u521b\u5efa Vlan \u5b50\u63a5\u53e3\u7684\u4f8b\u5b50 [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: vlan200 spec: gateway: 172 .200.0.1 ipVersion: 4 ips: - 172 .200.0.100-172.200.0.200 subnet: 172 .200.0.0/16 vlan: 200 EOF vlan200 \u7528\u4e8e\u52a8\u6001\u521b\u5efa Bond \u63a5\u53e3\u7684\u4f8b\u5b50","title":"\u5982\u4f55\u4f7f\u7528"},{"location":"usage/ifacer-zh_CN/#vlan","text":"\u6211\u4eec\u901a\u8fc7 SpiderMultusConfig \u4e3a ifacer \u751f\u6210\u4e00\u4e2a\u5bf9\u5e94\u7684 Multus network-attachment-definition \u914d\u7f6e: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay macvlan: master: - ens192 vlanID: 100 ippools: ipv4: - vlan100 EOF spidermultusconfig.spiderpool.spidernet.io/macvlan-conf created \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: [ root@controller1 ~ ] # kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o=jsonpath='{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-conf1\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"macvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" : [ \"vlan100\" ] } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u4ece\u914d\u7f6e\u53ef\u4ee5\u770b\u5230: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0cifacer \u5c06\u57fa\u4e8e ens192 \u521b\u5efa\u4e00\u4e2a vlan tag \u4e3a: 100 \u7684\u5b50\u63a5\u53e3 main cni: macvlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: ens192.100 , \u4e5f\u5c31\u662f\u901a\u8fc7 ifacer \u521b\u5efa\u7684 Vlan \u5b50\u63a5\u53e3: ens192.100 \u5982\u679c\u8282\u70b9\u4e0a\u5df2\u7ecf\u521b\u5efa\u597d Vlan \u5b50\u63a5\u53e3\uff0c\u4e0d\u9700\u8981\u4f7f\u7528 ifacer \u3002\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u914d\u7f6e master \u5b57\u6bb5\u4e3a: ens192.100 \uff0c\u5e76\u4e14\u4e0d\u914d\u7f6e vlanID \u5373\u53ef, \u5982\u4e0b: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100 \u63a5\u4e0b\u6765\u6211\u4eec\u521b\u5efa\u6d4b\u8bd5\u7684\u4e1a\u52a1 Pod: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: v1.multus-cni.io/default-network: macvlan-conf labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5f53 Pod \u6210\u529f\u7684\u88ab\u521b\u5efa\uff0c\u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u8be5\u8282\u70b9\u4e00\u4e2a\u540d\u4e3a ens192.100 \u7684 Vlan \u5b50\u63a5\u53e3\u88ab\u6210\u529f\u521b\u5efa: [ root@controller1 ~ ] # kubectl get po -o wide | grep nginx nginx-5d6dc85ff4-gdvkh 1 /1 Running 0 32s 172 .100.0.163 worker1 <none> <none> [ root@worker1 ~ ] # ip l show type vlan 135508 : ens192.100@ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 00 :50:56:b4:3c:82 brd ff:ff:ff:ff:ff:ff \u7ecf\u8fc7\u6d4b\u8bd5\uff0cPod \u5404\u79cd\u8fde\u901a\u6027\u6b63\u5e38\u3002","title":"\u52a8\u6001\u7684\u521b\u5efa Vlan \u5b50\u63a5\u53e3"},{"location":"usage/ifacer-zh_CN/#bond","text":"\u6211\u4eec\u901a\u8fc7 SpiderMultusConfig \u4e3a ifacer \u751f\u6210\u4e00\u4e2a\u5bf9\u5e94\u7684 Multus network-attachment-definition \u914d\u7f6e: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay macvlan: master: - ens192 - ens160 vlanID: 200 ippools: ipv4: - vlan200 bond: name: bond0 mode: 1 options: \"\" EOF spidermultusconfig.spiderpool.spidernet.io/macvlan-conf created \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: [ root@controller1 ~ ] # kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o jsonpath='{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens160\" ] , \"vlanID\" : 200 , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"macvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" : [ \"vlan100\" ] } , \"master\" : \"bond0.200\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u914d\u7f6e\u8bf4\u660e: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0cifacer \u5c06\u57fa\u4e8e [\"ens192\",\"ens160\"] \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 bond \u63a5\u53e3\uff0cmode \u4e3a 1(active-backup)\u3002\u7136\u540e\u518d\u57fa\u4e8e bond0 \u521b\u5efa\u540d\u4e3a bond0.200 \u7684 Vlan \u5b50\u63a5\u53e3, Vlan tag \u4e3a 200 main cni: macvlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: bond0.200 \u521b\u5efa Bond \u5982\u679c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options \u5b9e\u73b0. \u8f93\u5165\u683c\u5f0f\u4e3a: \"primary=ens160;arp_interval=1\",\u591a\u4e2a\u53c2\u6570\u7528\";\"\u8fde\u63a5 \u63a5\u4e0b\u6765\u6211\u4eec\u521b\u5efa\u6d4b\u8bd5\u7684\u4e1a\u52a1 Pod: [ root@controller1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: v1.multus-cni.io/default-network: macvlan-conf labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5f53 Pod \u6210\u529f\u7684\u88ab\u521b\u5efa\uff0c\u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u8be5\u8282\u70b9\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 Bond \u63a5\u53e3 \u548c bond0.100 \u7684 Vlan \u5b50\u63a5\u53e3 \u88ab\u6210\u529f\u521b\u5efa: [ root@controller1 ~ ] # kubectl get po -o wide | grep nginx nginx-1c8wcv2sg5-jcwla 1 /1 Running 0 32s 172 .200.0.163 worker1 <none> <none> [ root@worker1 ~ ] # ip --details link show type bond 135510 : bond0: <BROADCAST,MULTICAST,MASTER,UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:8f:14 brd ff:ff:ff:ff:ff:ff promiscuity 1 bond mode active-backup miimon 0 updelay 0 downdelay 0 use_carrier 1 arp_interval 1 arp_validate none arp_all_targets any primary ens192 primary_reselect always fail_over_mac none xmit_hash_policy layer2 resend_igmp 1 num_grat_arp 1 all_slaves_active 0 min_links 0 lp_interval 1 packets_per_slave 1 lacp_rate slow ad_select stable tlb_dynamic_lb 1 addrgenmode eui64 numtxqueues 16 numrxqueues 16 gso_max_size 65536 gso_max_segs 65535 [ root@worker1 ~ ] # ip link show type vlan 135508 : bond0.100@ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 00 :50:56:b4:3c:82 brd ff:ff:ff:ff:ff:ff \u7ecf\u8fc7\u6d4b\u8bd5\uff0cPod \u5404\u79cd\u8fde\u901a\u6027\u6b63\u5e38\u3002","title":"\u52a8\u6001\u7684\u521b\u5efa Bond \u63a5\u53e3"},{"location":"usage/ifacer/","text":"","title":"Plugin ifacer"},{"location":"usage/ippool-affinity-namespace/","text":"Namespace affinity of IPPool Spiderpool supports affinity between IP pools and Namespaces. It means only Pods running under these Namespaces can use the IP pools that have an affinity to these Namespaces. Namespace affinity should be regarded as a filtering mechanism rather than a pool selection rule . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started First, create a new Namespace test-ns . kubectl create namespace test-ns Create an IPPool that will be bound to it. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.41 namespaceAffinity : matchLabels : kubernetes.io/metadata.name : test-ns For convenience, this example uses a native Namespace label kubernetes.io/metadata.name as the matching condition of IPPool affinity. You can replace them with desired labels to match the corresponding Namespaces. Next, create two Deployments under test-ns and default Namespaces respectively, and configure the Pods therein to get IP addresses from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml You will find that the Deployment under Namespace test-ns is running. kubectl get deploy -n test-ns NAME READY UP-TO-DATE AVAILABLE AGE test-ns-deploy 1 /1 1 1 35s And its Pod has been assigned with an IP address from that IPPool. kubectl get se -n test-ns NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-ns-deploy-74c6784f9-dlkmx eth0 test-ns-ipv4-ippool 172 .18.41.41/24 spider-worker 46s However, the Deployment under Namespace default cannot work properly. You can troubleshoot with the Events of its Pod: kubectl describe po default-ns-deploy-5587c7bd47-xbmj2 -n default ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/default-ns-deploy-5587c7bd47-xbmj2 to spider-worker Warning FailedCreatePodSandBox 17s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"97f18ae3ee315f58347f8936f819dd20b29c2d0a3d457fc6f0022282bf513e91\" : [ default/default-ns-deploy-5587c7bd47-xbmj2:macvlan-cni-default ] : error adding container to network \"macvlan-cni-default\" : spiderpool IP allocation error: [ POST /ipam/ip ][ 500 ] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [ test-ns-ipv4-ippool ] of eth0 filtered out: unmatched Namespace affinity of IPPool test-ns-ipv4-ippool Obviously, this Pod has no permission to get IP addresses from IPPool test-ns-ipv4-ippool . You can specify a default IP pool for a Namespace and set the corresponding namespaceAffinity for the IPPool to achieve the effect of \"a Namespace static IP pool\". Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete ns test-ns kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml \\ --ignore-not-found = true","title":"Namespace affinity of IPPool"},{"location":"usage/ippool-affinity-namespace/#namespace-affinity-of-ippool","text":"Spiderpool supports affinity between IP pools and Namespaces. It means only Pods running under these Namespaces can use the IP pools that have an affinity to these Namespaces. Namespace affinity should be regarded as a filtering mechanism rather than a pool selection rule .","title":"Namespace affinity of IPPool"},{"location":"usage/ippool-affinity-namespace/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-affinity-namespace/#get-started","text":"First, create a new Namespace test-ns . kubectl create namespace test-ns Create an IPPool that will be bound to it. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.41 namespaceAffinity : matchLabels : kubernetes.io/metadata.name : test-ns For convenience, this example uses a native Namespace label kubernetes.io/metadata.name as the matching condition of IPPool affinity. You can replace them with desired labels to match the corresponding Namespaces. Next, create two Deployments under test-ns and default Namespaces respectively, and configure the Pods therein to get IP addresses from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml You will find that the Deployment under Namespace test-ns is running. kubectl get deploy -n test-ns NAME READY UP-TO-DATE AVAILABLE AGE test-ns-deploy 1 /1 1 1 35s And its Pod has been assigned with an IP address from that IPPool. kubectl get se -n test-ns NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-ns-deploy-74c6784f9-dlkmx eth0 test-ns-ipv4-ippool 172 .18.41.41/24 spider-worker 46s However, the Deployment under Namespace default cannot work properly. You can troubleshoot with the Events of its Pod: kubectl describe po default-ns-deploy-5587c7bd47-xbmj2 -n default ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/default-ns-deploy-5587c7bd47-xbmj2 to spider-worker Warning FailedCreatePodSandBox 17s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"97f18ae3ee315f58347f8936f819dd20b29c2d0a3d457fc6f0022282bf513e91\" : [ default/default-ns-deploy-5587c7bd47-xbmj2:macvlan-cni-default ] : error adding container to network \"macvlan-cni-default\" : spiderpool IP allocation error: [ POST /ipam/ip ][ 500 ] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [ test-ns-ipv4-ippool ] of eth0 filtered out: unmatched Namespace affinity of IPPool test-ns-ipv4-ippool Obviously, this Pod has no permission to get IP addresses from IPPool test-ns-ipv4-ippool . You can specify a default IP pool for a Namespace and set the corresponding namespaceAffinity for the IPPool to achieve the effect of \"a Namespace static IP pool\".","title":"Get started"},{"location":"usage/ippool-affinity-namespace/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete ns test-ns kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/test-ns-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-namespace/different-ns-deploys.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/ippool-affinity-node/","text":"Node affinity of IPPool Spiderpool supports affinity between IP pools and Nodes. It means only Pods running on these Nodes can use the IP pools that have an affinity to these Nodes. Node affinity should be regarded as a filtering mechanism rather than a pool selection rule . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started Since the cluster in this example has only two Nodes (1 master and 1 worker), it is required to remove the relevant taints on the master Node through kubectl taint , so that ordinary Pods can also be scheduled to it. If your cluster has two or more worker Nodes, please ignore the step above. Create two IPPools with 1 IP address each, one of which will provide IP addresses for all Pods running on the master Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40 gateway : 172.18.41.1 nodeAffinity : matchExpressions : - { key : node-role.kubernetes.io/master , operator : Exists } The other provides IP addresses for the Pods on the worker Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : worker-ipv4-ippool spec : subnet : 172.18.42.0/24 ips : - 172.18.42.40 gateway : 172.18.42.1 nodeAffinity : matchExpressions : - { key : node-role.kubernetes.io/master , operator : DoesNotExist } Here, the value of the Node annotation node-role.kubernetes.io/master distinguishes two Nodes with different roles (or different node regions). If there is no annotation node-role.kubernetes.io/master on your Nodes, you can change it to another one or add some annotations you want. Then, create a Deployment with 2 replicas, and set podAntiAffinity to ensure that the two Pods which select the above IPPools according to the syntax of alternative IP pools can be scheduled to different Nodes. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : node-affinity-deploy spec : replicas : 2 selector : matchLabels : app : node-affinity-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"master-ipv4-ippool\", \"worker-ipv4-ippool\"] } labels : app : node-affinity-deploy spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : app : node-affinity-deploy topologyKey : kubernetes.io/hostname containers : - name : node-affinity-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Finally, you will find that Pods on different Nodes will use different IPPools. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME node-affinity-deploy-66c9874465-rvdkm eth0 master-ipv4-ippool 172 .18.41.40/24 spider-control-plane 35s node-affinity-deploy-66c9874465-wb8ds eth0 worker-ipv4-ippool 172 .18.42.40/24 spider-worker 35s Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml \\ --ignore-not-found = true","title":"Node affinity of IPPool"},{"location":"usage/ippool-affinity-node/#node-affinity-of-ippool","text":"Spiderpool supports affinity between IP pools and Nodes. It means only Pods running on these Nodes can use the IP pools that have an affinity to these Nodes. Node affinity should be regarded as a filtering mechanism rather than a pool selection rule .","title":"Node affinity of IPPool"},{"location":"usage/ippool-affinity-node/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-affinity-node/#get-started","text":"Since the cluster in this example has only two Nodes (1 master and 1 worker), it is required to remove the relevant taints on the master Node through kubectl taint , so that ordinary Pods can also be scheduled to it. If your cluster has two or more worker Nodes, please ignore the step above. Create two IPPools with 1 IP address each, one of which will provide IP addresses for all Pods running on the master Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40 gateway : 172.18.41.1 nodeAffinity : matchExpressions : - { key : node-role.kubernetes.io/master , operator : Exists } The other provides IP addresses for the Pods on the worker Node. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : worker-ipv4-ippool spec : subnet : 172.18.42.0/24 ips : - 172.18.42.40 gateway : 172.18.42.1 nodeAffinity : matchExpressions : - { key : node-role.kubernetes.io/master , operator : DoesNotExist } Here, the value of the Node annotation node-role.kubernetes.io/master distinguishes two Nodes with different roles (or different node regions). If there is no annotation node-role.kubernetes.io/master on your Nodes, you can change it to another one or add some annotations you want. Then, create a Deployment with 2 replicas, and set podAntiAffinity to ensure that the two Pods which select the above IPPools according to the syntax of alternative IP pools can be scheduled to different Nodes. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : node-affinity-deploy spec : replicas : 2 selector : matchLabels : app : node-affinity-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"master-ipv4-ippool\", \"worker-ipv4-ippool\"] } labels : app : node-affinity-deploy spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : app : node-affinity-deploy topologyKey : kubernetes.io/hostname containers : - name : node-affinity-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Finally, you will find that Pods on different Nodes will use different IPPools. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME node-affinity-deploy-66c9874465-rvdkm eth0 master-ipv4-ippool 172 .18.41.40/24 spider-control-plane 35s node-affinity-deploy-66c9874465-wb8ds eth0 worker-ipv4-ippool 172 .18.42.40/24 spider-worker 35s","title":"Get started"},{"location":"usage/ippool-affinity-node/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/master-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/worker-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-node/node-affinity-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/ippool-affinity-pod/","text":"Pod affinity of IPPool IPPool supports affinity setting for Pods. when setting sepc.podAffinity in SpiderIPPool, only the selected Pods could get IP address from it, and another Pods could not even specifying the annotation. When setting the sepc.podAffinity , it helps to better implement the capability of static IP for workloads like Deployment, StatefulSet etc. When no sepc.podAffinity , all applications could share the IP address in the ippool Get started The example shows how sepc.podAffinity works. Set up Spiderpool Follow the installation guide to install Spiderpool. Shared IPPool Create an IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : ipVersion : 4 subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 Create two Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker Occupied IPPool Create an IPPool configured with podAffinity . The spec.podAffinity means only application labeled with app: static can get IP from this IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : occupied-static-ipv4-ippool spec : ipVersion : 4 subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.43 podAffinity : matchLabels : app : static Create a Deployment whose Pods are labeled with app: static , and set the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : occupied-static-ippool-deploy spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"static-ipv4-ippool\"] } labels : app : static spec : containers : - name : occupied-static-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE occupied-static-ippool-deploy-7f478cc7d7-l7wm5 1 /1 Running 0 20s 172 .18.41.42 spider-control-plane occupied-static-ippool-deploy-7f478cc7d7-vphw9 1 /1 Running 0 20s 172 .18.41.40 spider-worker Create a Deployment using the same IPPool occupied-static-ipv4-ippool to allocate IP addresses, but its Pods do not have the label app: static , it will fail to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml As a result, these Pods cannot run successfully because they do not have permission to use the IPPool. kubectl describe po wrong-static-ippool-deploy-6c496cfb7d-wptq5 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 35s default-scheduler Successfully assigned default/wrong-static-ippool-deploy-6c496cfb7d-wptq5 to spider-worker Warning FailedCreatePodSandBox 34s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"a6f717aede91a356b552ad38c66112a26e5f7a4f7d23b7067870f33f05d350bc\" : [ default/wrong-static-ippool-deploy-6c496cfb7d-wptq5:macvlan-cni-default ] : error adding container to network \"macvlan-cni-default\" : spiderpool IP allocation error: [ POST /ipam/ip ][ 500 ] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [ static-ipv4-ippool ] of eth0 filtered out: unmatched Pod affinity of IPPool static-ipv4-ippool Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml \\ --ignore-not-found = true","title":"Pod affinity of IPPool"},{"location":"usage/ippool-affinity-pod/#pod-affinity-of-ippool","text":"IPPool supports affinity setting for Pods. when setting sepc.podAffinity in SpiderIPPool, only the selected Pods could get IP address from it, and another Pods could not even specifying the annotation. When setting the sepc.podAffinity , it helps to better implement the capability of static IP for workloads like Deployment, StatefulSet etc. When no sepc.podAffinity , all applications could share the IP address in the ippool","title":"Pod affinity of IPPool"},{"location":"usage/ippool-affinity-pod/#get-started","text":"The example shows how sepc.podAffinity works.","title":"Get started"},{"location":"usage/ippool-affinity-pod/#set-up-spiderpool","text":"Follow the installation guide to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-affinity-pod/#shared-ippool","text":"Create an IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : ipVersion : 4 subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 Create two Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker","title":"Shared IPPool"},{"location":"usage/ippool-affinity-pod/#occupied-ippool","text":"Create an IPPool configured with podAffinity . The spec.podAffinity means only application labeled with app: static can get IP from this IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : occupied-static-ipv4-ippool spec : ipVersion : 4 subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.43 podAffinity : matchLabels : app : static Create a Deployment whose Pods are labeled with app: static , and set the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : occupied-static-ippool-deploy spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"static-ipv4-ippool\"] } labels : app : static spec : containers : - name : occupied-static-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE occupied-static-ippool-deploy-7f478cc7d7-l7wm5 1 /1 Running 0 20s 172 .18.41.42 spider-control-plane occupied-static-ippool-deploy-7f478cc7d7-vphw9 1 /1 Running 0 20s 172 .18.41.40 spider-worker Create a Deployment using the same IPPool occupied-static-ipv4-ippool to allocate IP addresses, but its Pods do not have the label app: static , it will fail to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml As a result, these Pods cannot run successfully because they do not have permission to use the IPPool. kubectl describe po wrong-static-ippool-deploy-6c496cfb7d-wptq5 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 35s default-scheduler Successfully assigned default/wrong-static-ippool-deploy-6c496cfb7d-wptq5 to spider-worker Warning FailedCreatePodSandBox 34s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"a6f717aede91a356b552ad38c66112a26e5f7a4f7d23b7067870f33f05d350bc\" : [ default/wrong-static-ippool-deploy-6c496cfb7d-wptq5:macvlan-cni-default ] : error adding container to network \"macvlan-cni-default\" : spiderpool IP allocation error: [ POST /ipam/ip ][ 500 ] postIpamIpFailure failed to allocate IP addresses in standard mode: no IPPool available, all IPv4 IPPools [ static-ipv4-ippool ] of eth0 filtered out: unmatched Pod affinity of IPPool static-ipv4-ippool","title":"Occupied IPPool"},{"location":"usage/ippool-affinity-pod/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/wrong-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/occupied-static-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/ippool-multi/","text":"Backup IPPool Multiple IP pools can be set for a Pod for the usage of backup IP resources. Get Started Set up Spiderpool Follow the guide installation to install Spiderpool. Backup IPPool effect Create two IPPools each containing 2 IP addresses. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml Create a Pod and allocate an IP address to it from these IPPools. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml You will find that you still have 3 available IP addresses, one in IPPool default-ipv4-ippool and two in IPPool backup-ipv4-ippool . kubectl get sp -l case = backup NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE backup-ipv4-ippool 4 172 .18.42.0/24 0 2 false default-ipv4-ippool 4 172 .18.41.0/24 1 2 false Then, create a Deployment with 2 replicas and allocate IP addresses to its Pods from the two IPPools above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : multi-ippool-deploy spec : replicas : 2 selector : matchLabels : app : multi-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"default-ipv4-ippool\", \"backup-ipv4-ippool\"] } labels : app : multi-ippool-deploy spec : containers : - name : multi-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Spiderpool will successively try to allocate IP addresses in the order of the elements in the \"IP pool array\" until the first allocation succeeds or all fail. Of course, you can specify the pool selection rules (that defines alternative IP pools) in many ways, the Pod annotation ipam.spidernet.io/ippool is used here to select IP pools. Finally, when addresses in IPPool default-ipv4-ippool are used up, the IPPool backup-ipv4-ippool takes over. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME dummy eth0 default-ipv4-ippool 172 .18.41.41/24 spider-worker 1m20s multi-ippool-deploy-669bf7cf79-4x88m eth0 default-ipv4-ippool 172 .18.41.40/24 spider-worker 2m31s multi-ippool-deploy-669bf7cf79-k7zkk eth0 backup-ipv4-ippool 172 .18.42.41/24 spider-worker 2m31s Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml \\ --ignore-not-found = true","title":"Backup IPPool"},{"location":"usage/ippool-multi/#backup-ippool","text":"Multiple IP pools can be set for a Pod for the usage of backup IP resources.","title":"Backup IPPool"},{"location":"usage/ippool-multi/#get-started","text":"","title":"Get Started"},{"location":"usage/ippool-multi/#set-up-spiderpool","text":"Follow the guide installation to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-multi/#backup-ippool-effect","text":"Create two IPPools each containing 2 IP addresses. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml Create a Pod and allocate an IP address to it from these IPPools. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml You will find that you still have 3 available IP addresses, one in IPPool default-ipv4-ippool and two in IPPool backup-ipv4-ippool . kubectl get sp -l case = backup NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE backup-ipv4-ippool 4 172 .18.42.0/24 0 2 false default-ipv4-ippool 4 172 .18.41.0/24 1 2 false Then, create a Deployment with 2 replicas and allocate IP addresses to its Pods from the two IPPools above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : multi-ippool-deploy spec : replicas : 2 selector : matchLabels : app : multi-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"default-ipv4-ippool\", \"backup-ipv4-ippool\"] } labels : app : multi-ippool-deploy spec : containers : - name : multi-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Spiderpool will successively try to allocate IP addresses in the order of the elements in the \"IP pool array\" until the first allocation succeeds or all fail. Of course, you can specify the pool selection rules (that defines alternative IP pools) in many ways, the Pod annotation ipam.spidernet.io/ippool is used here to select IP pools. Finally, when addresses in IPPool default-ipv4-ippool are used up, the IPPool backup-ipv4-ippool takes over. kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME dummy eth0 default-ipv4-ippool 172 .18.41.41/24 spider-worker 1m20s multi-ippool-deploy-669bf7cf79-4x88m eth0 default-ipv4-ippool 172 .18.41.40/24 spider-worker 2m31s multi-ippool-deploy-669bf7cf79-k7zkk eth0 backup-ipv4-ippool 172 .18.42.41/24 spider-worker 2m31s","title":"Backup IPPool effect"},{"location":"usage/ippool-multi/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/test-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/dummy-pod.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-multi/multi-ippool-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/ippool-namespace/","text":"Namespace default IPPool Spiderpool provides default IP pools at Namespace level. A Pod not configured with a pool selection rule of higher priority will be assigned with IP addresses from the default IP pools of its Namespace. Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get started Create a Namespace named as test-ns1 . kubectl create ns test-ns1 Create an IPPool to be bound with Namespace test-ns1 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml Check the status of this IPPool with the following command. kubectl get sp -l case = ns NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ns1-default-ipv4-ippool 4 172 .18.41.0/24 0 4 false Specify pool selection rules for Namespace test-ns1 with the following command and annotation. kubectl patch ns test-ns1 --patch-file https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-ippool-selection-patch.yaml metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns1-default-ipv4-ippool\"]' Create a Deployment with 3 replicas in the Namespace test-ns1 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ippool-deploy.yaml Now, all Pods in the Namespace should have been assigned with an IP address from the specified IPPool. Verify it with the following command: kubectl get se -n test-ns1 NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME ns1-default-ippool-deploy-7cd5449c88-9xncm eth0 ns1-default-ipv4-ippool 172 .18.41.41/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-dpfjs eth0 ns1-default-ipv4-ippool 172 .18.41.43/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-vjtdd eth0 ns1-default-ipv4-ippool 172 .18.41.42/24 spider-worker 58s The Namespace annotation ipam.spidernet.io/defaultv4ippool also supports the syntax of alternative IP pools , which means you can specify multiple default IP pools for a Namespace . In addition, one IPPool can be specified as the default IP pool for different Namespaces. If you want to bind an IPPool to a specific Namespace in an exclusive way, it means that no Namespace other than this (or a group of Namespaces) has permission to use this IPPool, please refer to SpiderIPPool namespace affinity . Clean up Clean relevant resources so that you can run this tutorial again. kubectl delete ns test-ns1 kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml --ignore-not-found = true","title":"Default IPPool at namespace"},{"location":"usage/ippool-namespace/#namespace-default-ippool","text":"Spiderpool provides default IP pools at Namespace level. A Pod not configured with a pool selection rule of higher priority will be assigned with IP addresses from the default IP pools of its Namespace.","title":"Namespace default IPPool"},{"location":"usage/ippool-namespace/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ippool-namespace/#get-started","text":"Create a Namespace named as test-ns1 . kubectl create ns test-ns1 Create an IPPool to be bound with Namespace test-ns1 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml Check the status of this IPPool with the following command. kubectl get sp -l case = ns NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ns1-default-ipv4-ippool 4 172 .18.41.0/24 0 4 false Specify pool selection rules for Namespace test-ns1 with the following command and annotation. kubectl patch ns test-ns1 --patch-file https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-ippool-selection-patch.yaml metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns1-default-ipv4-ippool\"]' Create a Deployment with 3 replicas in the Namespace test-ns1 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ippool-deploy.yaml Now, all Pods in the Namespace should have been assigned with an IP address from the specified IPPool. Verify it with the following command: kubectl get se -n test-ns1 NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME ns1-default-ippool-deploy-7cd5449c88-9xncm eth0 ns1-default-ipv4-ippool 172 .18.41.41/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-dpfjs eth0 ns1-default-ipv4-ippool 172 .18.41.43/24 spider-worker 57s ns1-default-ippool-deploy-7cd5449c88-vjtdd eth0 ns1-default-ipv4-ippool 172 .18.41.42/24 spider-worker 58s The Namespace annotation ipam.spidernet.io/defaultv4ippool also supports the syntax of alternative IP pools , which means you can specify multiple default IP pools for a Namespace . In addition, one IPPool can be specified as the default IP pool for different Namespaces. If you want to bind an IPPool to a specific Namespace in an exclusive way, it means that no Namespace other than this (or a group of Namespaces) has permission to use this IPPool, please refer to SpiderIPPool namespace affinity .","title":"Get started"},{"location":"usage/ippool-namespace/#clean-up","text":"Clean relevant resources so that you can run this tutorial again. kubectl delete ns test-ns1 kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-namespace/ns1-default-ipv4-ippool.yaml --ignore-not-found = true","title":"Clean up"},{"location":"usage/ipv6/","text":"IPv6 support Description Spiderpool supports: Dual stack Each workload can get IPv4 and IPv6 addresses, and can communicate over IPv4 or IPv6. IPv4 only Each workload can acquire IPv4 addresses, and can communicate over IPv4. IPv6 only Each workload can acquire IPv6 addresses, and can communicate over IPv6. Get Started Set up Spiderpool follow the guide installation to install Spiderpool. Create SpiderSubnet Create a SpiderSubnet and allocate IP addresses from the IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-subnet.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-subnet.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : custom-ipv4-subnet spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : custom-ipv6-subnet spec : subnet : fd00:172:18::/64 ips : - fd00:172:18::40-fd00:172:18::50 Create Deployment By Subnet create a Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/subnet to explicitly specify the subnet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-subnet-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-subnet-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-subnet-deploy template : metadata : annotations : ipam.spidernet.io/subnet : |- { \"ipv4\": [\"custom-ipv4-subnet\"],\"ipv6\": [\"custom-ipv6-subnet\"] } labels : app : custom-dual-subnet-deploy spec : containers : - name : custom-dual-subnet-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get pod -l app = custom-dual-subnet-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-dual-subnet-deploy-7fdbccfbb8-h5l4d 1 /1 Running 0 33s 172 .18.41.41 controller-node-1 <none> <none> custom-dual-subnet-deploy-7fdbccfbb8-rhdbd 1 /1 Running 0 33s 172 .18.41.42 controller-node-1 <none> <none> custom-dual-subnet-deploy-7fdbccfbb8-t6m5c 1 /1 Running 0 33s 172 .18.41.40 controller-node-1 <none> <none> View all IPs of Pods kubectl get pod -l app = custom-dual-subnet-deploy -o go-template = '{{range .items}}{{.metadata.name}}: {{range .status.podIPs}}{{.}} {{end}}{{\"\\n\"}}{{end}}' custom-dual-subnet-deploy-7fdbccfbb8-h5l4d: map [ ip:172.18.41.41 ] map [ ip:fd00:172:18::42 ] custom-dual-subnet-deploy-7fdbccfbb8-rhdbd: map [ ip:172.18.41.42 ] map [ ip:fd00:172:18::41 ] custom-dual-subnet-deploy-7fdbccfbb8-t6m5c: map [ ip:172.18.41.40 ] map [ ip:fd00:172:18::40 ] Create Deployment By IPPool Create IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv6-ippool spec : subnet : fd00:172:18::/64 ips : - fd00:172:18::40-fd00:172:18::50 Create Deployment create a Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get pod -owide -l app = custom-dual-ippool-deploy NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-dual-ippool-deploy-9bb6696c4-6wjnl 1 /1 Running 0 76s 172 .18.41.42 controller-node-1 <none> <none> custom-dual-ippool-deploy-9bb6696c4-8vtpf 1 /1 Running 0 76s 172 .18.41.45 controller-node-1 <none> <none> custom-dual-ippool-deploy-9bb6696c4-zbknv 1 /1 Running 0 76s 172 .18.41.43 controller-node-1 <none> <none> View all IPs of Pods kubectl get pod -l app = custom-dual-ippool-deploy -o go-template = '{{range .items}}{{.metadata.name}}: {{range .status.podIPs}}{{.}} {{end}}{{\"\\n\"}}{{end}}' custom-dual-ippool-deploy-9bb6696c4-6wjnl: map [ ip:172.18.41.42 ] map [ ip:fd00:172:18::4d ] custom-dual-ippool-deploy-9bb6696c4-8vtpf: map [ ip:172.18.41.45 ] map [ ip:fd00:172:18::4e ] custom-dual-ippool-deploy-9bb6696c4-zbknv: map [ ip:172.18.41.43 ] map [ ip:fd00:172:18::46 ] Clean up Clean the relevant resources so that you can run this tutorial again kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-subnet.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-subnet.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ --ignore-not-found = true","title":"IPv6 support"},{"location":"usage/ipv6/#ipv6-support","text":"","title":"IPv6 support"},{"location":"usage/ipv6/#description","text":"Spiderpool supports: Dual stack Each workload can get IPv4 and IPv6 addresses, and can communicate over IPv4 or IPv6. IPv4 only Each workload can acquire IPv4 addresses, and can communicate over IPv4. IPv6 only Each workload can acquire IPv6 addresses, and can communicate over IPv6.","title":"Description"},{"location":"usage/ipv6/#get-started","text":"","title":"Get Started"},{"location":"usage/ipv6/#set-up-spiderpool","text":"follow the guide installation to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/ipv6/#create-spidersubnet","text":"Create a SpiderSubnet and allocate IP addresses from the IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-subnet.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-subnet.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : custom-ipv4-subnet spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : custom-ipv6-subnet spec : subnet : fd00:172:18::/64 ips : - fd00:172:18::40-fd00:172:18::50","title":"Create SpiderSubnet"},{"location":"usage/ipv6/#create-deployment-by-subnet","text":"create a Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/subnet to explicitly specify the subnet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-subnet-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-subnet-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-subnet-deploy template : metadata : annotations : ipam.spidernet.io/subnet : |- { \"ipv4\": [\"custom-ipv4-subnet\"],\"ipv6\": [\"custom-ipv6-subnet\"] } labels : app : custom-dual-subnet-deploy spec : containers : - name : custom-dual-subnet-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get pod -l app = custom-dual-subnet-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-dual-subnet-deploy-7fdbccfbb8-h5l4d 1 /1 Running 0 33s 172 .18.41.41 controller-node-1 <none> <none> custom-dual-subnet-deploy-7fdbccfbb8-rhdbd 1 /1 Running 0 33s 172 .18.41.42 controller-node-1 <none> <none> custom-dual-subnet-deploy-7fdbccfbb8-t6m5c 1 /1 Running 0 33s 172 .18.41.40 controller-node-1 <none> <none> View all IPs of Pods kubectl get pod -l app = custom-dual-subnet-deploy -o go-template = '{{range .items}}{{.metadata.name}}: {{range .status.podIPs}}{{.}} {{end}}{{\"\\n\"}}{{end}}' custom-dual-subnet-deploy-7fdbccfbb8-h5l4d: map [ ip:172.18.41.41 ] map [ ip:fd00:172:18::42 ] custom-dual-subnet-deploy-7fdbccfbb8-rhdbd: map [ ip:172.18.41.42 ] map [ ip:fd00:172:18::41 ] custom-dual-subnet-deploy-7fdbccfbb8-t6m5c: map [ ip:172.18.41.40 ] map [ ip:fd00:172:18::40 ]","title":"Create Deployment By Subnet"},{"location":"usage/ipv6/#create-deployment-by-ippool","text":"Create IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv6-ippool spec : subnet : fd00:172:18::/64 ips : - fd00:172:18::40-fd00:172:18::50 Create Deployment create a Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get pod -owide -l app = custom-dual-ippool-deploy NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-dual-ippool-deploy-9bb6696c4-6wjnl 1 /1 Running 0 76s 172 .18.41.42 controller-node-1 <none> <none> custom-dual-ippool-deploy-9bb6696c4-8vtpf 1 /1 Running 0 76s 172 .18.41.45 controller-node-1 <none> <none> custom-dual-ippool-deploy-9bb6696c4-zbknv 1 /1 Running 0 76s 172 .18.41.43 controller-node-1 <none> <none> View all IPs of Pods kubectl get pod -l app = custom-dual-ippool-deploy -o go-template = '{{range .items}}{{.metadata.name}}: {{range .status.podIPs}}{{.}} {{end}}{{\"\\n\"}}{{end}}' custom-dual-ippool-deploy-9bb6696c4-6wjnl: map [ ip:172.18.41.42 ] map [ ip:fd00:172:18::4d ] custom-dual-ippool-deploy-9bb6696c4-8vtpf: map [ ip:172.18.41.45 ] map [ ip:fd00:172:18::4e ] custom-dual-ippool-deploy-9bb6696c4-zbknv: map [ ip:172.18.41.43 ] map [ ip:fd00:172:18::46 ]","title":"Create Deployment By IPPool"},{"location":"usage/ipv6/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-subnet.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-subnet.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv6-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/multi-interfaces-annotation/","text":"Pod annotation of multi-NIC When assigning multiple NICs to a Pod with Multus CNI , Spiderpool supports to specify the IP pools for each interface. This feature supports to implement by annotation ipam.spidernet.io/subnets and ipam.spidernet.io/ippools Get Started The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a Pod with two NICs with IP in different subnets. Set up Spiderpool Follow the guide installation to install Spiderpool. Set up Multus Configuration In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed: Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use. In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service . ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf-ens192 20s macvlan-conf-ens224 22s multiple NICs by subnet Create two Subnets to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-test-ens192 4 10 .6.0.1/16 0 10 subnet-test-ens224 4 10 .7.0.1/16 0 10 In the following example Yaml, 2 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-test-app-v4-eth0-b1a361c7e9df 4 10 .6.0.1/16 2 3 false false auto-test-app-v4-net1-b1a361c7e9df 4 10 .7.0.1/16 2 3 false false ~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.6.168.171-10.6.168.173\" ] ~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.7.168.171-10.7.168.173\" ] ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f4594ff67-fkqbw 1 /1 Running 0 40s 10 .6.168.172 node2 <none> <none> test-app-6f4594ff67-gwlx8 1 /1 Running 0 40s 10 .6.168.173 node1 <none> <none> ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a 3 : eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.168.172/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 26 :6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 5 : net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .7.168.173/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.173 lookup 100 32765 : from all to 10 .7.168.173/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 proto kernel scope link src 10 .7.168.173 10 .96.0.0/12 via 10 .6.168.123 dev veth0 multiple NICs by IPPool Create two IPPools to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 0 5 false false ippool-test-ens224 4 10 .7.0.1/16 0 5 false false In the following example Yaml, 1 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 1 5 false false ippool-test-ens224 4 10 .7.0.1/16 1 5 false false ~# kubectl get po -l app = ippool-test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-65f646574c-mpr47 1 /1 Running 0 6m18s 10 .6.168.175 node2 <none> <none> ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a ... 3 : eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff inet 10 .6.168.175/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 86 :ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff 5 : net1@eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff inet 10 .7.168.177/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.177 lookup 100 32765 : from all to 10 .7.168.177/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 scope link src 10 .7.168.177 10 .96.0.0/12 via 10 .6.168.123 dev veth0 Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\ --ignore-not-found = true","title":"Multiple interfaces"},{"location":"usage/multi-interfaces-annotation/#pod-annotation-of-multi-nic","text":"When assigning multiple NICs to a Pod with Multus CNI , Spiderpool supports to specify the IP pools for each interface. This feature supports to implement by annotation ipam.spidernet.io/subnets and ipam.spidernet.io/ippools","title":"Pod annotation of multi-NIC"},{"location":"usage/multi-interfaces-annotation/#get-started","text":"The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a Pod with two NICs with IP in different subnets.","title":"Get Started"},{"location":"usage/multi-interfaces-annotation/#set-up-spiderpool","text":"Follow the guide installation to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/multi-interfaces-annotation/#set-up-multus-configuration","text":"In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed: Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use. In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service . ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf-ens192 20s macvlan-conf-ens224 22s","title":"Set up Multus Configuration"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-subnet","text":"Create two Subnets to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-test-ens192 4 10 .6.0.1/16 0 10 subnet-test-ens224 4 10 .7.0.1/16 0 10 In the following example Yaml, 2 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-test-app-v4-eth0-b1a361c7e9df 4 10 .6.0.1/16 2 3 false false auto-test-app-v4-net1-b1a361c7e9df 4 10 .7.0.1/16 2 3 false false ~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.6.168.171-10.6.168.173\" ] ~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.7.168.171-10.7.168.173\" ] ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f4594ff67-fkqbw 1 /1 Running 0 40s 10 .6.168.172 node2 <none> <none> test-app-6f4594ff67-gwlx8 1 /1 Running 0 40s 10 .6.168.173 node1 <none> <none> ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a 3 : eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.168.172/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 26 :6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 5 : net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .7.168.173/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.173 lookup 100 32765 : from all to 10 .7.168.173/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 proto kernel scope link src 10 .7.168.173 10 .96.0.0/12 via 10 .6.168.123 dev veth0","title":"multiple NICs by subnet"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-ippool","text":"Create two IPPools to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 0 5 false false ippool-test-ens224 4 10 .7.0.1/16 0 5 false false In the following example Yaml, 1 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 1 5 false false ippool-test-ens224 4 10 .7.0.1/16 1 5 false false ~# kubectl get po -l app = ippool-test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-65f646574c-mpr47 1 /1 Running 0 6m18s 10 .6.168.175 node2 <none> <none> ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a ... 3 : eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff inet 10 .6.168.175/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 86 :ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff 5 : net1@eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff inet 10 .7.168.177/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.177 lookup 100 32765 : from all to 10 .7.168.177/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 scope link src 10 .7.168.177 10 .96.0.0/12 via 10 .6.168.123 dev veth0","title":"multiple NICs by IPPool"},{"location":"usage/multi-interfaces-annotation/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/network-topology-zh_CN/","text":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u968f\u7740\u6570\u636e\u4e2d\u5fc3\u79c1\u6709\u4e91\u7684\u4e0d\u65ad\u666e\u53ca\uff0cUnderlay \u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u4f20\u8f93\u548c\u66f4\u597d\u7684\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u80fd\u529b\u3002\u7531\u4e8e\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9760\u3001\u5b89\u5168\u7b49\u7279\u6027\uff0cUnderlay \u5728\u4e0b\u5217\u573a\u666f\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7684\u5e94\u7528\uff1a \u5ef6\u65f6\u654f\u611f\u7684\u5e94\u7528\uff1a\u67d0\u4e9b\u7279\u5b9a\u884c\u4e1a\u6216\u5e94\u7528\uff08\u5982\u91d1\u878d\u4ea4\u6613\u3001\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u7b49\uff09\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u975e\u5e38\u654f\u611f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cUnderlay \u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u7269\u7406\u548c\u94fe\u8def\u5c42\u7684\u8fde\u63a5\u6765\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u65f6\u95f4\u3002\u8fd9\u79cd\u4f4e\u5ef6\u8fdf\u7684\u7279\u6027\u4f7f\u5f97 Underlay \u7f51\u7edc\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u5e94\u7528\u9700\u6c42\u7684\u7406\u60f3\u9009\u62e9\u3002 \u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff1a\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7 Underlay \u7f51\u7edc\u7684 IPAM \u5bf9\u5e94\u7528\u56fa\u5b9a\u51fa\u53e3 IP \u5730\u5740\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u63a7\u5236\u96c6\u7fa4\u4e0e\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002 \u5728 Underlay \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5f53\u5b83\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u800c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u5b50\u7f51\u65f6\uff0c\u5c06\u5bf9 IP \u5730\u5740\u7ba1\u7406\uff08IPAM\uff09\u5e26\u6765\u6311\u6218\uff0c\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u80fd\u5b9e\u73b0\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u7684\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u3002 \u4e00\u4e2a\u96c6\u7fa4\uff0c\u4f46\u96c6\u7fa4\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 10.6.1.0/24\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.16.2.0/24\u3002 \u5728\u4e0a\u8bc9\u7684\u573a\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u4e0e\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\uff0cSpiderpool \u7684 CR\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5b9e\u73b0 IP \u6c60\u4e0e\u8282\u70b9\u4e4b\u95f4\u7684\u4eb2\u548c\u6027\uff0c\u8ba9 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e00\u8282\u70b9\u65f6\uff0c\u80fd\u4ece\u8282\u70b9\u6240\u5728\u7684 Underlay \u5b50\u7f51\u4e2d\u83b7\u5f97 IP \u5730\u5740\uff0c\u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c \u5b9e\u65bd\u8981\u6c42 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4 \u51c6\u5907\u4e00\u5957\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4\uff0c\u5982\u8282\u70b9 1 \u4f7f\u7528 10.6.0.0/16 \uff0c\u8282\u70b9 2 \u4f7f\u7528 10.7.0.0/16 \u5b50\u7f51\uff0c\u4ee5\u4e0b\u662f\u6240\u4f7f\u7528\u7684\u96c6\u7fa4\u4fe1\u606f\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u56fe\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 eth0 \u4f5c\u4e3a master \u7684\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u5e94\u4e0e\u96c6\u7fa4\u8de8\u8d8a\u7f51\u7edc\u533a\u7684\u8282\u70b9\u4e0a\u7684\u63a5\u53e3\u540d\u79f0\u5339\u914d\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m \u521b\u5efa IPPools Spiderpool \u7684 CRD\uff1aSpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a daemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0cSpiderpool \u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u8282\u70b9\u8de8\u7f51\u7edc\u533a\u57df\u7684\u573a\u666f\u5206\u914d IP \u65f6\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684 IPPool.spec.nodeAffinity \u6ce8\u89e3\uff0c Pod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u53ef\u4ee5\u901a\u8fc7\u5907\u9009\u6c60 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u3002 v1.multus-cni.io/default-network\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5b8c\u6210\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0 Pod \u7684 IP \u5c5e\u4e8e Pod \u6240\u5728\u8282\u70b9\u7684\u5b50\u7f51\u5185\uff0c\u6240\u5bf9\u5e94\u7684 IP \u6c60\u4e3a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e86 IP \u5730\u5740\u3002 ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false \u8de8\u7f51\u7edc\u533a\u57df\u7684 Pod \u4e0e Pod \u4e4b\u95f4\u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms \u603b\u7ed3 \u4e0d\u540c\u7f51\u7edc\u533a\u57df\u7684 Pod \u80fd\u591f\u6b63\u5e38\u901a\u4fe1\uff0cSpiderpool \u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u9700\u6c42\u3002","title":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d"},{"location":"usage/network-topology-zh_CN/#ip","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d"},{"location":"usage/network-topology-zh_CN/#_1","text":"\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u79c1\u6709\u4e91\u7684\u4e0d\u65ad\u666e\u53ca\uff0cUnderlay \u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u4f20\u8f93\u548c\u66f4\u597d\u7684\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u80fd\u529b\u3002\u7531\u4e8e\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9760\u3001\u5b89\u5168\u7b49\u7279\u6027\uff0cUnderlay \u5728\u4e0b\u5217\u573a\u666f\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7684\u5e94\u7528\uff1a \u5ef6\u65f6\u654f\u611f\u7684\u5e94\u7528\uff1a\u67d0\u4e9b\u7279\u5b9a\u884c\u4e1a\u6216\u5e94\u7528\uff08\u5982\u91d1\u878d\u4ea4\u6613\u3001\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u7b49\uff09\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u975e\u5e38\u654f\u611f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cUnderlay \u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u7269\u7406\u548c\u94fe\u8def\u5c42\u7684\u8fde\u63a5\u6765\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u65f6\u95f4\u3002\u8fd9\u79cd\u4f4e\u5ef6\u8fdf\u7684\u7279\u6027\u4f7f\u5f97 Underlay \u7f51\u7edc\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u5e94\u7528\u9700\u6c42\u7684\u7406\u60f3\u9009\u62e9\u3002 \u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff1a\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7 Underlay \u7f51\u7edc\u7684 IPAM \u5bf9\u5e94\u7528\u56fa\u5b9a\u51fa\u53e3 IP \u5730\u5740\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u63a7\u5236\u96c6\u7fa4\u4e0e\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002 \u5728 Underlay \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5f53\u5b83\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u800c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u5b50\u7f51\u65f6\uff0c\u5c06\u5bf9 IP \u5730\u5740\u7ba1\u7406\uff08IPAM\uff09\u5e26\u6765\u6311\u6218\uff0c\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u80fd\u5b9e\u73b0\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u7684\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/network-topology-zh_CN/#_2","text":"Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u3002 \u4e00\u4e2a\u96c6\u7fa4\uff0c\u4f46\u96c6\u7fa4\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 10.6.1.0/24\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.16.2.0/24\u3002 \u5728\u4e0a\u8bc9\u7684\u573a\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u4e0e\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\uff0cSpiderpool \u7684 CR\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5b9e\u73b0 IP \u6c60\u4e0e\u8282\u70b9\u4e4b\u95f4\u7684\u4eb2\u548c\u6027\uff0c\u8ba9 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e00\u8282\u70b9\u65f6\uff0c\u80fd\u4ece\u8282\u70b9\u6240\u5728\u7684 Underlay \u5b50\u7f51\u4e2d\u83b7\u5f97 IP \u5730\u5740\uff0c\u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/network-topology-zh_CN/#_3","text":"\u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/network-topology-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/network-topology-zh_CN/#_5","text":"\u51c6\u5907\u4e00\u5957\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4\uff0c\u5982\u8282\u70b9 1 \u4f7f\u7528 10.6.0.0/16 \uff0c\u8282\u70b9 2 \u4f7f\u7528 10.7.0.0/16 \u5b50\u7f51\uff0c\u4ee5\u4e0b\u662f\u6240\u4f7f\u7528\u7684\u96c6\u7fa4\u4fe1\u606f\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u56fe\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none>","title":"\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4"},{"location":"usage/network-topology-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/network-topology-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 eth0 \u4f5c\u4e3a master \u7684\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u5e94\u4e0e\u96c6\u7fa4\u8de8\u8d8a\u7f51\u7edc\u533a\u7684\u8282\u70b9\u4e0a\u7684\u63a5\u53e3\u540d\u79f0\u5339\u914d\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/network-topology-zh_CN/#ippools","text":"Spiderpool \u7684 CRD\uff1aSpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF","title":"\u521b\u5efa IPPools"},{"location":"usage/network-topology-zh_CN/#_6","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a daemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0cSpiderpool \u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u8282\u70b9\u8de8\u7f51\u7edc\u533a\u57df\u7684\u573a\u666f\u5206\u914d IP \u65f6\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684 IPPool.spec.nodeAffinity \u6ce8\u89e3\uff0c Pod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u53ef\u4ee5\u901a\u8fc7\u5907\u9009\u6c60 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u3002 v1.multus-cni.io/default-network\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5b8c\u6210\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0 Pod \u7684 IP \u5c5e\u4e8e Pod \u6240\u5728\u8282\u70b9\u7684\u5b50\u7f51\u5185\uff0c\u6240\u5bf9\u5e94\u7684 IP \u6c60\u4e3a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e86 IP \u5730\u5740\u3002 ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false \u8de8\u7f51\u7edc\u533a\u57df\u7684 Pod \u4e0e Pod \u4e4b\u95f4\u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/network-topology-zh_CN/#_7","text":"\u4e0d\u540c\u7f51\u7edc\u533a\u57df\u7684 Pod \u80fd\u591f\u6b63\u5e38\u901a\u4fe1\uff0cSpiderpool \u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u9700\u6c42\u3002","title":"\u603b\u7ed3"},{"location":"usage/network-topology/","text":"Based on IP allocation across network zones English | \u7b80\u4f53\u4e2d\u6587 Introduce The rising popularity of private cloud data centers has made underlay networks essential components of data center network architecture by offering efficient network transmission and improved network topology management capabilities. Underlay is widely used in the following scenarios due to its low latency, reliability, and security: Latency-sensitive applications: applications in specific industries, such as financial trading and real-time video transmission, are highly sensitive to network latency. Underlay networks directly control physical and link-layer connections to reduce data transmission time, providing an ideal solution for these applications. Firewall security control and management: firewalls are often used to manage north-south traffic, namely communication between internal and external networks, by checking, filtering, and restricting communication traffic. IP address management (IPAM) solutions of underlay networks that allocate fixed egress IP addresses for applications can provide better communication management and control between the cluster and external networks, further enhancing overall network security. In the cluster of the Underlay network, when its nodes are distributed in different regions or data centers, and some node regions can only use specific subnets, it will bring challenges to IP address management (IPAM). This article will introduce a method that can realize A complete underlay network solution for IP allocation across network regions. Project Functions Spiderpool provides the function of node topology, which can help solve the problem of IP allocation across network regions. Its implementation principle is as follows. A cluster, but the nodes of the cluster are distributed in different regions or data centers, some nodes' regions can only use the subnet 10.6.1.0/24, and some nodes' regions can only use the subnet 172.16.2.0/24. In the appeal scenario, when an application deploys copies across subnets, IPAM is required to assign IP addresses that match the subnet to different Pods under the same application on different nodes. Spiderpool's CR: SpiderIPPool The nodeName field is provided to realize the affinity between the IP pool and the node, so that when the Pod is scheduled to a certain node, it can obtain the IP address from the Underlay subnet where the node is located, and realize the node topology function. Implementation Requirements Installed Helm . Steps Clusters spanning network regions Prepare a set of clusters that span the network area. For example, node 1 uses 10.6.0.0/16 and node 2 uses 10.7.0.0/16 subnet. The following is the cluster information and network topology used: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> ~# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS controller-node-1 Ready control-plane,master 1h v1.25.3 node-subnet = subnet-6, ... worker-node-1 Ready <none> 1h v1.25.3 node-subnet = subnet-7, ... Install Spiderpool Install Spiderpool via helm. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the Multus clusterNetwork of the cluster through multus.multusCNI.defaultCniCRName , clusterNetwork is a specific field of the Multus plugin, which is used to specify the default network interface of the Pod. Verify the installation\uff1a ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m Install CNI configuration To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an IPvlan SpiderMultusConfig configuration: master: In this example the interface eth0 is used as the parameter for master, this parameter should match the interface name on the nodes where the cluster spans network zones. MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } In the example of this article, use the above configuration to create the following two Macvlan SpiderMultusConfig, which will be automatically generated based on the Multus NetworkAttachmentDefinition CR, which corresponds to the eth0 network card of the host. ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m Create IPPools CRD of Spiderpool: SpiderIPPool provides nodeName field. When nodeName is not empty, when Pod starts on a node and tries to allocate IP from SpiderIPPool, if the node where Pod is located matches the nodeName setting, it can be retrieved from SpiderIPPool The IP is allocated successfully, otherwise the IP cannot be allocated from the SpiderIPPool. When nodeName is empty, Spiderpool does not enforce any allocation limit on Pods. As above, using the following Yaml, create 2 SpiderIPPools that will provide IP addresses to Pods on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF Create the application The following sample yaml creates a daemonSet application where: ipam.spidernet.io/ippool: It is used to specify the IP pool of Spiderpool. Multiple IP pools can be set as alternative pools. Spiderpool will try to allocate IP addresses in sequence according to the order of elements in the \"IP pool array\". When assigning IP in the network area scenario, if the node to which the application copy is scheduled meets the IPPool.spec.nodeAffinity annotation of the first IP pool, the Pod will obtain the IP allocation from the pool. Select the IP pool in the selected pool and continue to allocate IPs for Pods until all candidate pools fail to be screened. You can learn more about usage with alternative pools. v1.multus-cni.io/default-network: Used to specify the NetworkAttachmentDefinition configuration of Multus, which will create a default network card for the application. ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF After creating an application, it can be observed that each Pod's IP address is assigned from an IP pool belonging to the same subnet as its node. ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false Communication between Pods across network zones: ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms Summarize Pods in different network areas can communicate normally, and Spiderpool can well meet the IP allocation requirements based on cross-network areas.","title":"node-based topology"},{"location":"usage/network-topology/#based-on-ip-allocation-across-network-zones","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Based on IP allocation across network zones"},{"location":"usage/network-topology/#introduce","text":"The rising popularity of private cloud data centers has made underlay networks essential components of data center network architecture by offering efficient network transmission and improved network topology management capabilities. Underlay is widely used in the following scenarios due to its low latency, reliability, and security: Latency-sensitive applications: applications in specific industries, such as financial trading and real-time video transmission, are highly sensitive to network latency. Underlay networks directly control physical and link-layer connections to reduce data transmission time, providing an ideal solution for these applications. Firewall security control and management: firewalls are often used to manage north-south traffic, namely communication between internal and external networks, by checking, filtering, and restricting communication traffic. IP address management (IPAM) solutions of underlay networks that allocate fixed egress IP addresses for applications can provide better communication management and control between the cluster and external networks, further enhancing overall network security. In the cluster of the Underlay network, when its nodes are distributed in different regions or data centers, and some node regions can only use specific subnets, it will bring challenges to IP address management (IPAM). This article will introduce a method that can realize A complete underlay network solution for IP allocation across network regions.","title":"Introduce"},{"location":"usage/network-topology/#project-functions","text":"Spiderpool provides the function of node topology, which can help solve the problem of IP allocation across network regions. Its implementation principle is as follows. A cluster, but the nodes of the cluster are distributed in different regions or data centers, some nodes' regions can only use the subnet 10.6.1.0/24, and some nodes' regions can only use the subnet 172.16.2.0/24. In the appeal scenario, when an application deploys copies across subnets, IPAM is required to assign IP addresses that match the subnet to different Pods under the same application on different nodes. Spiderpool's CR: SpiderIPPool The nodeName field is provided to realize the affinity between the IP pool and the node, so that when the Pod is scheduled to a certain node, it can obtain the IP address from the Underlay subnet where the node is located, and realize the node topology function.","title":"Project Functions"},{"location":"usage/network-topology/#implementation-requirements","text":"Installed Helm .","title":"Implementation Requirements"},{"location":"usage/network-topology/#steps","text":"","title":"Steps"},{"location":"usage/network-topology/#clusters-spanning-network-regions","text":"Prepare a set of clusters that span the network area. For example, node 1 uses 10.6.0.0/16 and node 2 uses 10.7.0.0/16 subnet. The following is the cluster information and network topology used: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> ~# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS controller-node-1 Ready control-plane,master 1h v1.25.3 node-subnet = subnet-6, ... worker-node-1 Ready <none> 1h v1.25.3 node-subnet = subnet-7, ...","title":"Clusters spanning network regions"},{"location":"usage/network-topology/#install-spiderpool","text":"Install Spiderpool via helm. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the Multus clusterNetwork of the cluster through multus.multusCNI.defaultCniCRName , clusterNetwork is a specific field of the Multus plugin, which is used to specify the default network interface of the Pod. Verify the installation\uff1a ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m","title":"Install Spiderpool"},{"location":"usage/network-topology/#install-cni-configuration","text":"To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an IPvlan SpiderMultusConfig configuration: master: In this example the interface eth0 is used as the parameter for master, this parameter should match the interface name on the nodes where the cluster spans network zones. MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } In the example of this article, use the above configuration to create the following two Macvlan SpiderMultusConfig, which will be automatically generated based on the Multus NetworkAttachmentDefinition CR, which corresponds to the eth0 network card of the host. ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"Install CNI configuration"},{"location":"usage/network-topology/#create-ippools","text":"CRD of Spiderpool: SpiderIPPool provides nodeName field. When nodeName is not empty, when Pod starts on a node and tries to allocate IP from SpiderIPPool, if the node where Pod is located matches the nodeName setting, it can be retrieved from SpiderIPPool The IP is allocated successfully, otherwise the IP cannot be allocated from the SpiderIPPool. When nodeName is empty, Spiderpool does not enforce any allocation limit on Pods. As above, using the following Yaml, create 2 SpiderIPPools that will provide IP addresses to Pods on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF","title":"Create IPPools"},{"location":"usage/network-topology/#create-the-application","text":"The following sample yaml creates a daemonSet application where: ipam.spidernet.io/ippool: It is used to specify the IP pool of Spiderpool. Multiple IP pools can be set as alternative pools. Spiderpool will try to allocate IP addresses in sequence according to the order of elements in the \"IP pool array\". When assigning IP in the network area scenario, if the node to which the application copy is scheduled meets the IPPool.spec.nodeAffinity annotation of the first IP pool, the Pod will obtain the IP allocation from the pool. Select the IP pool in the selected pool and continue to allocate IPs for Pods until all candidate pools fail to be screened. You can learn more about usage with alternative pools. v1.multus-cni.io/default-network: Used to specify the NetworkAttachmentDefinition configuration of Multus, which will create a default network card for the application. ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF After creating an application, it can be observed that each Pod's IP address is assigned from an IP pool belonging to the same subnet as its node. ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false Communication between Pods across network zones: ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms","title":"Create the application"},{"location":"usage/network-topology/#summarize","text":"Pods in different network areas can communicate normally, and Spiderpool can well meet the IP allocation requirements based on cross-network areas.","title":"Summarize"},{"location":"usage/reserved-ip/","text":"Reserved IP Spiderpool reserve some IP addresses for the whole Kubernetes cluster, which will not be used by any IPAM allocation results. Typically, these IP addresses are external IP addresses or cannot be used for network communication (e.g. broadcast address). IPPool excludeIPs You may have observed that there is a field excludeIPs in SpiderIPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main function is not like this. Field excludeIPs is more of a syntax sugar , so that users can more flexibly define the IP address ranges of the IPPool. For example, create an IPPool without using excludeIPs , which contains two IP ranges: 172.18.41.40-172.18.41.44 and 172.18.41.46-172.18.41.50 , you should define the ips as follows: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : not-use-excludeips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 But in fact, this semantics can be more succinctly described through excludeIPs : apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : use-excludeips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 excludeIPs : - 172.18.41.45 Field excludeIPs will make sure that any Pod that allocates IP addresses from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with excludeIPs defined. Use SpiderReservedIP Unlike configuring field excluedIPs in SpiderIPPool CR, creating a SpiderReservedIP CR is really a way to define the global reserved IP address rules of a Kubernetes cluster. The IP addresses defined in ReservedIP cannot be used by any Pod in the cluster, regardless of whether some IPPools have inadvertently defined them. More details refer to definition of SpiderReservedIP . Set up Spiderpool If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool. Get Started To understand how it works, let's do such a test. First, create an ReservedIP which reserves 9 IP addresses from 172.18.42.41 to 172.18.42.49 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderReservedIP metadata : name : test-ipv4-reservedip spec : ips : - 172.18.42.41-172.18.42.49 At the same time, create an IPPool with 10 IP addresses from 172.18.42.41 to 172.18.42.50 . Yes, we deliberately make it hold one more IP address than the ReservedIP above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ipv4-ippool spec : subnet : 172.18.42.0/24 ips : - 172.18.42.41-172.18.42.50 Then, create a Deployment with 3 replicas and allocate IP addresses to its Pods from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : reservedip-deploy spec : replicas : 3 selector : matchLabels : app : reservedip-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"test-ipv4-ippool\"] } labels : app : reservedip-deploy spec : containers : - name : reservedip-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] After a while, only one of these Pods using IP 172.18.42.50 can run successfully because \"all IP used out\". kubectl get po -l app = reservedip-deploy -o wide NAME READY STATUS RESTARTS AGE IP NODE reservedip-deploy-6cf9858886-cm7bp 0 /1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-lb7cr 0 /1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-pkcfl 1 /1 Running 0 35s 172 .18.42.50 spider-worker But when you delete this ReservedIP, everything will return to normal. kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle its IP addresses, but ReservedIP will still ensure that when the Pod is terminated, no other Pods can continue to use the reserved IP address. Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work. Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml \\ --ignore-not-found = true A Trap So, can you use IPPool's field excludeIPs to achieve the same effect as ReservedIP? The answer is NO ! Look at such a case, now you want to reserve an IP 172.18.43.31 for an external application of the Kubernetes cluster, which may be a Redis node. To achieve this, you created such an IPPool: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : already-in-use spec : subnet : 172.18.43.0/24 ips : - 172.18.43.1-172.18.43.31 excludeIPs : - 172.18.43.31 I believe that if there is only one IPPool under the subnet 172.18.43.0/24 network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : created-by-someone-else spec : subnet : 172.18.43.0/24 ips : - 172.18.43.31-172.18.43.50 Different IPPools allow to define the same field subnet , more details refer to validation of IPPool . After a period of time, a Pod may be allocated with IP 172.18.43.31 from the IPPool created-by-someone-else , and then it holds the same IP address as your Redis node. After that, the Redis may not work as well. So, if you really want to reserve an IP address instead of excluding an IP address, SpiderReservedIP makes life better.","title":"Reserved IP"},{"location":"usage/reserved-ip/#reserved-ip","text":"Spiderpool reserve some IP addresses for the whole Kubernetes cluster, which will not be used by any IPAM allocation results. Typically, these IP addresses are external IP addresses or cannot be used for network communication (e.g. broadcast address).","title":"Reserved IP"},{"location":"usage/reserved-ip/#ippool-excludeips","text":"You may have observed that there is a field excludeIPs in SpiderIPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main function is not like this. Field excludeIPs is more of a syntax sugar , so that users can more flexibly define the IP address ranges of the IPPool. For example, create an IPPool without using excludeIPs , which contains two IP ranges: 172.18.41.40-172.18.41.44 and 172.18.41.46-172.18.41.50 , you should define the ips as follows: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : not-use-excludeips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 But in fact, this semantics can be more succinctly described through excludeIPs : apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : use-excludeips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 excludeIPs : - 172.18.41.45 Field excludeIPs will make sure that any Pod that allocates IP addresses from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with excludeIPs defined.","title":"IPPool excludeIPs"},{"location":"usage/reserved-ip/#use-spiderreservedip","text":"Unlike configuring field excluedIPs in SpiderIPPool CR, creating a SpiderReservedIP CR is really a way to define the global reserved IP address rules of a Kubernetes cluster. The IP addresses defined in ReservedIP cannot be used by any Pod in the cluster, regardless of whether some IPPools have inadvertently defined them. More details refer to definition of SpiderReservedIP .","title":"Use SpiderReservedIP"},{"location":"usage/reserved-ip/#set-up-spiderpool","text":"If you have not deployed Spiderpool yet, follow the guide installation for instructions on how to deploy and easily configure Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/reserved-ip/#get-started","text":"To understand how it works, let's do such a test. First, create an ReservedIP which reserves 9 IP addresses from 172.18.42.41 to 172.18.42.49 . kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderReservedIP metadata : name : test-ipv4-reservedip spec : ips : - 172.18.42.41-172.18.42.49 At the same time, create an IPPool with 10 IP addresses from 172.18.42.41 to 172.18.42.50 . Yes, we deliberately make it hold one more IP address than the ReservedIP above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ipv4-ippool spec : subnet : 172.18.42.0/24 ips : - 172.18.42.41-172.18.42.50 Then, create a Deployment with 3 replicas and allocate IP addresses to its Pods from the IPPool above. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : reservedip-deploy spec : replicas : 3 selector : matchLabels : app : reservedip-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"test-ipv4-ippool\"] } labels : app : reservedip-deploy spec : containers : - name : reservedip-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] After a while, only one of these Pods using IP 172.18.42.50 can run successfully because \"all IP used out\". kubectl get po -l app = reservedip-deploy -o wide NAME READY STATUS RESTARTS AGE IP NODE reservedip-deploy-6cf9858886-cm7bp 0 /1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-lb7cr 0 /1 ContainerCreating 0 35s <none> spider-worker reservedip-deploy-6cf9858886-pkcfl 1 /1 Running 0 35s 172 .18.42.50 spider-worker But when you delete this ReservedIP, everything will return to normal. kubectl delete -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle its IP addresses, but ReservedIP will still ensure that when the Pod is terminated, no other Pods can continue to use the reserved IP address. Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work.","title":"Get Started"},{"location":"usage/reserved-ip/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-reservedip.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/test-ipv4-ippool.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/reserved-ip/reservedip-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/reserved-ip/#a-trap","text":"So, can you use IPPool's field excludeIPs to achieve the same effect as ReservedIP? The answer is NO ! Look at such a case, now you want to reserve an IP 172.18.43.31 for an external application of the Kubernetes cluster, which may be a Redis node. To achieve this, you created such an IPPool: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : already-in-use spec : subnet : 172.18.43.0/24 ips : - 172.18.43.1-172.18.43.31 excludeIPs : - 172.18.43.31 I believe that if there is only one IPPool under the subnet 172.18.43.0/24 network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : created-by-someone-else spec : subnet : 172.18.43.0/24 ips : - 172.18.43.31-172.18.43.50 Different IPPools allow to define the same field subnet , more details refer to validation of IPPool . After a period of time, a Pod may be allocated with IP 172.18.43.31 from the IPPool created-by-someone-else , and then it holds the same IP address as your Redis node. After that, the Redis may not work as well. So, if you really want to reserve an IP address instead of excluding an IP address, SpiderReservedIP makes life better.","title":"A Trap"},{"location":"usage/route/","text":"Route support Description Spiderpool supports the configuration of routing information. Get Started Set up Spiderpool follow the guide installation to install Spiderpool. Create Subnet Create a SpiderSubnet and set up a subnet routes, a Pod and get the IP address from the AutoIPPool of the subnet, then see the routes configured in the subnet that exist in the AutoIPPool and Pod. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : ipv4-subnet-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.41-172.18.41.60 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1 Create Deployment By SpiderSubnet Create a Deployment whose Pods sets the Pod annotation ipam.spidernet.io/subnet to explicitly specify the subnet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : subnet-test-app spec : replicas : 1 selector : matchLabels : app : subnet-test-app template : metadata : annotations : ipam.spidernet.io/subnet : |- { \"ipv4\": [\"ipv4-subnet-route\"] } labels : app : subnet-test-app spec : containers : - name : route-test image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderpool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-subnet-test-app-v4-eth0-d69f2fb7bccf 4 172 .18.41.0/24 1 1 false false ~# kubectl get spiderpool auto-subnet-test-app-v4-eth0-d69f2fb7bccf -oyaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool ... ips: - 172 .18.41.41 podAffinity: matchLabels: app: subnet-test-app routes: - dst: 172 .18.42.0/24 gw: 172 .18.41.1 subnet: 172 .18.41.0/24 ... The Pods are running. ~# kubectl get pod -l app = subnet-test-app -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES subnet-test-app-59df44fc57-clp8t 1 /1 Running 0 3m48s 172 .18.41.41 spider-worker <none> <none> After the created Pod has obtained an IP from the automatic IPPool, the route set in the subnet, which is inherited by the automatic pool and takes effect in the Pod, you can view it via IP r as follows: ~# kubectl exec -it route-test-app-bdc84f8f5-2bxbr -- ip r 172 .18.41.0/24 dev eth0 scope link src 172 .18.41.41 172 .18.42.0/24 via 172 .18.41.1 dev eth0 Create IPPool Create a SpiderIPPool and set up the routes for the IPPool, create the Pod and assign IP addresses from the IPPool, you can see the routing information in the ippool pool taking effect within the Pod. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1 Create Deployment By IPPool Create a Deployment whose Pods sets the Pod annotation ipam.spidernet.io/ippool to explicitly specify the IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : ippool-test-app spec : replicas : 1 selector : matchLabels : app : ippool-test-app template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"ipv4-ippool-route\"] } labels : app : ippool-test-app spec : containers : - name : route-test image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. ~# kubectl get pod -l app = ippool-test-app -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-66fd47d895-pthx5 1 /1 Running 0 45s 172 .18.41.53 spider-worker <none> <none> After the created Pod has obtained an IP from IPPool, the route set in IPPool is already in effect in the Pod and you can view it via IP r as follows: ~# kubectl exec -it ippool-test-app-66fd47d895-pthx5 -- ip r 172 .18.41.0/24 dev eth0 scope link src 172 .18.41.53 172 .18.42.0/24 via 172 .18.41.1 dev eth0 Clean up Clean the relevant resources so that you can run this tutorial again kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route-deploy.yaml \\ --ignore-not-found = true","title":"Route support"},{"location":"usage/route/#route-support","text":"","title":"Route support"},{"location":"usage/route/#description","text":"Spiderpool supports the configuration of routing information.","title":"Description"},{"location":"usage/route/#get-started","text":"","title":"Get Started"},{"location":"usage/route/#set-up-spiderpool","text":"follow the guide installation to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/route/#create-subnet","text":"Create a SpiderSubnet and set up a subnet routes, a Pod and get the IP address from the AutoIPPool of the subnet, then see the routes configured in the subnet that exist in the AutoIPPool and Pod. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : ipv4-subnet-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.41-172.18.41.60 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1","title":"Create Subnet"},{"location":"usage/route/#create-deployment-by-spidersubnet","text":"Create a Deployment whose Pods sets the Pod annotation ipam.spidernet.io/subnet to explicitly specify the subnet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : subnet-test-app spec : replicas : 1 selector : matchLabels : app : subnet-test-app template : metadata : annotations : ipam.spidernet.io/subnet : |- { \"ipv4\": [\"ipv4-subnet-route\"] } labels : app : subnet-test-app spec : containers : - name : route-test image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderpool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-subnet-test-app-v4-eth0-d69f2fb7bccf 4 172 .18.41.0/24 1 1 false false ~# kubectl get spiderpool auto-subnet-test-app-v4-eth0-d69f2fb7bccf -oyaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool ... ips: - 172 .18.41.41 podAffinity: matchLabels: app: subnet-test-app routes: - dst: 172 .18.42.0/24 gw: 172 .18.41.1 subnet: 172 .18.41.0/24 ... The Pods are running. ~# kubectl get pod -l app = subnet-test-app -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES subnet-test-app-59df44fc57-clp8t 1 /1 Running 0 3m48s 172 .18.41.41 spider-worker <none> <none> After the created Pod has obtained an IP from the automatic IPPool, the route set in the subnet, which is inherited by the automatic pool and takes effect in the Pod, you can view it via IP r as follows: ~# kubectl exec -it route-test-app-bdc84f8f5-2bxbr -- ip r 172 .18.41.0/24 dev eth0 scope link src 172 .18.41.41 172 .18.42.0/24 via 172 .18.41.1 dev eth0","title":"Create Deployment By SpiderSubnet"},{"location":"usage/route/#create-ippool","text":"Create a SpiderIPPool and set up the routes for the IPPool, create the Pod and assign IP addresses from the IPPool, you can see the routing information in the ippool pool taking effect within the Pod. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1","title":"Create IPPool"},{"location":"usage/route/#create-deployment-by-ippool","text":"Create a Deployment whose Pods sets the Pod annotation ipam.spidernet.io/ippool to explicitly specify the IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : ippool-test-app spec : replicas : 1 selector : matchLabels : app : ippool-test-app template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"ipv4-ippool-route\"] } labels : app : ippool-test-app spec : containers : - name : route-test image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. ~# kubectl get pod -l app = ippool-test-app -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-66fd47d895-pthx5 1 /1 Running 0 45s 172 .18.41.53 spider-worker <none> <none> After the created Pod has obtained an IP from IPPool, the route set in IPPool is already in effect in the Pod and you can view it via IP r as follows: ~# kubectl exec -it ippool-test-app-66fd47d895-pthx5 -- ip r 172 .18.41.0/24 dev eth0 scope link src 172 .18.41.53 172 .18.42.0/24 via 172 .18.41.1 dev eth0","title":"Create Deployment By IPPool"},{"location":"usage/route/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/subnet-route-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/route/ippool-route-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/spider-multus-config/","text":"SpiderMultusConfig TODO","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config/#spidermultusconfig","text":"TODO","title":"SpiderMultusConfig"},{"location":"usage/spider-subnet-zh_CN/","text":"SpiderSubnet \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 SpiderIPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002 SpiderSubnet \u529f\u80fd \u542f\u7528 Subnet \u529f\u80fd\u65f6\uff0c\u6bcf\u4e00\u4e2a IPPool \u5b9e\u4f8b\u90fd\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684 Subnet \u5b9e\u4f8b\uff0cIPPool \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u5fc5\u987b\u662f Subnet \u5b9e\u4f8b\u4e2d IP \u5730\u5740\u7684\u5b50\u96c6\uff0cIPPool \u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u51fa\u73b0\u91cd\u53e0 IP \u5730\u5740\uff0c\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\u7684\u5404\u79cd\u8def\u7531\u5c5e\u6027\uff0c\u9ed8\u8ba4\u7ee7\u627f Subnet \u5b9e\u4f8b\u4e2d\u7684\u8bbe\u7f6e\u3002 \u5728\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e26\u6765\u4e86\u5982\u4e0b\u4e24\u79cd\u5b9e\u8df5\u624b\u6bb5\uff0c\u4ece\u800c\u5b8c\u6210\u5e94\u7528\u7ba1\u7406\u5458\u548c\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u804c\u8d23\u89e3\u8026\uff1a \u624b\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\uff0c\u53ef\u57fa\u4e8e\u5bf9\u5e94\u7684 Subnet \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u7ea6\u675f\uff0c\u6765\u83b7\u77e5\u53ef\u4f7f\u7528\u54ea\u4e9b IP \u5730\u5740\u3002 \u81ea\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u53ef\u5728 Pod annotation \u4e2d\u6ce8\u660e\u4f7f\u7528\u7684 Subnet \u5b9e\u4f8b\u540d\uff0c\u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u81ea\u52a8\u6839\u636e Subnet \u5b9e\u4f8b\u4e2d\u7684\u53ef\u7528 IP \u5730\u5740\u6765\u521b\u5efa\u56fa\u5b9a IP \u7684 IPPool \u5b9e\u4f8b\uff0c\u4ece\u4e2d\u5206\u914d IP \u5730\u5740\u7ed9 Pod\u3002\u5e76\u4e14 Spiderpool \u80fd\u591f\u81ea\u52a8\u76d1\u63a7\u5e94\u7528\u7684\u6269\u7f29\u5bb9\u548c\u5220\u9664\u4e8b\u4ef6\uff0c\u81ea\u52a8\u5b8c\u6210 IPPool \u4e2d\u7684 IP \u5730\u5740\u6269\u7f29\u5bb9\u548c\u5220\u9664\u3002 SpiderSubnet \u529f\u80fd\u8fd8\u652f\u6301\u4f17\u591a\u7684\u63a7\u5236\u5668\uff0c\u5982\uff1aReplicaSet\u3001Deployment\u3001Statefulset\u3001Daemonset\u3001Job\u3001Cronjob\uff0c\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\u7b49\u3002\u5bf9\u4e8e\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 \u793a\u4f8b \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableSpiderSubnet = true --set multus.multusCNI.defaultCniCRName = \"macvlan-ens192\" \u5982\u679c\u60a8\u6240\u5728\u5730\u533a\u662f\u4e2d\u56fd\u5927\u9646\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectl get po -n kube-system | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u548c ens224 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 ens192 \u4e0e ens224 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m \u521b\u5efa Subnets ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF \u4f7f\u7528\u5982\u4e0a\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderSubnet\uff0c\u5e76\u5206\u522b\u4e3a\u5176\u914d\u7f6e\u7f51\u5173\u4e0e\u8def\u7531\u4fe1\u606f\u3002 ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } \u81ea\u52a8\u56fa\u5b9a\u5355\u7f51\u5361 IP \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnet \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 1 \u4e2a\u5bf9\u5e94\u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 ipam.spidernet.io/ippool-ip-number \uff1a\u7528\u4e8e\u6307\u5b9a\u521b\u5efa IP \u6c60 \u4e2d \u7684 IP \u6570\u91cf\u3002\u8be5 annotation \u7684\u5199\u6cd5\u652f\u6301\u4e24\u79cd\u65b9\u5f0f\uff1a\u4e00\u79cd\u662f\u6570\u5b57\u7684\u65b9\u5f0f\u6307\u5b9a IP \u6c60\u7684\u56fa\u5b9a\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a1 \uff1b\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u52a0\u53f7\u548c\u6570\u5b57\u6307\u5b9a IP \u6c60\u7684\u76f8\u5bf9\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a+1 \uff0c\u5373\u8868\u793a IP \u6c60\u4e2d\u7684\u6570\u91cf\u4f1a\u81ea\u52a8\u5b9e\u65f6\u4fdd\u6301\u5728\u5e94\u7528\u7684\u526f\u672c\u6570\u7684\u57fa\u7840\u4e0a\u591a 1 \u4e2a IP\uff0c\u4ee5\u89e3\u51b3\u5e94\u7528\u5728\u5f39\u6027\u6269\u7f29\u5bb9\u7684\u65f6\u6709\u4e34\u65f6\u7684 IP \u53ef\u7528\u3002 ipam.spidernet.io/ippool-reclaim \uff1a \u5176\u8868\u793a\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u662f\u5426\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\u800c\u88ab\u56de\u6536\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u56fa\u5b9a IP \u6c60 \u4e0e Pod \u7684\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u81ea\u52a8\u6c60\u4f1a\u81ea\u52a8\u7ee7\u627f\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u4e3a\u5b9e\u73b0\u56fa\u5b9a IP \u6c60\u6548\u679c\uff0cSpiderpool \u4f1a\u7ed9\u81ea\u52a8\u6c60\u8865\u5145\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u5185\u5efa Label \u548c PodAffinity\uff0c\u5b83\u4eec\u7528\u4e8e\u6307\u5411\u5e94\u7528\uff0c\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\uff0c\u660e\u786e\u8be5\u6c60\u53ea\u670d\u52a1\u4e8e\u6307\u5b9a\u7684\u5e94\u7528\u3002\u5f53 ipam.spidernet.io/ippool-reclaim: false \u65f6\uff0c\u5220\u9664\u5e94\u7528\u540e\uff0cIP\u4f1a\u88ab\u56de\u6536\uff0c\u4f46\u81ea\u52a8\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002\u5982\u679c\u671f\u671b\u8be5\u6c60\u80fd\u88ab\u5176\u4ed6\u5e94\u7528\u6240\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u6458\u9664\u8fd9\u4e9b\u5185\u5efa Label \u548c PodAffinity\u3002 Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name \u7ecf\u8fc7\u591a\u6b21\u6d4b\u8bd5\uff0c\u4e0d\u65ad\u91cd\u542f Pod\uff0c\u5176 Pod \u7684 IP \u90fd\u88ab\u56fa\u5b9a\u5728 IP \u6c60\u8303\u56f4\u5185: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none> \u56fa\u5b9a IP \u6c60 IP \u6570\u91cf\u7684\u52a8\u6001\u6269\u7f29\u5bb9 \u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-ip-number : '+1'\uff0c\u5176\u8868\u793a\u5e94\u7528\u5206\u914d\u5230\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u7684\u526f\u672c\u6570\u591a 1 \u4e2a\uff0c\u5728\u5e94\u7528\u6eda\u52a8\u66f4\u65b0\u65f6\uff0c\u80fd\u591f\u907f\u514d\u65e7 Pod \u672a\u5220\u9664\uff0c\u65b0 Pod \u6ca1\u6709\u53ef\u7528 IP \u7684\u95ee\u9898\u3002 \u4ee5\u4e0b\u6f14\u793a\u4e86\u6269\u5bb9\u573a\u666f\uff0c\u5c06\u5e94\u7528\u7684\u526f\u672c\u6570\u4ece 2 \u6269\u5bb9\u5230 3\uff0c\u5e94\u7528\u5bf9\u5e94\u7684\u4e24\u4e2a\u56fa\u5b9a IP \u6c60\u4f1a\u81ea\u52a8\u4ece 3 \u4e2a IP \u6269\u5bb9\u5230 4 \u4e2a IP\uff0c\u4e00\u76f4\u4fdd\u6301\u4e00\u4e2a\u5197\u4f59 IP\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false \u901a\u8fc7\u4e0a\u8ff0\uff0cSpiderpool \u5bf9\u4e8e\u5e94\u7528\u6269\u7f29\u5bb9\u7684\u573a\u666f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u5373\u53ef\u3002 \u81ea\u52a8\u56de\u6536 IP \u6c60 \u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-reclaim \uff0c\u8be5\u6ce8\u89e3\u9ed8\u8ba4\u503c\u4e3a true \uff0c\u4e3a true \u65f6\uff0c\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\uff0c\u5c06\u81ea\u52a8\u5220\u9664\u5bf9\u5e94\u7684\u81ea\u52a8\u6c60\u3002\u5728\u672c\u6587\u4e2d\u8bbe\u7f6e\u4e3a false \uff0c\u5176\u8868\u793a\u5220\u9664\u5e94\u7528\u65f6\uff0c\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u4f1a\u56de\u6536\u5176\u4e2d\u88ab\u5206\u914d\u7684 IP \uff0c\u4f46\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\uff0c\u5e76\u4e14\u5f53\u4f7f\u7528\u76f8\u540c\u914d\u7f6e\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\u65f6\uff0c\u4f1a\u81ea\u52a8\u7ee7\u627f\u8be5 IP \u6c60\u3002 ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false \u4f7f\u7528\u4e0a\u8ff0\u6240\u793a\u7684\u5e94\u7528 Yaml\uff0c\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u4e0d\u4f1a\u518d\u6b21\u521b\u5efa\u65b0\u7684 IP \u6c60\uff0c\u5c06\u81ea\u52a8\u590d\u7528\u65e7 IP \u6c60\uff0c\u5e76\u4e14\u5176\u526f\u672c\u6570\u548c IP \u6c60\u7684 IP \u5206\u914d\u60c5\u51b5\u4e0e\u5b9e\u9645\u76f8\u540c\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false \u81ea\u52a8\u56fa\u5b9a\u591a\u7f51\u5361 IP \u5982\u679c\u60a8\u5e0c\u671b\u4e3a Pod \u5b9e\u73b0\u591a\u7f51\u5361 IP \u7684\u56fa\u5b9a\uff0c\u53c2\u8003\u672c\u7ae0\u8282\u3002\u5728\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u6bcf\u4e2a\u526f\u672c\u62e5\u6709\u591a\u5f20\u7f51\u5361\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnets \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 2 \u4e2a\u5c5e\u4e8e\u4e0d\u540c Underlay \u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u53e6\u4e00\u5f20\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a 2 \u4e2a Underlay \u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u5bf9\u5e94\u7684\u56fa\u5b9a IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528 Pod \u7684\u4e24\u5f20\u7f51\u5361\u5206\u522b\u5f62\u6210\u7ed1\u5b9a\u3002\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u7684\u56fa\u5b9a\u6c60\u90fd\u5c06\u4f1a\u81ea\u52a8\u7ee7\u627f\u5176\u6240\u5f52\u5c5e\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet \u4e5f\u652f\u6301\u591a\u7f51\u5361\u7684\u52a8\u6001 IP \u6269\u7f29\u5bb9\u3001\u81ea\u52a8\u56de\u6536 IP \u6c60\u7b49\u529f\u80fd\u3002 \u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u7ee7\u627f\u5b50\u7f51\u5c5e\u6027 \u5982\u4e0b\u662f\u4e00\u4e2a\u5f52\u5c5e\u4e8e\u5b50\u7f51 subnet-6 \uff0c\u5b50\u7f51\u53f7\u4e3a\uff1a 10.6.0.0/16 \u7684 IPPool \u5b9e\u4f8b\u793a\u4f8b\u3002\u8be5 IPPool \u5b9e\u4f8b\u7684\u53ef\u7528 IP \u8303\u56f4\u5fc5\u987b\u662f\u5b50\u7f51 subnet-6.spec.ips \u7684\u5b50\u96c6\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF \u4f7f\u7528\u4e0a\u8ff0 Yaml\uff0c\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u5230\u5b83\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684\u5b50\u7f51\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u5bf9\u5e94\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u603b\u7ed3 SpiderSubnet \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u5c06\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u5458\u7684\u8d23\u4efb\u5206\u5f00\uff0c\u652f\u6301\u81ea\u52a8\u521b\u5efa\u548c\u52a8\u6001\u6269\u5c55\u56fa\u5b9a IPPool \u5230\u6bcf\u4e2a\u9700\u8981\u9759\u6001 IP \u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"SpiderSubnet"},{"location":"usage/spider-subnet-zh_CN/#spidersubnet","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderSubnet"},{"location":"usage/spider-subnet-zh_CN/#_1","text":"SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 SpiderIPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-subnet-zh_CN/#spidersubnet_1","text":"\u542f\u7528 Subnet \u529f\u80fd\u65f6\uff0c\u6bcf\u4e00\u4e2a IPPool \u5b9e\u4f8b\u90fd\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684 Subnet \u5b9e\u4f8b\uff0cIPPool \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u5fc5\u987b\u662f Subnet \u5b9e\u4f8b\u4e2d IP \u5730\u5740\u7684\u5b50\u96c6\uff0cIPPool \u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u51fa\u73b0\u91cd\u53e0 IP \u5730\u5740\uff0c\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\u7684\u5404\u79cd\u8def\u7531\u5c5e\u6027\uff0c\u9ed8\u8ba4\u7ee7\u627f Subnet \u5b9e\u4f8b\u4e2d\u7684\u8bbe\u7f6e\u3002 \u5728\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e26\u6765\u4e86\u5982\u4e0b\u4e24\u79cd\u5b9e\u8df5\u624b\u6bb5\uff0c\u4ece\u800c\u5b8c\u6210\u5e94\u7528\u7ba1\u7406\u5458\u548c\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u804c\u8d23\u89e3\u8026\uff1a \u624b\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\uff0c\u53ef\u57fa\u4e8e\u5bf9\u5e94\u7684 Subnet \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u7ea6\u675f\uff0c\u6765\u83b7\u77e5\u53ef\u4f7f\u7528\u54ea\u4e9b IP \u5730\u5740\u3002 \u81ea\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u53ef\u5728 Pod annotation \u4e2d\u6ce8\u660e\u4f7f\u7528\u7684 Subnet \u5b9e\u4f8b\u540d\uff0c\u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u81ea\u52a8\u6839\u636e Subnet \u5b9e\u4f8b\u4e2d\u7684\u53ef\u7528 IP \u5730\u5740\u6765\u521b\u5efa\u56fa\u5b9a IP \u7684 IPPool \u5b9e\u4f8b\uff0c\u4ece\u4e2d\u5206\u914d IP \u5730\u5740\u7ed9 Pod\u3002\u5e76\u4e14 Spiderpool \u80fd\u591f\u81ea\u52a8\u76d1\u63a7\u5e94\u7528\u7684\u6269\u7f29\u5bb9\u548c\u5220\u9664\u4e8b\u4ef6\uff0c\u81ea\u52a8\u5b8c\u6210 IPPool \u4e2d\u7684 IP \u5730\u5740\u6269\u7f29\u5bb9\u548c\u5220\u9664\u3002 SpiderSubnet \u529f\u80fd\u8fd8\u652f\u6301\u4f17\u591a\u7684\u63a7\u5236\u5668\uff0c\u5982\uff1aReplicaSet\u3001Deployment\u3001Statefulset\u3001Daemonset\u3001Job\u3001Cronjob\uff0c\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\u7b49\u3002\u5bf9\u4e8e\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 \u793a\u4f8b","title":"SpiderSubnet \u529f\u80fd"},{"location":"usage/spider-subnet-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/spider-subnet-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/spider-subnet-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableSpiderSubnet = true --set multus.multusCNI.defaultCniCRName = \"macvlan-ens192\" \u5982\u679c\u60a8\u6240\u5728\u5730\u533a\u662f\u4e2d\u56fd\u5927\u9646\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectl get po -n kube-system | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/spider-subnet-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u548c ens224 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 ens192 \u4e0e ens224 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/spider-subnet-zh_CN/#subnets","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF \u4f7f\u7528\u5982\u4e0a\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderSubnet\uff0c\u5e76\u5206\u522b\u4e3a\u5176\u914d\u7f6e\u7f51\u5173\u4e0e\u8def\u7531\u4fe1\u606f\u3002 ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 }","title":"\u521b\u5efa Subnets"},{"location":"usage/spider-subnet-zh_CN/#ip","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnet \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 1 \u4e2a\u5bf9\u5e94\u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 ipam.spidernet.io/ippool-ip-number \uff1a\u7528\u4e8e\u6307\u5b9a\u521b\u5efa IP \u6c60 \u4e2d \u7684 IP \u6570\u91cf\u3002\u8be5 annotation \u7684\u5199\u6cd5\u652f\u6301\u4e24\u79cd\u65b9\u5f0f\uff1a\u4e00\u79cd\u662f\u6570\u5b57\u7684\u65b9\u5f0f\u6307\u5b9a IP \u6c60\u7684\u56fa\u5b9a\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a1 \uff1b\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u52a0\u53f7\u548c\u6570\u5b57\u6307\u5b9a IP \u6c60\u7684\u76f8\u5bf9\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a+1 \uff0c\u5373\u8868\u793a IP \u6c60\u4e2d\u7684\u6570\u91cf\u4f1a\u81ea\u52a8\u5b9e\u65f6\u4fdd\u6301\u5728\u5e94\u7528\u7684\u526f\u672c\u6570\u7684\u57fa\u7840\u4e0a\u591a 1 \u4e2a IP\uff0c\u4ee5\u89e3\u51b3\u5e94\u7528\u5728\u5f39\u6027\u6269\u7f29\u5bb9\u7684\u65f6\u6709\u4e34\u65f6\u7684 IP \u53ef\u7528\u3002 ipam.spidernet.io/ippool-reclaim \uff1a \u5176\u8868\u793a\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u662f\u5426\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\u800c\u88ab\u56de\u6536\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u56fa\u5b9a IP \u6c60 \u4e0e Pod \u7684\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u81ea\u52a8\u6c60\u4f1a\u81ea\u52a8\u7ee7\u627f\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u4e3a\u5b9e\u73b0\u56fa\u5b9a IP \u6c60\u6548\u679c\uff0cSpiderpool \u4f1a\u7ed9\u81ea\u52a8\u6c60\u8865\u5145\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u5185\u5efa Label \u548c PodAffinity\uff0c\u5b83\u4eec\u7528\u4e8e\u6307\u5411\u5e94\u7528\uff0c\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\uff0c\u660e\u786e\u8be5\u6c60\u53ea\u670d\u52a1\u4e8e\u6307\u5b9a\u7684\u5e94\u7528\u3002\u5f53 ipam.spidernet.io/ippool-reclaim: false \u65f6\uff0c\u5220\u9664\u5e94\u7528\u540e\uff0cIP\u4f1a\u88ab\u56de\u6536\uff0c\u4f46\u81ea\u52a8\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002\u5982\u679c\u671f\u671b\u8be5\u6c60\u80fd\u88ab\u5176\u4ed6\u5e94\u7528\u6240\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u6458\u9664\u8fd9\u4e9b\u5185\u5efa Label \u548c PodAffinity\u3002 Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name \u7ecf\u8fc7\u591a\u6b21\u6d4b\u8bd5\uff0c\u4e0d\u65ad\u91cd\u542f Pod\uff0c\u5176 Pod \u7684 IP \u90fd\u88ab\u56fa\u5b9a\u5728 IP \u6c60\u8303\u56f4\u5185: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none>","title":"\u81ea\u52a8\u56fa\u5b9a\u5355\u7f51\u5361 IP"},{"location":"usage/spider-subnet-zh_CN/#ip-ip","text":"\u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-ip-number : '+1'\uff0c\u5176\u8868\u793a\u5e94\u7528\u5206\u914d\u5230\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u7684\u526f\u672c\u6570\u591a 1 \u4e2a\uff0c\u5728\u5e94\u7528\u6eda\u52a8\u66f4\u65b0\u65f6\uff0c\u80fd\u591f\u907f\u514d\u65e7 Pod \u672a\u5220\u9664\uff0c\u65b0 Pod \u6ca1\u6709\u53ef\u7528 IP \u7684\u95ee\u9898\u3002 \u4ee5\u4e0b\u6f14\u793a\u4e86\u6269\u5bb9\u573a\u666f\uff0c\u5c06\u5e94\u7528\u7684\u526f\u672c\u6570\u4ece 2 \u6269\u5bb9\u5230 3\uff0c\u5e94\u7528\u5bf9\u5e94\u7684\u4e24\u4e2a\u56fa\u5b9a IP \u6c60\u4f1a\u81ea\u52a8\u4ece 3 \u4e2a IP \u6269\u5bb9\u5230 4 \u4e2a IP\uff0c\u4e00\u76f4\u4fdd\u6301\u4e00\u4e2a\u5197\u4f59 IP\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false \u901a\u8fc7\u4e0a\u8ff0\uff0cSpiderpool \u5bf9\u4e8e\u5e94\u7528\u6269\u7f29\u5bb9\u7684\u573a\u666f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u5373\u53ef\u3002","title":"\u56fa\u5b9a IP \u6c60 IP \u6570\u91cf\u7684\u52a8\u6001\u6269\u7f29\u5bb9"},{"location":"usage/spider-subnet-zh_CN/#ip_1","text":"\u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-reclaim \uff0c\u8be5\u6ce8\u89e3\u9ed8\u8ba4\u503c\u4e3a true \uff0c\u4e3a true \u65f6\uff0c\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\uff0c\u5c06\u81ea\u52a8\u5220\u9664\u5bf9\u5e94\u7684\u81ea\u52a8\u6c60\u3002\u5728\u672c\u6587\u4e2d\u8bbe\u7f6e\u4e3a false \uff0c\u5176\u8868\u793a\u5220\u9664\u5e94\u7528\u65f6\uff0c\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u4f1a\u56de\u6536\u5176\u4e2d\u88ab\u5206\u914d\u7684 IP \uff0c\u4f46\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\uff0c\u5e76\u4e14\u5f53\u4f7f\u7528\u76f8\u540c\u914d\u7f6e\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\u65f6\uff0c\u4f1a\u81ea\u52a8\u7ee7\u627f\u8be5 IP \u6c60\u3002 ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false \u4f7f\u7528\u4e0a\u8ff0\u6240\u793a\u7684\u5e94\u7528 Yaml\uff0c\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u4e0d\u4f1a\u518d\u6b21\u521b\u5efa\u65b0\u7684 IP \u6c60\uff0c\u5c06\u81ea\u52a8\u590d\u7528\u65e7 IP \u6c60\uff0c\u5e76\u4e14\u5176\u526f\u672c\u6570\u548c IP \u6c60\u7684 IP \u5206\u914d\u60c5\u51b5\u4e0e\u5b9e\u9645\u76f8\u540c\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false","title":"\u81ea\u52a8\u56de\u6536 IP \u6c60"},{"location":"usage/spider-subnet-zh_CN/#ip_2","text":"\u5982\u679c\u60a8\u5e0c\u671b\u4e3a Pod \u5b9e\u73b0\u591a\u7f51\u5361 IP \u7684\u56fa\u5b9a\uff0c\u53c2\u8003\u672c\u7ae0\u8282\u3002\u5728\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u6bcf\u4e2a\u526f\u672c\u62e5\u6709\u591a\u5f20\u7f51\u5361\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnets \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 2 \u4e2a\u5c5e\u4e8e\u4e0d\u540c Underlay \u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u53e6\u4e00\u5f20\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a 2 \u4e2a Underlay \u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u5bf9\u5e94\u7684\u56fa\u5b9a IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528 Pod \u7684\u4e24\u5f20\u7f51\u5361\u5206\u522b\u5f62\u6210\u7ed1\u5b9a\u3002\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u7684\u56fa\u5b9a\u6c60\u90fd\u5c06\u4f1a\u81ea\u52a8\u7ee7\u627f\u5176\u6240\u5f52\u5c5e\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet \u4e5f\u652f\u6301\u591a\u7f51\u5361\u7684\u52a8\u6001 IP \u6269\u7f29\u5bb9\u3001\u81ea\u52a8\u56de\u6536 IP \u6c60\u7b49\u529f\u80fd\u3002","title":"\u81ea\u52a8\u56fa\u5b9a\u591a\u7f51\u5361 IP"},{"location":"usage/spider-subnet-zh_CN/#ippool","text":"\u5982\u4e0b\u662f\u4e00\u4e2a\u5f52\u5c5e\u4e8e\u5b50\u7f51 subnet-6 \uff0c\u5b50\u7f51\u53f7\u4e3a\uff1a 10.6.0.0/16 \u7684 IPPool \u5b9e\u4f8b\u793a\u4f8b\u3002\u8be5 IPPool \u5b9e\u4f8b\u7684\u53ef\u7528 IP \u8303\u56f4\u5fc5\u987b\u662f\u5b50\u7f51 subnet-6.spec.ips \u7684\u5b50\u96c6\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF \u4f7f\u7528\u4e0a\u8ff0 Yaml\uff0c\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u5230\u5b83\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684\u5b50\u7f51\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u5bf9\u5e94\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 }","title":"\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u7ee7\u627f\u5b50\u7f51\u5c5e\u6027"},{"location":"usage/spider-subnet-zh_CN/#_4","text":"SpiderSubnet \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u5c06\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u5458\u7684\u8d23\u4efb\u5206\u5f00\uff0c\u652f\u6301\u81ea\u52a8\u521b\u5efa\u548c\u52a8\u6001\u6269\u5c55\u56fa\u5b9a IPPool \u5230\u6bcf\u4e2a\u9700\u8981\u9759\u6001 IP \u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-subnet/","text":"SpiderSubnet The Spiderpool owns a CRD SpiderSubnet, which can help applications (such as Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet) to create a corresponding SpiderIPPool. Here are some annotations that you should write down on the application template Pod annotation: Annotation Description Example ipam.spidernet.io/subnet Choose one SpiderSubnet V4 and V6 CR to use {\"interface\":\"eth0\", \"ipv4\":[\"subnet-demo-v4\"], \"ipv6\":[\"subnet-demo-v6\"]} ipam.spidernet.io/subnets Choose multiple SpiderSubnet V4 and V6 CR to use (the current version only supports to use the first one) [{\"interface\":\"eth0\", \"ipv4\":[\"v4-subnet1\"], \"ipv6\":[\"v6-subnet1\"]}] ipam.spidernet.io/ippool-ip-number The IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1') +2 ipam.spidernet.io/ippool-reclaim Specify the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true') true Notice The annotation ipam.spidernet.io/subnets has higher priority over ipam.spidernet.io/subnet . If you specify both of them two, it only uses ipam.spidernet.io/subnets mode. In annotation ipam.spidernet.io/subnet mode, it will use default interface name eth0 if you do not set interface property. For annotation ipam.spidernet.io/ippool-ip-number , you can use '2' for fixed IP number or '+2' for flexible mode. The value '+2' means the SpiderSubnet auto-created IPPool will add 2 more IPs based on your application replicas. If you choose to use flexible mode, the auto-created IPPool IPs will expand or shrink dynamically by your application replicas. This is an optional annotation. If left unset, it will use the clusterSubnetDefaultFlexibleIPNumber property from the spiderpool-conf ConfigMap as the flexible IP number in flexible mode. Refer to config for details. The current version only supports using one SpiderSubnet V4/V6 CR for one Interface. You shouldn't specify two or more SpiderSubnet V4 CRs. The system will choose the first one to use. It's invalid to modify the Auto-created IPPool Spec.IPs by users. The auto-created IPPool will only serve your specified application, and the system will bind a special Spiderpool podAffinity to it. If you want to use ipam.spidernet.io/ippool or ipam.spidernet.io/ippools annotations to specify the 'reserved' auto-created IPPool, you should edit the IPPool to remove its special Spiderpool podAffinity and annotations ipam.spidernet.io/owner-application* . Get Started Enable SpiderSubnet feature Firstly, please ensure you have installed the Spiderpool and configure the CNI file, refer to install for details. Check configmap spiderpool-conf property enableSpiderSubnet whether is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , just execute helm upgrade spiderpool spiderpool/spiderpool --set ipam.enableSpiderSubnet=true -n kube-system . Create a SpiderSubnet Install a SpiderSubnet example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/subnet-demo.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/deploy-use-subnet.yaml Check the related CR data Here's the SpiderSubnet, SpiderIPPool, and Pod information. $ kubectl get ss NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 3 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 3 200 $ kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-demo-deploy-subnet-eth0-337bc 4 172.16.0.0/16 1 3 false false default auto6-demo-deploy-subnet-eth0-337bc 6 fc00:f853:ccd:e790::/64 1 3 false false default ------------------------------------------------------------------------------------------ $ kubectl get sp auto4-demo-deploy-subnet-eth0-337bc -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: annotations: ipam.spidernet.io/ippool-ip-number: \"+2\" creationTimestamp: \"2023-05-09T09:25:24Z\" finalizers: - spiderpool.spidernet.io generation: 1 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172-16-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: Deployment ipam.spidernet.io/owner-application-name: demo-deploy-subnet ipam.spidernet.io/owner-application-namespace: default ipam.spidernet.io/owner-application-uid: bea129aa-acf5-40cb-8669-337bc1e95a41 ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto4-demo-deploy-subnet-eth0-337bc ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: 3f5882a0-12e6-40f5-a103-431de700195f resourceVersion: \"1381\" uid: 4ebc5725-5d3d-448a-889f-ce9e3943b2aa spec: default: false disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.3 podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: Deployment ipam.spidernet.io/app-name: demo-deploy-subnet ipam.spidernet.io/app-namespace: default subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 1 allocatedIPs: '{\"172.16.41.1\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-kls7s\",\"podUid\":\"cc310a87-8c5a-4bff-9a84-0c4975bd5921\"}}' totalIPCount: 3 $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-778786474b-kls7s 1/1 Running 0 3m10s 172.16.41.1 spider-worker <none> <none> Try to scale the deployment replicas and check the SpiderIPPool. $ kubectl patch deploy demo-deploy-subnet --patch '{\"spec\": {\"replicas\": 2}}' deployment.apps/demo-deploy-subnet patched ------------------------------------------------------------------------------------------ $ kubectl get ss NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 4 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 4 200 ------------------------------------------------------------------------------------------ $ kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-demo-deploy-subnet-eth0-337bc 4 172.16.0.0/16 2 4 false false default auto6-demo-deploy-subnet-eth0-337bc 6 fc00:f853:ccd:e790::/64 2 4 false false default ------------------------------------------------------------------------------------------ $ kubectl get sp auto4-demo-deploy-subnet-eth0-337bc -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: annotations: ipam.spidernet.io/ippool-ip-number: \"+2\" creationTimestamp: \"2023-05-09T09:25:24Z\" finalizers: - spiderpool.spidernet.io generation: 2 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172-16-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: Deployment ipam.spidernet.io/owner-application-name: demo-deploy-subnet ipam.spidernet.io/owner-application-namespace: default ipam.spidernet.io/owner-application-uid: bea129aa-acf5-40cb-8669-337bc1e95a41 ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto4-demo-deploy-subnet-eth0-337bc ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: 3f5882a0-12e6-40f5-a103-431de700195f resourceVersion: \"2004\" uid: 4ebc5725-5d3d-448a-889f-ce9e3943b2aa spec: default: false disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.4 podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: Deployment ipam.spidernet.io/app-name: demo-deploy-subnet ipam.spidernet.io/app-namespace: default subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 2 allocatedIPs: '{\"172.16.41.1\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-kls7s\",\"podUid\":\"cc310a87-8c5a-4bff-9a84-0c4975bd5921\"},\"172.16.41.2\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-2kf24\",\"podUid\":\"0a239f85-be47-4b98-bf29-c8bf02b6b59e\"}}' totalIPCount: 4 ------------------------------------------------------------------------------------------ $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-778786474b-2kf24 1/1 Running 0 63s 172.16.41.2 spider-control-plane <none> <none> demo-deploy-subnet-778786474b-kls7s 1/1 Running 0 4m39s 172.16.41.1 spider-worker <none> <none> As you can see, the SpiderSubnet object subnet-demo-v4 allocates another IP to SpiderIPPool auto4-demo-deploy-subnet-eth0-337bc and SpiderSubnet object subnet-demo-v6 allocates another IP to SpiderIPPool auto6-demo-deploy-subnet-eth0-337bc . SpiderSubnet with multiple interfaces Make sure the interface name and use ipam.spidernet.io/subnets annotation just like this: annotations: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni2 ipam.spidernet.io/subnets: |- [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}] Install the example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/multiple-interfaces.yaml","title":"SpiderSubnet"},{"location":"usage/spider-subnet/#spidersubnet","text":"The Spiderpool owns a CRD SpiderSubnet, which can help applications (such as Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet) to create a corresponding SpiderIPPool. Here are some annotations that you should write down on the application template Pod annotation: Annotation Description Example ipam.spidernet.io/subnet Choose one SpiderSubnet V4 and V6 CR to use {\"interface\":\"eth0\", \"ipv4\":[\"subnet-demo-v4\"], \"ipv6\":[\"subnet-demo-v6\"]} ipam.spidernet.io/subnets Choose multiple SpiderSubnet V4 and V6 CR to use (the current version only supports to use the first one) [{\"interface\":\"eth0\", \"ipv4\":[\"v4-subnet1\"], \"ipv6\":[\"v6-subnet1\"]}] ipam.spidernet.io/ippool-ip-number The IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1') +2 ipam.spidernet.io/ippool-reclaim Specify the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true') true","title":"SpiderSubnet"},{"location":"usage/spider-subnet/#notice","text":"The annotation ipam.spidernet.io/subnets has higher priority over ipam.spidernet.io/subnet . If you specify both of them two, it only uses ipam.spidernet.io/subnets mode. In annotation ipam.spidernet.io/subnet mode, it will use default interface name eth0 if you do not set interface property. For annotation ipam.spidernet.io/ippool-ip-number , you can use '2' for fixed IP number or '+2' for flexible mode. The value '+2' means the SpiderSubnet auto-created IPPool will add 2 more IPs based on your application replicas. If you choose to use flexible mode, the auto-created IPPool IPs will expand or shrink dynamically by your application replicas. This is an optional annotation. If left unset, it will use the clusterSubnetDefaultFlexibleIPNumber property from the spiderpool-conf ConfigMap as the flexible IP number in flexible mode. Refer to config for details. The current version only supports using one SpiderSubnet V4/V6 CR for one Interface. You shouldn't specify two or more SpiderSubnet V4 CRs. The system will choose the first one to use. It's invalid to modify the Auto-created IPPool Spec.IPs by users. The auto-created IPPool will only serve your specified application, and the system will bind a special Spiderpool podAffinity to it. If you want to use ipam.spidernet.io/ippool or ipam.spidernet.io/ippools annotations to specify the 'reserved' auto-created IPPool, you should edit the IPPool to remove its special Spiderpool podAffinity and annotations ipam.spidernet.io/owner-application* .","title":"Notice"},{"location":"usage/spider-subnet/#get-started","text":"","title":"Get Started"},{"location":"usage/spider-subnet/#enable-spidersubnet-feature","text":"Firstly, please ensure you have installed the Spiderpool and configure the CNI file, refer to install for details. Check configmap spiderpool-conf property enableSpiderSubnet whether is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , just execute helm upgrade spiderpool spiderpool/spiderpool --set ipam.enableSpiderSubnet=true -n kube-system .","title":"Enable SpiderSubnet feature"},{"location":"usage/spider-subnet/#create-a-spidersubnet","text":"Install a SpiderSubnet example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/subnet-demo.yaml kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/deploy-use-subnet.yaml","title":"Create a SpiderSubnet"},{"location":"usage/spider-subnet/#check-the-related-cr-data","text":"Here's the SpiderSubnet, SpiderIPPool, and Pod information. $ kubectl get ss NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 3 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 3 200 $ kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-demo-deploy-subnet-eth0-337bc 4 172.16.0.0/16 1 3 false false default auto6-demo-deploy-subnet-eth0-337bc 6 fc00:f853:ccd:e790::/64 1 3 false false default ------------------------------------------------------------------------------------------ $ kubectl get sp auto4-demo-deploy-subnet-eth0-337bc -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: annotations: ipam.spidernet.io/ippool-ip-number: \"+2\" creationTimestamp: \"2023-05-09T09:25:24Z\" finalizers: - spiderpool.spidernet.io generation: 1 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172-16-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: Deployment ipam.spidernet.io/owner-application-name: demo-deploy-subnet ipam.spidernet.io/owner-application-namespace: default ipam.spidernet.io/owner-application-uid: bea129aa-acf5-40cb-8669-337bc1e95a41 ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto4-demo-deploy-subnet-eth0-337bc ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: 3f5882a0-12e6-40f5-a103-431de700195f resourceVersion: \"1381\" uid: 4ebc5725-5d3d-448a-889f-ce9e3943b2aa spec: default: false disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.3 podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: Deployment ipam.spidernet.io/app-name: demo-deploy-subnet ipam.spidernet.io/app-namespace: default subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 1 allocatedIPs: '{\"172.16.41.1\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-kls7s\",\"podUid\":\"cc310a87-8c5a-4bff-9a84-0c4975bd5921\"}}' totalIPCount: 3 $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-778786474b-kls7s 1/1 Running 0 3m10s 172.16.41.1 spider-worker <none> <none> Try to scale the deployment replicas and check the SpiderIPPool. $ kubectl patch deploy demo-deploy-subnet --patch '{\"spec\": {\"replicas\": 2}}' deployment.apps/demo-deploy-subnet patched ------------------------------------------------------------------------------------------ $ kubectl get ss NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-demo-v4 4 172.16.0.0/16 4 200 subnet-demo-v6 6 fc00:f853:ccd:e790::/64 4 200 ------------------------------------------------------------------------------------------ $ kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-demo-deploy-subnet-eth0-337bc 4 172.16.0.0/16 2 4 false false default auto6-demo-deploy-subnet-eth0-337bc 6 fc00:f853:ccd:e790::/64 2 4 false false default ------------------------------------------------------------------------------------------ $ kubectl get sp auto4-demo-deploy-subnet-eth0-337bc -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: annotations: ipam.spidernet.io/ippool-ip-number: \"+2\" creationTimestamp: \"2023-05-09T09:25:24Z\" finalizers: - spiderpool.spidernet.io generation: 2 labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172-16-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: Deployment ipam.spidernet.io/owner-application-name: demo-deploy-subnet ipam.spidernet.io/owner-application-namespace: default ipam.spidernet.io/owner-application-uid: bea129aa-acf5-40cb-8669-337bc1e95a41 ipam.spidernet.io/owner-spider-subnet: subnet-demo-v4 name: auto4-demo-deploy-subnet-eth0-337bc ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderSubnet name: subnet-demo-v4 uid: 3f5882a0-12e6-40f5-a103-431de700195f resourceVersion: \"2004\" uid: 4ebc5725-5d3d-448a-889f-ce9e3943b2aa spec: default: false disable: false ipVersion: 4 ips: - 172.16.41.1-172.16.41.4 podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: Deployment ipam.spidernet.io/app-name: demo-deploy-subnet ipam.spidernet.io/app-namespace: default subnet: 172.16.0.0/16 vlan: 0 status: allocatedIPCount: 2 allocatedIPs: '{\"172.16.41.1\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-kls7s\",\"podUid\":\"cc310a87-8c5a-4bff-9a84-0c4975bd5921\"},\"172.16.41.2\":{\"interface\":\"eth0\",\"pod\":\"default/demo-deploy-subnet-778786474b-2kf24\",\"podUid\":\"0a239f85-be47-4b98-bf29-c8bf02b6b59e\"}}' totalIPCount: 4 ------------------------------------------------------------------------------------------ $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deploy-subnet-778786474b-2kf24 1/1 Running 0 63s 172.16.41.2 spider-control-plane <none> <none> demo-deploy-subnet-778786474b-kls7s 1/1 Running 0 4m39s 172.16.41.1 spider-worker <none> <none> As you can see, the SpiderSubnet object subnet-demo-v4 allocates another IP to SpiderIPPool auto4-demo-deploy-subnet-eth0-337bc and SpiderSubnet object subnet-demo-v6 allocates another IP to SpiderIPPool auto6-demo-deploy-subnet-eth0-337bc .","title":"Check the related CR data"},{"location":"usage/spider-subnet/#spidersubnet-with-multiple-interfaces","text":"Make sure the interface name and use ipam.spidernet.io/subnets annotation just like this: annotations: k8s.v1.cni.cncf.io/networks: kube-system/macvlan-cni2 ipam.spidernet.io/subnets: |- [{\"interface\": \"eth0\", \"ipv4\": [\"subnet-demo-v4-1\"], \"ipv6\": [\"subnet-demo-v6-1\"]}, {\"interface\": \"net2\", \"ipv4\": [\"subnet-demo-v4-2\"], \"ipv6\": [\"subnet-demo-v6-2\"]}] Install the example: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/spider-subnet/multiple-interfaces.yaml","title":"SpiderSubnet with multiple interfaces"},{"location":"usage/statefulset/","text":"StatefulSet Description The spiderpool supports IP assignment for StatefulSet. When the number of statefulset replicas is not scaled up or down, all Pods could hold same IP address even when the Pod is restarting or rebuilding. Pod restarts Once a Pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID. In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property. Pod is deleted and re-created After deleting a StatefulSet Pod, kubernetes will re-create a Pod with the same name. In this case, Spiderpool will also keep the previous IP and update the ContainerID. Notice Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its Pods are running. When the statefulset is scaled down and then scaled up, the scaled-up Pod is not guaranteed to get the previous IP. The IP-GC feature (reclaim IP for the Pod of graceful-period timeout) does work for StatefulSet Pod. Get Started Enable StatefulSet support Firstly, please ensure you have installed the spiderpool and configure the CNI file. Refer to install for details. Check whether the property enableStatefulSet of the configmap spiderpool-conf is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , run helm upgrade spiderpool spiderpool/spiderpool --set ipam.enableStatefulSet=true -n kube-system . Create a StatefulSet This is an example to install a StatefulSet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset/statefulset-demo.yaml Validate the Spiderpool related CR data Here's the created Pod, spiderippool, and spiderendpoint CR information: $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 8s 172.22.40.181 spider-worker <none> <none> --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml ... \"172.22.40.181\":{\"interface\":\"eth0\",\"pod\":\"default/demo-sts-0\",\"podUid\":\"fa50a7c2-99e7-4e97-a3f1-8d6503067b54\"} ... --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml ... status: current: ips: - interface: eth0 ipv4: 172.22.40.181/16 ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::d/64 ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker uid: fa50a7c2-99e7-4e97-a3f1-8d6503067b54 ownerControllerName: demo-sts ownerControllerType: StatefulSet ... Try to delete Pod demo-sts-0 and check whether the rebuilding Pod keeps the previous IP or not. $ kubectl delete po demo-sts-0 pod \"demo-sts-0\" deleted --------------------------------------------------------------------------------------------------------------------- $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 12s 172.22.40.181 spider-worker <none> <none> --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml ... \"172.22.40.181\":{\"interface\":\"eth0\",\"pod\":\"default/demo-sts-0\",\"podUid\":\"425d6552-63bb-4b4c-aab2-b2db95de0ab1\"} ... --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml ... status: current: ips: - interface: eth0 ipv4: 172.22.40.181/16 ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::d/64 ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker uid: 425d6552-63bb-4b4c-aab2-b2db95de0ab1 ownerControllerName: demo-sts ownerControllerType: StatefulSet ... And you can see, the re-created Pod still holds the previous IP, spiderippool, and spiderendpoint updated containerID property. clean up Delete the StatefulSet object: demo-sts . $ kubectl delete sts demo-sts statefulset.apps \"demo-sts\" deleted --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0 --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml Error from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found The related data is cleaned up now.","title":"StatefulSet"},{"location":"usage/statefulset/#statefulset","text":"","title":"StatefulSet"},{"location":"usage/statefulset/#description","text":"The spiderpool supports IP assignment for StatefulSet. When the number of statefulset replicas is not scaled up or down, all Pods could hold same IP address even when the Pod is restarting or rebuilding. Pod restarts Once a Pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID. In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property. Pod is deleted and re-created After deleting a StatefulSet Pod, kubernetes will re-create a Pod with the same name. In this case, Spiderpool will also keep the previous IP and update the ContainerID.","title":"Description"},{"location":"usage/statefulset/#notice","text":"Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its Pods are running. When the statefulset is scaled down and then scaled up, the scaled-up Pod is not guaranteed to get the previous IP. The IP-GC feature (reclaim IP for the Pod of graceful-period timeout) does work for StatefulSet Pod.","title":"Notice"},{"location":"usage/statefulset/#get-started","text":"","title":"Get Started"},{"location":"usage/statefulset/#enable-statefulset-support","text":"Firstly, please ensure you have installed the spiderpool and configure the CNI file. Refer to install for details. Check whether the property enableStatefulSet of the configmap spiderpool-conf is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , run helm upgrade spiderpool spiderpool/spiderpool --set ipam.enableStatefulSet=true -n kube-system .","title":"Enable StatefulSet support"},{"location":"usage/statefulset/#create-a-statefulset","text":"This is an example to install a StatefulSet. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset/statefulset-demo.yaml","title":"Create a StatefulSet"},{"location":"usage/statefulset/#validate-the-spiderpool-related-cr-data","text":"Here's the created Pod, spiderippool, and spiderendpoint CR information: $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 8s 172.22.40.181 spider-worker <none> <none> --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml ... \"172.22.40.181\":{\"interface\":\"eth0\",\"pod\":\"default/demo-sts-0\",\"podUid\":\"fa50a7c2-99e7-4e97-a3f1-8d6503067b54\"} ... --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml ... status: current: ips: - interface: eth0 ipv4: 172.22.40.181/16 ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::d/64 ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker uid: fa50a7c2-99e7-4e97-a3f1-8d6503067b54 ownerControllerName: demo-sts ownerControllerType: StatefulSet ... Try to delete Pod demo-sts-0 and check whether the rebuilding Pod keeps the previous IP or not. $ kubectl delete po demo-sts-0 pod \"demo-sts-0\" deleted --------------------------------------------------------------------------------------------------------------------- $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 12s 172.22.40.181 spider-worker <none> <none> --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml ... \"172.22.40.181\":{\"interface\":\"eth0\",\"pod\":\"default/demo-sts-0\",\"podUid\":\"425d6552-63bb-4b4c-aab2-b2db95de0ab1\"} ... --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml ... status: current: ips: - interface: eth0 ipv4: 172.22.40.181/16 ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::d/64 ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker uid: 425d6552-63bb-4b4c-aab2-b2db95de0ab1 ownerControllerName: demo-sts ownerControllerType: StatefulSet ... And you can see, the re-created Pod still holds the previous IP, spiderippool, and spiderendpoint updated containerID property.","title":"Validate the Spiderpool related CR data"},{"location":"usage/statefulset/#clean-up","text":"Delete the StatefulSet object: demo-sts . $ kubectl delete sts demo-sts statefulset.apps \"demo-sts\" deleted --------------------------------------------------------------------------------------------------------------------- $ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0 --------------------------------------------------------------------------------------------------------------------- $ kubectl get se demo-sts-0 -o yaml Error from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found The related data is cleaned up now.","title":"clean up"},{"location":"usage/third-party-controller/","text":"Spiderpool supports third-party controllers Description Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to Pods created not by kubernetes-native controller. There are two ways to do this: Manual ippool The administrator could create ippool object and assign IP to Pods. Automatical ippool Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application. This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet. This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: does not support automatically scale up and down the IP does not support automatically delete the ippool In the future, spiderpool may support all operation of automatical ippool. Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as stateless Pod like Deployment , this means Pods created by none kubernetes-native controller is able to fix the IP range like Deployment , but not able to bind each Pod to a specific IP address like Statefulset . Get Started It will use OpenKruise to demonstrate how Spiderpool supports third-party controllers. Set up Spiderpool See installation for more details. Set up OpenKruise Please refer to OpenKruise Create Pod by Manual ippool way Create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations ipam.spidernet.io/ippool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2 Create Pod by Automatical ippool way Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations ipam.spidernet.io/subnet apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] NOTICE: You must specify a fixed IP number for auto-created IPPool like ipam.spidernet.io/ippool-ip-number: \"5\" . Because Spiderpool has no idea about the replica number, so it does not support annotation like ipam.spidernet.io/ippool-ip-number: \"+5\" . Check status As expected, Spiderpool will create auto-created IPPool from subnet-demo-v4 and subnet-demo-v6 objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"Third-party controllers"},{"location":"usage/third-party-controller/#spiderpool-supports-third-party-controllers","text":"","title":"Spiderpool supports third-party controllers"},{"location":"usage/third-party-controller/#description","text":"Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to Pods created not by kubernetes-native controller. There are two ways to do this: Manual ippool The administrator could create ippool object and assign IP to Pods. Automatical ippool Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application. This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet. This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: does not support automatically scale up and down the IP does not support automatically delete the ippool In the future, spiderpool may support all operation of automatical ippool. Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as stateless Pod like Deployment , this means Pods created by none kubernetes-native controller is able to fix the IP range like Deployment , but not able to bind each Pod to a specific IP address like Statefulset .","title":"Description"},{"location":"usage/third-party-controller/#get-started","text":"It will use OpenKruise to demonstrate how Spiderpool supports third-party controllers.","title":"Get Started"},{"location":"usage/third-party-controller/#set-up-spiderpool","text":"See installation for more details.","title":"Set up Spiderpool"},{"location":"usage/third-party-controller/#set-up-openkruise","text":"Please refer to OpenKruise","title":"Set up OpenKruise"},{"location":"usage/third-party-controller/#create-pod-by-manual-ippool-way","text":"Create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations ipam.spidernet.io/ippool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2","title":"Create Pod by Manual ippool way"},{"location":"usage/third-party-controller/#create-pod-by-automatical-ippool-way","text":"Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations ipam.spidernet.io/subnet apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] NOTICE: You must specify a fixed IP number for auto-created IPPool like ipam.spidernet.io/ippool-ip-number: \"5\" . Because Spiderpool has no idea about the replica number, so it does not support annotation like ipam.spidernet.io/ippool-ip-number: \"+5\" . Check status As expected, Spiderpool will create auto-created IPPool from subnet-demo-v4 and subnet-demo-v6 objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"Create Pod by Automatical ippool way"},{"location":"usage/install/certificate/","text":"Certificates Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways. Auto certificates Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = auto Note that the default value of parameter spiderpoolController.tls.method is auto . Provided certificates If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA = ` cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT = ` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY = ` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = provided \\ --set spiderpoolController.tls.provided.tlsCa = ${ CA } \\ --set spiderpoolController.tls.provided.tlsCert = ${ SERVER_CERT } \\ --set spiderpoolController.tls.provided.tlsKey = ${ SERVER_KEY } Cert-manager certificates It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = certmanager \\ --set spiderpoolController.tls.certmanager.issuerName = ${ CERT_MANAGER_ISSUER_NAME }","title":"Certificates"},{"location":"usage/install/certificate/#certificates","text":"Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways.","title":"Certificates"},{"location":"usage/install/certificate/#auto-certificates","text":"Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = auto Note that the default value of parameter spiderpoolController.tls.method is auto .","title":"Auto certificates"},{"location":"usage/install/certificate/#provided-certificates","text":"If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA = ` cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT = ` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY = ` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = provided \\ --set spiderpoolController.tls.provided.tlsCa = ${ CA } \\ --set spiderpoolController.tls.provided.tlsCert = ${ SERVER_CERT } \\ --set spiderpoolController.tls.provided.tlsKey = ${ SERVER_KEY }","title":"Provided certificates"},{"location":"usage/install/certificate/#cert-manager-certificates","text":"It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = certmanager \\ --set spiderpoolController.tls.certmanager.issuerName = ${ CERT_MANAGER_ISSUER_NAME }","title":"Cert-manager certificates"},{"location":"usage/install/install-zh_CN/","text":"\u5b89\u88c5 English | \u7b80\u4f53\u4e2d\u6587 \u7528\u6cd5 \u5b89\u88c5 Spiderpool \u6709\u4e24\u79cd\u573a\u666f\uff1a \u5728 Underlay NICs \u4e0b\u5b89\u88c5 Spiderpool \u5bf9\u4e8e\u8fd9\u4e00\u4f7f\u7528\u573a\u666f\uff0c\u96c6\u7fa4\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a\u6216\u591a\u4e2a Underlay CNI \u6765\u8fd0\u884c Pod\u3002 \u5f53 Pod \u4e2d\u6709\u4e00\u4e2a\u6216\u591a\u4e2a Underlay CNI \u65f6\uff0cSpiderpool \u53ef\u4ee5\u5e2e\u52a9\u5176\u5206\u914d IP \u5730\u5740\u3001\u8c03\u6574\u8def\u7531\u3001\u8fde\u63a5 Pod \u548c\u672c\u5730\u8282\u70b9\u3001\u68c0\u6d4b IP \u51b2\u7a81\u7b49\u3002 \u4e3a Overlay CNI \u7684 Pod \u6dfb\u52a0 Underaly CNI \u7684\u8f85\u52a9\u7f51\u5361 \u5bf9\u4e8e\u8fd9\u4e00\u4f7f\u7528\u573a\u666f\uff0c\u96c6\u7fa4\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a Overlay CNI \u548c\u5176\u4ed6 Underlay CNI \u6765\u8fd0\u884c Pod\u3002 \u5f53\u4e00\u4e2a Pod \u4e2d\u6709\u4e00\u4e2a\u6216\u591a\u4e2a\u4e0d\u540c\u7684\u7f51\u5361\u65f6\uff0cSpiderpool \u53ef\u4ee5\u5e2e\u52a9\u5206\u914d IP \u5730\u5740\u3001\u8c03\u6574\u8def\u7531\u3001\u8fde\u63a5 Pod \u548c\u672c\u5730\u8282\u70b9\u3001\u68c0\u6d4b IP \u51b2\u7a81\u7b49\u3002 \u5728 Underlay NICs \u4e0b\u5b89\u88c5 Spiderpool \u4efb\u4f55\u4e0e\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u517c\u5bb9\u7684 CNI \u9879\u76ee\u90fd\u53ef\u4ee5\u4e0e Spiderpool \u826f\u597d\u914d\u5408\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI , calico CNI , weave CNI \u4ee5\u4e0b\u662f Underlay NICs \u4e0b\u5b89\u88c5 Spiderpool \u7684\u793a\u4f8b\uff1a macvlan in Kind SRIOV CNI , \u9002\u5408\u88f8\u673a\u4e3b\u673a\u8fd9\u6837\u7684\u573a\u666f\u3002 calico CNI weave CNI ovs CNI , \u9002\u5408\u88f8\u673a\u4e3b\u673a\u8fd9\u6837\u7684\u573a\u666f\u3002 \u4ee5\u4e0b\u793a\u4f8b\u662f\u5728\u96c6\u7fa4\u4e2d\u4f7f\u7528\u4e24\u4e2a CNI \u7684\u9ad8\u7ea7\u793a\u4f8b\uff1a SRIOV and macvlan \uff0c\u8fd9\u4e2a\u9002\u7528\u4e8e\u88f8\u673a\u4e3b\u673a\u7b49\u573a\u666f\uff0c\u6709\u4e9b\u8282\u70b9\u6709 SRIOV \u7f51\u5361\u800c\u6709\u4e9b\u8282\u70b9\u6ca1\u6709 \u5728\u4e91\u57fa\u7840\u8bbe\u65bd\u4e0a\u5b89\u88c5 Underlay CNI alibaba cloud vmware vsphere openstack \u4e3a Overlay CNI \u7684 Pod \u6dfb\u52a0 Underaly CNI \u7684\u8f85\u52a9\u7f51\u5361 \u4ee5\u4e0b\u793a\u4f8b\u662f\u5b89\u88c5 Spiderpool \u7684\u6307\u5357\uff1a calico and macvlan CNI cilium and macvlan CNI \u5378\u8f7d \u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5378\u8f7d\u5f53\u524d\u7684 Spiderpool \u7248\u672c\uff1a helm uninstall spiderpool -n kube-system \u7136\u800c\uff0cSpiderpool \u7684\u67d0\u4e9b CR \u4e2d\u5b58\u5728 finalizers \uff0c helm uninstall cmd \u53ef\u80fd\u65e0\u6cd5\u6e05\u7406\u6240\u6709\u76f8\u5173\u7684 CR\u3002 \u83b7\u53d6\u4e0b\u5217\u793a\u4f8b\u7684\u6e05\u7406\u811a\u672c\u5e76\u6267\u884c\u5b83\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"\u5b89\u88c5"},{"location":"usage/install/install-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"\u5b89\u88c5"},{"location":"usage/install/install-zh_CN/#_2","text":"\u5b89\u88c5 Spiderpool \u6709\u4e24\u79cd\u573a\u666f\uff1a \u5728 Underlay NICs \u4e0b\u5b89\u88c5 Spiderpool \u5bf9\u4e8e\u8fd9\u4e00\u4f7f\u7528\u573a\u666f\uff0c\u96c6\u7fa4\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a\u6216\u591a\u4e2a Underlay CNI \u6765\u8fd0\u884c Pod\u3002 \u5f53 Pod \u4e2d\u6709\u4e00\u4e2a\u6216\u591a\u4e2a Underlay CNI \u65f6\uff0cSpiderpool \u53ef\u4ee5\u5e2e\u52a9\u5176\u5206\u914d IP \u5730\u5740\u3001\u8c03\u6574\u8def\u7531\u3001\u8fde\u63a5 Pod \u548c\u672c\u5730\u8282\u70b9\u3001\u68c0\u6d4b IP \u51b2\u7a81\u7b49\u3002 \u4e3a Overlay CNI \u7684 Pod \u6dfb\u52a0 Underaly CNI \u7684\u8f85\u52a9\u7f51\u5361 \u5bf9\u4e8e\u8fd9\u4e00\u4f7f\u7528\u573a\u666f\uff0c\u96c6\u7fa4\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a Overlay CNI \u548c\u5176\u4ed6 Underlay CNI \u6765\u8fd0\u884c Pod\u3002 \u5f53\u4e00\u4e2a Pod \u4e2d\u6709\u4e00\u4e2a\u6216\u591a\u4e2a\u4e0d\u540c\u7684\u7f51\u5361\u65f6\uff0cSpiderpool \u53ef\u4ee5\u5e2e\u52a9\u5206\u914d IP \u5730\u5740\u3001\u8c03\u6574\u8def\u7531\u3001\u8fde\u63a5 Pod \u548c\u672c\u5730\u8282\u70b9\u3001\u68c0\u6d4b IP \u51b2\u7a81\u7b49\u3002","title":"\u7528\u6cd5"},{"location":"usage/install/install-zh_CN/#underlay-nics-spiderpool","text":"\u4efb\u4f55\u4e0e\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u517c\u5bb9\u7684 CNI \u9879\u76ee\u90fd\u53ef\u4ee5\u4e0e Spiderpool \u826f\u597d\u914d\u5408\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI , calico CNI , weave CNI \u4ee5\u4e0b\u662f Underlay NICs \u4e0b\u5b89\u88c5 Spiderpool \u7684\u793a\u4f8b\uff1a macvlan in Kind SRIOV CNI , \u9002\u5408\u88f8\u673a\u4e3b\u673a\u8fd9\u6837\u7684\u573a\u666f\u3002 calico CNI weave CNI ovs CNI , \u9002\u5408\u88f8\u673a\u4e3b\u673a\u8fd9\u6837\u7684\u573a\u666f\u3002 \u4ee5\u4e0b\u793a\u4f8b\u662f\u5728\u96c6\u7fa4\u4e2d\u4f7f\u7528\u4e24\u4e2a CNI \u7684\u9ad8\u7ea7\u793a\u4f8b\uff1a SRIOV and macvlan \uff0c\u8fd9\u4e2a\u9002\u7528\u4e8e\u88f8\u673a\u4e3b\u673a\u7b49\u573a\u666f\uff0c\u6709\u4e9b\u8282\u70b9\u6709 SRIOV \u7f51\u5361\u800c\u6709\u4e9b\u8282\u70b9\u6ca1\u6709","title":"\u5728 Underlay NICs \u4e0b\u5b89\u88c5 Spiderpool"},{"location":"usage/install/install-zh_CN/#underlay-cni","text":"alibaba cloud vmware vsphere openstack","title":"\u5728\u4e91\u57fa\u7840\u8bbe\u65bd\u4e0a\u5b89\u88c5 Underlay CNI"},{"location":"usage/install/install-zh_CN/#overlay-cni-pod-underaly-cni","text":"\u4ee5\u4e0b\u793a\u4f8b\u662f\u5b89\u88c5 Spiderpool \u7684\u6307\u5357\uff1a calico and macvlan CNI cilium and macvlan CNI","title":"\u4e3a Overlay CNI \u7684 Pod \u6dfb\u52a0 Underaly CNI \u7684\u8f85\u52a9\u7f51\u5361"},{"location":"usage/install/install-zh_CN/#_3","text":"\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5378\u8f7d\u5f53\u524d\u7684 Spiderpool \u7248\u672c\uff1a helm uninstall spiderpool -n kube-system \u7136\u800c\uff0cSpiderpool \u7684\u67d0\u4e9b CR \u4e2d\u5b58\u5728 finalizers \uff0c helm uninstall cmd \u53ef\u80fd\u65e0\u6cd5\u6e05\u7406\u6240\u6709\u76f8\u5173\u7684 CR\u3002 \u83b7\u53d6\u4e0b\u5217\u793a\u4f8b\u7684\u6e05\u7406\u811a\u672c\u5e76\u6267\u884c\u5b83\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"\u5378\u8f7d"},{"location":"usage/install/install/","text":"Installation English | \u7b80\u4f53\u4e2d\u6587 usage It could have two kinds of scenes to install spiderpool: spiderpool for underlay NICs For this use case, the cluster could use one or more underlay CNI to run pods. When one or more underlay NIC in a pod, spiderpool could help assign IP address, tune routes, connect the pod and local node, detect IP conflict etc. spiderpool for overlay and underlay NICs For this use case, the cluster could use one overlay CNI and other underlay CNI to run pods. When one or more NIC of different NIC in a pod, spiderpool could help assign IP address, tune routes, connect the pod and local node, detect IP conflict etc. Install Spiderpool in Underlay NICs Any CNI project compatible with third-party IPAM plugins, can work well with spiderpool, such as: macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI , calico CNI , weave CNI The following examples are guides to install spiderpool: macvlan in Kind SRIOV CNI , this is suitable for scenes like bare metal host. calico CNI weave CNI ovs CNI , this is suitable for scenes like bare metal host. The following examples are advanced to use two CNI in a cluster: SRIOV and macvlan , this is suitable for scenes like bare metal hosts, some nodes has SRIOV NIC and some nodes do not have Installation for underlay CNI on Cloud infrastruct alibaba cloud vmware vsphere openstack Installation for adding an auxiliary underlay CNI NIC for overlay CNI The following examples are guides to install spiderpool: calico and macvlan CNI cilium and macvlan CNI Uninstall Generally, you can uninstall Spiderpool release in this way: helm uninstall spiderpool -n kube-system However, there are finalizers in some CRs of Spiderpool, the helm uninstall cmd may not clean up all relevant CRs. Get this cleanup script and execute it to ensure that unexpected errors will not occur when deploying Spiderpool next time. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Installation"},{"location":"usage/install/install/#installation","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Installation"},{"location":"usage/install/install/#usage","text":"It could have two kinds of scenes to install spiderpool: spiderpool for underlay NICs For this use case, the cluster could use one or more underlay CNI to run pods. When one or more underlay NIC in a pod, spiderpool could help assign IP address, tune routes, connect the pod and local node, detect IP conflict etc. spiderpool for overlay and underlay NICs For this use case, the cluster could use one overlay CNI and other underlay CNI to run pods. When one or more NIC of different NIC in a pod, spiderpool could help assign IP address, tune routes, connect the pod and local node, detect IP conflict etc.","title":"usage"},{"location":"usage/install/install/#install-spiderpool-in-underlay-nics","text":"Any CNI project compatible with third-party IPAM plugins, can work well with spiderpool, such as: macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI , calico CNI , weave CNI The following examples are guides to install spiderpool: macvlan in Kind SRIOV CNI , this is suitable for scenes like bare metal host. calico CNI weave CNI ovs CNI , this is suitable for scenes like bare metal host. The following examples are advanced to use two CNI in a cluster: SRIOV and macvlan , this is suitable for scenes like bare metal hosts, some nodes has SRIOV NIC and some nodes do not have","title":"Install Spiderpool in Underlay NICs"},{"location":"usage/install/install/#installation-for-underlay-cni-on-cloud-infrastruct","text":"alibaba cloud vmware vsphere openstack","title":"Installation for underlay CNI on Cloud infrastruct"},{"location":"usage/install/install/#installation-for-adding-an-auxiliary-underlay-cni-nic-for-overlay-cni","text":"The following examples are guides to install spiderpool: calico and macvlan CNI cilium and macvlan CNI","title":"Installation for adding an auxiliary underlay CNI NIC for overlay CNI"},{"location":"usage/install/install/#uninstall","text":"Generally, you can uninstall Spiderpool release in this way: helm uninstall spiderpool -n kube-system However, there are finalizers in some CRs of Spiderpool, the helm uninstall cmd may not clean up all relevant CRs. Get this cleanup script and execute it to ensure that unexpected errors will not occur when deploying Spiderpool next time. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Uninstall"},{"location":"usage/install/upgrade/","text":"Upgrading Spiderpool Versions This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool. Upgrade to 0.3.6 from (<=0.3.5) Description There's a design flaw for SpiderSubnet feature in auto-created IPPool label. The previous label ipam.spidernet.io/owner-application corresponding value uses '-' as separative sign. For example, we have deployment ns398-174835790/deploy398-82311862 and the corresponding label value is Deployment-ns398-174835790-deploy398-82311862 . It's very hard to unpack it to trace back what the application namespace and name is. Now, we use '_' rather than '-' as slash for SpiderSubnet feature label ipam.spidernet.io/owner-application , and the upper case will be like Deployment_ns398-174835790_deploy398-82311862 . Reference PR: #1162 In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool. The key is ipam.spidernet.io/interface , and the value is the corresponding interface name. Operation steps Find all auto-created IPPools, their name format is auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid} such as auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41 . Replace their label, just like this: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}' Add one more label kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Update your Spiderpool components version and restart them all. Upgrade to 0.4.0 from (<0.4.0) Description Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod . Operation steps Find all SpiderEndpoint objects that their Status OwnerControllerType is None Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from None to Pod","title":"Upgrading"},{"location":"usage/install/upgrade/#upgrading-spiderpool-versions","text":"This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool.","title":"Upgrading Spiderpool Versions"},{"location":"usage/install/upgrade/#upgrade-to-036-from-035","text":"","title":"Upgrade to 0.3.6 from (&lt;=0.3.5)"},{"location":"usage/install/upgrade/#description","text":"There's a design flaw for SpiderSubnet feature in auto-created IPPool label. The previous label ipam.spidernet.io/owner-application corresponding value uses '-' as separative sign. For example, we have deployment ns398-174835790/deploy398-82311862 and the corresponding label value is Deployment-ns398-174835790-deploy398-82311862 . It's very hard to unpack it to trace back what the application namespace and name is. Now, we use '_' rather than '-' as slash for SpiderSubnet feature label ipam.spidernet.io/owner-application , and the upper case will be like Deployment_ns398-174835790_deploy398-82311862 . Reference PR: #1162 In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool. The key is ipam.spidernet.io/interface , and the value is the corresponding interface name.","title":"Description"},{"location":"usage/install/upgrade/#operation-steps","text":"Find all auto-created IPPools, their name format is auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid} such as auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41 . Replace their label, just like this: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}' Add one more label kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Update your Spiderpool components version and restart them all.","title":"Operation steps"},{"location":"usage/install/upgrade/#upgrade-to-040-from-040","text":"","title":"Upgrade to 0.4.0 from (&lt;0.4.0)"},{"location":"usage/install/upgrade/#description_1","text":"Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod .","title":"Description"},{"location":"usage/install/upgrade/#operation-steps_1","text":"Find all SpiderEndpoint objects that their Status OwnerControllerType is None Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from None to Pod","title":"Operation steps"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/","text":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u6709\u8282\u70b9\u62d3\u6251\u3001\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u3001\u5bf9\u63a5\u57fa\u4e8e spec.externalTrafficPolicy \u4e3a Local \u6a21\u5f0f\u7684 service \u7b49\u529f\u80fd\u3002Spiderpool \u80fd\u57fa\u4e8e IPVlan Underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u4e0a\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001IPvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u5904\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d IPVlan CNI \u8fdb\u884c\u89e3\u51b3\u3002IPVlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 IPvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002 \u5728 service \u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP \uff0c\u4f46\u516c\u6709\u4e91\u81ea\u5efa\u96c6\u7fa4\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5e73\u53f0\u7684 Loadbalancer \u7ec4\u4ef6\u8fdb\u884c nodePort \u8f6c\u53d1\u65f6\uff0c\u4f1a\u51fa\u73b0\u8bbf\u95ee\u4e0d\u901a\u3002\u9488\u5bf9\u8be5\u95ee\u9898 Spiderpool \u63d0\u4f9b\u4e86 coordinator \u63d2\u4ef6\uff0c\u8be5\u63d2\u4ef6\u901a\u8fc7 iptables \u5728\u6570\u636e\u5305\u4e2d\u6253\u6807\u8bb0\uff0c\u786e\u8ba4\u4ece veth0 \u8fdb\u5165\u7684\u6570\u636e\u7684\u56de\u590d\u5305\u4ecd\u4ece veth0 \u8f6c\u53d1\uff0c\u8fdb\u800c\u89e3\u51b3\u5728\u8be5\u6a21\u5f0f\u4e0b nodeport \u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\u3002 \u5b9e\u65bd\u8981\u6c42 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u963f\u91cc\u4e91\u73af\u5883 \u51c6\u5907\u4e00\u5957\u963f\u91cc\u4e91\u73af\u5883\uff0c\u7ed9\u865a\u62df\u673a\u5206\u914d 2 \u4e2a\u7f51\u5361\uff0c\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe\uff1a \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a IPvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u4e0e eth1 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m \u521b\u5efa IPPools Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u8be5\u5b57\u6bb5\u7684\u503c\u5fc5\u987b\u8bbe\u7f6e\u3002\u7531\u4e8e\u963f\u91cc\u4e91\u9650\u5236\u4e86\u8282\u70b9\u53ef\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u60a8\u53ef\u4ee5\u4ece\u963f\u91cc\u4e91\u7684\u5f39\u6027\u7f51\u5361\u754c\u9762\u83b7\u53d6\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( eth0\u3001eth1 )\u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u7ec4 DaemonSet \u5e94\u7528\u548c 1 \u4e2a type \u4e3a ClusterIP \u7684 service \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u5206\u522b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-b7765b8d8-422sb 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-b7765b8d8-qjgpj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-7c56876fc6-7brhf 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-7c56876fc6-zlxxt 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\uff0c\u5e94\u7528\u7684 IP \u5747\u5728\u671f\u671b\u7684 IP \u6c60\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true \u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027 \u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes \u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027 \u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee \u963f\u91cc\u4e91\u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u73af\u5883\u4e0b\u6784\u5efa\u4e00\u4e2a\u516c\u7f51\u6216\u79c1\u7f51\u6d41\u91cf\u7684\u51fa\u5165\u53e3\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u6d4b\u8bd5\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 \u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee \u90e8\u7f72 Cloud Controller Manager CCM\uff08Cloud Controller Manager\uff09\u662f\u963f\u91cc\u4e91\u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e\u963f\u91cc\u4e91\u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e CCM \u6587\u6863 \u5b8c\u6210 CCM \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u5206\u522b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4ece\u800c\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5404\u81ea\u7684 providerID \u3002 http://100.100.100.200/latest/meta-data \u662f\u963f\u91cc\u4e91 CLI \u63d0\u4f9b\u83b7\u53d6\u5b9e\u4f8b\u5143\u6570\u636e\u7684 API \u5165\u53e3\uff0c\u5728\u4e0b\u5217\u793a\u4f8b\u4e2d\u65e0\u9700\u4fee\u6539\u5b83\u3002\u66f4\u591a\u7528\u6cd5\u53ef\u53c2\u8003 \u5b9e\u4f8b\u5143\u6570\u636e ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` \u5728\u96c6\u7fa4\u7684 master \u8282\u70b9\u901a\u8fc7 kubectl patch \u547d\u4ee4\u4e3a\u96c6\u7fa4\u4e2d\u7684 \u6bcf\u4e2a\u8282\u70b9 \u8865\u5145\u5404\u81ea\u7684 providerID \uff0c\u8be5\u6b65\u9aa4\u5fc5\u987b\u88ab\u6267\u884c\uff0c\u5426\u5219\u5bf9\u5e94\u8282\u70b9\u7684 CCM Pod \u5c06\u65e0\u6cd5\u8fd0\u884c\u3002 ~# kubectl get nodes ~# kubectl patch node ${ NODE_NAME } -p '{\"spec\":{\"providerID\": \"${provider_id}\"}}' \u521b\u5efa\u963f\u91cc\u4e91\u7684 RAM \u7528\u6237\uff0c\u5e76\u6388\u6743\u3002 RAM \u7528\u6237\u662f RAM \u4e2d\u7684\u4e00\u79cd\u5b9e\u4f53\u8eab\u4efd\uff0c\u4ee3\u8868\u9700\u8981\u8bbf\u95ee\u963f\u91cc\u4e91\u7684\u4eba\u5458\u6216\u5e94\u7528\u7a0b\u5e8f\u3002\u901a\u8fc7\u53c2\u9605 RAM \u8bbf\u95ee\u63a7\u5236 \u521b\u5efa RAM \u7528\u6237\uff0c\u5e76\u6388\u4e8e\u9700\u8981\u8bbf\u95ee\u8d44\u6e90\u7684\u6743\u9650\u3002 \u4e3a\u786e\u4fdd\u540e\u7eed\u6b65\u9aa4\u4e2d\u6240\u4f7f\u7528\u7684 RAM \u7528\u6237\u5177\u5907\u8db3\u591f\u7684\u6743\u9650\uff0c\u8bf7\u4e0e\u672c\u6587\u4fdd\u6301\u4e00\u81f4\uff0c\u7ed9\u4e88 RAM \u7528\u6237 AdministratorAccess \u548c AliyunSLBFullAccess \u6743\u9650\u3002 \u83b7\u53d6 RAM \u7528\u6237\u7684 AccessKey & AccessKeySecret \u767b\u5f55 RAM \u7528\u6237\uff0c\u8bbf\u95ee \u7528\u6237\u4e2d\u5fc3 \uff0c\u83b7\u53d6\u5bf9\u5e94 RAM \u7528\u7684 AccessKey & AccessKeySecret\u3002 \u521b\u5efa CCM \u7684 Cloud ConfigMap\u3002 \u5c06\u6b65\u9aa4 3 \u83b7\u53d6\u7684 AccessKey & AccessKeySecret\uff0c\u53c2\u8003\u4e0b\u5217\u65b9\u5f0f\u5199\u5165\u73af\u5883\u53d8\u91cf\u3002 ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u5b8c\u6210\u521b\u5efa cloud-config\u3002 accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF \u83b7\u53d6 Yaml \uff0c\u5e76\u901a\u8fc7 kubectl apply -f cloud-controller-manager.yaml \u65b9\u5f0f\u5b89\u88c5 CCM\uff0c\u672c\u6587\u4e2d\u5b89\u88c5\u7684\u7248\u672c\u4e3a v2.5.0 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u83b7\u53d6 cloud-controller-manager.yaml\uff0c\u5e76\u66ff\u6362\u5176\u4e2d <<cluster_cidr>> \u4e3a\u60a8\u771f\u5b9e\u96c6\u7fa4\u7684 cluster cidr \u3002 ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml \u68c0\u67e5 CCM \u5b89\u88c5\u5b8c\u6210\u3002 ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s \u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3 \u5982\u4e0b\u7684 Yaml \u5c06\u521b\u5efa spec.type \u4e3a LoadBalancer \u7684 2 \u7ec4 service\uff0c\u4e00\u7ec4\u4e3a tcp \uff08\u56db\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u4e00\u7ec4\u4e3a http \uff08\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\u3002 service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port \uff1aCCM \u63d0\u4f9b\u7684\u521b\u5efa\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\u6ce8\u89e3\u3002\u53ef\u4ee5\u901a\u8fc7\u5b83\u81ea\u5b9a\u4e49\u66b4\u9732\u7aef\u53e3\u3002\u66f4\u591a\u7528\u6cd5\u53c2\u8003 CCM \u4f7f\u7528\u6587\u6863 \u3002 .spec.externalTrafficPolicy \uff1a\u8868\u793a\u6b64 Service \u662f\u5426\u5e0c\u671b\u5c06\u5916\u90e8\u6d41\u91cf\u8def\u7531\u5230\u8282\u70b9\u672c\u5730\u6216\u96c6\u7fa4\u8303\u56f4\u7684\u7aef\u70b9\u3002\u5b83\u6709\u4e24\u4e2a\u53ef\u7528\u9009\u9879\uff1aCluster\uff08\u9ed8\u8ba4\uff09\u548c Local\u3002\u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM \u5c06\u81ea\u52a8\u5728 IaaS \u5c42\u521b\u5efa\u56db\u5c42\u4e0e\u4e03\u5c42\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u53ef\u4ee5\u901a\u8fc7\u963f\u91cc\u4e91\u754c\u9762\u8fdb\u884c\u67e5\u770b\uff0c\u5982\u4e0b\uff1a \u9a8c\u8bc1\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee \u5728\u516c\u7f51\u7684\u673a\u5668\u4e0a\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u7684 \u516c\u7f51 IP + \u7aef\u53e3 \u5b9e\u73b0\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee # \u8bbf\u95ee\u56db\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # \u8bbf\u95ee\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes \u603b\u7ed3 Spiderpool \u80fd\u591f\u8fd0\u884c\u5728\u963f\u91cc\u4e91\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\u3002","title":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_2","text":"\u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_3","text":"Spiderpool \u6709\u8282\u70b9\u62d3\u6251\u3001\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u3001\u5bf9\u63a5\u57fa\u4e8e spec.externalTrafficPolicy \u4e3a Local \u6a21\u5f0f\u7684 service \u7b49\u529f\u80fd\u3002Spiderpool \u80fd\u57fa\u4e8e IPVlan Underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u4e0a\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001IPvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u5904\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d IPVlan CNI \u8fdb\u884c\u89e3\u51b3\u3002IPVlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 IPvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002 \u5728 service \u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP \uff0c\u4f46\u516c\u6709\u4e91\u81ea\u5efa\u96c6\u7fa4\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5e73\u53f0\u7684 Loadbalancer \u7ec4\u4ef6\u8fdb\u884c nodePort \u8f6c\u53d1\u65f6\uff0c\u4f1a\u51fa\u73b0\u8bbf\u95ee\u4e0d\u901a\u3002\u9488\u5bf9\u8be5\u95ee\u9898 Spiderpool \u63d0\u4f9b\u4e86 coordinator \u63d2\u4ef6\uff0c\u8be5\u63d2\u4ef6\u901a\u8fc7 iptables \u5728\u6570\u636e\u5305\u4e2d\u6253\u6807\u8bb0\uff0c\u786e\u8ba4\u4ece veth0 \u8fdb\u5165\u7684\u6570\u636e\u7684\u56de\u590d\u5305\u4ecd\u4ece veth0 \u8f6c\u53d1\uff0c\u8fdb\u800c\u89e3\u51b3\u5728\u8be5\u6a21\u5f0f\u4e0b nodeport \u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_4","text":"\u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_5","text":"","title":"\u6b65\u9aa4"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_6","text":"\u51c6\u5907\u4e00\u5957\u963f\u91cc\u4e91\u73af\u5883\uff0c\u7ed9\u865a\u62df\u673a\u5206\u914d 2 \u4e2a\u7f51\u5361\uff0c\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe\uff1a \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a","title":"\u963f\u91cc\u4e91\u73af\u5883"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a IPvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u4e0e eth1 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#ippools","text":"Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u8be5\u5b57\u6bb5\u7684\u503c\u5fc5\u987b\u8bbe\u7f6e\u3002\u7531\u4e8e\u963f\u91cc\u4e91\u9650\u5236\u4e86\u8282\u70b9\u53ef\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u60a8\u53ef\u4ee5\u4ece\u963f\u91cc\u4e91\u7684\u5f39\u6027\u7f51\u5361\u754c\u9762\u83b7\u53d6\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( eth0\u3001eth1 )\u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF","title":"\u521b\u5efa IPPools"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_7","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u7ec4 DaemonSet \u5e94\u7528\u548c 1 \u4e2a type \u4e3a ClusterIP \u7684 service \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u5206\u522b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-b7765b8d8-422sb 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-b7765b8d8-qjgpj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-7c56876fc6-7brhf 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-7c56876fc6-zlxxt 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\uff0c\u5e94\u7528\u7684 IP \u5747\u5728\u671f\u671b\u7684 IP \u6c60\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_8","text":"\u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes","title":"\u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_9","text":"","title":"\u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#pod","text":"\u963f\u91cc\u4e91\u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u73af\u5883\u4e0b\u6784\u5efa\u4e00\u4e2a\u516c\u7f51\u6216\u79c1\u7f51\u6d41\u91cf\u7684\u51fa\u5165\u53e3\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u6d4b\u8bd5\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18","title":"\u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_10","text":"","title":"\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#cloud-controller-manager","text":"CCM\uff08Cloud Controller Manager\uff09\u662f\u963f\u91cc\u4e91\u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e\u963f\u91cc\u4e91\u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e CCM \u6587\u6863 \u5b8c\u6210 CCM \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u5206\u522b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4ece\u800c\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5404\u81ea\u7684 providerID \u3002 http://100.100.100.200/latest/meta-data \u662f\u963f\u91cc\u4e91 CLI \u63d0\u4f9b\u83b7\u53d6\u5b9e\u4f8b\u5143\u6570\u636e\u7684 API \u5165\u53e3\uff0c\u5728\u4e0b\u5217\u793a\u4f8b\u4e2d\u65e0\u9700\u4fee\u6539\u5b83\u3002\u66f4\u591a\u7528\u6cd5\u53ef\u53c2\u8003 \u5b9e\u4f8b\u5143\u6570\u636e ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` \u5728\u96c6\u7fa4\u7684 master \u8282\u70b9\u901a\u8fc7 kubectl patch \u547d\u4ee4\u4e3a\u96c6\u7fa4\u4e2d\u7684 \u6bcf\u4e2a\u8282\u70b9 \u8865\u5145\u5404\u81ea\u7684 providerID \uff0c\u8be5\u6b65\u9aa4\u5fc5\u987b\u88ab\u6267\u884c\uff0c\u5426\u5219\u5bf9\u5e94\u8282\u70b9\u7684 CCM Pod \u5c06\u65e0\u6cd5\u8fd0\u884c\u3002 ~# kubectl get nodes ~# kubectl patch node ${ NODE_NAME } -p '{\"spec\":{\"providerID\": \"${provider_id}\"}}' \u521b\u5efa\u963f\u91cc\u4e91\u7684 RAM \u7528\u6237\uff0c\u5e76\u6388\u6743\u3002 RAM \u7528\u6237\u662f RAM \u4e2d\u7684\u4e00\u79cd\u5b9e\u4f53\u8eab\u4efd\uff0c\u4ee3\u8868\u9700\u8981\u8bbf\u95ee\u963f\u91cc\u4e91\u7684\u4eba\u5458\u6216\u5e94\u7528\u7a0b\u5e8f\u3002\u901a\u8fc7\u53c2\u9605 RAM \u8bbf\u95ee\u63a7\u5236 \u521b\u5efa RAM \u7528\u6237\uff0c\u5e76\u6388\u4e8e\u9700\u8981\u8bbf\u95ee\u8d44\u6e90\u7684\u6743\u9650\u3002 \u4e3a\u786e\u4fdd\u540e\u7eed\u6b65\u9aa4\u4e2d\u6240\u4f7f\u7528\u7684 RAM \u7528\u6237\u5177\u5907\u8db3\u591f\u7684\u6743\u9650\uff0c\u8bf7\u4e0e\u672c\u6587\u4fdd\u6301\u4e00\u81f4\uff0c\u7ed9\u4e88 RAM \u7528\u6237 AdministratorAccess \u548c AliyunSLBFullAccess \u6743\u9650\u3002 \u83b7\u53d6 RAM \u7528\u6237\u7684 AccessKey & AccessKeySecret \u767b\u5f55 RAM \u7528\u6237\uff0c\u8bbf\u95ee \u7528\u6237\u4e2d\u5fc3 \uff0c\u83b7\u53d6\u5bf9\u5e94 RAM \u7528\u7684 AccessKey & AccessKeySecret\u3002 \u521b\u5efa CCM \u7684 Cloud ConfigMap\u3002 \u5c06\u6b65\u9aa4 3 \u83b7\u53d6\u7684 AccessKey & AccessKeySecret\uff0c\u53c2\u8003\u4e0b\u5217\u65b9\u5f0f\u5199\u5165\u73af\u5883\u53d8\u91cf\u3002 ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u5b8c\u6210\u521b\u5efa cloud-config\u3002 accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF \u83b7\u53d6 Yaml \uff0c\u5e76\u901a\u8fc7 kubectl apply -f cloud-controller-manager.yaml \u65b9\u5f0f\u5b89\u88c5 CCM\uff0c\u672c\u6587\u4e2d\u5b89\u88c5\u7684\u7248\u672c\u4e3a v2.5.0 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u83b7\u53d6 cloud-controller-manager.yaml\uff0c\u5e76\u66ff\u6362\u5176\u4e2d <<cluster_cidr>> \u4e3a\u60a8\u771f\u5b9e\u96c6\u7fa4\u7684 cluster cidr \u3002 ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml \u68c0\u67e5 CCM \u5b89\u88c5\u5b8c\u6210\u3002 ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s","title":"\u90e8\u7f72 Cloud Controller Manager"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#loadbalancer","text":"\u5982\u4e0b\u7684 Yaml \u5c06\u521b\u5efa spec.type \u4e3a LoadBalancer \u7684 2 \u7ec4 service\uff0c\u4e00\u7ec4\u4e3a tcp \uff08\u56db\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u4e00\u7ec4\u4e3a http \uff08\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\u3002 service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port \uff1aCCM \u63d0\u4f9b\u7684\u521b\u5efa\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\u6ce8\u89e3\u3002\u53ef\u4ee5\u901a\u8fc7\u5b83\u81ea\u5b9a\u4e49\u66b4\u9732\u7aef\u53e3\u3002\u66f4\u591a\u7528\u6cd5\u53c2\u8003 CCM \u4f7f\u7528\u6587\u6863 \u3002 .spec.externalTrafficPolicy \uff1a\u8868\u793a\u6b64 Service \u662f\u5426\u5e0c\u671b\u5c06\u5916\u90e8\u6d41\u91cf\u8def\u7531\u5230\u8282\u70b9\u672c\u5730\u6216\u96c6\u7fa4\u8303\u56f4\u7684\u7aef\u70b9\u3002\u5b83\u6709\u4e24\u4e2a\u53ef\u7528\u9009\u9879\uff1aCluster\uff08\u9ed8\u8ba4\uff09\u548c Local\u3002\u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM \u5c06\u81ea\u52a8\u5728 IaaS \u5c42\u521b\u5efa\u56db\u5c42\u4e0e\u4e03\u5c42\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u53ef\u4ee5\u901a\u8fc7\u963f\u91cc\u4e91\u754c\u9762\u8fdb\u884c\u67e5\u770b\uff0c\u5982\u4e0b\uff1a","title":"\u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_11","text":"\u5728\u516c\u7f51\u7684\u673a\u5668\u4e0a\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u7684 \u516c\u7f51 IP + \u7aef\u53e3 \u5b9e\u73b0\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee # \u8bbf\u95ee\u56db\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # \u8bbf\u95ee\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes","title":"\u9a8c\u8bc1\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_12","text":"Spiderpool \u80fd\u591f\u8fd0\u884c\u5728\u963f\u91cc\u4e91\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\u3002","title":"\u603b\u7ed3"},{"location":"usage/install/cloud/get-started-alibaba/","text":"Running On Alibaba Cloud English | \u7b80\u4f53\u4e2d\u6587 Introduction With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using Underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized Underlay solutions for public clouds. This page introduces Spiderpool , an Underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios. Features Spiderpool offers features such as node topology, MAC address validation, and integration with services that use spec.externalTrafficPolicy set to Local mode. Spiderpool can run on public cloud environments using the IPVlan Underlay CNI. Here's an overview of its implementation: When using Underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and IPvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based Underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with IPVlan CNI . The IPVlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating IPVlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved. When the .spec.externalTrafficPolicy of a service is set to Local , the client's source IP can be reserved. However, in self-managed public cloud clusters, using the platform's LoadBalancer component for nodePort forwarding in this mode will lead to access failures. To tackle this problem, Spiderpool offers the coordinator plugin. This plugin utilizes iptables to apply packet marking, ensuring that response packets coming from veth0 are still forwarded through veth0. This resolves the problem of nodePort access in this mode. Prerequisites The system kernel version must be greater than 4.2 when using IPVlan as the cluster's CNI. Helm is installed. Steps Alibaba Cloud Environment Prepare an Alibaba Cloud environment with virtual machines that have 2 network interfaces. Assign a set of auxiliary private IP addresses to each network interface, as shown in the picture: Utilize the configured VMs to build a Kubernetes cluster. The available IP addresses for the nodes and the network topology of the cluster are depicted below: Install Spiderpool Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the Multus clusterNetwork for the cluster using multus.multusCNI.defaultCniCRName . clusterNetwork is a specific field within the Multus plugin used to define the default network interface for Pods. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Here is an example of creating an IPvlan SpiderMultusConfig configuration: IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two IPvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth0 and eth1 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m Create IPPools The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips \uff1athis field must not be empty. Due to Alibaba Cloud's limitations on available IP addresses for nodes, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . You can obtain this information from the Elastic Network Interface page in the Alibaba Cloud console. Based on the provided information, use the following YAML configuration to create a SpiderIPPool for each network interface (eth0 and eth1) on every node. These SpiderIPPools will assign IP addresses to Pods running on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF Create Applications In the following example YAML, there are 2 sets of DaemonSet applications and 1 service with a type of ClusterIP: v1.multus-cni.io/default-network : specify the subnet that each application will use. In the example, the applications are assigned different subnets. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF Check the status of the running Pods: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-b7765b8d8-422sb 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-b7765b8d8-qjgpj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-7c56876fc6-7brhf 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-7c56876fc6-zlxxt 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool automatically assigns IP addresses to the applications, ensuring that the assigned IPs are within the expected IP pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true Test East-West Connectivity Test communication between Pods and their hosts: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms Test communication between Pods across different nodes and subnets: ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms Test communication between Pods and ClusterIP services: ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes Test North-South Connectivity Test egress traffic from Pods to external destinations Alibaba Cloud's NAT Gateway provides an ingress and egress gateway for public or private network traffic within a VPC environment. By utilizing NAT Gateway, the cluster can have egress connectivity. Please refer to the NAT Gateway documentation for creating a NAT Gateway as depicted in the picture: Test egress traffic from Pods ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 Load Balancer Traffic Ingress Access Deploy Cloud Controller Manager Cloud Controller Manager (CCM) is an Alibaba Cloud's component that enables integration between Kubernetes and Alibaba Cloud services. We will use CCM along with Alibaba Cloud infrastructure to facilitate load balancer traffic ingress access. Follow the steps below and refer to the CCM documentation for deploying CCM. Configure providerID on Cluster Nodes On each node in the cluster, run the following command to obtain the providerID for each node. http://100.100.100.200/latest/meta-data is the API entry point provided by Alibaba Cloud CLI for retrieving instance metadata. You don't need to modify it in the provided example. For more information, please refer to ECS instance metadata . ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` On the master node of the cluster, use the kubectl patch command to add the providerID for each node in the cluster. This step is necessary to ensure the proper functioning of the CCM Pod on each corresponding node. Failure to run this step will result in the CCM Pod being unable to run correctly. ~# kubectl get nodes ~# kubectl patch node ${ NODE_NAME } -p '{\"spec\":{\"providerID\": \"${provider_id}\"}}' Create an Alibaba Cloud RAM user and grant authorization. A RAM user is an entity within Alibaba Cloud's Resource Access Management (RAM) that represents individuals or applications requiring access to Alibaba Cloud resources. Refer to Overview of RAM users to create a RAM user and assign the necessary permissions for accessing resources. To ensure that the RAM user used in the subsequent steps has sufficient privileges, grant the AdministratorAccess and AliyunSLBFullAccess permissions to the RAM user, following the instructions provided here. Obtain the AccessKey & AccessKeySecret for the RAM user. Log in to the RAM User account and go to User Center to retrieve the corresponding AccessKey & AccessKeySecret for the RAM User. Create the Cloud ConfigMap for CCM. Use the following method to write the AccessKey & AccessKeySecret obtained in step 3 as environment variables. ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** Run the following command to create cloud-config: accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF Retrieve the YAML file and install CCM by running the command kubectl apply -f cloud-controller-manager.yaml . The version of CCM being installed here is v2.5.0. Use the following command to obtain the cloud-controller-manager.yaml file and replace <<cluster_cidr>> with the actual cluster CIDR. ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml Verify if CCM is installed. ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s Create Load Balancer Ingress for Applications The following YAML will create two sets of services, one for TCP (layer 4 load balancing) and one for HTTP (layer 7 load balancing), with spec.type set to LoadBalancer . service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port : this annotation provided by CCM allows you to customize the exposed ports for layer 7 load balancing. For more information, refer to the CCM Usage Documentation . .spec.externalTrafficPolicy : indicates whether the service prefers to route external traffic to local or cluster-wide endpoints. It has two options: Cluster (default) and Local. Setting .spec.externalTrafficPolicy to Local preserves the client source IP. ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF After the creation is complete, you can view the following: ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM will automatically create layer 4 and layer 7 load balancers at its IaaS services. You can easily access and manage them through the Alibaba Cloud console, as shown below: Verify Load Balancer Traffic Ingress Access On a public machine, access the load balancer's public IP + port to test the traffic ingress: # Access layer 4 load balancing $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # Access layer 7 load balancing $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes Summary Spiderpool is successfully running in an Alibaba Cloud cluster, ensuring normal east-west and north-south traffic.","title":"alibaba cloud"},{"location":"usage/install/cloud/get-started-alibaba/#running-on-alibaba-cloud","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Alibaba Cloud"},{"location":"usage/install/cloud/get-started-alibaba/#introduction","text":"With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using Underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized Underlay solutions for public clouds. This page introduces Spiderpool , an Underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios.","title":"Introduction"},{"location":"usage/install/cloud/get-started-alibaba/#features","text":"Spiderpool offers features such as node topology, MAC address validation, and integration with services that use spec.externalTrafficPolicy set to Local mode. Spiderpool can run on public cloud environments using the IPVlan Underlay CNI. Here's an overview of its implementation: When using Underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and IPvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based Underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with IPVlan CNI . The IPVlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating IPVlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved. When the .spec.externalTrafficPolicy of a service is set to Local , the client's source IP can be reserved. However, in self-managed public cloud clusters, using the platform's LoadBalancer component for nodePort forwarding in this mode will lead to access failures. To tackle this problem, Spiderpool offers the coordinator plugin. This plugin utilizes iptables to apply packet marking, ensuring that response packets coming from veth0 are still forwarded through veth0. This resolves the problem of nodePort access in this mode.","title":"Features"},{"location":"usage/install/cloud/get-started-alibaba/#prerequisites","text":"The system kernel version must be greater than 4.2 when using IPVlan as the cluster's CNI. Helm is installed.","title":"Prerequisites"},{"location":"usage/install/cloud/get-started-alibaba/#steps","text":"","title":"Steps"},{"location":"usage/install/cloud/get-started-alibaba/#alibaba-cloud-environment","text":"Prepare an Alibaba Cloud environment with virtual machines that have 2 network interfaces. Assign a set of auxiliary private IP addresses to each network interface, as shown in the picture: Utilize the configured VMs to build a Kubernetes cluster. The available IP addresses for the nodes and the network topology of the cluster are depicted below:","title":"Alibaba Cloud Environment"},{"location":"usage/install/cloud/get-started-alibaba/#install-spiderpool","text":"Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the Multus clusterNetwork for the cluster using multus.multusCNI.defaultCniCRName . clusterNetwork is a specific field within the Multus plugin used to define the default network interface for Pods.","title":"Install Spiderpool"},{"location":"usage/install/cloud/get-started-alibaba/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Here is an example of creating an IPvlan SpiderMultusConfig configuration: IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan coordinator: mode: underlay tunePodRoutes: true podCIDRType: cluster enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two IPvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth0 and eth1 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m","title":"Install CNI"},{"location":"usage/install/cloud/get-started-alibaba/#create-ippools","text":"The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips \uff1athis field must not be empty. Due to Alibaba Cloud's limitations on available IP addresses for nodes, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . You can obtain this information from the Elastic Network Interface page in the Alibaba Cloud console. Based on the provided information, use the following YAML configuration to create a SpiderIPPool for each network interface (eth0 and eth1) on every node. These SpiderIPPools will assign IP addresses to Pods running on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF","title":"Create IPPools"},{"location":"usage/install/cloud/get-started-alibaba/#create-applications","text":"In the following example YAML, there are 2 sets of DaemonSet applications and 1 service with a type of ClusterIP: v1.multus-cni.io/default-network : specify the subnet that each application will use. In the example, the applications are assigned different subnets. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF Check the status of the running Pods: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-b7765b8d8-422sb 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-b7765b8d8-qjgpj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-7c56876fc6-7brhf 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-7c56876fc6-zlxxt 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool automatically assigns IP addresses to the applications, ensuring that the assigned IPs are within the expected IP pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true","title":"Create Applications"},{"location":"usage/install/cloud/get-started-alibaba/#test-east-west-connectivity","text":"Test communication between Pods and their hosts: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms Test communication between Pods across different nodes and subnets: ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-b7765b8d8-422sb -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms Test communication between Pods and ClusterIP services: ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes","title":"Test East-West Connectivity"},{"location":"usage/install/cloud/get-started-alibaba/#test-north-south-connectivity","text":"","title":"Test North-South Connectivity"},{"location":"usage/install/cloud/get-started-alibaba/#test-egress-traffic-from-pods-to-external-destinations","text":"Alibaba Cloud's NAT Gateway provides an ingress and egress gateway for public or private network traffic within a VPC environment. By utilizing NAT Gateway, the cluster can have egress connectivity. Please refer to the NAT Gateway documentation for creating a NAT Gateway as depicted in the picture: Test egress traffic from Pods ~# kubectl exec -ti test-app-2-7c56876fc6-7brhf -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18","title":"Test egress traffic from Pods to external destinations"},{"location":"usage/install/cloud/get-started-alibaba/#load-balancer-traffic-ingress-access","text":"","title":"Load Balancer Traffic Ingress Access"},{"location":"usage/install/cloud/get-started-alibaba/#deploy-cloud-controller-manager","text":"Cloud Controller Manager (CCM) is an Alibaba Cloud's component that enables integration between Kubernetes and Alibaba Cloud services. We will use CCM along with Alibaba Cloud infrastructure to facilitate load balancer traffic ingress access. Follow the steps below and refer to the CCM documentation for deploying CCM. Configure providerID on Cluster Nodes On each node in the cluster, run the following command to obtain the providerID for each node. http://100.100.100.200/latest/meta-data is the API entry point provided by Alibaba Cloud CLI for retrieving instance metadata. You don't need to modify it in the provided example. For more information, please refer to ECS instance metadata . ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` On the master node of the cluster, use the kubectl patch command to add the providerID for each node in the cluster. This step is necessary to ensure the proper functioning of the CCM Pod on each corresponding node. Failure to run this step will result in the CCM Pod being unable to run correctly. ~# kubectl get nodes ~# kubectl patch node ${ NODE_NAME } -p '{\"spec\":{\"providerID\": \"${provider_id}\"}}' Create an Alibaba Cloud RAM user and grant authorization. A RAM user is an entity within Alibaba Cloud's Resource Access Management (RAM) that represents individuals or applications requiring access to Alibaba Cloud resources. Refer to Overview of RAM users to create a RAM user and assign the necessary permissions for accessing resources. To ensure that the RAM user used in the subsequent steps has sufficient privileges, grant the AdministratorAccess and AliyunSLBFullAccess permissions to the RAM user, following the instructions provided here. Obtain the AccessKey & AccessKeySecret for the RAM user. Log in to the RAM User account and go to User Center to retrieve the corresponding AccessKey & AccessKeySecret for the RAM User. Create the Cloud ConfigMap for CCM. Use the following method to write the AccessKey & AccessKeySecret obtained in step 3 as environment variables. ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** Run the following command to create cloud-config: accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF Retrieve the YAML file and install CCM by running the command kubectl apply -f cloud-controller-manager.yaml . The version of CCM being installed here is v2.5.0. Use the following command to obtain the cloud-controller-manager.yaml file and replace <<cluster_cidr>> with the actual cluster CIDR. ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml Verify if CCM is installed. ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s","title":"Deploy Cloud Controller Manager"},{"location":"usage/install/cloud/get-started-alibaba/#create-load-balancer-ingress-for-applications","text":"The following YAML will create two sets of services, one for TCP (layer 4 load balancing) and one for HTTP (layer 7 load balancing), with spec.type set to LoadBalancer . service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port : this annotation provided by CCM allows you to customize the exposed ports for layer 7 load balancing. For more information, refer to the CCM Usage Documentation . .spec.externalTrafficPolicy : indicates whether the service prefers to route external traffic to local or cluster-wide endpoints. It has two options: Cluster (default) and Local. Setting .spec.externalTrafficPolicy to Local preserves the client source IP. ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF After the creation is complete, you can view the following: ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM will automatically create layer 4 and layer 7 load balancers at its IaaS services. You can easily access and manage them through the Alibaba Cloud console, as shown below:","title":"Create Load Balancer Ingress for Applications"},{"location":"usage/install/cloud/get-started-alibaba/#verify-load-balancer-traffic-ingress-access","text":"On a public machine, access the load balancer's public IP + port to test the traffic ingress: # Access layer 4 load balancing $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # Access layer 7 load balancing $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes","title":"Verify Load Balancer Traffic Ingress Access"},{"location":"usage/install/cloud/get-started-alibaba/#summary","text":"Spiderpool is successfully running in an Alibaba Cloud cluster, ensuring normal east-west and north-south traffic.","title":"Summary"},{"location":"usage/install/cloud/get-started-openstack-zh_CN/","text":"openstack \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English","title":"openstack \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-openstack-zh_CN/#openstack","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"openstack \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-openstack/","text":"Running On Openstack English | \u7b80\u4f53\u4e2d\u6587","title":"openstack"},{"location":"usage/install/cloud/get-started-openstack/#running-on-openstack","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Openstack"},{"location":"usage/install/cloud/get-started-vmware-zh_CN/","text":"vmware \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English","title":"vmware \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-vmware-zh_CN/#vmware","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"vmware \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-vmware/","text":"Running On Vmware English | \u7b80\u4f53\u4e2d\u6587","title":"vmware vsphere"},{"location":"usage/install/cloud/get-started-vmware/#running-on-vmware","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Vmware"},{"location":"usage/install/overlay/get-started-calico-zh_cn/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Calico \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Calico \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0c \u5b83\u7684\u597d\u5904\u662f\uff1a \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u5e2e\u52a9\u89e3\u51b3 Macvlan \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u96c6\u7fa4\u5916\u90e8\u8bbf\u95ee NodePort \u65f6\uff0c\u53ef\u501f\u52a9 Calico \u6570\u636e\u8def\u5f84\u8fdb\u884c\u8f6c\u53d1\uff0c\u65e0\u9700\u5916\u90e8\u8def\u7531\u3002\u5426\u5219 Macvlan \u4f5c\u4e3a CNI \u65f6\uff0c\u53ea\u80fd\u501f\u52a9\u5916\u90e8\u8def\u7531\u8f6c\u53d1\u624d\u80fd\u5b9e\u73b0\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u4fdd\u8bc1 Pod \u8bbf\u95ee\u65f6\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u8054\u901a\u6027\u6b63\u5e38\u3002 \u6ce8: \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR\u7684\u7b80\u5199\u3002 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system \u5982\u679c\u4f60\u7684\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9 /opt/cni/bin \u4e0b\u672a\u5b89\u88c5 cni plugins \u3002\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u5b89\u88c5: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5 Spiderpool \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5b89\u88c5 Multus \u7ec4\u4ef6, \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u672c\u5730\u5b89\u88c5\u4e86 Multus, \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8df3\u8fc7\u5b89\u88c5 Multus: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false \u9700\u8981\u6307\u5b9a coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-m2kbt 1 /1 Running 0 1m spiderpool-multus-sl65s 1 /1 Running 0 1m \u521b\u5efa SpiderIPPool \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 ips \u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u521b\u5efa SpiderMultusConfig \u672c\u6587\u901a\u8fc7 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.spiderpoolConfigPools.IPv4IPPool \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d IP\u3001\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link \u4ee5\u4e0a\u8868\u9879\u89e3\u91ca: Pod \u4e2d\u5206\u914d\u4e86 Calico(eth0) \u548c Macvlan(net1) \u4e24\u5f20\u7f51\u5361, IPv4 \u5730\u5740\u5206\u522b\u662f: 10.233.73.210 \u548c 10.6.212.145 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod\u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.132 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u4e0b\u9762\u6d4b\u8bd5 Pod \u57fa\u672c\u7f51\u7edc\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 Pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Calico \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Calico \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0c \u5b83\u7684\u597d\u5904\u662f\uff1a \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u5e2e\u52a9\u89e3\u51b3 Macvlan \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u96c6\u7fa4\u5916\u90e8\u8bbf\u95ee NodePort \u65f6\uff0c\u53ef\u501f\u52a9 Calico \u6570\u636e\u8def\u5f84\u8fdb\u884c\u8f6c\u53d1\uff0c\u65e0\u9700\u5916\u90e8\u8def\u7531\u3002\u5426\u5219 Macvlan \u4f5c\u4e3a CNI \u65f6\uff0c\u53ea\u80fd\u501f\u52a9\u5916\u90e8\u8def\u7531\u8f6c\u53d1\u624d\u80fd\u5b9e\u73b0\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u4fdd\u8bc1 Pod \u8bbf\u95ee\u65f6\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u8054\u901a\u6027\u6b63\u5e38\u3002 \u6ce8: \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR\u7684\u7b80\u5199\u3002","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#_1","text":"\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system \u5982\u679c\u4f60\u7684\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9 /opt/cni/bin \u4e0b\u672a\u5b89\u88c5 cni plugins \u3002\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u5b89\u88c5: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm \u4e8c\u8fdb\u5236","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spiderpool","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5b89\u88c5 Multus \u7ec4\u4ef6, \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u672c\u5730\u5b89\u88c5\u4e86 Multus, \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8df3\u8fc7\u5b89\u88c5 Multus: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false \u9700\u8981\u6307\u5b9a coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-m2kbt 1 /1 Running 0 1m spiderpool-multus-sl65s 1 /1 Running 0 1m","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spiderippool","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 ips \u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002","title":"\u521b\u5efa SpiderIPPool"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spidermultusconfig","text":"\u672c\u6587\u901a\u8fc7 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.spiderpoolConfigPools.IPv4IPPool \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"\u521b\u5efa SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d IP\u3001\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link \u4ee5\u4e0a\u8868\u9879\u89e3\u91ca: Pod \u4e2d\u5206\u914d\u4e86 Calico(eth0) \u548c Macvlan(net1) \u4e24\u5f20\u7f51\u5361, IPv4 \u5730\u5740\u5206\u522b\u662f: 10.233.73.210 \u548c 10.6.212.145 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod\u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.132 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u4e0b\u9762\u6d4b\u8bd5 Pod \u57fa\u672c\u7f51\u7edc\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 Pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/overlay/get-started-calico/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Calico serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . This setup enables Pod's east-west traffic to be forwarded through the Calico-created NIC (eth0). The advantages offered by Spiderpool's solution are: Solve Macvlan's problem for accessing ClusterIP when Pods have both Calico and Macvlan NICs attached. Facilitate the forwarding of external access to NodePort through Calico's data path, eliminating the need for external routing. Whereas, external routing is typically required for forwarding when Macvlan is used as the CNI. Coordinate subnet routing for Pods with multiple Calico and Macvlan NICs, guaranteeing consistent traffic forwarding path for Pods and uninterrupted network connectivity. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR. Prerequisites A ready Kubernetes cluster. Calico has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system If the cni plugins are not installed under /opt/cni/bin on each node of your cluster, follow the commands below for installation: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm binary Install Spiderpool Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait By default, Spiderpool automatically installs Multus. However, if Multus has been already installed, you can skip the installation via the following command: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false It is necessary to specify that the coordinator operates in overlay mode Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-m2kbt 1 /1 Running 0 1m spiderpool-multus-sl65s 1 /1 Running 0 1m Create SpiderIPPool The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. Create SpiderMultusConfig The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 \u3002 Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' Create an application Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller Enter the Pod and use the command ip to view information such as IP addresses and routes within the Pod: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link Explanation of the above: The Pod is allocated two interfaces: eth0 (Calico) and net1 (Macvlan), having IPv4 addresses of 10.233.73.210 and 10.6.212.145, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, it will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the basic network connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Calico"},{"location":"usage/install/overlay/get-started-calico/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Calico serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . This setup enables Pod's east-west traffic to be forwarded through the Calico-created NIC (eth0). The advantages offered by Spiderpool's solution are: Solve Macvlan's problem for accessing ClusterIP when Pods have both Calico and Macvlan NICs attached. Facilitate the forwarding of external access to NodePort through Calico's data path, eliminating the need for external routing. Whereas, external routing is typically required for forwarding when Macvlan is used as the CNI. Coordinate subnet routing for Pods with multiple Calico and Macvlan NICs, guaranteeing consistent traffic forwarding path for Pods and uninterrupted network connectivity. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR.","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico/#prerequisites","text":"A ready Kubernetes cluster. Calico has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system If the cni plugins are not installed under /opt/cni/bin on each node of your cluster, follow the commands below for installation: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm binary","title":"Prerequisites"},{"location":"usage/install/overlay/get-started-calico/#install-spiderpool","text":"Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait By default, Spiderpool automatically installs Multus. However, if Multus has been already installed, you can skip the installation via the following command: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false It is necessary to specify that the coordinator operates in overlay mode Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-m2kbt 1 /1 Running 0 1m spiderpool-multus-sl65s 1 /1 Running 0 1m","title":"Install Spiderpool"},{"location":"usage/install/overlay/get-started-calico/#create-spiderippool","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones.","title":"Create SpiderIPPool"},{"location":"usage/install/overlay/get-started-calico/#create-spidermultusconfig","text":"The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 \u3002 Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"Create SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-calico/#create-an-application","text":"Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller Enter the Pod and use the command ip to view information such as IP addresses and routes within the Pod: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link Explanation of the above: The Pod is allocated two interfaces: eth0 (Calico) and net1 (Macvlan), having IPv4 addresses of 10.233.73.210 and 10.6.212.145, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, it will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the basic network connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Create an application"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/","text":"Cilium Quick Start English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Cilium \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u5f20\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0\u4ee5\u4e0b\u6548\u679c: Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u4e24\u5f20\u7f51\u5361 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Cilium \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0cPod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u4ece Macvlan \u521b\u5efa\u7684\u7f51\u5361(net1)\u8f6c\u53d1\u3002 Pod \u591a\u7f51\u5361\u7684\u8def\u7531\u8c03\u534f\uff0c\u4f7f Pod \u5bf9\u5185\u5bf9\u5916\u8bbf\u95ee\u6b63\u5e38 \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR \u7684\u7b80\u5199\u3002 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Cilium \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system \u5982\u679c\u4f60\u7684\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9 /opt/cni/bin \u4e0b\u672a\u5b89\u88c5 cni plugins \u3002\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u5b89\u88c5: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5 Spiderpool \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5b89\u88c5 Multus \u7ec4\u4ef6, \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u672c\u5730\u5b89\u88c5\u4e86 Multus, \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8df3\u8fc7\u5b89\u88c5 Multus: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false \u9700\u8981\u6307\u5b9a coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-hkxb6 1 /1 Running 0 1m spiderpool-multus-l9dcs 1 /1 Running 0 1m \u521b\u5efa SpiderIPPool \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF Note: subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u521b\u5efa SpiderMultusConfig \u672c\u6587\u4f7f\u7528 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Note: spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.ippools.ipv4 \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 \u4ee5\u4e0a\u4fe1\u606f\u89e3\u91ca: Pod \u5206\u914d\u4e86\u4e24\u5f20\u7f51\u5361: eth0(cilium)\u3001net1(macvlan),\u5bf9\u5e94\u7684 IPv4 \u5730\u5740\u5206\u522b\u4e3a: 10.233.120.101 \u548c 10.6.212.202 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod \u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.131 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u7684\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#cilium-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Cilium \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u5f20\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0\u4ee5\u4e0b\u6548\u679c: Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u4e24\u5f20\u7f51\u5361 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Cilium \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0cPod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u4ece Macvlan \u521b\u5efa\u7684\u7f51\u5361(net1)\u8f6c\u53d1\u3002 Pod \u591a\u7f51\u5361\u7684\u8def\u7531\u8c03\u534f\uff0c\u4f7f Pod \u5bf9\u5185\u5bf9\u5916\u8bbf\u95ee\u6b63\u5e38 \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR \u7684\u7b80\u5199\u3002","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#_1","text":"\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Cilium \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system \u5982\u679c\u4f60\u7684\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9 /opt/cni/bin \u4e0b\u672a\u5b89\u88c5 cni plugins \u3002\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u547d\u4ee4\u5b89\u88c5: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm \u4e8c\u8fdb\u5236","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spiderpool","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5b89\u88c5 Multus \u7ec4\u4ef6, \u5982\u679c\u60a8\u5df2\u7ecf\u5728\u672c\u5730\u5b89\u88c5\u4e86 Multus, \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8df3\u8fc7\u5b89\u88c5 Multus: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false \u9700\u8981\u6307\u5b9a coordinator \u8fd0\u884c\u5728 overlay \u6a21\u5f0f \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-hkxb6 1 /1 Running 0 1m spiderpool-multus-l9dcs 1 /1 Running 0 1m","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spiderippool","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF Note: subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002","title":"\u521b\u5efa SpiderIPPool"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spidermultusconfig","text":"\u672c\u6587\u4f7f\u7528 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Note: spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.ippools.ipv4 \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"\u521b\u5efa SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 \u4ee5\u4e0a\u4fe1\u606f\u89e3\u91ca: Pod \u5206\u914d\u4e86\u4e24\u5f20\u7f51\u5361: eth0(cilium)\u3001net1(macvlan),\u5bf9\u5e94\u7684 IPv4 \u5730\u5740\u5206\u522b\u4e3a: 10.233.120.101 \u548c 10.6.212.202 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod \u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.131 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u7684\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/overlay/get-started-cilium/","text":"Cilium Quick Start English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Cilium serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . The advantages offered by Spiderpool's solution are: Pods have both Cilium and Macvlan NICs. East-west traffic is routed through the Cilium NIC (eth0), while north-south traffic is routed through the Macvlan NIC (net1). The routing coordination among multiple NICs of the Pod ensures seamless connectivity for both internal and external access. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR. Prerequisites A ready Kubernetes cluster. Cilium has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system If the cni plugins are not installed under /opt/cni/bin on each node of your cluster, follow the commands below for installation: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm binary Install Spiderpool Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait By default, Spiderpool automatically installs Multus. However, if Multus has been already installed, you can skip the installation via the following command: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false It is necessary to specify that coordinator operates in overlay mode Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-hkxb6 1 /1 Running 0 1m spiderpool-multus-l9dcs 1 /1 Running 0 1m Create SpiderIPPool The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. Create SpiderMultusConfig The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 . Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' Create an application Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller Use the command ip to view the Pod's information such as routes: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (cilium) and net1 (macvlan), having IPv4 addresses of 10.233.120.101 and 10.6.212.202, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, traffic will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the east-west connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Cilium"},{"location":"usage/install/overlay/get-started-cilium/#cilium-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Cilium serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . The advantages offered by Spiderpool's solution are: Pods have both Cilium and Macvlan NICs. East-west traffic is routed through the Cilium NIC (eth0), while north-south traffic is routed through the Macvlan NIC (net1). The routing coordination among multiple NICs of the Pod ensures seamless connectivity for both internal and external access. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR.","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium/#prerequisites","text":"A ready Kubernetes cluster. Cilium has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system If the cni plugins are not installed under /opt/cni/bin on each node of your cluster, follow the commands below for installation: ~# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.3.0.tgz -C /opt/cni/bin Helm binary","title":"Prerequisites"},{"location":"usage/install/overlay/get-started-cilium/#install-spiderpool","text":"Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait By default, Spiderpool automatically installs Multus. However, if Multus has been already installed, you can skip the installation via the following command: helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.install=false It is necessary to specify that coordinator operates in overlay mode Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-multus-hkxb6 1 /1 Running 0 1m spiderpool-multus-l9dcs 1 /1 Running 0 1m","title":"Install Spiderpool"},{"location":"usage/install/overlay/get-started-cilium/#create-spiderippool","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ipVersion: 4 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones.","title":"Create SpiderIPPool"},{"location":"usage/install/overlay/get-started-cilium/#create-spidermultusconfig","text":"The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 . Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"Create SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-cilium/#create-an-application","text":"Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller Use the command ip to view the Pod's information such as routes: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (cilium) and net1 (macvlan), having IPv4 addresses of 10.233.120.101 and 10.6.212.202, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, traffic will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the east-west connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Create an application"},{"location":"usage/install/underlay/get-started-calico-zh_CN/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3: \u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740 IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9 \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528 \u5148\u51b3\u6761\u4ef6 \u4e00\u4e2a Kubernetes \u96c6\u7fa4(\u63a8\u8350 k8s version > 1.22), \u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002 \u786e\u8ba4 calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002 \u786e\u8ba4 calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002 Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177 \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u521b\u5efa Pod \u4f7f\u7528\u7684 SpiderIPPool \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602 \u914d\u7f6e Calico BGP [\u53ef\u9009] \u672c\u4f8b\u5e0c\u671b calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u7684 IP \u6c60\u6240\u5728\u7684\u5b50\u7f51( 10.244.0.0/16 )\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 Pod IP\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002 \u7f51\u7edc\u62d3\u6251\u5982\u4e0b: \u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router \u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR: root@router:~# apt install -y frr FRR \u5f00\u542f BGP \u529f\u80fd: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr \u914d\u7f6e FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy \u914d\u7f6e\u89e3\u91ca: Router \u4fa7\u7684 AS \u4e3a 23000 , \u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a 64512 \u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a ebgp , \u8282\u70b9\u4e4b\u95f4\u4e3a ibgp \u9700\u8981\u5173\u95ed ebgp-requires-policy , \u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb 172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP \u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting \u3002 \u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45 Calico \u9700\u8981\u914d\u7f6e calico_backend: bird , \u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 \u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh : [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' \u521b\u5efa BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740 asNumber \u4e3a BGP Router \u7684 AS \u53f7 \u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. \u66f4\u591a Calico BGP \u914d\u7f6e, \u8bf7\u53c2\u8003 Calico BGP \u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60 \u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60, \u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-ippool spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51: 10.244.0.0/16 \u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a: Never \u5207\u6362 Calico \u7684 IPAM \u4e3a Spiderpool \u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a Calico \u7684 CNI \u914d\u7f6e\u6587\u4ef6: /etc/cni/net.d/10-calico.conflist , \u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" }, \u521b\u5efa\u5e94\u7528 \u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : \u4ece \"nginx-ippool-v4\" SpiderIPPool \u4e2d\u5206\u914d\u56fa\u5b9a IP \u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annnotations \u6307\u5b9a\u7684 ippool: nginx-ippool-v4 \u4e2d\u7ed9 Pod \u5206\u914d IP\u3002 [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false \u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 nginx-ippool-v4 \u7684 IP \u6c60\u8303\u56f4\u5185: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> \u6269\u5bb9\u526f\u672c\u6570\u5230 3 , \u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece IP \u6c60: nginx-ippool-v4 \u4e2d\u5206\u914d: [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> \u67e5\u770b IP \u6c60: nginx-ippool-v4 \u7684 ALLOCATED-IP-COUNT \u65b0\u589e 1 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 25602 false false \u7ed3\u8bba \u7ecf\u8fc7\u6d4b\u8bd5: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38(\u5305\u62ec\u8de8 Calico \u5b50\u7f51)\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3: \u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740 IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9 \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_1","text":"\u4e00\u4e2a Kubernetes \u96c6\u7fa4(\u63a8\u8350 k8s version > 1.22), \u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002 \u786e\u8ba4 calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002 \u786e\u8ba4 calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002 Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#spiderpool","text":"helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u521b\u5efa Pod \u4f7f\u7528\u7684 SpiderIPPool \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-bgp","text":"\u672c\u4f8b\u5e0c\u671b calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u7684 IP \u6c60\u6240\u5728\u7684\u5b50\u7f51( 10.244.0.0/16 )\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 Pod IP\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002 \u7f51\u7edc\u62d3\u6251\u5982\u4e0b: \u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router \u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR: root@router:~# apt install -y frr FRR \u5f00\u542f BGP \u529f\u80fd: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr \u914d\u7f6e FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy \u914d\u7f6e\u89e3\u91ca: Router \u4fa7\u7684 AS \u4e3a 23000 , \u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a 64512 \u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a ebgp , \u8282\u70b9\u4e4b\u95f4\u4e3a ibgp \u9700\u8981\u5173\u95ed ebgp-requires-policy , \u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb 172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP \u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting \u3002 \u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45 Calico \u9700\u8981\u914d\u7f6e calico_backend: bird , \u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 \u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh : [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' \u521b\u5efa BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740 asNumber \u4e3a BGP Router \u7684 AS \u53f7 \u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. \u66f4\u591a Calico BGP \u914d\u7f6e, \u8bf7\u53c2\u8003 Calico BGP","title":"\u914d\u7f6e Calico BGP [\u53ef\u9009]"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-ip","text":"\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60, \u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-ippool spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51: 10.244.0.0/16 \u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a: Never","title":"\u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-ipam-spiderpool","text":"\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a Calico \u7684 CNI \u914d\u7f6e\u6587\u4ef6: /etc/cni/net.d/10-calico.conflist , \u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" },","title":"\u5207\u6362 Calico \u7684 IPAM \u4e3a Spiderpool"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_2","text":"\u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : \u4ece \"nginx-ippool-v4\" SpiderIPPool \u4e2d\u5206\u914d\u56fa\u5b9a IP \u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annnotations \u6307\u5b9a\u7684 ippool: nginx-ippool-v4 \u4e2d\u7ed9 Pod \u5206\u914d IP\u3002 [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false \u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 nginx-ippool-v4 \u7684 IP \u6c60\u8303\u56f4\u5185: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> \u6269\u5bb9\u526f\u672c\u6570\u5230 3 , \u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece IP \u6c60: nginx-ippool-v4 \u4e2d\u5206\u914d: [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> \u67e5\u770b IP \u6c60: nginx-ippool-v4 \u7684 ALLOCATED-IP-COUNT \u65b0\u589e 1 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 25602 false false","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_3","text":"\u7ecf\u8fc7\u6d4b\u8bd5: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38(\u5305\u62ec\u8de8 Calico \u5b50\u7f51)\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002","title":"\u7ed3\u8bba"},{"location":"usage/install/underlay/get-started-calico/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements: Assign static IP addresses to applications Scale IP pools dynamically based on replica counts Enable external clients outside the cluster to access applications without their IPs Prerequisites An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI. Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks. Confirm that Calico has enabled BGP configuration in full-mesh mode. Helm and Calicoctl Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Create the SpiderIPPool instance used by the Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF Verify the installation\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602 Configure Calico BGP [optional] In this example, we want Calico to work in underlay mode and announce the subnet where Spiderpool's IPPool resides ( 10.244.0.0/16 ) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router. If you don't need external clients to access pod IPs directly, skip this step. The network topology is as follows: Configure a host outside the cluster as BGP Router We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand: root@router:~# apt install -y frr FRR enable BGP: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr Configure FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy Configuration descriptions: The AS on the router side is 23000 , and the AS on the cluster node side is 64512 . The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp. ebgp-requires-policy needs to be disabled, otherwise the BGP session cannot be established. 172.16.13.11/21 is the IP address of the cluster node. For more information, refer to frrouting . Configure BGP neighbor for Calico calico_backend: bird needs to be configured to establish a BGP session: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled: [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' Create BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP is the IP address of BGP Router asNumber is the AS number of BGP Router Check if the BGP session is established: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. For more information on Calico BGP configuration, refer to Calico BGP . Create a Calico IP pool in the same subnet Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-subnet spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF The CIDR needs to correspond to the subnet of Spiderpool: 10.244.0.0/16 Set ipipMode and vxlanMode to: Never Switch Calico's IPAM to Spiderpool Change the Calico CNI configuration file /etc/cni/net.d/10-calico.conflist on each node to switch the ipam field to Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" }, Create applications Take the Nginx application as an example: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : Assign static IPs from \"nginx-ippool-v4\" SpiderIPPool When the application Pod is created, Spiderpool assigns the IP to the Pod from the ippool: nginx-ippool-v4 specified in the annnotations. [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false When replicas are restarted, their IPs are fixed within the range of the nginx-ippool-v4 IPPool: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> Expand the number of replicas to 3 , the IP address of the new replica is still allocated from the IPPool: nginx-ippool-v4 : [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> View IP pool: Added 1 to ALLOCATED-IP-COUNT of nginx-ippool-v4 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 5 false false Conclusion The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.","title":"Calico"},{"location":"usage/install/underlay/get-started-calico/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements: Assign static IP addresses to applications Scale IP pools dynamically based on replica counts Enable external clients outside the cluster to access applications without their IPs","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico/#prerequisites","text":"An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI. Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks. Confirm that Calico has enabled BGP configuration in full-mesh mode. Helm and Calicoctl","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-calico/#install-spiderpool","text":"helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Create the SpiderIPPool instance used by the Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF Verify the installation\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-calico/#configure-calico-bgp-optional","text":"In this example, we want Calico to work in underlay mode and announce the subnet where Spiderpool's IPPool resides ( 10.244.0.0/16 ) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router. If you don't need external clients to access pod IPs directly, skip this step. The network topology is as follows: Configure a host outside the cluster as BGP Router We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand: root@router:~# apt install -y frr FRR enable BGP: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr Configure FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy Configuration descriptions: The AS on the router side is 23000 , and the AS on the cluster node side is 64512 . The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp. ebgp-requires-policy needs to be disabled, otherwise the BGP session cannot be established. 172.16.13.11/21 is the IP address of the cluster node. For more information, refer to frrouting . Configure BGP neighbor for Calico calico_backend: bird needs to be configured to establish a BGP session: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled: [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' Create BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP is the IP address of BGP Router asNumber is the AS number of BGP Router Check if the BGP session is established: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. For more information on Calico BGP configuration, refer to Calico BGP .","title":"Configure Calico BGP [optional]"},{"location":"usage/install/underlay/get-started-calico/#create-a-calico-ip-pool-in-the-same-subnet","text":"Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-subnet spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF The CIDR needs to correspond to the subnet of Spiderpool: 10.244.0.0/16 Set ipipMode and vxlanMode to: Never","title":"Create a Calico IP pool in the same subnet"},{"location":"usage/install/underlay/get-started-calico/#switch-calicos-ipam-to-spiderpool","text":"Change the Calico CNI configuration file /etc/cni/net.d/10-calico.conflist on each node to switch the ipam field to Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" },","title":"Switch Calico's IPAM to Spiderpool"},{"location":"usage/install/underlay/get-started-calico/#create-applications","text":"Take the Nginx application as an example: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : Assign static IPs from \"nginx-ippool-v4\" SpiderIPPool When the application Pod is created, Spiderpool assigns the IP to the Pod from the ippool: nginx-ippool-v4 specified in the annnotations. [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false When replicas are restarted, their IPs are fixed within the range of the nginx-ippool-v4 IPPool: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> Expand the number of replicas to 3 , the IP address of the new replica is still allocated from the IPPool: nginx-ippool-v4 : [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> View IP pool: Added 1 to ALLOCATED-IP-COUNT of nginx-ippool-v4 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 5 false false","title":"Create applications"},{"location":"usage/install/underlay/get-started-calico/#conclusion","text":"The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.","title":"Conclusion"},{"location":"usage/install/underlay/get-started-kind-zh_CN/","text":"Kind Quick Start English | \u7b80\u4f53\u4e2d\u6587 Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u90e8\u7f72\u7b26\u5408\u60a8\u9700\u6c42\u7684\u96c6\u7fa4\uff0c\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002 \u5148\u51b3\u6761\u4ef6 \u5df2\u5b89\u88c5 Go \u514b\u9686 Spiderpool \u4ee3\u7801\u4ed3\u5e93\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002 git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u83b7\u53d6 Spiderpool \u7684\u6700\u65b0\u955c\u50cf\u3002 ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name | select((\"^v1.[0-9]*.[0-9]*$\"))' | head -n 1 ) \u6267\u884c make dev-doctor \uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\uff0c\u5982\u679c\u7f3a\u5c11\u7ec4\u4ef6\u4f1a\u4e3a\u60a8\u81ea\u52a8\u5b89\u88c5\u3002 Spiderpool \u811a\u672c\u652f\u6301\u7684\u591a\u79cd\u5b89\u88c5\u6a21\u5f0f \u5982\u679c\u60a8\u5728\u4e2d\u56fd\u5927\u9646\uff0c\u5b89\u88c5\u65f6\u53ef\u4ee5\u989d\u5916\u6307\u5b9a\u53c2\u6570 -e E2E_CHINA_IMAGE_REGISTRY=true \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u5b89\u88c5 Spiderpool \u5728 Underlay CNI\uff08Macvlan\uff09 \u96c6\u7fa4 ~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u5b89\u88c5 Spiderpool \u5728 Calico Overlay CNI \u96c6\u7fa4 ~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u5b89\u88c5 Spiderpool \u5728 Cilium Overlay CNI \u96c6\u7fa4 ~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u9a8c\u8bc1\u5b89\u88c5 \u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002 ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config \u60a8\u53ef\u4ee5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m spiderpool-multus-66xnx 1 /1 Running 0 3m spiderpool-multus-xwxv4 1 /1 Running 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> \u60a8\u4e5f\u53ef\u4ee5\u624b\u52a8\u521b\u5efa\u5e94\u7528\u9a8c\u8bc1 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> \u901a\u8fc7\u6d4b\u8bd5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u5b83\u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684\u66f4\u591a\u529f\u80fd\u3002 \u5378\u8f7d \u5378\u8f7d Kind \u96c6\u7fa4 \u6267\u884c make clean \u5378\u8f7d Kind \u96c6\u7fa4\u3002 \u5220\u9664\u6d4b\u8bd5\u955c\u50cf ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Kind Quick Start"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#kind-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u90e8\u7f72\u7b26\u5408\u60a8\u9700\u6c42\u7684\u96c6\u7fa4\uff0c\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002","title":"Kind Quick Start"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#_1","text":"\u5df2\u5b89\u88c5 Go \u514b\u9686 Spiderpool \u4ee3\u7801\u4ed3\u5e93\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002 git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u83b7\u53d6 Spiderpool \u7684\u6700\u65b0\u955c\u50cf\u3002 ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name | select((\"^v1.[0-9]*.[0-9]*$\"))' | head -n 1 ) \u6267\u884c make dev-doctor \uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\uff0c\u5982\u679c\u7f3a\u5c11\u7ec4\u4ef6\u4f1a\u4e3a\u60a8\u81ea\u52a8\u5b89\u88c5\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool","text":"\u5982\u679c\u60a8\u5728\u4e2d\u56fd\u5927\u9646\uff0c\u5b89\u88c5\u65f6\u53ef\u4ee5\u989d\u5916\u6307\u5b9a\u53c2\u6570 -e E2E_CHINA_IMAGE_REGISTRY=true \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002","title":"Spiderpool \u811a\u672c\u652f\u6301\u7684\u591a\u79cd\u5b89\u88c5\u6a21\u5f0f"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool-underlay-cnimacvlan","text":"~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5b89\u88c5 Spiderpool \u5728 Underlay CNI\uff08Macvlan\uff09 \u96c6\u7fa4"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool-calico-overlay-cni","text":"~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5b89\u88c5 Spiderpool \u5728 Calico Overlay CNI \u96c6\u7fa4"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool-cilium-overlay-cni","text":"~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5b89\u88c5 Spiderpool \u5728 Cilium Overlay CNI \u96c6\u7fa4"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#_2","text":"\u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002 ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config \u60a8\u53ef\u4ee5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m spiderpool-multus-66xnx 1 /1 Running 0 3m spiderpool-multus-xwxv4 1 /1 Running 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> \u60a8\u4e5f\u53ef\u4ee5\u624b\u52a8\u521b\u5efa\u5e94\u7528\u9a8c\u8bc1 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> \u901a\u8fc7\u6d4b\u8bd5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u5b83\u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684\u66f4\u591a\u529f\u80fd\u3002","title":"\u9a8c\u8bc1\u5b89\u88c5"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#_3","text":"\u5378\u8f7d Kind \u96c6\u7fa4 \u6267\u884c make clean \u5378\u8f7d Kind \u96c6\u7fa4\u3002 \u5220\u9664\u6d4b\u8bd5\u955c\u50cf ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"\u5378\u8f7d"},{"location":"usage/install/underlay/get-started-kind/","text":"Kind Quick Start English | \u7b80\u4f53\u4e2d\u6587 Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Spiderpool provides a script to install the Kind cluster, you can use it to deploy a cluster that meets your needs, and test and experience Spiderpool. Prerequisites Go has already been installed. Clone the Spiderpool code repository to the local host and go to the root directory of the Spiderpool project. ~# git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool Get the latest image of Spiderpool. ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name | select((\"^v1.[0-9]*.[0-9]*$\"))' | head -n 1 ) Execute make dev-doctor to check that the development tools on the local host meet the conditions for deploying a Kind cluster with Spiderpool, and that the components are automatically installed for you if they are missing. Various installation modes supported by Spiderpool script If you are mainland user who is not available to access ghcr.io, Additional parameter -e E2E_CHINA_IMAGE_REGISTRY=true can be specified during installation to help you pull images faster. Install Spiderpool in Underlay CNI (Macvlan) cluster ~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Install Spiderpool on Calico Overlay CNI cluster ~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Install Spiderpool on Cilium Overlay CNI cluster ~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Check that everything is working Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl. ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config It should be possible to observe the following: ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m spiderpool-multus-66xnx 1 /1 Running 0 3m spiderpool-multus-xwxv4 1 /1 Running 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application: ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> You can also manually create an application to verify that the cluster is available, the following command will create 1 copy of Deployment: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> As tested, everything works fine with the Kind cluster. You can test and experience more features of Spiderpool based on kind clusters. Uninstall Uninstall a Kind cluster Execute make clean to uninstall the Kind cluster. Delete test's images ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Macvlan"},{"location":"usage/install/underlay/get-started-kind/#kind-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Spiderpool provides a script to install the Kind cluster, you can use it to deploy a cluster that meets your needs, and test and experience Spiderpool.","title":"Kind Quick Start"},{"location":"usage/install/underlay/get-started-kind/#prerequisites","text":"Go has already been installed. Clone the Spiderpool code repository to the local host and go to the root directory of the Spiderpool project. ~# git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool Get the latest image of Spiderpool. ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name | select((\"^v1.[0-9]*.[0-9]*$\"))' | head -n 1 ) Execute make dev-doctor to check that the development tools on the local host meet the conditions for deploying a Kind cluster with Spiderpool, and that the components are automatically installed for you if they are missing.","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-kind/#various-installation-modes-supported-by-spiderpool-script","text":"If you are mainland user who is not available to access ghcr.io, Additional parameter -e E2E_CHINA_IMAGE_REGISTRY=true can be specified during installation to help you pull images faster.","title":"Various installation modes supported by Spiderpool script"},{"location":"usage/install/underlay/get-started-kind/#install-spiderpool-in-underlay-cni-macvlan-cluster","text":"~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Install Spiderpool in Underlay CNI (Macvlan) cluster"},{"location":"usage/install/underlay/get-started-kind/#install-spiderpool-on-calico-overlay-cni-cluster","text":"~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Install Spiderpool on Calico Overlay CNI cluster"},{"location":"usage/install/underlay/get-started-kind/#install-spiderpool-on-cilium-overlay-cni-cluster","text":"~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Install Spiderpool on Cilium Overlay CNI cluster"},{"location":"usage/install/underlay/get-started-kind/#check-that-everything-is-working","text":"Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl. ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config It should be possible to observe the following: ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m spiderpool-multus-66xnx 1 /1 Running 0 3m spiderpool-multus-xwxv4 1 /1 Running 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application: ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> You can also manually create an application to verify that the cluster is available, the following command will create 1 copy of Deployment: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> As tested, everything works fine with the Kind cluster. You can test and experience more features of Spiderpool based on kind clusters.","title":"Check that everything is working"},{"location":"usage/install/underlay/get-started-kind/#uninstall","text":"Uninstall a Kind cluster Execute make clean to uninstall the Kind cluster. Delete test's images ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Uninstall"},{"location":"usage/install/underlay/get-started-macvlan-and-sriov/","text":"","title":"Get started macvlan and sriov"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/","text":"Macvlan Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Macvlan \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5df2\u5b89\u88c5 Helm \u5b89\u88c5 Macvlan Macvlan \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u80fd\u591f\u4e3a Pod \u5206\u914d Macvlan \u865a\u62df\u7f51\u5361\uff0c\u53ef\u7528\u4e8e\u5bf9\u63a5 Underlay \u7f51\u7edc\u3002 \u4e00\u4e9b Kubernetes \u5b89\u88c5\u5668\u9879\u76ee\uff0c\u9ed8\u8ba4\u5b89\u88c5\u4e86 Macvlan \u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u786e\u8ba4\u8282\u70b9\u4e0a\u5b58\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6 /opt/cni/bin/macvlan \u3002\u5982\u679c\u8282\u70b9\u4e0a\u4e0d\u5b58\u5728\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u5728\u6240\u6709\u8282\u70b9\u4e0a\u4e0b\u8f7d\u5b89\u88c5\uff1a ~# wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin ~# chmod +x /opt/cni/bin/macvlan \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 \u521b\u5efa\u4e0e\u7f51\u7edc\u63a5\u53e3 eth0 \u5728\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u6c60\u4ee5\u4f9b Pod \u4f7f\u7528\uff0c\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - macvlan-conf EOF \u9a8c\u8bc1\u5b89\u88c5 ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false \u521b\u5efa CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528 MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms \u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a \u67e5\u770b service \u7684 IP\uff1a ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#macvlan-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Macvlan \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#_1","text":"\u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5df2\u5b89\u88c5 Helm","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#macvlan","text":"Macvlan \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u80fd\u591f\u4e3a Pod \u5206\u914d Macvlan \u865a\u62df\u7f51\u5361\uff0c\u53ef\u7528\u4e8e\u5bf9\u63a5 Underlay \u7f51\u7edc\u3002 \u4e00\u4e9b Kubernetes \u5b89\u88c5\u5668\u9879\u76ee\uff0c\u9ed8\u8ba4\u5b89\u88c5\u4e86 Macvlan \u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u786e\u8ba4\u8282\u70b9\u4e0a\u5b58\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6 /opt/cni/bin/macvlan \u3002\u5982\u679c\u8282\u70b9\u4e0a\u4e0d\u5b58\u5728\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u5728\u6240\u6709\u8282\u70b9\u4e0a\u4e0b\u8f7d\u5b89\u88c5\uff1a ~# wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz ~# tar xvfzp ./cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin ~# chmod +x /opt/cni/bin/macvlan","title":"\u5b89\u88c5 Macvlan"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 \u521b\u5efa\u4e0e\u7f51\u7edc\u63a5\u53e3 eth0 \u5728\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u6c60\u4ee5\u4f9b Pod \u4f7f\u7528\uff0c\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - macvlan-conf EOF \u9a8c\u8bc1\u5b89\u88c5 ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528 MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"\u521b\u5efa CNI \u914d\u7f6e"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#_2","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms \u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a \u67e5\u770b service \u7684 IP\uff1a ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-macvlan/","text":"Macvlan Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Macvlan and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort. Prerequisites Make sure a Kubernetes cluster is ready. Helm has been already installed. Install Macvlan Macvlan is a CNI plugin that allows pods to be assigned Macvlan virtual NICs for connecting to Underlay networks. Some Kubernetes installers include the Macvlan binary file by default that can be found at \"/opt/cni/bin/macvlan\" on your nodes. If the binary file is missing, you can download and install it on all nodes using the following command: wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz tar xvfzp ./cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin chmod +x /opt/cni/bin/macvlan Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the Multus clusterNetwork of the cluster through multus.multusCNI.defaultCniCRName , clusterNetwork is a specific field of the Multus plugin, which is used to specify the default network interface of the Pod. Create a SpiderIPPool instance. Create an IP Pool in the same subnet as the network interface eth0 for Pods to use, the following is an example of creating a related SpiderIPPool: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - macvlan-conf EOF Verify installation ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false Create CNI configuration To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating a Macvlan SpiderMultusConfig configuration: Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0. MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF In the example of this article, use the above configuration to create the following Macvlan SpiderMultusConfig, which will automatically generate Multus NetworkAttachmentDefinition CR based on it, which corresponds to the eth0 network card of the host. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m Create applications Create test Pods and service via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the status of Pods: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s Test the communication between Pods: ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms Test the communication between Pods and service IP: ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan/#macvlan-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Macvlan and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan/#prerequisites","text":"Make sure a Kubernetes cluster is ready. Helm has been already installed.","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-macvlan/#install-macvlan","text":"Macvlan is a CNI plugin that allows pods to be assigned Macvlan virtual NICs for connecting to Underlay networks. Some Kubernetes installers include the Macvlan binary file by default that can be found at \"/opt/cni/bin/macvlan\" on your nodes. If the binary file is missing, you can download and install it on all nodes using the following command: wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz tar xvfzp ./cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin chmod +x /opt/cni/bin/macvlan","title":"Install Macvlan"},{"location":"usage/install/underlay/get-started-macvlan/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the Multus clusterNetwork of the cluster through multus.multusCNI.defaultCniCRName , clusterNetwork is a specific field of the Multus plugin, which is used to specify the default network interface of the Pod. Create a SpiderIPPool instance. Create an IP Pool in the same subnet as the network interface eth0 for Pods to use, the following is an example of creating a related SpiderIPPool: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - macvlan-conf EOF Verify installation ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-macvlan/#create-cni-configuration","text":"To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating a Macvlan SpiderMultusConfig configuration: Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0. MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF In the example of this article, use the above configuration to create the following Macvlan SpiderMultusConfig, which will automatically generate Multus NetworkAttachmentDefinition CR based on it, which corresponds to the eth0 network card of the host. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"Create CNI configuration"},{"location":"usage/install/underlay/get-started-macvlan/#create-applications","text":"Create test Pods and service via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the status of Pods: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s Test the communication between Pods: ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms Test the communication between Pods and service IP: ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Create applications"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/","text":"Ovs-cni Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Ovs-cni \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u5b89\u88c5\u8bf4\u660e \u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002 ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch \u5b89\u88c5 Ovs-cni ovs-cni \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002 \u786e\u8ba4\u8282\u70b9\u4e0a\u662f\u5426\u5b58\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6 /opt/cni/bin/ovs \u3002\u5982\u679c\u8282\u70b9\u4e0a\u4e0d\u5b58\u5728\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u5728\u6240\u6709\u8282\u70b9\u4e0a\u4e0b\u8f7d\u5b89\u88c5\uff1a ~# wget https://github.com/k8snetworkplumbingwg/ovs-cni/releases/download/v0.31.1/plugin ~# mv ./plugin /opt/cni/bin/ovs ~# chmod +x /opt/cni/bin/ovs Ovs-cni \u4e0d\u4f1a\u914d\u7f6e\u7f51\u6865\uff0c\u7531\u7528\u6237\u521b\u5efa\u5b83\u4eec\uff0c\u5e76\u5c06\u5b83\u4eec\u8fde\u63a5\u5230 L2\u3001L3 \u7f51\u7edc\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u7f51\u6865\u7684\u793a\u4f8b\uff0c\u8bf7\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\uff1a \u521b\u5efa Open vSwitch \u7f51\u6865\u3002 ~# ovs-vsctl add-br br1 \u7f51\u7edc\u63a5\u53e3\u8fde\u63a5\u5230\u7f51\u6865 \u6b64\u8fc7\u7a0b\u53d6\u51b3\u4e8e\u60a8\u7684\u5e73\u53f0\uff0c\u4ee5\u4e0b\u547d\u4ee4\u53ea\u662f\u793a\u4f8b\u8bf4\u660e\uff0c\u5b83\u53ef\u80fd\u4f1a\u7834\u574f\u60a8\u7684\u7cfb\u7edf\u3002\u9996\u5148\u4f7f\u7528 ip link show \u67e5\u8be2\u4e3b\u673a\u7684\u53ef\u7528\u63a5\u53e3\uff0c\u793a\u4f8b\u4e2d\u4f7f\u7528\u4e3b\u673a\u4e0a\u7684\u63a5\u53e3\uff1a eth0 \u4e3a\u4f8b\u3002 ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801> dev br1 ~# ip link set br1 up ~# ip route add default via <\u9ed8\u8ba4\u7f51\u5173IP> dev br1 \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u4ee5\u5e2e\u52a9\u60a8\u5feb\u901f\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5 IP \u6c60\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5 IP \u6c60\u7684\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ipVersion: 4 ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - ovs-conf EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Ovs SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u7f51\u6865\u540d\u79f0\uff0c\u672c\u4f8b\u5b50\u4ee5 br1 \u4e3a\u4f8b: shell BRIDGE_NAME=\"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool \u4e3a\u5e94\u7528\u5206\u914d\u4e86 IP\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#ovs-cni-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Ovs-cni \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#_1","text":"\u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u5b89\u88c5\u8bf4\u660e \u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002 ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#ovs-cni","text":"ovs-cni \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002 \u786e\u8ba4\u8282\u70b9\u4e0a\u662f\u5426\u5b58\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6 /opt/cni/bin/ovs \u3002\u5982\u679c\u8282\u70b9\u4e0a\u4e0d\u5b58\u5728\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u5728\u6240\u6709\u8282\u70b9\u4e0a\u4e0b\u8f7d\u5b89\u88c5\uff1a ~# wget https://github.com/k8snetworkplumbingwg/ovs-cni/releases/download/v0.31.1/plugin ~# mv ./plugin /opt/cni/bin/ovs ~# chmod +x /opt/cni/bin/ovs Ovs-cni \u4e0d\u4f1a\u914d\u7f6e\u7f51\u6865\uff0c\u7531\u7528\u6237\u521b\u5efa\u5b83\u4eec\uff0c\u5e76\u5c06\u5b83\u4eec\u8fde\u63a5\u5230 L2\u3001L3 \u7f51\u7edc\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u7f51\u6865\u7684\u793a\u4f8b\uff0c\u8bf7\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\uff1a \u521b\u5efa Open vSwitch \u7f51\u6865\u3002 ~# ovs-vsctl add-br br1 \u7f51\u7edc\u63a5\u53e3\u8fde\u63a5\u5230\u7f51\u6865 \u6b64\u8fc7\u7a0b\u53d6\u51b3\u4e8e\u60a8\u7684\u5e73\u53f0\uff0c\u4ee5\u4e0b\u547d\u4ee4\u53ea\u662f\u793a\u4f8b\u8bf4\u660e\uff0c\u5b83\u53ef\u80fd\u4f1a\u7834\u574f\u60a8\u7684\u7cfb\u7edf\u3002\u9996\u5148\u4f7f\u7528 ip link show \u67e5\u8be2\u4e3b\u673a\u7684\u53ef\u7528\u63a5\u53e3\uff0c\u793a\u4f8b\u4e2d\u4f7f\u7528\u4e3b\u673a\u4e0a\u7684\u63a5\u53e3\uff1a eth0 \u4e3a\u4f8b\u3002 ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801> dev br1 ~# ip link set br1 up ~# ip route add default via <\u9ed8\u8ba4\u7f51\u5173IP> dev br1 \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\"","title":"\u5b89\u88c5 Ovs-cni"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u4ee5\u5e2e\u52a9\u60a8\u5feb\u901f\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a\u96c6\u7fa4\u7684 Multus clusterNetwork\uff0cclusterNetwork \u662f Multus \u63d2\u4ef6\u7684\u4e00\u4e2a\u7279\u5b9a\u5b57\u6bb5\uff0c\u7528\u4e8e\u6307\u5b9a Pod \u7684\u9ed8\u8ba4\u7f51\u7edc\u63a5\u53e3\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5 IP \u6c60\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5 IP \u6c60\u7684\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ipVersion: 4 ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - ovs-conf EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Ovs SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u7f51\u6865\u540d\u79f0\uff0c\u672c\u4f8b\u5b50\u4ee5 br1 \u4e3a\u4f8b: shell BRIDGE_NAME=\"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#_2","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool \u4e3a\u5e94\u7528\u5206\u914d\u4e86 IP\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-ovs/","text":"Ovs-cni Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus , Ovs-cni , and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster. Prerequisites Make sure a multi-node Kubernetes cluster is ready. Helm has been already installed. Open vSwitch must be installed and running on the host. It could refer to Installation . The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system. ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch Install Ovs-cni ovs-cni is a Kubernetes CNI plugin based on Open vSwitch (OVS) that provides a way to use OVS for network virtualization in a Kubernetes cluster in a Kubernetes cluster. Verify that the binary /opt/cni/bin/ovs exists on the node. if the binary is missing, you can download it and install it on all nodes using the following command: ~# wget https://github.com/k8snetworkplumbingwg/ovs-cni/releases/download/v0.31.1/plugin ~# mv ./plugin /opt/cni/bin/ovs ~# chmod +x /opt/cni/bin/ovs Note: Ovs-cni does not configure bridges, it is up to the user to create them and connect them to L2, L3, The following is an example of creating a bridge, to be executed on each node: Create an Open vSwitch bridge. ~# ovs-vsctl add-br br1 Network interface connected to the bridge\uff1a This procedure depends on your platform, the following commands are only example instructions and it may break your system. First use ip link show to query the host for available interfaces, the example uses the interface on the host: eth0 as an example. ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801> dev br1 ~# ip link set br1 up ~# ip route add default via <\u9ed8\u8ba4\u7f51\u5173IP> dev br1 Once created, the following bridge information can be viewed on each node: ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" Install Spiderpool Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the Multus clusterNetwork of the cluster through multus.multusCNI.defaultCniCRName , clusterNetwork is a specific field of the Multus plugin, which is used to specify the default network interface of the Pod. Create a SpiderIPPool instance. The Pod will obtain an IP address from the IPPool for underlying network communication, so the subnet of the IPPool needs to correspond to the underlying subnet being accessed. Here is an example of creating a SpiderSubnet instance: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ipVersion: 4 ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - ovs-conf EOF Verify the installation\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an ovs-cni SpiderMultusConfig configuration: Confirm the required host bridge for ovs-cni, for example based on the command ovs-vsctl show , this example takes the host bridge: br1 as an example. shell BRIDGE_NAME=\"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF Create applications In the following example Yaml, 2 copies of the Deployment are created, of which: v1.multus-cni.io/default-network : used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool assigns an IP to the application, and the application's IP will be automatically fixed within this IP range: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane Testing Pod communication with cross-node Pods: ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Ovs"},{"location":"usage/install/underlay/get-started-ovs/#ovs-cni-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus , Ovs-cni , and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster.","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs/#prerequisites","text":"Make sure a multi-node Kubernetes cluster is ready. Helm has been already installed. Open vSwitch must be installed and running on the host. It could refer to Installation . The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system. ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-ovs/#install-ovs-cni","text":"ovs-cni is a Kubernetes CNI plugin based on Open vSwitch (OVS) that provides a way to use OVS for network virtualization in a Kubernetes cluster in a Kubernetes cluster. Verify that the binary /opt/cni/bin/ovs exists on the node. if the binary is missing, you can download it and install it on all nodes using the following command: ~# wget https://github.com/k8snetworkplumbingwg/ovs-cni/releases/download/v0.31.1/plugin ~# mv ./plugin /opt/cni/bin/ovs ~# chmod +x /opt/cni/bin/ovs Note: Ovs-cni does not configure bridges, it is up to the user to create them and connect them to L2, L3, The following is an example of creating a bridge, to be executed on each node: Create an Open vSwitch bridge. ~# ovs-vsctl add-br br1 Network interface connected to the bridge\uff1a This procedure depends on your platform, the following commands are only example instructions and it may break your system. First use ip link show to query the host for available interfaces, the example uses the interface on the host: eth0 as an example. ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801> dev br1 ~# ip link set br1 up ~# ip route add default via <\u9ed8\u8ba4\u7f51\u5173IP> dev br1 Once created, the following bridge information can be viewed on each node: ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\"","title":"Install Ovs-cni"},{"location":"usage/install/underlay/get-started-ovs/#install-spiderpool","text":"Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the Multus clusterNetwork of the cluster through multus.multusCNI.defaultCniCRName , clusterNetwork is a specific field of the Multus plugin, which is used to specify the default network interface of the Pod. Create a SpiderIPPool instance. The Pod will obtain an IP address from the IPPool for underlying network communication, so the subnet of the IPPool needs to correspond to the underlying subnet being accessed. Here is an example of creating a SpiderSubnet instance: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ipVersion: 4 ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - ovs-conf EOF Verify the installation\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m spiderpool-multus-7vkm2 1 /1 Running 0 13m spiderpool-multus-rwzjn 1 /1 Running 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an ovs-cni SpiderMultusConfig configuration: Confirm the required host bridge for ovs-cni, for example based on the command ovs-vsctl show , this example takes the host bridge: br1 as an example. shell BRIDGE_NAME=\"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-ovs/#create-applications","text":"In the following example Yaml, 2 copies of the Deployment are created, of which: v1.multus-cni.io/default-network : used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool assigns an IP to the application, and the application's IP will be automatically fixed within this IP range: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane Testing Pod communication with cross-node Pods: ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Create applications"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/","text":"SRIOV Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Sriov \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u7684\u7f51\u5361\u5177\u6709 Sriov \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1 \u5148\u51b3\u6761\u4ef6 \u4e00\u4e2a Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361 \u67e5\u8be2\u7f51\u5361 bus-info\uff1a ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 \u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 Single Root I/O Virtualization (SR-IOV) \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) \u5b89\u88c5 Sriov-network-operator Sriov-network-operator \u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u81ea\u52a8\u5b89\u88c5\u3001\u914d\u7f6e sriov-cni \u548c sriov-device-plugin\u3002 \u5b89\u88c5 sriov-network-operator git clone https://github.com/k8snetworkplumbingwg/sriov-network-operator.git && cd sriov-network-operator/deployment helm install -n sriov-network-operator --create-namespace --set operator.resourcePrefix = spidernet.io --wait sriov-network-operator ./ \u5fc5\u987b\u7ed9 sriov \u5de5\u4f5c\u8282\u70b9\u6253\u4e0a label: 'node-role.kubernetes.io/worker=\"\"'\uff0csriov-operator \u76f8\u5173\u7ec4\u4ef6\u624d\u4f1a\u5c31\u7eea\u3002 sriov-network-operator \u9ed8\u8ba4\u5b89\u88c5\u5728 sriov-network-operator \u547d\u540d\u7a7a\u95f4\u4e0b \u914d\u7f6e sriov-network-operator \u5982\u679c SriovNetworkNodeState CRs \u7684\u72b6\u6001\u4e3a InProgress , \u8bf4\u660e sriov-operator \u6b63\u5728\u540c\u6b65\u8282\u70b9\u72b6\u6001\uff0c\u7b49\u5f85\u72b6\u6001\u4e3a Suncceeded \u8bf4\u660e\u540c\u6b65\u5b8c\u6210\u3002\u67e5\u770b CR, \u786e\u8ba4 sriov-network-operator \u5df2\u7ecf\u53d1\u73b0\u8282\u70b9\u4e0a\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361\u3002 $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:87 mtu: 1500 name: enp4s0f1np1 pciAddress: \"0000:04:00.1\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded \u4ece\u4e0a\u9762\u53ef\u77e5\uff0c\u8282\u70b9 node-1 \u4e0a\u7684\u63a5\u53e3 enp4s0f0np0 \u548c enp4s0f1np1 \u90fd\u5177\u6709 SR-IOV \u529f\u80fd\uff0c\u5e76\u4e14\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\u4e3a 8\u3002 \u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7\u521b\u5efa SriovNetworkNodePolicy CRs \u6765\u914d\u7f6e VFs\uff0c\u5e76\u4e14\u5b89\u88c5 sriov-device-plugin : $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nicSelector: pfNames: - enp4s0f0np0 nodeSelector: kubernetes.io/hostname: node-1 # \u53ea\u4f5c\u7528\u4e8e 10-20-1-240 \u8fd9\u4e2a\u8282\u70b9 numVfs: 8 # \u6e34\u671b\u7684 VFs \u6570\u91cf resourceName: sriov_netdevice EOF \u4e0b\u53d1\u540e, \u56e0\u4e3a\u9700\u8981\u914d\u7f6e\u8282\u70b9\u542f\u7528 SR-IOV \u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u91cd\u542f\u8282\u70b9\u3002\u5982\u6709\u9700\u8981\uff0c\u6307\u5b9a\u5de5\u4f5c\u8282\u70b9\u800c\u975e Master \u8282\u70b9\u3002 resourceName \u4e0d\u80fd\u4e3a\u7279\u6b8a\u5b57\u7b26\uff0c\u652f\u6301\u7684\u5b57\u7b26: [0-9],[a-zA-Z] \u548c \"_\"\u3002 \u5728\u4e0b\u53d1 SriovNetworkNodePolicy CRs \u4e4b\u540e\uff0c\u518d\u6b21\u67e5\u770b SriovNetworkNodeState CRs \u7684\u72b6\u6001, \u53ef\u4ee5\u770b\u89c1 status \u4e2d VFs \u5df2\u7ecf\u5f97\u5230\u914d\u7f6e: $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... \u67e5\u770b Node \u53d1\u73b0\u540d\u4e3a spidernet.io/sriov_netdevice \u7684 sriov \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d VF \u7684\u6570\u91cf\u4e3a 8: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF \u521b\u5efa SpiderMultusConfig \u5b9e\u4f8b\u3002 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice EOF \u6ce8\u610f: SpiderIPPool.Spec.multusName: kube-system/sriov-test \u8981\u548c\u521b\u5efa\u7684 SpiderMultusConfig \u5b9e\u4f8b\u7684 Name \u548c Namespace \u76f8\u5339\u914d resourceName: spidernet.io/sriov_netdevice \u7531\u5b89\u88c5 sriov-operator \u6307\u5b9a\u7684 resourcePrefix: spidernet.io \u548c\u521b\u5efa SriovNetworkNodePolicy CR \u65f6\u6307\u5b9a\u7684 resourceName: sriov_netdevice \u62fc\u63a5\u800c\u6210 \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet.io/sriov_netdevice: '1' limits: spidernet.io/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF \u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a spidernet/sriov_netdevice : \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 Sriov \u8d44\u6e90\u3002 v1.multus-cni.io/default-network \uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002 \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3 \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001 ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185: ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms \u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf \u67e5\u770b Service \u7684 IP\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"SRIOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#sriov-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Sriov \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u7684\u7f51\u5361\u5177\u6709 Sriov \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1","title":"SRIOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#_1","text":"\u4e00\u4e2a Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361 \u67e5\u8be2\u7f51\u5361 bus-info\uff1a ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 \u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 Single Root I/O Virtualization (SR-IOV) \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV )","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#sriov-network-operator","text":"Sriov-network-operator \u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u81ea\u52a8\u5b89\u88c5\u3001\u914d\u7f6e sriov-cni \u548c sriov-device-plugin\u3002 \u5b89\u88c5 sriov-network-operator git clone https://github.com/k8snetworkplumbingwg/sriov-network-operator.git && cd sriov-network-operator/deployment helm install -n sriov-network-operator --create-namespace --set operator.resourcePrefix = spidernet.io --wait sriov-network-operator ./ \u5fc5\u987b\u7ed9 sriov \u5de5\u4f5c\u8282\u70b9\u6253\u4e0a label: 'node-role.kubernetes.io/worker=\"\"'\uff0csriov-operator \u76f8\u5173\u7ec4\u4ef6\u624d\u4f1a\u5c31\u7eea\u3002 sriov-network-operator \u9ed8\u8ba4\u5b89\u88c5\u5728 sriov-network-operator \u547d\u540d\u7a7a\u95f4\u4e0b \u914d\u7f6e sriov-network-operator \u5982\u679c SriovNetworkNodeState CRs \u7684\u72b6\u6001\u4e3a InProgress , \u8bf4\u660e sriov-operator \u6b63\u5728\u540c\u6b65\u8282\u70b9\u72b6\u6001\uff0c\u7b49\u5f85\u72b6\u6001\u4e3a Suncceeded \u8bf4\u660e\u540c\u6b65\u5b8c\u6210\u3002\u67e5\u770b CR, \u786e\u8ba4 sriov-network-operator \u5df2\u7ecf\u53d1\u73b0\u8282\u70b9\u4e0a\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361\u3002 $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:87 mtu: 1500 name: enp4s0f1np1 pciAddress: \"0000:04:00.1\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded \u4ece\u4e0a\u9762\u53ef\u77e5\uff0c\u8282\u70b9 node-1 \u4e0a\u7684\u63a5\u53e3 enp4s0f0np0 \u548c enp4s0f1np1 \u90fd\u5177\u6709 SR-IOV \u529f\u80fd\uff0c\u5e76\u4e14\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\u4e3a 8\u3002 \u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7\u521b\u5efa SriovNetworkNodePolicy CRs \u6765\u914d\u7f6e VFs\uff0c\u5e76\u4e14\u5b89\u88c5 sriov-device-plugin : $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nicSelector: pfNames: - enp4s0f0np0 nodeSelector: kubernetes.io/hostname: node-1 # \u53ea\u4f5c\u7528\u4e8e 10-20-1-240 \u8fd9\u4e2a\u8282\u70b9 numVfs: 8 # \u6e34\u671b\u7684 VFs \u6570\u91cf resourceName: sriov_netdevice EOF \u4e0b\u53d1\u540e, \u56e0\u4e3a\u9700\u8981\u914d\u7f6e\u8282\u70b9\u542f\u7528 SR-IOV \u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u91cd\u542f\u8282\u70b9\u3002\u5982\u6709\u9700\u8981\uff0c\u6307\u5b9a\u5de5\u4f5c\u8282\u70b9\u800c\u975e Master \u8282\u70b9\u3002 resourceName \u4e0d\u80fd\u4e3a\u7279\u6b8a\u5b57\u7b26\uff0c\u652f\u6301\u7684\u5b57\u7b26: [0-9],[a-zA-Z] \u548c \"_\"\u3002 \u5728\u4e0b\u53d1 SriovNetworkNodePolicy CRs \u4e4b\u540e\uff0c\u518d\u6b21\u67e5\u770b SriovNetworkNodeState CRs \u7684\u72b6\u6001, \u53ef\u4ee5\u770b\u89c1 status \u4e2d VFs \u5df2\u7ecf\u5f97\u5230\u914d\u7f6e: $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... \u67e5\u770b Node \u53d1\u73b0\u540d\u4e3a spidernet.io/sriov_netdevice \u7684 sriov \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d VF \u7684\u6570\u91cf\u4e3a 8: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" }","title":"\u5b89\u88c5 Sriov-network-operator"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF \u521b\u5efa SpiderMultusConfig \u5b9e\u4f8b\u3002 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice EOF \u6ce8\u610f: SpiderIPPool.Spec.multusName: kube-system/sriov-test \u8981\u548c\u521b\u5efa\u7684 SpiderMultusConfig \u5b9e\u4f8b\u7684 Name \u548c Namespace \u76f8\u5339\u914d resourceName: spidernet.io/sriov_netdevice \u7531\u5b89\u88c5 sriov-operator \u6307\u5b9a\u7684 resourcePrefix: spidernet.io \u548c\u521b\u5efa SriovNetworkNodePolicy CR \u65f6\u6307\u5b9a\u7684 resourceName: sriov_netdevice \u62fc\u63a5\u800c\u6210","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#_2","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet.io/sriov_netdevice: '1' limits: spidernet.io/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF \u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a spidernet/sriov_netdevice : \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 Sriov \u8d44\u6e90\u3002 v1.multus-cni.io/default-network \uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002 \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3 \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001 ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185: ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms \u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf \u67e5\u770b Service \u7684 IP\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-sriov/","text":"SRIOV Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Sriov , Veth , and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort. Prerequisites Make sure a Kubernetes cluster is ready Helm has already been installed A SR-IOV-enabled NIC Check the NIC's bus-info: ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 Check whether the NIC supports SR-IOV via bus-info. If the Single Root I/O Virtualization (SR-IOV) field appears, it means that SR-IOV is supported: ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) Install Sriov-network-operator SriovNetwork helps us install sriov-cni and sriov-device-plugin components, making it easier to use sriov-cni: Install sriov-network-operator git clone https://github.com/k8snetworkplumbingwg/sriov-network-operator.git && cd sriov-network-operator/deployment helm install -n sriov-network-operator --create-namespace --set operator.resourcePrefix = spidernet.io --wait sriov-network-operator ./ You may need to label SR-IOV worker nodes using node-role.kubernetes.io/worker=\"\" label, if not already. By default, SR-IOV Operator will be deployed in namespace 'openshift-sriov-network-operator'. After installation, the node may reboot automatically. If necessary, install sriov-network-operator to the designated worker nodes. Configure sriov-network-operator Firstly, check the status of SriovNetworkNodeState CRs to confirm that sriov-network-operator has found a network card on the node that supports the SR-IOV feature. ```shell $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04:3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04:3f:72:d0:d2:87 mtu: 1500 name: enp4s0f1np1 pciAddress: \"0000:04:00.1\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded ``` As can be seen from the above, interfaces 'enp4s0f0np0' and 'enp4s0f1np1' on node 'node-1' both have SR-IOV capability and support a maximum number of VFs of 8. Below we will configure the VFs by creating SriovNetworkNodePolicy CRs and install sriov-device-plugin: ```shell $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nicSelector: pfNames: - enp4s0f0np0 nodeSelector: kubernetes.io/hostname: node-1 numVfs: 8 # The number of VFs desired resourceName: sriov_netdevice ``` After creating the SriovNetworkNodePolicy CRs, check the status of the SriovNetworkNodeState CRs again to see that the VFs in status have been configured: ```shell $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000:04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000:04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000:04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000:04:00.0 totalvfs: 8 vendor: \"8086\" ... ``` You can see the sriov resource named 'spidernet/sriov_netdevice' has been configured in Node object, where the number of VFs is 8: ```shell ~# kubectl get node node-1 -o json |jq '.status.allocatable' { \"cpu\": \"24\", \"ephemeral-storage\": \"94580335255\", \"hugepages-1Gi\": \"0\", \"hugepages-2Mi\": \"0\", \"spidernet/sriov_netdevice\": \"8\", \"memory\": \"16247944Ki\", \"pods\": \"110\" } Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Create a SpiderIPPool instance. The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF Create a SpiderMultusConfig instance. shell $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice Note: SpiderIPPool.Spec.multusName: 'kube-system/sriov-test' must be to match the Name and Namespace of the SpiderMultusConfig instance created. Create applications Create test Pods and Services via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet/sriov_netdevice: '1' limits: spidernet/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF Spec descriptions: spidernet/sriov_netdevice : Sriov resources used. v1.multus-cni.io/default-network : specifies the CNI configuration for Multus. For more information on Multus annotations, refer to Multus Quickstart . Check the status of Pods: ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> Spiderpool ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 Test the communication between Pods: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms Test the communication between Pods and Services: Check Services' IPs\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Access its own service within the Pod: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"SRIOV"},{"location":"usage/install/underlay/get-started-sriov/#sriov-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Sriov , Veth , and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.","title":"SRIOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov/#prerequisites","text":"Make sure a Kubernetes cluster is ready Helm has already been installed A SR-IOV-enabled NIC Check the NIC's bus-info: ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 Check whether the NIC supports SR-IOV via bus-info. If the Single Root I/O Virtualization (SR-IOV) field appears, it means that SR-IOV is supported: ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV )","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-sriov/#install-sriov-network-operator","text":"SriovNetwork helps us install sriov-cni and sriov-device-plugin components, making it easier to use sriov-cni: Install sriov-network-operator git clone https://github.com/k8snetworkplumbingwg/sriov-network-operator.git && cd sriov-network-operator/deployment helm install -n sriov-network-operator --create-namespace --set operator.resourcePrefix = spidernet.io --wait sriov-network-operator ./ You may need to label SR-IOV worker nodes using node-role.kubernetes.io/worker=\"\" label, if not already. By default, SR-IOV Operator will be deployed in namespace 'openshift-sriov-network-operator'. After installation, the node may reboot automatically. If necessary, install sriov-network-operator to the designated worker nodes. Configure sriov-network-operator Firstly, check the status of SriovNetworkNodeState CRs to confirm that sriov-network-operator has found a network card on the node that supports the SR-IOV feature. ```shell $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04:3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04:3f:72:d0:d2:87 mtu: 1500 name: enp4s0f1np1 pciAddress: \"0000:04:00.1\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded ``` As can be seen from the above, interfaces 'enp4s0f0np0' and 'enp4s0f1np1' on node 'node-1' both have SR-IOV capability and support a maximum number of VFs of 8. Below we will configure the VFs by creating SriovNetworkNodePolicy CRs and install sriov-device-plugin: ```shell $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nicSelector: pfNames: - enp4s0f0np0 nodeSelector: kubernetes.io/hostname: node-1 numVfs: 8 # The number of VFs desired resourceName: sriov_netdevice ``` After creating the SriovNetworkNodePolicy CRs, check the status of the SriovNetworkNodeState CRs again to see that the VFs in status have been configured: ```shell $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000:04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000:04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000:04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000:04:00.0 totalvfs: 8 vendor: \"8086\" ... ``` You can see the sriov resource named 'spidernet/sriov_netdevice' has been configured in Node object, where the number of VFs is 8: ```shell ~# kubectl get node node-1 -o json |jq '.status.allocatable' { \"cpu\": \"24\", \"ephemeral-storage\": \"94580335255\", \"hugepages-1Gi\": \"0\", \"hugepages-2Mi\": \"0\", \"spidernet/sriov_netdevice\": \"8\", \"memory\": \"16247944Ki\", \"pods\": \"110\" }","title":"Install Sriov-network-operator"},{"location":"usage/install/underlay/get-started-sriov/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Create a SpiderIPPool instance. The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF Create a SpiderMultusConfig instance. shell $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice Note: SpiderIPPool.Spec.multusName: 'kube-system/sriov-test' must be to match the Name and Namespace of the SpiderMultusConfig instance created.","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-sriov/#create-applications","text":"Create test Pods and Services via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet/sriov_netdevice: '1' limits: spidernet/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF Spec descriptions: spidernet/sriov_netdevice : Sriov resources used. v1.multus-cni.io/default-network : specifies the CNI configuration for Multus. For more information on Multus annotations, refer to Multus Quickstart . Check the status of Pods: ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> Spiderpool ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 Test the communication between Pods: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms Test the communication between Pods and Services: Check Services' IPs\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Access its own service within the Pod: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"Create applications"},{"location":"usage/install/underlay/get-started-weave-zh_CN/","text":"Weave Quick Start English | \u7b80\u4f53\u4e2d\u6587 Weave \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c Weave \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 IPAM \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 IPAM \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d Weave , \u5728\u4fdd\u7559 Weave \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 Spiderpool \u6269\u5c55 Weave \u7684 IPAM \u80fd\u529b\u3002 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177 \u5b89\u88c5 \u5b89\u88c5 Weave : kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml \u7b49\u5f85 Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u6240\u4f7f\u7528\u7684 IP \u6c60: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave \u4f7f\u7528 10.32.0.0/12 \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u5185 SpiderIPPool\u3002 \u9a8c\u8bc1\u5b89\u88c5 [ root@node1 ~ ] # kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 0 12901 false \u5207\u6362 Weave \u7684 IPAM \u4e3a Spiderpool \u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: /etc/cni/net.d/10-weave.conflist \u7684 ipam \u5b57\u6bb5: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } \u4fee\u6539\u4e3a: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } \u6216\u53ef\u901a\u8fc7 jq \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 jq \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: # \u4ee5 centos7 \u4e3a\u4f8b yum -y install jq \u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist \u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u6ce8\u89e3: ipam.spidernet.io/ippool \u6307\u5b9a Pod \u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d IP: [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4ece SpiderIPPool: weave-ippool-v4 \u4e2d\u5206\u914d IP Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool \u4e2d\u5206\u914d IP \u5730\u5740: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false \u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002 \u901a\u8fc7 Spiderpool , \u786e\u5b9e\u6269\u5c55\u4e86 Weave \u7684 IPAM \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528 \uff0c\u4f53\u9a8c Spiderpool \u5176\u4ed6\u7684\u529f\u80fd\u3002","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#weave-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Weave \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c Weave \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 IPAM \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 IPAM \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d Weave , \u5728\u4fdd\u7559 Weave \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 Spiderpool \u6269\u5c55 Weave \u7684 IPAM \u80fd\u529b\u3002","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_1","text":"\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_2","text":"\u5b89\u88c5 Weave : kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml \u7b49\u5f85 Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u6240\u4f7f\u7528\u7684 IP \u6c60: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave \u4f7f\u7528 10.32.0.0/12 \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u5185 SpiderIPPool\u3002 \u9a8c\u8bc1\u5b89\u88c5 [ root@node1 ~ ] # kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 0 12901 false","title":"\u5b89\u88c5"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#weave-ipam-spiderpool","text":"\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: /etc/cni/net.d/10-weave.conflist \u7684 ipam \u5b57\u6bb5: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } \u4fee\u6539\u4e3a: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } \u6216\u53ef\u901a\u8fc7 jq \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 jq \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: # \u4ee5 centos7 \u4e3a\u4f8b yum -y install jq \u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist \u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c","title":"\u5207\u6362 Weave \u7684 IPAM \u4e3a Spiderpool"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_3","text":"\u4f7f\u7528\u6ce8\u89e3: ipam.spidernet.io/ippool \u6307\u5b9a Pod \u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d IP: [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4ece SpiderIPPool: weave-ippool-v4 \u4e2d\u5206\u914d IP Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool \u4e2d\u5206\u914d IP \u5730\u5740: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false \u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002 \u901a\u8fc7 Spiderpool , \u786e\u5b9e\u6269\u5c55\u4e86 Weave \u7684 IPAM \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528 \uff0c\u4f53\u9a8c Spiderpool \u5176\u4ed6\u7684\u529f\u80fd\u3002","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-weave/","text":"Weave Quick Start English | \u7b80\u4f53\u4e2d\u6587 Weave , an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, Weave utilizes the built-in IPAM to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how Weave and Spiderpool can be integrated to extend Weave 's IPAM capabilities while preserving its original functions. Prerequisites A ready Kubernetes cluster without any CNI installed Helm, Kubectl and Jq (optional) Install Install Weave: kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml Wait for Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Wait for Pod Running and create the IPPool used by Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave uses 10.32.0.0/12 as the cluster's default subnet, and thus a SpiderIPPool with the same subnet needs to be created in this case. Verify installation shell [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1/1 Running 0 13m spiderpool-agent-kxf27 1/1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1/1 Running 0 13m spiderpool-init 0/1 Completed 0 13m [root@node1 ~]# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10.32.0.0/12 0 12901 false Switch Weave 's IPAM to Spiderpool Change the ipam field of /etc/cni/net.d/10-weave.conflist on each node: Change the following: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } To: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } Alternatively, it can be changed with jq in one step. If jq is not installed, you can use the following command to install it: # Take centos7 as an example yum -y install jq Change the CNI configuration file: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist Make sure to run this command at each node Create applications Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation ipam.spidernet.io/ippool : [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/subnet : specifies that the Pods will be assigned IPs from SpiderSubnet: weave-ippool-v4 . The Pods have been created and allocated IP addresses from Spiderpool Subnets: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false To test connectivity, let's use inter-node communication between Pods as an example: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms The test results indicate that IP allocation and network connectivity are normal. Spiderpool has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of Spiderpool .","title":"Weave"},{"location":"usage/install/underlay/get-started-weave/#weave-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Weave , an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, Weave utilizes the built-in IPAM to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how Weave and Spiderpool can be integrated to extend Weave 's IPAM capabilities while preserving its original functions.","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave/#prerequisites","text":"A ready Kubernetes cluster without any CNI installed Helm, Kubectl and Jq (optional)","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-weave/#install","text":"Install Weave: kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml Wait for Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Wait for Pod Running and create the IPPool used by Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave uses 10.32.0.0/12 as the cluster's default subnet, and thus a SpiderIPPool with the same subnet needs to be created in this case. Verify installation shell [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1/1 Running 0 13m spiderpool-agent-kxf27 1/1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1/1 Running 0 13m spiderpool-init 0/1 Completed 0 13m [root@node1 ~]# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10.32.0.0/12 0 12901 false","title":"Install"},{"location":"usage/install/underlay/get-started-weave/#switch-weaves-ipam-to-spiderpool","text":"Change the ipam field of /etc/cni/net.d/10-weave.conflist on each node: Change the following: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } To: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } Alternatively, it can be changed with jq in one step. If jq is not installed, you can use the following command to install it: # Take centos7 as an example yum -y install jq Change the CNI configuration file: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist Make sure to run this command at each node","title":"Switch Weave's IPAM to Spiderpool"},{"location":"usage/install/underlay/get-started-weave/#create-applications","text":"Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation ipam.spidernet.io/ippool : [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/subnet : specifies that the Pods will be assigned IPs from SpiderSubnet: weave-ippool-v4 . The Pods have been created and allocated IP addresses from Spiderpool Subnets: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false To test connectivity, let's use inter-node communication between Pods as an example: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms The test results indicate that IP allocation and network connectivity are normal. Spiderpool has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of Spiderpool .","title":"Create applications"}]}
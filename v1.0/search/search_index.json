{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"English | \u7b80\u4f53\u4e2d\u6587 We are a Cloud Native Computing Foundation sandbox project Spiderpool is the underlay and RDMA network solution of the Kubernetes, for bare metal, VM and any public cloud Introduction Spiderpool is an underlay and RDMA network solution for the Kubernetes. It enhances the capabilities of Macvlan CNI , ipvlan CNI , SR-IOV CNI , fulfills various networking needs, and supports to run on bare metal, virtual machine, and public cloud environments . Spiderpool delivers exceptional network performance, particularly benefiting network I/O-intensive and low-latency applications like storage, middleware, and AI . It could refer to website for more details. The Advantages Of Underlay CNI The underlay CNI is mainly including macvlan, ipvlan, and SR-IOV, which cloud access the layer 2 network of the node. It has some advantages: macvlan, ipvlan, and SR-IOV is crucial for supporting RDMA network acceleration. RDMA significantly enhances performance for AI applicaitons, latency-sensitive and network I/O-intensive applications, surpassing overlay network solutions in terms of network performance. Unlike CNI solutions based on veth virtual interfaces, underlay networks eliminate layer 3 network forwarding on the host, avoiding tunnel encapsulation overhead. This translates to excellent network performance with high throughput, low latency, and reduced CPU utilization for network forwarding. Connecting seamlessly with underlay layer 2 VLAN networks enables both layer 2 and layer 3 communication for applications. It supports multicast and broadcast communication, while allowing packets to be controlled by firewalls. Data packages carry the actual IP addresses of Pods, enabling direct north-south communication based on Pod IPs. This connectivity across multi-cloud networks enhances flexibility and ease of use. Underlay CNI can create virtual interfaces using different parent network interfaces on the host, providing isolated subnets for applications with high network overhead, such as storage and observability. Major Features Simplified installation and usage Through eliminating the need for manually installing multiple components such as Multus CNI , RDMA and SR-IOV, Spiderpool simplifies the installation process and decreases the number of running PODs. It provides streamlined installation procedures, encapsulates relevant CRDs, and offers comprehensive documentation for easy setup and management. CRD-based dual-stack IPAM Spiderpool provides exclusive and shared IP address pools, supporting various affinity settings. It supports to assign static IP addresses for stateful applications such as mysql , redis , and kubevirt , while enabling fixed IP address ranges for stateless ones. Spiderpool automates the management of exclusive IP pools, ensuring excellent IP reclamation to avoid IP leakage. In additions, it provides wonderful IPAM performance . The IPAM of Spiderpool could be available for any main CNI supporting third-party IPAM plugin, not only including Macvlan CNI , ipvlan CNI , and SR-IOV CNI , but also calico and weave as static IP usage. Multiple network interfaces from underlay and overlay CNI Spiderpool enables scenarios where Pods can have multiple underlay CNI interfaces or a combination of overlay and underlay CNI interfaces. It ensures proper IP addressing for each CNI interface and effectively manages policy routing to maintain consistent data paths, eliminating packet loss concerns. It could strengthen cilium , calico , and kubevirt . Enhanced network connectivity As we know, native CNI of macvlan ipvlan SR-IOV has lots of communication limits. However, Spiderpool establishes seamless connectivity between Pods and host machines, ensuring smooth functioning of Pod health checks. It enables Pods to access services through kube-proxy or eBPF-based kube-proxy replacement. Additionally, it supports advanced features like IP conflict detection and gateway reachability checks. The network of Multi-cluster could be connected by a same underlay network, or Submariner . eBPF enhancements The eBPF-based kube-proxy replacement significantly accelerates service access, while socket short-circuiting technology improves local Pod communication efficiency within the same node. Compared with kube-proxy manner, the improvement of the performance is Up to 25% on network delay, up to 50% on network throughput . RDMA support Spiderpool offers RDMA solutions based on RoCE and InfiniBand technologies. POD could use the RDMA device in shared or exclusive mode, and it is fit for AI workloads. Dual-stack network support Spiderpool supports IPv4-only, IPv6-only, and dual-stack environments. Good network performance of latency and throughput Spiderpool performs better than overlay CNI on network latency and throughput, refer to performance report . Metrics Application Scenarios Spiderpool, powered by underlay CNI, offers unparalleled network performance compared to overlay CNI solutions, as evidenced in I/O Performance . It can be effectively applied in various scenarios, including: Support to deploy on the environments of bare metal, virtual machine, and public cloud, especially offer a unified underlay CNI solution for hybrid cloud. Traditional host applications. They hope to directly use the underlay network, with the reasons such as direct access to the underlay multi-subnet, multicast, multicast, layer 2 network communication, etc. They cannot accept the NAT of the overlay network and hope to seamlessly migrate to Kubernetes. Network I/O-intensive applications such as middleware, data storage, log observability, and AI training. Applications which needs a separate network bandwidth. Latency-sensitive application. Quick start Refer to Quick start to explore Spiderpool quickly. Refer to Usage Index for usage details. Refer to Spiderpool Architecture for more detailed information. Roadmap Features macvlan ipvlan SR-IOV Service By Kubeproxy Beta Beta Beta Service By Kubeproxy Replacement Alpha Alpha Alpha Network Policy In-plan Alpha In-plan Bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta Multi-Cluster Alpha Alpha Alpha Egress Policy Alpha Alpha Alpha Multiple NIC And Routing Coordination Beta Beta Beta Scenarios Bare metal Bare metal and VM Bare metal For detailed information about all the planned features, please refer to the roadmap . Blogs Refer to Blogs . Governance The project is governed by a group of Maintainers and Committers . How they are selected and govern is outlined in our Governance Document . Adopters A list of adopters who are deploying Spiderpool in production, and of their use cases, can be found in file . Contribution Refer to Contribution to join us for developing Spiderppol. Community The Spiderpool community is committed to fostering an open and welcoming environment, with several ways to engage with other users and developers. You can find out more information by visiting our community repository . Slack: join the #Spiderpool channel on CNCF Slack by requesting an invitation from CNCF Slack. Once you have access to CNCF Slack, you can join the Spiderpool channel. Community Meeting: Welcome to our community meeting held on the 1st of every month. Feel free to join and discuss any questions or topics related to Spiderpool. Email: refer to the MAINTAINERS.md to find the email addresses of all maintainers. Feel free to contact them via email to report any issues or ask questions. WeChat group: scan the QR code below to join the Spiderpool technical discussion group and engage in further conversations with us. License Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Others Copyright The Spiderpool Authors We are a Cloud Native Computing Foundation sandbox project . The Linux Foundation\u00ae (TLF) has registered trademarks and uses trademarks. For a list of TLF trademarks, see Trademark Usage .","title":"Home"},{"location":"#_1","text":"English | \u7b80\u4f53\u4e2d\u6587 We are a Cloud Native Computing Foundation sandbox project Spiderpool is the underlay and RDMA network solution of the Kubernetes, for bare metal, VM and any public cloud","title":""},{"location":"#introduction","text":"Spiderpool is an underlay and RDMA network solution for the Kubernetes. It enhances the capabilities of Macvlan CNI , ipvlan CNI , SR-IOV CNI , fulfills various networking needs, and supports to run on bare metal, virtual machine, and public cloud environments . Spiderpool delivers exceptional network performance, particularly benefiting network I/O-intensive and low-latency applications like storage, middleware, and AI . It could refer to website for more details.","title":"Introduction"},{"location":"#the-advantages-of-underlay-cni","text":"The underlay CNI is mainly including macvlan, ipvlan, and SR-IOV, which cloud access the layer 2 network of the node. It has some advantages: macvlan, ipvlan, and SR-IOV is crucial for supporting RDMA network acceleration. RDMA significantly enhances performance for AI applicaitons, latency-sensitive and network I/O-intensive applications, surpassing overlay network solutions in terms of network performance. Unlike CNI solutions based on veth virtual interfaces, underlay networks eliminate layer 3 network forwarding on the host, avoiding tunnel encapsulation overhead. This translates to excellent network performance with high throughput, low latency, and reduced CPU utilization for network forwarding. Connecting seamlessly with underlay layer 2 VLAN networks enables both layer 2 and layer 3 communication for applications. It supports multicast and broadcast communication, while allowing packets to be controlled by firewalls. Data packages carry the actual IP addresses of Pods, enabling direct north-south communication based on Pod IPs. This connectivity across multi-cloud networks enhances flexibility and ease of use. Underlay CNI can create virtual interfaces using different parent network interfaces on the host, providing isolated subnets for applications with high network overhead, such as storage and observability.","title":"The Advantages Of Underlay CNI"},{"location":"#major-features","text":"Simplified installation and usage Through eliminating the need for manually installing multiple components such as Multus CNI , RDMA and SR-IOV, Spiderpool simplifies the installation process and decreases the number of running PODs. It provides streamlined installation procedures, encapsulates relevant CRDs, and offers comprehensive documentation for easy setup and management. CRD-based dual-stack IPAM Spiderpool provides exclusive and shared IP address pools, supporting various affinity settings. It supports to assign static IP addresses for stateful applications such as mysql , redis , and kubevirt , while enabling fixed IP address ranges for stateless ones. Spiderpool automates the management of exclusive IP pools, ensuring excellent IP reclamation to avoid IP leakage. In additions, it provides wonderful IPAM performance . The IPAM of Spiderpool could be available for any main CNI supporting third-party IPAM plugin, not only including Macvlan CNI , ipvlan CNI , and SR-IOV CNI , but also calico and weave as static IP usage. Multiple network interfaces from underlay and overlay CNI Spiderpool enables scenarios where Pods can have multiple underlay CNI interfaces or a combination of overlay and underlay CNI interfaces. It ensures proper IP addressing for each CNI interface and effectively manages policy routing to maintain consistent data paths, eliminating packet loss concerns. It could strengthen cilium , calico , and kubevirt . Enhanced network connectivity As we know, native CNI of macvlan ipvlan SR-IOV has lots of communication limits. However, Spiderpool establishes seamless connectivity between Pods and host machines, ensuring smooth functioning of Pod health checks. It enables Pods to access services through kube-proxy or eBPF-based kube-proxy replacement. Additionally, it supports advanced features like IP conflict detection and gateway reachability checks. The network of Multi-cluster could be connected by a same underlay network, or Submariner . eBPF enhancements The eBPF-based kube-proxy replacement significantly accelerates service access, while socket short-circuiting technology improves local Pod communication efficiency within the same node. Compared with kube-proxy manner, the improvement of the performance is Up to 25% on network delay, up to 50% on network throughput . RDMA support Spiderpool offers RDMA solutions based on RoCE and InfiniBand technologies. POD could use the RDMA device in shared or exclusive mode, and it is fit for AI workloads. Dual-stack network support Spiderpool supports IPv4-only, IPv6-only, and dual-stack environments. Good network performance of latency and throughput Spiderpool performs better than overlay CNI on network latency and throughput, refer to performance report . Metrics","title":"Major Features"},{"location":"#application-scenarios","text":"Spiderpool, powered by underlay CNI, offers unparalleled network performance compared to overlay CNI solutions, as evidenced in I/O Performance . It can be effectively applied in various scenarios, including: Support to deploy on the environments of bare metal, virtual machine, and public cloud, especially offer a unified underlay CNI solution for hybrid cloud. Traditional host applications. They hope to directly use the underlay network, with the reasons such as direct access to the underlay multi-subnet, multicast, multicast, layer 2 network communication, etc. They cannot accept the NAT of the overlay network and hope to seamlessly migrate to Kubernetes. Network I/O-intensive applications such as middleware, data storage, log observability, and AI training. Applications which needs a separate network bandwidth. Latency-sensitive application.","title":"Application Scenarios"},{"location":"#quick-start","text":"Refer to Quick start to explore Spiderpool quickly. Refer to Usage Index for usage details. Refer to Spiderpool Architecture for more detailed information.","title":"Quick start"},{"location":"#roadmap","text":"Features macvlan ipvlan SR-IOV Service By Kubeproxy Beta Beta Beta Service By Kubeproxy Replacement Alpha Alpha Alpha Network Policy In-plan Alpha In-plan Bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta Multi-Cluster Alpha Alpha Alpha Egress Policy Alpha Alpha Alpha Multiple NIC And Routing Coordination Beta Beta Beta Scenarios Bare metal Bare metal and VM Bare metal For detailed information about all the planned features, please refer to the roadmap .","title":"Roadmap"},{"location":"#blogs","text":"Refer to Blogs .","title":"Blogs"},{"location":"#governance","text":"The project is governed by a group of Maintainers and Committers . How they are selected and govern is outlined in our Governance Document .","title":"Governance"},{"location":"#adopters","text":"A list of adopters who are deploying Spiderpool in production, and of their use cases, can be found in file .","title":"Adopters"},{"location":"#contribution","text":"Refer to Contribution to join us for developing Spiderppol.","title":"Contribution"},{"location":"#community","text":"The Spiderpool community is committed to fostering an open and welcoming environment, with several ways to engage with other users and developers. You can find out more information by visiting our community repository . Slack: join the #Spiderpool channel on CNCF Slack by requesting an invitation from CNCF Slack. Once you have access to CNCF Slack, you can join the Spiderpool channel. Community Meeting: Welcome to our community meeting held on the 1st of every month. Feel free to join and discuss any questions or topics related to Spiderpool. Email: refer to the MAINTAINERS.md to find the email addresses of all maintainers. Feel free to contact them via email to report any issues or ask questions. WeChat group: scan the QR code below to join the Spiderpool technical discussion group and engage in further conversations with us.","title":"Community"},{"location":"#license","text":"Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.","title":"License"},{"location":"#others","text":"Copyright The Spiderpool Authors We are a Cloud Native Computing Foundation sandbox project . The Linux Foundation\u00ae (TLF) has registered trademarks and uses trademarks. For a list of TLF trademarks, see Trademark Usage .","title":"Others"},{"location":"README-zh_CN/","text":"Spiderpool English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u662f CNCF \u7684\u4e00\u4e2a Sandbox \u9879\u76ee \u3002 Spiderpool \u63d0\u4f9b\u4e86\u4e00\u4e2a Kubernetes \u7684 underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u80fd\u8fd0\u884c\u5728\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u4e0a\u3002 Spiderpool \u4ecb\u7ecd Spiderpool \u662f\u4e00\u4e2a kubernetes \u7684 underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u589e\u5f3a\u4e86 Macvlan CNI \u3001 ipvlan CNI \u548c SR-IOV CNI \u7684\u529f\u80fd\uff0c\u6ee1\u8db3\u4e86\u5404\u79cd\u7f51\u7edc\u9700\u6c42\uff0c\u4f7f\u5f97 underlay \u7f51\u7edc\u65b9\u6848\u53ef\u5e94\u7528\u5728**\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883**\u4e2d\uff0c\u53ef\u4e3a\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u3001\u4f4e\u5ef6\u65f6\u5e94\u7528\u5e26\u6765\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec**\u5b58\u50a8\u3001\u4e2d\u95f4\u4ef6\u3001AI \u7b49\u5e94\u7528**\u3002\u8be6\u7ec6\u7684\u6587\u6863\u53ef\u53c2\u8003 \u6587\u6863\u7ad9 \u3002 Underlay CNI \u7684\u4f18\u52bf underlay CNI \u4e3b\u8981\u6307 macvlan\u3001ipvlan\u3001SR-IOV \u7b49\u80fd\u591f\u76f4\u63a5\u8bbf\u95ee\u5bbf\u4e3b\u673a\u4e8c\u5c42\u7f51\u7edc\u7684 CNI \u6280\u672f\uff0c\u5b83\u6709\u5982\u4e0b\u4f18\u52bf\uff1a macvlan\u3001ipvlan\u3001SR-IOV \u662f\u627f\u8f7d RDMA \u7f51\u7edc\u52a0\u901f\u7684\u91cd\u8981\u6280\u672f\uff0cRDMA \u80fd\u4e3a AI \u5e94\u7528\u3001\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528\u3001\u7f51\u7edc I/O \u5bc6\u96c6\u578b\u5e94\u7528\u5e26\u6765\u6781\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u7f51\u7edc\u6027\u80fd\u5927\u5e45\u8d85\u8fc7 overlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u533a\u522b\u4e8e\u57fa\u4e8e veth \u865a\u62df\u7f51\u5361\u7684 CNI \u89e3\u51b3\u65b9\u6848\uff0cunderlay \u7f51\u7edc\u6570\u636e\u5305\u907f\u514d\u4e86\u5bbf\u4e3b\u673a\u7684\u4e09\u5c42\u7f51\u7edc\u8f6c\u53d1\uff0c\u6ca1\u6709\u96a7\u9053\u5c01\u88c5\u5f00\u9500\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u80fd\u4e3a\u5e94\u7528\u63d0\u4f9b\u4e86\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec\u4f18\u79c0\u7684\u7f51\u7edc\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u65f6\uff0c\u8282\u7701\u4e86 CPU \u7684\u7f51\u7edc\u8f6c\u53d1\u5f00\u9500\u3002 \u53ef\u76f4\u63a5\u5bf9\u63a5 underlay \u4e8c\u5c42 VLAN \u7f51\u7edc\uff0c\u5e94\u7528\u53ef\u8fdb\u884c\u4e8c\u5c42\u3001\u4e09\u5c42\u7f51\u7edc\u901a\u4fe1\uff0c\u53ef\u8fdb\u884c\u7ec4\u64ad\u3001\u591a\u64ad\u901a\u4fe1\uff0c\u6570\u636e\u5305\u53ef\u53d7\u9632\u706b\u5899\u7ba1\u63a7\u3002 \u6570\u636e\u5305\u643a\u5e26 Pod \u7684\u771f\u6b63 IP \u5730\u5740\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u57fa\u4e8e Pod IP \u8fdb\u884c\u5357\u5317\u5411\u901a\u4fe1\uff0c\u591a\u4e91\u7f51\u7edc\u5929\u7136\u8054\u901a\u3002 underlay CNI \u53ef\u57fa\u4e8e\u5bbf\u4e3b\u673a\u4e0d\u540c\u7684\u7236\u7f51\u5361\u6765\u521b\u5efa\u865a\u62df\u673a\u63a5\u53e3\uff0c\u56e0\u6b64\u53ef\u4e3a\u5b58\u50a8\u3001\u89c2\u6d4b\u6027\u7b49\u7f51\u7edc\u5f00\u9500\u5927\u7684\u5e94\u7528\u63d0\u4f9b\u9694\u79bb\u7684\u5b50\u7f51\u3002 Spiderpool \u6838\u5fc3\u529f\u80fd \u7b80\u5316\u5b89\u88c5\u548c\u4f7f\u7528 \u5f53\u524d\u5f00\u6e90\u793e\u533a\u5bf9\u4e8e underlay CNI \u548c RDMA \u7684\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u5b89\u88c5 Multus CNI \u3001RDMA\u3001SR-IOV \u7b49\u8bf8\u591a\u76f8\u5173\u7ec4\u4ef6\uff0cSpiderpool \u7b80\u5316\u4e86\u5b89\u88c5\u6d41\u7a0b\u548c\u8fd0\u884c POD \u6570\u91cf\uff0c\u5bf9\u76f8\u5173\u7684 CRD \u8fdb\u884c\u4e86\u5c01\u88c5\uff0c\u63d0\u4f9b\u4e86\u5404\u79cd\u573a\u666f\u7684\u5b8c\u5907\u6587\u6863\uff0c\u4f7f\u5f97\u4f7f\u7528\u3001\u7ba1\u7406\u66f4\u52a0\u4fbf\u6377\u3002 \u57fa\u4e8e CRD \u7684\u53cc\u6808 IPAM \u80fd\u529b \u63d0\u4f9b\u4e86\u72ec\u4eab\u3001\u5171\u4eab\u7684 IP \u5730\u5740\u6c60\uff0c\u652f\u6301\u8bbe\u7f6e\u5404\u79cd\u4eb2\u548c\u6027\uff0c\u4e3a\u4e2d\u95f4\u4ef6\u7b49\u6709\u72b6\u6001\u5e94\u7528\u548c kubevirt \u7b49\u56fa\u5b9a IP \u5730\u5740\u503c\uff0c\u4e3a\u65e0\u72b6\u6001\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\u8303\u56f4\uff0c\u81ea\u52a8\u5316\u7ba1\u7406\u72ec\u4eab\u7684 IP \u6c60\uff0c\u4f18\u79c0\u7684 IP \u56de\u6536\u907f\u514d IP \u6cc4\u9732\u7b49\u3002\u5e76\u4e14\uff0c\u5177\u5907\u4f18\u79c0\u7684 IPAM \u5206\u914d\u6027\u80fd \u3002 Spiderpool IPAM \u7ec4\u4ef6\u80fd\u591f\u4e3a\u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u7684 main CNI \u4f7f\u7528\uff0c\u4e0d\u4ec5\u5305\u542b\u4e86 Macvlan CNI \u3001 ipvlan CNI \u548c SR-IOV CNI \uff0c\u4e5f\u5305\u62ec\u4e86 calico \u548c weave \u4f5c\u4e3a\u9759\u6001 IP \u573a\u666f\u4f7f\u7528\u3002 underlay \u548c overlay CNI \u7684\u591a\u7f51\u5361\u63a5\u5165 \u5b83\u5305\u62ec\u4e86 \u201cPod \u63d2\u5165\u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u3001\u201cPod \u63d2\u5165\u4e00\u4e2a overlay CNI \u548c \u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u4e24\u79cd\u573a\u666f\uff0cPod \u5177\u5907\u591a\u79cd CNI \u7f51\u5361\uff0cSpiderpool \u80fd\u591f\u4e3a\u591a\u4e2a underlay CNI \u7f51\u5361\u5b9a\u5236\u4e0d\u540c\u7684 IP \u5730\u5740\uff0c\u8c03\u534f\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\u800c\u907f\u514d\u4e22\u5305\u3002\u5b83\u80fd\u591f\u4e3a cilium \u3001 calico \u548c kubevirt \u7b49\u9879\u76ee\u8fdb\u884c\u589e\u5f3a\u3002 \u589e\u5f3a\u7f51\u7edc\u8fde\u901a\u6027 \u4f17\u6240\u5468\u77e5\uff0c\u539f\u751f\u7684 macvlan ipvlan SR-IOV \u5b58\u5728\u8bf8\u591a\u901a\u4fe1\u9650\u5236\u3002\u4f46\u662f\uff0cSpiderpool \u6253\u901a Pod \u548c\u5bbf\u4e3b\u673a\u7684\u8fde\u901a\u6027\uff0c\u786e\u4fdd Pod \u5065\u5eb7\u68c0\u6d4b\u5de5\u4f5c\u6b63\u5e38\uff0c\u5e76\u53ef\u901a\u8fc7 kube-proxy \u6216 eBPF kube-proxy replacement \u4f7f\u5f97 Pod \u8bbf\u95ee service\uff0c\u652f\u6301 Pod \u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u7b49\u3002\u591a\u96c6\u7fa4\u7f51\u7edc\u53ef\u57fa\u4e8e\u76f8\u540c\u7684 underlay \u7f51\u7edc\u6216\u8005 Submariner \u5b9e\u73b0\u8054\u901a\u3002 eBPF \u589e\u5f3a kube-proxy replacement \u6280\u672f\u6781\u5927\u52a0\u901f\u4e86\u8bbf\u95ee service \u573a\u666f\uff0c\u540c\u8282\u70b9\u4e0a\u7684 socket \u77ed\u8def\u6280\u672f\u52a0\u901f\u4e86\u672c\u5730 Pod \u7684\u901a\u4fe1\u6548\u7387\u3002\u76f8\u6bd4 kube proxy \u89e3\u6790\u65b9\u5f0f\uff0c \u7f51\u7edc\u5ef6\u65f6\u6709\u6700\u5927 25% \u7684\u6539\u5584\uff0c\u7f51\u7edc\u541e\u5410\u6709 50% \u7684\u63d0\u9ad8 \u3002 RDMA \u63d0\u4f9b\u4e86\u57fa\u4e8e RoCE\u3001infiniband \u573a\u666f\u4e0b\u7684 RDMA \u89e3\u51b3\u65b9\u6848\uff0cPOD \u80fd\u591f\u72ec\u4eab\u6216\u5171\u4eab\u4f7f\u7528 RDMA \u8bbe\u5907\uff0c\u9002\u5408 AI \u7b49\u7f51\u7edc\u6027\u80fd\u9700\u6c42\u9ad8\u7684\u5e94\u7528\u3002 \u7f51\u7edc\u53cc\u6808\u652f\u6301 Spiderpool \u7ec4\u4ef6\u548c\u5176\u63d0\u4f9b\u7684\u6240\u6709\u529f\u80fd\uff0c\u652f\u6301 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u3002 \u4f18\u79c0\u7684\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u6027\u80fd Spiderpool \u5728\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86 overlay CNI\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u3002 \u6307\u6807 \u5e94\u7528\u573a\u666f Spiderpool \u57fa\u4e8e underlay CNI \u63d0\u4f9b\u4e86\u6bd4 overlay CNI \u8fd8\u4f18\u8d8a\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u3002\u5177\u4f53\u53ef\u5e94\u7528\u5728\u5982\u4e0b\uff1a \u652f\u6301\u8fd0\u884c\u5728\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u3001\u5404\u5927\u516c\u6709\u4e91\u5382\u5546\u7b49\u73af\u5883\uff0c\u5c24\u5176\u4e3a\u6df7\u5408\u4e91\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684 underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u4f20\u7edf\u7684\u4e3b\u673a\u5e94\u7528\u3002\u5b83\u4eec\u5e0c\u671b\u76f4\u63a5\u4f7f\u7528 underlay \u7f51\u7edc\u8fdb\u884c\u901a\u4fe1\uff0c\u4f8b\u5982\u76f4\u63a5\u8bbf\u95ee underlay \u591a\u5b50\u7f51\u3001\u591a\u64ad\u3001\u7ec4\u64ad\u3001\u4e8c\u5c42\u7f51\u7edc\u901a\u4fe1\u7b49\uff0c\u5b83\u4eec\u4e0d\u80fd\u63a5\u53d7 overlay \u7f51\u7edc\u7684 NAT\uff0c\u5e0c\u671b\u8fdb\u884c\u65e0\u7f1d\u79fb\u690d\u7684 Kubernetes\u3002 \u4e2d\u95f4\u4ef6\u3001\u6570\u636e\u5b58\u50a8\u3001\u65e5\u5fd7\u89c2\u6d4b\u3001AI \u8bad\u7ec3\u7b49\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u5e94\u7528\u3002 \u7f51\u7edc\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528\u3002 \u5feb\u901f\u5f00\u59cb \u53c2\u8003 \u5feb\u901f\u642d\u5efa \u6765\u4f7f\u7528 Spiderpool \u53c2\u8003 \u4f7f\u7528 \u6765\u4e86\u89e3\u5404\u79cd\u529f\u80fd\u7684\u4f7f\u7528\u65b9\u6cd5 \u53c2\u8003 \u67b6\u6784 \u6765\u4e86\u89e3\u67b6\u6784\u8bbe\u8ba1 Roadmap \u529f\u80fd macvlan ipvlan SR-IOV Service By Kubeproxy Beta Beta Beta Service By Kubeproxy Replacement Alpha Alpha Alpha Network Policy In-plan Alpha In-plan Bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta Multi-Cluster Alpha Alpha Alpha Egress Policy Alpha Alpha Alpha \u591a\u7f51\u5361\u548c\u8def\u7531\u8c03\u8c10 Beta Beta Beta \u9002\u7528\u573a\u666f \u88f8\u91d1\u5c5e \u88f8\u91d1\u5c5e\u548c\u865a\u62df\u673a \u88f8\u91d1\u5c5e \u5173\u4e8e\u6240\u6709\u7684\u529f\u80fd\u89c4\u5212\uff0c\u5177\u4f53\u53ef\u53c2\u8003 roadmap \u3002 Blogs \u53ef\u53c2\u8003 Blog \u3002 Governance Spiderpool \u9879\u76ee\u7531\u4e00\u7ec4 \u7ef4\u62a4\u8005\u548c\u63d0\u4ea4\u8005 \u7ba1\u7406\uff0c\u6211\u4eec\u7684 Governance Document \u6982\u8ff0\u4e86\u5982\u4f55\u6cbb\u7406\u6539\u9879\u76ee\u3002 \u4f7f\u7528\u8005 \u4f7f\u7528 Spiderpool \u9879\u76ee\u7684 \u7528\u6237 \u3002 \u53c2\u4e0e\u5f00\u53d1 \u53ef\u53c2\u8003 \u5f00\u53d1\u642d\u5efa\u6587\u6863 \u3002 \u793e\u533a Spiderpool \u793e\u533a\u81f4\u529b\u4e8e\u8425\u9020\u4e00\u4e2a\u5f00\u653e\u548c\u70ed\u60c5\u7684\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u4e0e\u5176\u4ed6\u7528\u6237\u548c\u5f00\u53d1\u4eba\u5458\u4e92\u52a8\u3002\u60a8\u53ef\u4ee5\u8bbf\u95ee\u6211\u4eec\u7684 \u793e\u533a\u7f51\u7ad9 \u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 Slack\uff1a\u5982\u679c\u4f60\u60f3\u5728 CNCF Slack \u52a0\u5165 Spiderpool \u7684\u9891\u9053\uff0c\u8bf7\u5148\u5f97\u5230 CNCF Slack \u7684 \u9080\u8bf7 \u7136\u540e\u52a0\u5165 #Spiderpool \u7684\u9891\u9053\u3002 \u90ae\u4ef6\uff1a\u60a8\u53ef\u4ee5\u67e5\u770b MAINTAINERS.md \u83b7\u53d6\u6240\u6709\u7ef4\u62a4\u8005\u7684\u90ae\u7bb1\u5730\u5740\uff0c \u8054\u7cfb\u90ae\u7bb1\u5730\u5740\u4ee5\u62a5\u544a\u4efb\u4f55\u95ee\u9898\u3002 \u793e\u533a\u4f1a\u8bae\uff1a\u6b22\u8fce\u52a0\u5165\u5230\u6211\u4eec\u6bcf\u4e2a\u67081\u53f7\u4e3e\u884c\u7684 \u793e\u533a\u4f1a\u8bae \uff0c\u53ef\u4ee5\u5728\u8fd9\u91cc\u8ba8\u8bba\u4efb\u4f55\u6709\u5173 Spiderpool \u7684\u95ee\u9898\u3002 \u5fae\u4fe1\u7fa4\uff1a\u60a8\u53ef\u4ee5\u626b\u63cf\u5fae\u4fe1\u4e8c\u7ef4\u7801\uff0c\u52a0\u5165\u5230 Spiderpool \u6280\u672f\u4ea4\u6d41\u7fa4\u4e0e\u6211\u4eec\u8fdb\u4e00\u6b65\u4ea4\u6d41\u3002 License Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Others Copyright The Spiderpool Authors We are a Cloud Native Computing Foundation sandbox project . The Linux Foundation\u00ae (TLF) has registered trademarks and uses trademarks. For a list of TLF trademarks, see Trademark Usage .","title":"Spiderpool"},{"location":"README-zh_CN/#spiderpool","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u662f CNCF \u7684\u4e00\u4e2a Sandbox \u9879\u76ee \u3002 Spiderpool \u63d0\u4f9b\u4e86\u4e00\u4e2a Kubernetes \u7684 underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u80fd\u8fd0\u884c\u5728\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u4e0a\u3002","title":"Spiderpool"},{"location":"README-zh_CN/#spiderpool_1","text":"Spiderpool \u662f\u4e00\u4e2a kubernetes \u7684 underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u589e\u5f3a\u4e86 Macvlan CNI \u3001 ipvlan CNI \u548c SR-IOV CNI \u7684\u529f\u80fd\uff0c\u6ee1\u8db3\u4e86\u5404\u79cd\u7f51\u7edc\u9700\u6c42\uff0c\u4f7f\u5f97 underlay \u7f51\u7edc\u65b9\u6848\u53ef\u5e94\u7528\u5728**\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883**\u4e2d\uff0c\u53ef\u4e3a\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u3001\u4f4e\u5ef6\u65f6\u5e94\u7528\u5e26\u6765\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec**\u5b58\u50a8\u3001\u4e2d\u95f4\u4ef6\u3001AI \u7b49\u5e94\u7528**\u3002\u8be6\u7ec6\u7684\u6587\u6863\u53ef\u53c2\u8003 \u6587\u6863\u7ad9 \u3002","title":"Spiderpool \u4ecb\u7ecd"},{"location":"README-zh_CN/#underlay-cni","text":"underlay CNI \u4e3b\u8981\u6307 macvlan\u3001ipvlan\u3001SR-IOV \u7b49\u80fd\u591f\u76f4\u63a5\u8bbf\u95ee\u5bbf\u4e3b\u673a\u4e8c\u5c42\u7f51\u7edc\u7684 CNI \u6280\u672f\uff0c\u5b83\u6709\u5982\u4e0b\u4f18\u52bf\uff1a macvlan\u3001ipvlan\u3001SR-IOV \u662f\u627f\u8f7d RDMA \u7f51\u7edc\u52a0\u901f\u7684\u91cd\u8981\u6280\u672f\uff0cRDMA \u80fd\u4e3a AI \u5e94\u7528\u3001\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528\u3001\u7f51\u7edc I/O \u5bc6\u96c6\u578b\u5e94\u7528\u5e26\u6765\u6781\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u7f51\u7edc\u6027\u80fd\u5927\u5e45\u8d85\u8fc7 overlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u533a\u522b\u4e8e\u57fa\u4e8e veth \u865a\u62df\u7f51\u5361\u7684 CNI \u89e3\u51b3\u65b9\u6848\uff0cunderlay \u7f51\u7edc\u6570\u636e\u5305\u907f\u514d\u4e86\u5bbf\u4e3b\u673a\u7684\u4e09\u5c42\u7f51\u7edc\u8f6c\u53d1\uff0c\u6ca1\u6709\u96a7\u9053\u5c01\u88c5\u5f00\u9500\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u80fd\u4e3a\u5e94\u7528\u63d0\u4f9b\u4e86\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec\u4f18\u79c0\u7684\u7f51\u7edc\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u65f6\uff0c\u8282\u7701\u4e86 CPU \u7684\u7f51\u7edc\u8f6c\u53d1\u5f00\u9500\u3002 \u53ef\u76f4\u63a5\u5bf9\u63a5 underlay \u4e8c\u5c42 VLAN \u7f51\u7edc\uff0c\u5e94\u7528\u53ef\u8fdb\u884c\u4e8c\u5c42\u3001\u4e09\u5c42\u7f51\u7edc\u901a\u4fe1\uff0c\u53ef\u8fdb\u884c\u7ec4\u64ad\u3001\u591a\u64ad\u901a\u4fe1\uff0c\u6570\u636e\u5305\u53ef\u53d7\u9632\u706b\u5899\u7ba1\u63a7\u3002 \u6570\u636e\u5305\u643a\u5e26 Pod \u7684\u771f\u6b63 IP \u5730\u5740\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u57fa\u4e8e Pod IP \u8fdb\u884c\u5357\u5317\u5411\u901a\u4fe1\uff0c\u591a\u4e91\u7f51\u7edc\u5929\u7136\u8054\u901a\u3002 underlay CNI \u53ef\u57fa\u4e8e\u5bbf\u4e3b\u673a\u4e0d\u540c\u7684\u7236\u7f51\u5361\u6765\u521b\u5efa\u865a\u62df\u673a\u63a5\u53e3\uff0c\u56e0\u6b64\u53ef\u4e3a\u5b58\u50a8\u3001\u89c2\u6d4b\u6027\u7b49\u7f51\u7edc\u5f00\u9500\u5927\u7684\u5e94\u7528\u63d0\u4f9b\u9694\u79bb\u7684\u5b50\u7f51\u3002","title":"Underlay CNI \u7684\u4f18\u52bf"},{"location":"README-zh_CN/#spiderpool_2","text":"\u7b80\u5316\u5b89\u88c5\u548c\u4f7f\u7528 \u5f53\u524d\u5f00\u6e90\u793e\u533a\u5bf9\u4e8e underlay CNI \u548c RDMA \u7684\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u5b89\u88c5 Multus CNI \u3001RDMA\u3001SR-IOV \u7b49\u8bf8\u591a\u76f8\u5173\u7ec4\u4ef6\uff0cSpiderpool \u7b80\u5316\u4e86\u5b89\u88c5\u6d41\u7a0b\u548c\u8fd0\u884c POD \u6570\u91cf\uff0c\u5bf9\u76f8\u5173\u7684 CRD \u8fdb\u884c\u4e86\u5c01\u88c5\uff0c\u63d0\u4f9b\u4e86\u5404\u79cd\u573a\u666f\u7684\u5b8c\u5907\u6587\u6863\uff0c\u4f7f\u5f97\u4f7f\u7528\u3001\u7ba1\u7406\u66f4\u52a0\u4fbf\u6377\u3002 \u57fa\u4e8e CRD \u7684\u53cc\u6808 IPAM \u80fd\u529b \u63d0\u4f9b\u4e86\u72ec\u4eab\u3001\u5171\u4eab\u7684 IP \u5730\u5740\u6c60\uff0c\u652f\u6301\u8bbe\u7f6e\u5404\u79cd\u4eb2\u548c\u6027\uff0c\u4e3a\u4e2d\u95f4\u4ef6\u7b49\u6709\u72b6\u6001\u5e94\u7528\u548c kubevirt \u7b49\u56fa\u5b9a IP \u5730\u5740\u503c\uff0c\u4e3a\u65e0\u72b6\u6001\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\u8303\u56f4\uff0c\u81ea\u52a8\u5316\u7ba1\u7406\u72ec\u4eab\u7684 IP \u6c60\uff0c\u4f18\u79c0\u7684 IP \u56de\u6536\u907f\u514d IP \u6cc4\u9732\u7b49\u3002\u5e76\u4e14\uff0c\u5177\u5907\u4f18\u79c0\u7684 IPAM \u5206\u914d\u6027\u80fd \u3002 Spiderpool IPAM \u7ec4\u4ef6\u80fd\u591f\u4e3a\u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u7684 main CNI \u4f7f\u7528\uff0c\u4e0d\u4ec5\u5305\u542b\u4e86 Macvlan CNI \u3001 ipvlan CNI \u548c SR-IOV CNI \uff0c\u4e5f\u5305\u62ec\u4e86 calico \u548c weave \u4f5c\u4e3a\u9759\u6001 IP \u573a\u666f\u4f7f\u7528\u3002 underlay \u548c overlay CNI \u7684\u591a\u7f51\u5361\u63a5\u5165 \u5b83\u5305\u62ec\u4e86 \u201cPod \u63d2\u5165\u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u3001\u201cPod \u63d2\u5165\u4e00\u4e2a overlay CNI \u548c \u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u4e24\u79cd\u573a\u666f\uff0cPod \u5177\u5907\u591a\u79cd CNI \u7f51\u5361\uff0cSpiderpool \u80fd\u591f\u4e3a\u591a\u4e2a underlay CNI \u7f51\u5361\u5b9a\u5236\u4e0d\u540c\u7684 IP \u5730\u5740\uff0c\u8c03\u534f\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\u800c\u907f\u514d\u4e22\u5305\u3002\u5b83\u80fd\u591f\u4e3a cilium \u3001 calico \u548c kubevirt \u7b49\u9879\u76ee\u8fdb\u884c\u589e\u5f3a\u3002 \u589e\u5f3a\u7f51\u7edc\u8fde\u901a\u6027 \u4f17\u6240\u5468\u77e5\uff0c\u539f\u751f\u7684 macvlan ipvlan SR-IOV \u5b58\u5728\u8bf8\u591a\u901a\u4fe1\u9650\u5236\u3002\u4f46\u662f\uff0cSpiderpool \u6253\u901a Pod \u548c\u5bbf\u4e3b\u673a\u7684\u8fde\u901a\u6027\uff0c\u786e\u4fdd Pod \u5065\u5eb7\u68c0\u6d4b\u5de5\u4f5c\u6b63\u5e38\uff0c\u5e76\u53ef\u901a\u8fc7 kube-proxy \u6216 eBPF kube-proxy replacement \u4f7f\u5f97 Pod \u8bbf\u95ee service\uff0c\u652f\u6301 Pod \u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u7b49\u3002\u591a\u96c6\u7fa4\u7f51\u7edc\u53ef\u57fa\u4e8e\u76f8\u540c\u7684 underlay \u7f51\u7edc\u6216\u8005 Submariner \u5b9e\u73b0\u8054\u901a\u3002 eBPF \u589e\u5f3a kube-proxy replacement \u6280\u672f\u6781\u5927\u52a0\u901f\u4e86\u8bbf\u95ee service \u573a\u666f\uff0c\u540c\u8282\u70b9\u4e0a\u7684 socket \u77ed\u8def\u6280\u672f\u52a0\u901f\u4e86\u672c\u5730 Pod \u7684\u901a\u4fe1\u6548\u7387\u3002\u76f8\u6bd4 kube proxy \u89e3\u6790\u65b9\u5f0f\uff0c \u7f51\u7edc\u5ef6\u65f6\u6709\u6700\u5927 25% \u7684\u6539\u5584\uff0c\u7f51\u7edc\u541e\u5410\u6709 50% \u7684\u63d0\u9ad8 \u3002 RDMA \u63d0\u4f9b\u4e86\u57fa\u4e8e RoCE\u3001infiniband \u573a\u666f\u4e0b\u7684 RDMA \u89e3\u51b3\u65b9\u6848\uff0cPOD \u80fd\u591f\u72ec\u4eab\u6216\u5171\u4eab\u4f7f\u7528 RDMA \u8bbe\u5907\uff0c\u9002\u5408 AI \u7b49\u7f51\u7edc\u6027\u80fd\u9700\u6c42\u9ad8\u7684\u5e94\u7528\u3002 \u7f51\u7edc\u53cc\u6808\u652f\u6301 Spiderpool \u7ec4\u4ef6\u548c\u5176\u63d0\u4f9b\u7684\u6240\u6709\u529f\u80fd\uff0c\u652f\u6301 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u3002 \u4f18\u79c0\u7684\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u6027\u80fd Spiderpool \u5728\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86 overlay CNI\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u3002 \u6307\u6807","title":"Spiderpool \u6838\u5fc3\u529f\u80fd"},{"location":"README-zh_CN/#_1","text":"Spiderpool \u57fa\u4e8e underlay CNI \u63d0\u4f9b\u4e86\u6bd4 overlay CNI \u8fd8\u4f18\u8d8a\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u3002\u5177\u4f53\u53ef\u5e94\u7528\u5728\u5982\u4e0b\uff1a \u652f\u6301\u8fd0\u884c\u5728\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u3001\u5404\u5927\u516c\u6709\u4e91\u5382\u5546\u7b49\u73af\u5883\uff0c\u5c24\u5176\u4e3a\u6df7\u5408\u4e91\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684 underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u4f20\u7edf\u7684\u4e3b\u673a\u5e94\u7528\u3002\u5b83\u4eec\u5e0c\u671b\u76f4\u63a5\u4f7f\u7528 underlay \u7f51\u7edc\u8fdb\u884c\u901a\u4fe1\uff0c\u4f8b\u5982\u76f4\u63a5\u8bbf\u95ee underlay \u591a\u5b50\u7f51\u3001\u591a\u64ad\u3001\u7ec4\u64ad\u3001\u4e8c\u5c42\u7f51\u7edc\u901a\u4fe1\u7b49\uff0c\u5b83\u4eec\u4e0d\u80fd\u63a5\u53d7 overlay \u7f51\u7edc\u7684 NAT\uff0c\u5e0c\u671b\u8fdb\u884c\u65e0\u7f1d\u79fb\u690d\u7684 Kubernetes\u3002 \u4e2d\u95f4\u4ef6\u3001\u6570\u636e\u5b58\u50a8\u3001\u65e5\u5fd7\u89c2\u6d4b\u3001AI \u8bad\u7ec3\u7b49\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u5e94\u7528\u3002 \u7f51\u7edc\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528\u3002","title":"\u5e94\u7528\u573a\u666f"},{"location":"README-zh_CN/#_2","text":"\u53c2\u8003 \u5feb\u901f\u642d\u5efa \u6765\u4f7f\u7528 Spiderpool \u53c2\u8003 \u4f7f\u7528 \u6765\u4e86\u89e3\u5404\u79cd\u529f\u80fd\u7684\u4f7f\u7528\u65b9\u6cd5 \u53c2\u8003 \u67b6\u6784 \u6765\u4e86\u89e3\u67b6\u6784\u8bbe\u8ba1","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"README-zh_CN/#roadmap","text":"\u529f\u80fd macvlan ipvlan SR-IOV Service By Kubeproxy Beta Beta Beta Service By Kubeproxy Replacement Alpha Alpha Alpha Network Policy In-plan Alpha In-plan Bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta Multi-Cluster Alpha Alpha Alpha Egress Policy Alpha Alpha Alpha \u591a\u7f51\u5361\u548c\u8def\u7531\u8c03\u8c10 Beta Beta Beta \u9002\u7528\u573a\u666f \u88f8\u91d1\u5c5e \u88f8\u91d1\u5c5e\u548c\u865a\u62df\u673a \u88f8\u91d1\u5c5e \u5173\u4e8e\u6240\u6709\u7684\u529f\u80fd\u89c4\u5212\uff0c\u5177\u4f53\u53ef\u53c2\u8003 roadmap \u3002","title":"Roadmap"},{"location":"README-zh_CN/#blogs","text":"\u53ef\u53c2\u8003 Blog \u3002","title":"Blogs"},{"location":"README-zh_CN/#governance","text":"Spiderpool \u9879\u76ee\u7531\u4e00\u7ec4 \u7ef4\u62a4\u8005\u548c\u63d0\u4ea4\u8005 \u7ba1\u7406\uff0c\u6211\u4eec\u7684 Governance Document \u6982\u8ff0\u4e86\u5982\u4f55\u6cbb\u7406\u6539\u9879\u76ee\u3002","title":"Governance"},{"location":"README-zh_CN/#_3","text":"\u4f7f\u7528 Spiderpool \u9879\u76ee\u7684 \u7528\u6237 \u3002","title":"\u4f7f\u7528\u8005"},{"location":"README-zh_CN/#_4","text":"\u53ef\u53c2\u8003 \u5f00\u53d1\u642d\u5efa\u6587\u6863 \u3002","title":"\u53c2\u4e0e\u5f00\u53d1"},{"location":"README-zh_CN/#_5","text":"Spiderpool \u793e\u533a\u81f4\u529b\u4e8e\u8425\u9020\u4e00\u4e2a\u5f00\u653e\u548c\u70ed\u60c5\u7684\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u4e0e\u5176\u4ed6\u7528\u6237\u548c\u5f00\u53d1\u4eba\u5458\u4e92\u52a8\u3002\u60a8\u53ef\u4ee5\u8bbf\u95ee\u6211\u4eec\u7684 \u793e\u533a\u7f51\u7ad9 \u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002 Slack\uff1a\u5982\u679c\u4f60\u60f3\u5728 CNCF Slack \u52a0\u5165 Spiderpool \u7684\u9891\u9053\uff0c\u8bf7\u5148\u5f97\u5230 CNCF Slack \u7684 \u9080\u8bf7 \u7136\u540e\u52a0\u5165 #Spiderpool \u7684\u9891\u9053\u3002 \u90ae\u4ef6\uff1a\u60a8\u53ef\u4ee5\u67e5\u770b MAINTAINERS.md \u83b7\u53d6\u6240\u6709\u7ef4\u62a4\u8005\u7684\u90ae\u7bb1\u5730\u5740\uff0c \u8054\u7cfb\u90ae\u7bb1\u5730\u5740\u4ee5\u62a5\u544a\u4efb\u4f55\u95ee\u9898\u3002 \u793e\u533a\u4f1a\u8bae\uff1a\u6b22\u8fce\u52a0\u5165\u5230\u6211\u4eec\u6bcf\u4e2a\u67081\u53f7\u4e3e\u884c\u7684 \u793e\u533a\u4f1a\u8bae \uff0c\u53ef\u4ee5\u5728\u8fd9\u91cc\u8ba8\u8bba\u4efb\u4f55\u6709\u5173 Spiderpool \u7684\u95ee\u9898\u3002 \u5fae\u4fe1\u7fa4\uff1a\u60a8\u53ef\u4ee5\u626b\u63cf\u5fae\u4fe1\u4e8c\u7ef4\u7801\uff0c\u52a0\u5165\u5230 Spiderpool \u6280\u672f\u4ea4\u6d41\u7fa4\u4e0e\u6211\u4eec\u8fdb\u4e00\u6b65\u4ea4\u6d41\u3002","title":"\u793e\u533a"},{"location":"README-zh_CN/#license","text":"Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.","title":"License"},{"location":"README-zh_CN/#others","text":"Copyright The Spiderpool Authors We are a Cloud Native Computing Foundation sandbox project . The Linux Foundation\u00ae (TLF) has registered trademarks and uses trademarks. For a list of TLF trademarks, see Trademark Usage .","title":"Others"},{"location":"USERS/","text":"Who is using Spiderpool Adding yourself as a user Sharing experiences and learning from other users is essential. If you are using Spiderpool or it is integrated into your product, service, or platform, please consider adding yourself as a user with a quick description of your use case by opening a pull request to this file and adding a section describing your usage of Spiderpool. Users orgnization description a bank in ShangHai, China a cluster with a size of 2000+ pods, production environment a bank in SiChuan, China about 80 clusters , not more thant 100 nodes for each cluster a broker in ShangHai, China a cluster with a size of about 50 nodes a broker in ShangHai, China a cluster with a size of about 50 nodes VIVO, a smart phone vendor in China leverage Spiderpool in AI clusters for LLM a telcom company of Beijing branch, China leverage Spiderpool in development environment a telcom company of Suzhou branch, China leverage Spiderpool in development environment","title":"Who is using Spiderpool"},{"location":"USERS/#who-is-using-spiderpool","text":"","title":"Who is using Spiderpool"},{"location":"USERS/#adding-yourself-as-a-user","text":"Sharing experiences and learning from other users is essential. If you are using Spiderpool or it is integrated into your product, service, or platform, please consider adding yourself as a user with a quick description of your use case by opening a pull request to this file and adding a section describing your usage of Spiderpool.","title":"Adding yourself as a user"},{"location":"USERS/#users","text":"orgnization description a bank in ShangHai, China a cluster with a size of 2000+ pods, production environment a bank in SiChuan, China about 80 clusters , not more thant 100 nodes for each cluster a broker in ShangHai, China a cluster with a size of about 50 nodes a broker in ShangHai, China a cluster with a size of about 50 nodes VIVO, a smart phone vendor in China leverage Spiderpool in AI clusters for LLM a telcom company of Beijing branch, China leverage Spiderpool in development environment a telcom company of Suzhou branch, China leverage Spiderpool in development environment","title":"Users"},{"location":"concepts/arch-zh_CN/","text":"\u67b6\u6784 \u7b80\u4f53\u4e2d\u6587 | English \u67b6\u6784\u4ecb\u7ecd Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool-controller \u4e00\u7ec4 deployment\uff0c\u4e0e API-Server \u4ea4\u4e92, \u7ba1\u7406\u591a\u4e2a CRD \u8d44\u6e90: \u5982 SpiderIPPool \u3001 SpiderSubnet \u3001 SpiderMultusConfig \u7b49, \u5b9e\u65bd\u8fd9\u4e9b CRD \u7684\u6821\u9a8c\u3001\u521b\u5efa\u3001\u72b6\u6001\u3002 \u5e76\u4e14\u54cd\u5e94\u6765\u81ea Spiderpool-agent Pod \u7684\u8bf7\u6c42\uff0c\u5206\u914d\u3001\u91ca\u653e\u3001\u56de\u6536\u3001\u81ea\u52a8IP \u6c60\u7b49\u529f\u80fd\u3002 Spiderpool-agent \u4e00\u7ec4 daemonset\uff0c\u8fd0\u884c\u5728\u6bcf\u4e2a\u8282\u70b9\u3002\u5e2e\u52a9\u5b89\u88c5 Multus\u3001Coordinator\u3001IPAM\u3001CNI \u7b49\u63d2\u4ef6\u5230\u6bcf\u4e2a\u8282\u70b9\u3002\u5e76\u54cd\u5e94 CNI \u521b\u5efa Pod \u65f6\u5206\u914d IP \u7684\u8bf7\u6c42\uff0c\u5e76\u4e0e Spiderpool-controller \u4ea4\u4e92\uff0c\u5b8c\u6210 Pod IP \u7684\u5206\u914d\u4e0e\u91ca\u653e\u3002\u540c\u65f6\u4e0e Coordinator \u4ea4\u4e92, \u7684\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u914d\u7f6e\u540c\u6b65\u3002 CNI plugins\uff0c\u5b83\u4eec\u5305\u62ec\u5982\u4e0b\uff1a Spiderpool IPAM plugin\uff1a\u4f9b main CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d\u3002\u53c2\u8003 IPAM plugin coordinator plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u8c10\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001MAC \u5730\u5740\u56fa\u5b9a\u7b49\u3002\u53c2\u8003 coordinator ifacer plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u53ef\u81ea\u52a8\u521b\u5efa bond\u3001vlan \u865a\u62df\u63a5\u53e3\uff0c\u4f5c\u4e3a macvlan\u3001ipvlan \u7b49\u63d2\u4ef6\u7684\u7236\u63a5\u53e3\u4f7f\u7528\u3002\u53c2\u8003 Ifacer \u63d2\u4ef6 Multus CNI : CNI plugin \u7684\u8c03\u5ea6\u5668 CNI plugins: \u5305\u62ec Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI \u7b49\u3002 SR-IOV \u7ec4\u4ef6\uff1a SR-IOV network operator : \u4fbf\u4e8e\u5b89\u88c5\u548c\u914d\u7f6e\u4f7f\u7528 sriov-cni\uff0c\u66f4\u591a\u53c2\u8003 sriov-cni \u4f7f\u7528 RDMA \u7ec4\u4ef6: RDMA shared device plugin : \u7528\u4e8e\u53d1\u73b0\u4e3b\u673a\u4e0a\u7684\u5171\u4eab RDMA \u8bbe\u5907\uff0c\u5e76\u4e0a\u62a5\u7ed9 Kubelet, \u4ee5\u4f9b RDMA CNI \u4f7f\u7528 RDMA CNI : \u5b9e\u73b0 RDMA \u7f51\u5361\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u9694\u79bb SR-IOV network operator : \u4fbf\u4e8e\u5b89\u88c5\u548c\u914d\u7f6e\u4f7f\u7528 sriov-cni \u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u4e00\u4e2a overlay CNI \u548c\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361 \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a\u4e3a Pod \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361\uff08\u4f8b\u5982 calico , cilium \uff09\u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\uff0c\u53ef\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u8c10\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u8c10\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 Pod \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c\u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 Pod \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002 \u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c\u5728\u4fdd\u969c\u96c6\u7fa4\u5185 Pod \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90 \u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361 \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI \uff08\u4f8b\u5982 macvlan CNI , SR-IOV CNI \uff09\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u8c10\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c\u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 Pod \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c\u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c\u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002 \u5e94\u7528\u573a\u666f \uff1aunderlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a \u5728\u516c\u6709\u4e91\u3001OpenStack\u3001vmvare \u7b49\u73af\u5883\u4e0b\u5b9e\u65bd underlay CNI\uff0c\u901a\u5e38\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u7684\u5382\u5546 CNI \u63d2\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u73af\u5883\u901a\u5e38\u6709\u5982\u4e0b\u9650\u5236\uff1a IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e8c\u5c42\u62a5\u5934\u4e2d\u7684 MAC \u9650\u5236\uff0c\u4e00\u65b9\u9762\uff0c\u5bf9\u6e90 MAC \u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c \u4ee5\u786e\u4fdd\u6e90 MAC \u5730\u5740\u4e0e\u865a\u62df\u673a\u7f51\u5361 MAC \u76f8\u540c\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u76ee\u7684 MAC \u505a\u4e86\u9650\u5236\uff0c\u53ea\u652f\u6301\u8f6c\u53d1 IAAS \u4e2d\u6240\u6709\u865a\u62df\u673a\u7f51\u5361\u7684 MAC\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0cPod \u5206\u914d\u7684\u7f51\u5361\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e09\u5c42\u62a5\u5934\u7684 IP \u9650\u5236\uff0c\u53ea\u6709\u6570\u636e\u5305\u7684\u76ee\u7684\u548c\u6e90 IP \u662f\u5728 IAAS \u4e2d\u5206\u914d\u7ed9\u4e86\u865a\u62df\u673a\u7f51\u5361\u65f6\uff0c\u6570\u636e\u5305\u624d\u80fd\u5f97\u5230\u8f6c\u53d1\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0c\u7ed9 Pod \u5206\u914d\u7684 IP \u5730\u5740\u4e0d\u7b26\u5408 IAAS \u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u4e0e\u865a\u62df\u673a\u7684\u76f8\u540c IP \u5206\u914d\u8bbe\u7f6e\u5bf9\u9f50\uff0c\u518d\u914d\u5408 ipvlan CNI\uff0c \u4ece\u800c\u80fd\u591f\u4e3a\u5404\u79cd\u516c\u6709\u4e91\u73af\u5883\u63d0\u4f9b underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u5e94\u7528\u573a\u666f \uff1a\u4f7f\u7528 RDMA \u8fdb\u884c\u7f51\u7edc\u4f20\u8f93\u7684\u5e94\u7528 RDMA \u529f\u80fd\u4f7f\u5f97\u7f51\u5361\u80fd\u591f\u76f4\u63a5\u8bfb\u5199\u5185\u5b58\uff0c\u964d\u4f4e\u4e86 CPU \u7684\u8d1f\u62c5\u548c\u5185\u6838\u534f\u8bae\u6808\u7684\u5904\u7406\uff0c\u662f\u4e00\u79cd\u7f51\u7edc\u534f\u8bae\u6808 offload \u5230\u7f51\u5361\u7684\u6280\u672f\uff0c\u5b83\u80fd\u6709\u6548\u964d\u4f4e\u7f51\u7edc\u4f20\u8f93\u5ef6\u65f6\u3001\u63d0\u9ad8\u541e\u5410\u91cf\u3002 \u5f53\u524d\uff0cRDMA \u6280\u672f\u5728 AI \u8ba1\u7b97\u3001\u5b58\u50a8\u7b49\u5e94\u7528\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u3002Macvlan\u3001IPvlan \u548c SR-IOV CNI\uff0c\u5b83\u4eec\u80fd\u591f\u5728 kubernetes \u5e73\u53f0\u4e0b\u628a RDMA \u7f51\u5361\u900f\u4f20\u7ed9 Pod \u4f7f\u7528\uff0cSpiderpool \u589e\u5f3a\u4e86\u8fd9\u4e9b CNI \u80fd\u529b\uff0c\u5305\u62ec IPAM\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001ClusterIP \u8bbf\u95ee\u7b49\uff0c\u5e76\u4e14\u7b80\u5316\u4e86\u793e\u533a\u4e2d\u7684\u4f9d\u8d56\u7ec4\u4ef6\u5b89\u88c5\u6d41\u7a0b\u548c\u4f7f\u7528\u6b65\u9aa4\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u6613\u7528\u6027\u3002","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#_2","text":"Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool-controller \u4e00\u7ec4 deployment\uff0c\u4e0e API-Server \u4ea4\u4e92, \u7ba1\u7406\u591a\u4e2a CRD \u8d44\u6e90: \u5982 SpiderIPPool \u3001 SpiderSubnet \u3001 SpiderMultusConfig \u7b49, \u5b9e\u65bd\u8fd9\u4e9b CRD \u7684\u6821\u9a8c\u3001\u521b\u5efa\u3001\u72b6\u6001\u3002 \u5e76\u4e14\u54cd\u5e94\u6765\u81ea Spiderpool-agent Pod \u7684\u8bf7\u6c42\uff0c\u5206\u914d\u3001\u91ca\u653e\u3001\u56de\u6536\u3001\u81ea\u52a8IP \u6c60\u7b49\u529f\u80fd\u3002 Spiderpool-agent \u4e00\u7ec4 daemonset\uff0c\u8fd0\u884c\u5728\u6bcf\u4e2a\u8282\u70b9\u3002\u5e2e\u52a9\u5b89\u88c5 Multus\u3001Coordinator\u3001IPAM\u3001CNI \u7b49\u63d2\u4ef6\u5230\u6bcf\u4e2a\u8282\u70b9\u3002\u5e76\u54cd\u5e94 CNI \u521b\u5efa Pod \u65f6\u5206\u914d IP \u7684\u8bf7\u6c42\uff0c\u5e76\u4e0e Spiderpool-controller \u4ea4\u4e92\uff0c\u5b8c\u6210 Pod IP \u7684\u5206\u914d\u4e0e\u91ca\u653e\u3002\u540c\u65f6\u4e0e Coordinator \u4ea4\u4e92, \u7684\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u914d\u7f6e\u540c\u6b65\u3002 CNI plugins\uff0c\u5b83\u4eec\u5305\u62ec\u5982\u4e0b\uff1a Spiderpool IPAM plugin\uff1a\u4f9b main CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d\u3002\u53c2\u8003 IPAM plugin coordinator plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u8c10\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001MAC \u5730\u5740\u56fa\u5b9a\u7b49\u3002\u53c2\u8003 coordinator ifacer plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u53ef\u81ea\u52a8\u521b\u5efa bond\u3001vlan \u865a\u62df\u63a5\u53e3\uff0c\u4f5c\u4e3a macvlan\u3001ipvlan \u7b49\u63d2\u4ef6\u7684\u7236\u63a5\u53e3\u4f7f\u7528\u3002\u53c2\u8003 Ifacer \u63d2\u4ef6 Multus CNI : CNI plugin \u7684\u8c03\u5ea6\u5668 CNI plugins: \u5305\u62ec Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI \u7b49\u3002 SR-IOV \u7ec4\u4ef6\uff1a SR-IOV network operator : \u4fbf\u4e8e\u5b89\u88c5\u548c\u914d\u7f6e\u4f7f\u7528 sriov-cni\uff0c\u66f4\u591a\u53c2\u8003 sriov-cni \u4f7f\u7528 RDMA \u7ec4\u4ef6: RDMA shared device plugin : \u7528\u4e8e\u53d1\u73b0\u4e3b\u673a\u4e0a\u7684\u5171\u4eab RDMA \u8bbe\u5907\uff0c\u5e76\u4e0a\u62a5\u7ed9 Kubelet, \u4ee5\u4f9b RDMA CNI \u4f7f\u7528 RDMA CNI : \u5b9e\u73b0 RDMA \u7f51\u5361\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u9694\u79bb SR-IOV network operator : \u4fbf\u4e8e\u5b89\u88c5\u548c\u914d\u7f6e\u4f7f\u7528 sriov-cni","title":"\u67b6\u6784\u4ecb\u7ecd"},{"location":"concepts/arch-zh_CN/#pod-overlay-cni-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a\u4e3a Pod \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361\uff08\u4f8b\u5982 calico , cilium \uff09\u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\uff0c\u53ef\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u8c10\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u8c10\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 Pod \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c\u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 Pod \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002 \u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c\u5728\u4fdd\u969c\u96c6\u7fa4\u5185 Pod \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90","title":"\u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u4e00\u4e2a overlay CNI \u548c\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361"},{"location":"concepts/arch-zh_CN/#pod-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI \uff08\u4f8b\u5982 macvlan CNI , SR-IOV CNI \uff09\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u8c10\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c\u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 Pod \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c\u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c\u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002","title":"\u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361"},{"location":"concepts/arch-zh_CN/#underlay-cni","text":"\u5728\u516c\u6709\u4e91\u3001OpenStack\u3001vmvare \u7b49\u73af\u5883\u4e0b\u5b9e\u65bd underlay CNI\uff0c\u901a\u5e38\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u7684\u5382\u5546 CNI \u63d2\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u73af\u5883\u901a\u5e38\u6709\u5982\u4e0b\u9650\u5236\uff1a IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e8c\u5c42\u62a5\u5934\u4e2d\u7684 MAC \u9650\u5236\uff0c\u4e00\u65b9\u9762\uff0c\u5bf9\u6e90 MAC \u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c \u4ee5\u786e\u4fdd\u6e90 MAC \u5730\u5740\u4e0e\u865a\u62df\u673a\u7f51\u5361 MAC \u76f8\u540c\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u76ee\u7684 MAC \u505a\u4e86\u9650\u5236\uff0c\u53ea\u652f\u6301\u8f6c\u53d1 IAAS \u4e2d\u6240\u6709\u865a\u62df\u673a\u7f51\u5361\u7684 MAC\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0cPod \u5206\u914d\u7684\u7f51\u5361\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e09\u5c42\u62a5\u5934\u7684 IP \u9650\u5236\uff0c\u53ea\u6709\u6570\u636e\u5305\u7684\u76ee\u7684\u548c\u6e90 IP \u662f\u5728 IAAS \u4e2d\u5206\u914d\u7ed9\u4e86\u865a\u62df\u673a\u7f51\u5361\u65f6\uff0c\u6570\u636e\u5305\u624d\u80fd\u5f97\u5230\u8f6c\u53d1\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0c\u7ed9 Pod \u5206\u914d\u7684 IP \u5730\u5740\u4e0d\u7b26\u5408 IAAS \u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u4e0e\u865a\u62df\u673a\u7684\u76f8\u540c IP \u5206\u914d\u8bbe\u7f6e\u5bf9\u9f50\uff0c\u518d\u914d\u5408 ipvlan CNI\uff0c \u4ece\u800c\u80fd\u591f\u4e3a\u5404\u79cd\u516c\u6709\u4e91\u73af\u5883\u63d0\u4f9b underlay CNI \u89e3\u51b3\u65b9\u6848\u3002","title":"\u5e94\u7528\u573a\u666f \uff1aunderlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a"},{"location":"concepts/arch-zh_CN/#rdma","text":"RDMA \u529f\u80fd\u4f7f\u5f97\u7f51\u5361\u80fd\u591f\u76f4\u63a5\u8bfb\u5199\u5185\u5b58\uff0c\u964d\u4f4e\u4e86 CPU \u7684\u8d1f\u62c5\u548c\u5185\u6838\u534f\u8bae\u6808\u7684\u5904\u7406\uff0c\u662f\u4e00\u79cd\u7f51\u7edc\u534f\u8bae\u6808 offload \u5230\u7f51\u5361\u7684\u6280\u672f\uff0c\u5b83\u80fd\u6709\u6548\u964d\u4f4e\u7f51\u7edc\u4f20\u8f93\u5ef6\u65f6\u3001\u63d0\u9ad8\u541e\u5410\u91cf\u3002 \u5f53\u524d\uff0cRDMA \u6280\u672f\u5728 AI \u8ba1\u7b97\u3001\u5b58\u50a8\u7b49\u5e94\u7528\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u3002Macvlan\u3001IPvlan \u548c SR-IOV CNI\uff0c\u5b83\u4eec\u80fd\u591f\u5728 kubernetes \u5e73\u53f0\u4e0b\u628a RDMA \u7f51\u5361\u900f\u4f20\u7ed9 Pod \u4f7f\u7528\uff0cSpiderpool \u589e\u5f3a\u4e86\u8fd9\u4e9b CNI \u80fd\u529b\uff0c\u5305\u62ec IPAM\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001ClusterIP \u8bbf\u95ee\u7b49\uff0c\u5e76\u4e14\u7b80\u5316\u4e86\u793e\u533a\u4e2d\u7684\u4f9d\u8d56\u7ec4\u4ef6\u5b89\u88c5\u6d41\u7a0b\u548c\u4f7f\u7528\u6b65\u9aa4\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u6613\u7528\u6027\u3002","title":"\u5e94\u7528\u573a\u666f \uff1a\u4f7f\u7528 RDMA \u8fdb\u884c\u7f51\u7edc\u4f20\u8f93\u7684\u5e94\u7528"},{"location":"concepts/arch/","text":"Spiderpool Architecture English | \u7b80\u4f53\u4e2d\u6587 Architecture Spiderpool consists of the following components: Spiderpool-controller A set of deployments that interact with the API Server, managing multiple CRD resources such as SpiderIPPool, SpiderSubnet, SpiderMultusConfig, etc. It implements validation, creation, and status updates for these CRDs. Additionally, it responds to requests from Spiderpool-agent Pods, performing functions like allocation, release, reclamation, and managing automatic IP pools. Spiderpool-agent A set of daemonsets running on each node, assisting in the installation of plugins such as Multus, Coordinator, IPAM, and CNI on each node. It responds to CNI requests for IP allocation during Pod creation and interacts with Spiderpool-controller to handle Pod IP allocation and release. It also interacts with Coordinator, assisting the Spiderpool plugin in implementing IP allocation and helping the coordinator plugin with configuration synchronization. CNI plugins include: Spiderpool IPAM plugin: a main CNI used to handle IP allocation. refer to IPAM plugin coordinator plugin: as a chain plugin, it performs various functions such as routing coordination for multiple network interfaces, checking for IP conflicts, ensuring host connectivity, and fixing MAC addresses. refer to coordinator ifacer plugin: as a chain plugin, it automates the creation of bond and VLAN virtual interfaces that serve as parent interfaces for plugins like macvlan and ipvlan. refer to Ifacer Plugin Multus CNI : a scheduler for other CNI plugins. CNI plugins: include Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI , etc. SR-IOV related components: SR-IOV network operator : Facilitates the installation and configuration of sriov-cni. For more details, refer to sriov-cni usage . RDMA components: RDMA shared device plugin : Used to discover shared RDMA devices on the host and report them to Kubelet for use by the RDMA CNI. RDMA CNI : It implements network isolation for RDMA device. SR-IOV network operator : Facilitates the installation and configuration of sriov-cni. ipoib CNI : It implements ipoib cni for infiniband scenario. Use case: Pod with one overlay interface and multiple underlay interfaces In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes. Use case: Pod with multiple underlay CNI interfaces In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI and SR-IOV CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding). Use case: underlay CNI on public cloud and VM It is hard to implement underlay CNI in public cloud, OpenStack, VMware. It requires the vendor underlay CNI on specific environments, as these environments typically have the following limitations: The IAAS network infrastructure implements MAC restrictions for packets. On the one hand, security checks are conducted on the source MAC to ensure that the source MAC address is the same as the MAC address of VM network interface. On the other hand, restrictions have been placed on the destination MAC, which only supports packet forwarding by the MAC address of VM network interfaces. The MAC address of the Pod in the common CNI plugin is newly generated, which leads to Pod communication failure. The IAAS network infrastructure implements IP restrictions on packets. Only when the destination and source IP of the packet are assigned to VM, packet could be forwarded rightly. The common CNI plugin assigns IP addresses to Pods that do not comply with IAAS settings, which leads to Pod communication failure. Spiderpool provides IP pool based on node topology, aligning with IP allocation settings of VMs. In conjunction with ipvlan CNI, it provides underlay CNI solutions for various public cloud environments. Use case: utilize RDMA for network transmission RDMA (Remote Direct Memory Access) allows network cards to directly interact with memory, reducing CPU overhead and alleviating the burden on the kernel protocol stack. This technology offloads the network protocol stack to the network card, resulting in effective reduction of network transmission latency and increased throughput. Currently, RDMA finds extensive applications in fields such as AI computing and storage. Macvlan, IPvlan, and SR-IOV CNIs enable transparent RDMA network card passthrough to Pods within the Kubernetes platform. Spiderpool enhances these CNIs by providing additional capabilities including IPAM, host connectivity, clusterIP access, as well as simplifying the installation process and usage steps of dependent components in the community.","title":"Architecture"},{"location":"concepts/arch/#spiderpool-architecture","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Spiderpool Architecture"},{"location":"concepts/arch/#architecture","text":"Spiderpool consists of the following components: Spiderpool-controller A set of deployments that interact with the API Server, managing multiple CRD resources such as SpiderIPPool, SpiderSubnet, SpiderMultusConfig, etc. It implements validation, creation, and status updates for these CRDs. Additionally, it responds to requests from Spiderpool-agent Pods, performing functions like allocation, release, reclamation, and managing automatic IP pools. Spiderpool-agent A set of daemonsets running on each node, assisting in the installation of plugins such as Multus, Coordinator, IPAM, and CNI on each node. It responds to CNI requests for IP allocation during Pod creation and interacts with Spiderpool-controller to handle Pod IP allocation and release. It also interacts with Coordinator, assisting the Spiderpool plugin in implementing IP allocation and helping the coordinator plugin with configuration synchronization. CNI plugins include: Spiderpool IPAM plugin: a main CNI used to handle IP allocation. refer to IPAM plugin coordinator plugin: as a chain plugin, it performs various functions such as routing coordination for multiple network interfaces, checking for IP conflicts, ensuring host connectivity, and fixing MAC addresses. refer to coordinator ifacer plugin: as a chain plugin, it automates the creation of bond and VLAN virtual interfaces that serve as parent interfaces for plugins like macvlan and ipvlan. refer to Ifacer Plugin Multus CNI : a scheduler for other CNI plugins. CNI plugins: include Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI , etc. SR-IOV related components: SR-IOV network operator : Facilitates the installation and configuration of sriov-cni. For more details, refer to sriov-cni usage . RDMA components: RDMA shared device plugin : Used to discover shared RDMA devices on the host and report them to Kubelet for use by the RDMA CNI. RDMA CNI : It implements network isolation for RDMA device. SR-IOV network operator : Facilitates the installation and configuration of sriov-cni. ipoib CNI : It implements ipoib cni for infiniband scenario.","title":"Architecture"},{"location":"concepts/arch/#use-case-pod-with-one-overlay-interface-and-multiple-underlay-interfaces","text":"In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes.","title":"Use case: Pod with one overlay interface and multiple underlay interfaces"},{"location":"concepts/arch/#use-case-pod-with-multiple-underlay-cni-interfaces","text":"In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI and SR-IOV CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding).","title":"Use case: Pod with multiple underlay CNI interfaces"},{"location":"concepts/arch/#use-case-underlay-cni-on-public-cloud-and-vm","text":"It is hard to implement underlay CNI in public cloud, OpenStack, VMware. It requires the vendor underlay CNI on specific environments, as these environments typically have the following limitations: The IAAS network infrastructure implements MAC restrictions for packets. On the one hand, security checks are conducted on the source MAC to ensure that the source MAC address is the same as the MAC address of VM network interface. On the other hand, restrictions have been placed on the destination MAC, which only supports packet forwarding by the MAC address of VM network interfaces. The MAC address of the Pod in the common CNI plugin is newly generated, which leads to Pod communication failure. The IAAS network infrastructure implements IP restrictions on packets. Only when the destination and source IP of the packet are assigned to VM, packet could be forwarded rightly. The common CNI plugin assigns IP addresses to Pods that do not comply with IAAS settings, which leads to Pod communication failure. Spiderpool provides IP pool based on node topology, aligning with IP allocation settings of VMs. In conjunction with ipvlan CNI, it provides underlay CNI solutions for various public cloud environments.","title":"Use case: underlay CNI on public cloud and VM"},{"location":"concepts/arch/#use-case-utilize-rdma-for-network-transmission","text":"RDMA (Remote Direct Memory Access) allows network cards to directly interact with memory, reducing CPU overhead and alleviating the burden on the kernel protocol stack. This technology offloads the network protocol stack to the network card, resulting in effective reduction of network transmission latency and increased throughput. Currently, RDMA finds extensive applications in fields such as AI computing and storage. Macvlan, IPvlan, and SR-IOV CNIs enable transparent RDMA network card passthrough to Pods within the Kubernetes platform. Spiderpool enhances these CNIs by providing additional capabilities including IPAM, host connectivity, clusterIP access, as well as simplifying the installation process and usage steps of dependent components in the community.","title":"Use case: utilize RDMA for network transmission"},{"location":"concepts/blog-zh_CN/","text":"Blogs \u7b80\u4f53\u4e2d\u6587 | English \u516c\u5f00\u5206\u4eab KCD \u6df1\u5733 2023, \u591a\u7f51\u5361\u5bb9\u5668\u7f51\u7edc\u52a0\u901f\u79bb\u7ebf\u8bad\u7ec3\u7684\u5b9e\u8df5\uff0c\u6765\u81ea Vivo CNCF \u5927\u4f7f\u63a2\u8ba8 Spiderpool \u5982\u4f55\u63d0\u4f9b IPAM \u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3 Kubernetes \u4e2d\u7684\u7f51\u7edc\u6311\u6218\uff0c\u540c\u65f6\u5b9e\u73b0\u7075\u6d3b\u90e8\u7f72 Spiderpool CNI \u4ecb\u7ecd CNCF-hosted Co-located Events Europe 2024, \u95ea\u7535\u6f14\u8bb2: \u4e91\u539f\u751f AI \u7f51\u7edc : CNI \u7684 RDMA \u589e\u5f3a\uff0c \u6765\u81ea\u9053\u5ba2 KubeCon + CloudNative North America 2023, \u6613\u7528\u548c\u5f3a\u5927 Underlay CNI\uff0c \u6765\u81ea\u9053\u5ba2 KCD \u4e0a\u6d77 2024, RDMA \u5bb9\u5668\u7f51\u7edc\u4e0b\u7684\u5927\u89c4\u6a21AI\u8bad\u7ec3\u63a2\u7d22\uff0c\u6765\u81ea\u9053\u5ba2\u548c\u4e2d\u56fd\u79fb\u52a8 \u6280\u672f\u6587\u7ae0 Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 \u4e91\u539f\u751f Spiderpool\uff1a\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d SpiderPool\uff1aCalico \u56fa\u5b9a\u5e94\u7528 IP \u7684\u4e00\u79cd\u65b0\u9009\u62e9 \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6 KubeEdge EdgeMesh v1.15 \u8fb9\u7f18 CNI \u91c7\u7528 Spiderpool \u5b9e\u65bd IPAM","title":"Blogs"},{"location":"concepts/blog-zh_CN/#blogs","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Blogs"},{"location":"concepts/blog-zh_CN/#_1","text":"KCD \u6df1\u5733 2023, \u591a\u7f51\u5361\u5bb9\u5668\u7f51\u7edc\u52a0\u901f\u79bb\u7ebf\u8bad\u7ec3\u7684\u5b9e\u8df5\uff0c\u6765\u81ea Vivo CNCF \u5927\u4f7f\u63a2\u8ba8 Spiderpool \u5982\u4f55\u63d0\u4f9b IPAM \u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3 Kubernetes \u4e2d\u7684\u7f51\u7edc\u6311\u6218\uff0c\u540c\u65f6\u5b9e\u73b0\u7075\u6d3b\u90e8\u7f72 Spiderpool CNI \u4ecb\u7ecd CNCF-hosted Co-located Events Europe 2024, \u95ea\u7535\u6f14\u8bb2: \u4e91\u539f\u751f AI \u7f51\u7edc : CNI \u7684 RDMA \u589e\u5f3a\uff0c \u6765\u81ea\u9053\u5ba2 KubeCon + CloudNative North America 2023, \u6613\u7528\u548c\u5f3a\u5927 Underlay CNI\uff0c \u6765\u81ea\u9053\u5ba2 KCD \u4e0a\u6d77 2024, RDMA \u5bb9\u5668\u7f51\u7edc\u4e0b\u7684\u5927\u89c4\u6a21AI\u8bad\u7ec3\u63a2\u7d22\uff0c\u6765\u81ea\u9053\u5ba2\u548c\u4e2d\u56fd\u79fb\u52a8","title":"\u516c\u5f00\u5206\u4eab"},{"location":"concepts/blog-zh_CN/#_2","text":"Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 \u4e91\u539f\u751f Spiderpool\uff1a\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d SpiderPool\uff1aCalico \u56fa\u5b9a\u5e94\u7528 IP \u7684\u4e00\u79cd\u65b0\u9009\u62e9 \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6 KubeEdge EdgeMesh v1.15 \u8fb9\u7f18 CNI \u91c7\u7528 Spiderpool \u5b9e\u65bd IPAM","title":"\u6280\u672f\u6587\u7ae0"},{"location":"concepts/blog/","text":"Blogs English | \u7b80\u4f53\u4e2d\u6587 Public Speech KCD Shenzhen 2023, Practice of Multi-NIC Container Network Acceleration for Offline Training, from Vivo CNCF Ambassador explores how Spiderpool provides a IPAM solution, addressing the challenges of networking in Kubernetes while allowing for flexible deployment Spiderpool CNI intro, from telco-cloud-native CNCF-hosted Co-located Events Europe 2024, Lightning Talk: Cloud Native Networking for AI : Strengthen CNI for RDMA, from Daocloud KubeCon + CloudNative North America 2023, Make Underlay CNI to Be Powerful and Simple, from Daocloud KCD ShangHai 2024, Exploration of large-scale AI training under RDMA container network, from Daocloud and China Mobile Technical Articles Spiderpool v0.6.0: Unified Cloud-Native Underlay Network Solution for Public Cloud Scenarios Spiderpool: How to Solve the Zombie IP Recycling Issue Cloud-Native Spiderpool: IP Allocation Across Network Zones Spiderpool: A New Solution to Fixed Application IPs for Calico New Cloud-Native Network Idea: An Underlay Network Solution Supporting Fixed Multi-NIC IPs SpiderPool - Cloud-Native Container Network IPAM Plugin KubeEdge EdgeMesh v1.15 Edge CNI with Spiderpool IPAM","title":"Blogs"},{"location":"concepts/blog/#blogs","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Blogs"},{"location":"concepts/blog/#public-speech","text":"KCD Shenzhen 2023, Practice of Multi-NIC Container Network Acceleration for Offline Training, from Vivo CNCF Ambassador explores how Spiderpool provides a IPAM solution, addressing the challenges of networking in Kubernetes while allowing for flexible deployment Spiderpool CNI intro, from telco-cloud-native CNCF-hosted Co-located Events Europe 2024, Lightning Talk: Cloud Native Networking for AI : Strengthen CNI for RDMA, from Daocloud KubeCon + CloudNative North America 2023, Make Underlay CNI to Be Powerful and Simple, from Daocloud KCD ShangHai 2024, Exploration of large-scale AI training under RDMA container network, from Daocloud and China Mobile","title":"Public Speech"},{"location":"concepts/blog/#technical-articles","text":"Spiderpool v0.6.0: Unified Cloud-Native Underlay Network Solution for Public Cloud Scenarios Spiderpool: How to Solve the Zombie IP Recycling Issue Cloud-Native Spiderpool: IP Allocation Across Network Zones Spiderpool: A New Solution to Fixed Application IPs for Calico New Cloud-Native Network Idea: An Underlay Network Solution Supporting Fixed Multi-NIC IPs SpiderPool - Cloud-Native Container Network IPAM Plugin KubeEdge EdgeMesh v1.15 Edge CNI with Spiderpool IPAM","title":"Technical Articles"},{"location":"concepts/coordinator-zh_CN/","text":"Coordinator \u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u5185\u7f6e\u4e00\u4e2a\u53eb coordinator \u7684 CNI meta-plugin, \u5b83\u5728 Main CNI \u88ab\u8c03\u7528\u4e4b\u540e\u518d\u5de5\u4f5c\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u529f\u80fd: \u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u5728 Pod \u591a\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u8def\u7531\uff0c\u786e\u4fdd\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e00\u81f4 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff08\u9057\u5f03\uff0c\u73b0\u5728\u7531 IPAM \u5b8c\u6210\uff09 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \uff08\u9057\u5f03\uff0c\u73b0\u5728\u7531 IPAM \u5b8c\u6210\uff09 \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u6ce8\u610f: \u5982\u679c\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u662f\u4f7f\u7528 NetworkManager \u7684 OS\uff0c\u6bd4\u5982 Fedora\u3001Centos\u7b49\uff0c\u5f3a\u70c8\u5efa\u8bae\u914d\u7f6e NetworkManager \u7684\u914d\u7f6e\u6587\u4ef6(/etc/NetworkManager/conf.d/spidernet.conf)\uff0c\u907f\u514d NetworkManager \u5e72\u6270 coordinator \u521b\u5efa\u7684 Veth \u865a\u62df\u63a5\u53e3\uff0c\u5f71\u54cd\u901a\u4fe1: ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecd coordinator \u5982\u4f55\u89e3\u51b3\u6216\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u3002 CNI \u914d\u7f6e\u5b57\u6bb5\u8bf4\u660e Field Description Schema Validation Default type CNI \u7684\u7c7b\u578b \u5b57\u7b26\u4e32 required coordinator mode coordinator \u8fd0\u884c\u7684\u6a21\u5f0f. \"auto\": coordinator \u81ea\u52a8\u5224\u65ad\u8fd0\u884c\u5728 Underlay \u6216\u8005 Overlay; \"underlay\": \u4e3a Pod \u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002\u7531 Pod \u7684 Underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"overlay\": \u4e0d\u989d\u5916\u521b\u5efa veth \u8bbe\u5907\uff0c\u8fd0\u884c\u5728\u591a\u7f51\u5361\u6a21\u5f0f\u3002\u7531 overlay \u7c7b\u578b\u7684 CNI(calico\uff0ccilium) \u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\uff0c\u7531 underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"disable\": \u7981\u7528 coordinator \u5b57\u7b26\u4e32 optional auto tunePodRoutes Pod \u591a\u7f51\u5361\u6a21\u5f0f\u4e0b\uff0c\u662f\u5426\u8c03\u534f Pod \u7684\u8def\u7531\uff0c\u89e3\u51b3\u8bbf\u95ee\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898 \u5e03\u5c14\u578b optional true podDefaultRouteNic Pod \u591a\u7f51\u5361\u65f6\uff0c\u914d\u7f6e Pod \u7684\u9ed8\u8ba4\u8def\u7531\u7f51\u5361\u3002\u9ed8\u8ba4\u4e3a \"\", \u5176 value \u5b9e\u9645\u4e3a Pod \u7b2c\u4e00\u5f20\u62e5\u6709\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361 \u5b57\u7b26\u4e32 optional \"\" podDefaultCniNic K8s \u4e2d Pod \u9ed8\u8ba4\u7684\u7b2c\u4e00\u5f20\u7f51\u5361 \u5e03\u5c14\u578b optional eth0 detectGateway \u9057\u5f03\uff0c\u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u5e03\u5c14\u578b optional false detectIPConflict \u9057\u5f03\uff0c\u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5 Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u5e03\u5c14\u578b optional false podMACPrefix \u662f\u5426\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00, \u524d\u7f00\u957f\u5ea6\u4e3a\u4e24\u4e2a\u5b57\u8282, \u7531\":\"\u62fc\u63a5\u3002\u6ce8\u610f\uff1a\u9996\u5b57\u8282\u7684\u6700\u4f4e\u4f4d\u5fc5\u987b\u662f \"0\"\u3002\u6bd4\u5982 \"0a:1b\"\u3002 \u5b57\u7b26\u4e32 optional \"\" overlayPodCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Pod \u7684\u5b50\u7f51\uff0c\u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 serviceCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Service \u5b50\u7f51\uff0c \u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 hijackCIDR \u989d\u5916\u7684\u9700\u8981\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\u3002\u6bd4\u5982nodelocaldns \u7684\u5730\u5740: 169.254.20.10/32 []stirng optional \u7a7a hostRuleTable \u7b56\u7565\u8def\u7531\u8868\u53f7\uff0c\u540c\u4e3b\u673a\u4e0e Pod \u901a\u4fe1\u7684\u8def\u7531\u5c06\u4f1a\u5b58\u653e\u4e8e\u8fd9\u4e2a\u8868\u53f7 \u6574\u6570\u578b optional 500 podRPFilter \u8bbe\u7f6e Pod \u7684 sysctl \u53c2\u6570 rp_filter \u6574\u6570\u578b optional 0 txQueueLen \u8bbe\u7f6e Pod \u7684\u7f51\u5361\u4f20\u8f93\u961f\u5217 \u6574\u6570\u578b optional 0 detectOptions \u68c0\u6d4b\u5730\u5740\u51b2\u7a81\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u7684\u9ad8\u7ea7\u914d\u7f6e\u9879: \u5305\u62ec\u53d1\u9001\u63a2\u6d4b\u62a5\u6587\u6b21\u6570(retries: \u9ed8\u8ba4\u4e3a 3 \u6b21), \u548c\u54cd\u5e94\u7684\u8d85\u65f6\u65f6\u95f4(timeout: \u9ed8\u8ba4\u4e3a 100ms)\uff0c\u8fd8\u6709\u53d1\u9001\u62a5\u6587\u7684\u95f4\u9694(interval:\u9ed8\u8ba4\u4e3a 10ms, \u5c06\u4f1a\u5728\u672a\u6765\u7248\u672c\u4e2d\u79fb\u9664) \u5bf9\u8c61\u7c7b\u578b optional \u7a7a logOptions \u65e5\u5fd7\u914d\u7f6e\uff0c\u5305\u62ec logLevel(\u9ed8\u8ba4\u4e3a debug) \u548c logFile(\u9ed8\u8ba4\u4e3a /var/log/spidernet/coordinator.log) \u5bf9\u8c61\u7c7b\u578b optional - \u5982\u679c\u60a8\u901a\u8fc7 SpinderMultusConfig CR \u5e2e\u52a9\u521b\u5efa NetworkAttachmentDefinition CR\uff0c\u60a8\u53ef\u4ee5\u5728 SpinderMultusConfig \u4e2d\u914d\u7f6e coordinator (\u6240\u6709\u5b57\u6bb5)\u3002\u53c2\u8003: SpinderMultusConfig \u3002 Spidercoordinators CR \u4f5c\u4e3a coordinator \u63d2\u4ef6\u7684\u5168\u5c40\u7f3a\u7701\u914d\u7f6e(\u6240\u6709\u5b57\u6bb5)\uff0c\u5176\u4f18\u5148\u7ea7\u4f4e\u4e8e NetworkAttachmentDefinition CR \u4e2d\u7684\u914d\u7f6e\u3002 \u5982\u679c\u5728 NetworkAttachmentDefinition CR \u672a\u914d\u7f6e, \u5c06\u4f7f\u7528 Spidercoordinator CR \u4f5c\u4e3a\u7f3a\u7701\u503c\u3002\u66f4\u591a\u8be6\u60c5\u53c2\u8003: Spidercoordinator \u3002 \u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u6211\u4eec\u5728\u4f7f\u7528\u4e00\u4e9b\u5982 Macvlan\u3001IPvlan\u3001SR-IOV \u7b49 Underlay CNI\u65f6\uff0c\u4f1a\u9047\u5230\u5176 Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8fd9\u5e38\u5e38\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee CLusterIP \u9700\u8981\u7ecf\u8fc7\u5728\u4ea4\u6362\u673a\u7684\u7f51\u5173\uff0c\u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 ClusterIP \u7684\u8def\u7531\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bbf\u95ee\u3002 \u5173\u4e8e Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8bf7\u53c2\u8003 Underlay-CNI\u8bbf\u95ee Service \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81( alpha \u9636\u6bb5) \u5bf9\u4e8e Underlay \u7f51\u7edc\uff0cIP \u51b2\u7a81\u662f\u65e0\u6cd5\u63a5\u53d7\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u95ee\u9898\u3002\u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff0c\u652f\u6301\u540c\u65f6\u68c0\u6d4b IPv4 \u548c IPv6 \u5730\u5740\u3002\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\uff0c \u5982\u679c\u53d1\u73b0\u56de\u590d\u62a5\u6587\u7684 Mac \u5730\u5740\u4e0d\u662f\u6765\u81ea Pod \u672c\u8eab\u7684\u7f51\u5361\uff0c\u90a3\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e2a IP \u662f\u51b2\u7a81\u7684\uff0c\u5e76\u62d2\u7edd IP \u51b2\u7a81\u7684 Pod \u88ab\u521b\u5efa\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u9ed8\u8ba4\u8fd8\u4f1a\u5bf9\u53d1\u751f IP \u51b2\u7a81\u7684**\u65e0\u72b6\u6001**\u7684 Pod \u91ca\u653e\u6240\u6709\u7684\u5df2\u5206\u914d\u7684 IP \u4f7f\u5176\u91cd\u65b0\u5206\u914d\uff0c\u4f7f\u5f97 Pod \u5728\u4e0b\u4e00\u6b21\u91cd\u65b0\u8c03\u7528 CNI \u65f6\u80fd\u591f\u5c1d\u8bd5\u5206\u914d\u5230\u5176\u5b83\u975e\u51b2\u7a81\u7684 IP\u3002\u5bf9\u4e8e\u53d1\u751f IP \u51b2\u7a81\u7684**\u6709\u72b6\u6001**\u7684 Pod\uff0c\u4e3a\u4e86\u4fdd\u6301 IP \u5730\u5740\u4e5f\u662f\u6709\u72b6\u6001\u8bbe\u8ba1\uff0c\u6211\u4eec\u4e0d\u4f1a\u5bf9\u5176\u91ca\u653e\u3002\u60a8\u53ef\u901a\u8fc7 spiderpool-agent \u73af\u5883\u53d8\u91cf SPIDERPOOL_ENABLED_RELEASE_CONFLICT_IPS \u6765\u63a7\u5236\u6b64\u529f\u80fd\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-ip namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] coordinator : detectIPConflict : true # Enable detectIPConflict \u82e5 IP \u51b2\u7a81\u68c0\u67e5\u53d1\u73b0\u67d0 IP \u5df2\u88ab\u5360\u7528\uff0c\u8bf7\u68c0\u67e5\u662f\u5426\u88ab\u96c6\u7fa4\u4e2d\u5176\u4ed6\u5904\u4e8e Terminating \u9636\u6bb5\u7684 \u65e0\u72b6\u6001 Pod \u6240\u5360\u7528\uff0c\u5e76\u914d\u5408 IP \u56de\u6536\u673a\u5236 \u76f8\u5173\u53c2\u6570\u8fdb\u884c\u914d\u7f6e\u3002 \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00(alpha) \u6709\u4e00\u4e9b\u4f20\u7edf\u5e94\u7528\u53ef\u80fd\u9700\u8981\u901a\u8fc7\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216\u8005 IP \u5730\u5740\u6765\u8026\u5408\u5e94\u7528\u7684\u884c\u4e3a\u3002\u6bd4\u5982 License Server \u53ef\u80fd\u9700\u8981\u5e94\u7528\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216 IP \u5730\u5740\u4e3a\u5e94\u7528\u9881\u53d1 License\u3002\u5982\u679c Pod \u7684 Mac \u5730\u5740\u53d1\u751f\u6539\u53d8\uff0c\u5df2\u9881\u53d1\u7684 License \u53ef\u80fd\u65e0\u6548\u3002 \u6240\u4ee5\u9700\u8981\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u3002 Spiderpool \u53ef\u901a\u8fc7 coordinator \u56fa\u5b9a\u5e94\u7528\u7684 Mac \u5730\u5740\uff0c\u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002 \u6ce8\u610f: \u76ee\u524d\u652f\u6301\u4fee\u6539 Macvlan \u548c SR-IOV \u4f5c\u4e3a CNI \u7684 Pod\u3002 IPVlan L2 \u6a21\u5f0f\u4e0b\u4e3b\u63a5\u53e3\u4e0e\u5b50\u63a5\u53e3 Mac \u5730\u5740\u4e00\u81f4\uff0c\u4e0d\u652f\u6301\u4fee\u6539 \u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002\u4e00\u4e2a IPv4 \u5730\u5740\u957f\u5ea6 4 \u5b57\u8282\uff0c\u53ef\u4ee5\u5b8c\u5168\u8f6c\u6362\u4e3a2 \u4e2a 16 \u8fdb\u5236\u6570\u3002\u5bf9\u4e8e IPv6 \u5730\u5740\uff0c\u53ea\u53d6\u6700\u540e 4 \u4e2a\u5b57\u8282\u3002 \u56fa\u5b9a Mac \u5730\u5740\u540e\uff0c\u4e3a\u907f\u514d\u8fc7\u65f6\u7684 ARP \u7f13\u5b58\u8868\u5bfc\u81f4\u8bbf\u95ee\u5931\u8d25\uff0cCoordinator \u63d2\u4ef6\u4f1a\u53d1\u9001\u4e00\u4e2a\u514d\u8d39 ARP\uff0c\u901a\u544a\u65b0\u7684 Mac \u5730\u5740\u5230\u5c40\u57df\u7f51\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u6211\u4eec\u53ef\u4ee5\u68c0\u6d4b Pod \u7684 Mac \u5730\u5740\u7684\u524d\u7f00\u662f\u5426\u662f \"0a:1b\" \u914d\u7f6e\u7f51\u5361\u4f20\u8f93\u961f\u5217(txQueueLen) \u4f20\u8f93\u961f\u5217\u957f\u5ea6\uff08txqueuelen\uff09\u662fTCP/IP\u534f\u8bae\u6808\u7f51\u7edc\u63a5\u53e3\u7684\u4e00\u4e2a\u503c\uff0c\u5b83\u8bbe\u7f6e\u4e86\u7f51\u7edc\u63a5\u53e3\u8bbe\u5907\u5185\u6838\u4f20\u8f93\u961f\u5217\u4e2d\u5141\u8bb8\u7684\u6570\u636e\u5305\u6570\u91cf\u3002\u5982\u679ctxqueuelen\u503c\u8fc7\u5c0f\uff0c\u53ef\u80fd\u5bfc\u81f4\u5728Pod\u4e4b\u95f4\u7684\u901a\u4fe1\u4e2d\u4e22\u5931\u6570\u636e\u5305\u3002\u5982\u679c\u9700\u8981\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c\u914d\u7f6e: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : txqueue-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : txQueueLen : 2000 \u4e3a Pod \u7684 veth0 \u7f51\u5361\u914d\u7f6e\u672c\u5730\u94fe\u8def\u5730\u5740\uff0c\u652f\u6301\u670d\u52a1\u7f51\u683c\u573a\u666f \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cCoordinator \u4e0d\u4f1a\u4e3a veth0 \u7f51\u5361\u914d\u7f6e\u672c\u5730\u94fe\u8def\u5730\u5740\u3002\u4f46\u6709\u4e9b\u573a\u666f\u4e0b(\u6bd4\u5982\u670d\u52a1\u7f51\u683c)\uff0c\u7ecf\u8fc7 veth0 \u7f51\u5361\u6d41\u5165\u7684\u7f51\u683c\u6d41\u91cf\u4f1a\u968f istio \u8bbe\u7f6e\u7684 iptables \u89c4\u5219\u91cd\u5b9a\u5411\uff0c\u5982\u679c veth0 \u6ca1\u6709 IP \u5730\u5740\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8fd9\u90e8\u5206\u6d41\u91cf\u88ab\u4e22\u5f03(\u89c1 #Issue3568 )\u3002\u6240\u4ee5\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u4e3a veth0 \u914d\u7f6e\u4e00\u4e2a\u672c\u5730\u94fe\u8def\u5730\u5740\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : istio-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : vethLinkAddress : \"169.254.200.1\" vethLinkAddress \u9ed8\u8ba4\u4e3a\u7a7a\uff0c\u8868\u793a\u4e0d\u914d\u7f6e\u3002\u4e0d\u4e3a\u7a7a\u5219\u5fc5\u987b\u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u672c\u5730\u94fe\u8def\u5730\u5740\u3002 \u81ea\u52a8\u83b7\u53d6\u96c6\u7fa4 Service \u7684 CIDR Kubernetes 1.29 \u5f00\u59cb\u652f\u6301\u4ee5 ServiceCIDR \u8d44\u6e90\u7684\u65b9\u5f0f\u914d\u7f6e\u96c6\u7fa4 Service \u7684 CIDR\uff0c\u66f4\u591a\u4fe1\u606f\u53c2\u8003 KEP 1880 \u3002\u5982\u679c\u60a8\u7684\u96c6\u7fa4\u652f\u6301 ServiceCIDR\uff0cSpiderpool-controller \u7ec4\u4ef6 \u81ea\u52a8\u76d1\u542c ServiceCIDR \u8d44\u6e90\u7684\u53d8\u5316\uff0c\u5c06\u8bfb\u53d6\u5230\u7684 Service \u5b50\u7f51\u4fe1\u606f\u81ea\u52a8\u66f4\u65b0\u5230 Spidercoordinator \u7684 Status \u4e2d\u3002 ~# kubectl get servicecidr kubernetes -o yaml apiVersion: networking.k8s.io/v1alpha1 kind: ServiceCIDR metadata: creationTimestamp: \"2024-01-25T08:36:00Z\" finalizers: - networking.k8s.io/service-cidr-finalizer name: kubernetes resourceVersion: \"504422\" uid: 72461b7d-fddd-409d-bdf2-83d1a2c067ca spec: cidrs: - 10 .233.0.0/18 - fd00:10:233::/116 status: conditions: - lastTransitionTime: \"2024-01-28T06:38:55Z\" message: Kubernetes Service CIDR is ready reason: \"\" status: \"True\" type: Ready ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-01-25T08:41:50Z\" finalizers: - spiderpool.spidernet.io generation: 1 name: default resourceVersion: \"41645\" uid: d1e095db-d6e8-4413-b60e-fcf31ad2bf5e spec: detectGateway: false detectIPConflict: false hijackCIDR: - 10 .244.64.0/18 - fd00:10:244::/112 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: auto podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true txQueueLen: 0 status: phase: Synced serviceCIDR: - 10 .233.0.0/18 - fd00:10:233::/116 \u5df2\u77e5\u95ee\u9898 underlay \u6a21\u5f0f\u4e0b\uff0cunderlay Pod \u4e0e Overlay Pod(calico or cilium) \u8fdb\u884c TCP \u901a\u4fe1\u5931\u8d25 \u6b64\u95ee\u9898\u662f\u56e0\u4e3a\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\uff0c\u53d1\u51fa\u7684\u8bf7\u6c42\u62a5\u6587\u5339\u914d\u6e90Pod \u4fa7\u7684\u8def\u7531\uff0c\u4f1a\u901a\u8fc7 veth0 \u8f6c\u53d1\u5230\u4e3b\u673a\u4fa7\uff0c\u518d\u7531\u4e3b\u673a\u4fa7\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u76ee\u6807 Pod \u770b\u89c1\u6570\u636e\u5305\u7684\u6e90 IP \u4e3a \u6e90 Pod \u7684 Underlay IP\uff0c\u76f4\u63a5\u8d70 Underlay \u7f51\u7edc\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u6e90 Pod \u6240\u5728\u4e3b\u673a\u3002 \u5728\u8be5\u4e3b\u673a\u770b\u6765\u8fd9\u662f\u4e00\u4e2a\u975e\u6cd5\u7684\u6570\u636e\u5305(\u610f\u5916\u7684\u6536\u5230 TCP \u7684\u7b2c\u4e8c\u6b21\u63e1\u624b\u62a5\u6587\uff0c\u8ba4\u4e3a\u662f conntrack table invalid), \u6240\u4ee5\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u663e\u5f0f\u7684 drop \u3002 \u76ee\u524d\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 kube-proxy \u7684\u6a21\u5f0f\u4e3a ipvs \u89c4\u907f\u3002\u8fd9\u4e2a\u95ee\u9898\u9884\u8ba1\u5728 k8s 1.29 \u4fee\u590d\u3002 \u5f53 sysctl nf_conntrack_tcp_be_liberal \u8bbe\u7f6e\u4e3a 1 \u65f6\uff0ckube-proxy \u5c06\u4e0d\u4f1a\u4e0b\u53d1\u8fd9\u6761 DROP \u89c4\u5219\u3002 overlay \u6a21\u5f0f\u4e0b, \u5f53 Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\u3002\u5982\u679c\u96c6\u7fa4\u7684\u7f3a\u7701CNI \u4e3a Cilium, Pod \u7684 underlay \u7f51\u5361 \u65e0\u6cd5\u4e0e\u8282\u70b9\u901a\u4fe1\u3002 \u6211\u4eec\u501f\u52a9\u7f3a\u7701CNI\u521b\u5efa Veth \u8bbe\u5907\uff0c\u5b9e\u73b0 Pod \u7684 underlay IP \u4e0e\u8282\u70b9\u901a\u4fe1(\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0cmacvlan \u5728 bridge \u6a21\u5f0f\u4e0b\uff0c \u5176\u7236\u5b50\u63a5\u53e3\u65e0\u6cd5\u76f4\u63a5)\uff0c\u4f46 Cilium \u4e0d\u5141\u8bb8\u975e Cilium \u5b50\u7f51\u7684 IP \u4ece Veth \u8bbe\u5907\u8f6c\u53d1\u3002","title":"Coordinator"},{"location":"concepts/coordinator-zh_CN/#coordinator","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u5185\u7f6e\u4e00\u4e2a\u53eb coordinator \u7684 CNI meta-plugin, \u5b83\u5728 Main CNI \u88ab\u8c03\u7528\u4e4b\u540e\u518d\u5de5\u4f5c\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u529f\u80fd: \u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u5728 Pod \u591a\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u8def\u7531\uff0c\u786e\u4fdd\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e00\u81f4 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff08\u9057\u5f03\uff0c\u73b0\u5728\u7531 IPAM \u5b8c\u6210\uff09 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \uff08\u9057\u5f03\uff0c\u73b0\u5728\u7531 IPAM \u5b8c\u6210\uff09 \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u6ce8\u610f: \u5982\u679c\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u662f\u4f7f\u7528 NetworkManager \u7684 OS\uff0c\u6bd4\u5982 Fedora\u3001Centos\u7b49\uff0c\u5f3a\u70c8\u5efa\u8bae\u914d\u7f6e NetworkManager \u7684\u914d\u7f6e\u6587\u4ef6(/etc/NetworkManager/conf.d/spidernet.conf)\uff0c\u907f\u514d NetworkManager \u5e72\u6270 coordinator \u521b\u5efa\u7684 Veth \u865a\u62df\u63a5\u53e3\uff0c\u5f71\u54cd\u901a\u4fe1: ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecd coordinator \u5982\u4f55\u89e3\u51b3\u6216\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u3002","title":"Coordinator"},{"location":"concepts/coordinator-zh_CN/#cni","text":"Field Description Schema Validation Default type CNI \u7684\u7c7b\u578b \u5b57\u7b26\u4e32 required coordinator mode coordinator \u8fd0\u884c\u7684\u6a21\u5f0f. \"auto\": coordinator \u81ea\u52a8\u5224\u65ad\u8fd0\u884c\u5728 Underlay \u6216\u8005 Overlay; \"underlay\": \u4e3a Pod \u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002\u7531 Pod \u7684 Underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"overlay\": \u4e0d\u989d\u5916\u521b\u5efa veth \u8bbe\u5907\uff0c\u8fd0\u884c\u5728\u591a\u7f51\u5361\u6a21\u5f0f\u3002\u7531 overlay \u7c7b\u578b\u7684 CNI(calico\uff0ccilium) \u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\uff0c\u7531 underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"disable\": \u7981\u7528 coordinator \u5b57\u7b26\u4e32 optional auto tunePodRoutes Pod \u591a\u7f51\u5361\u6a21\u5f0f\u4e0b\uff0c\u662f\u5426\u8c03\u534f Pod \u7684\u8def\u7531\uff0c\u89e3\u51b3\u8bbf\u95ee\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898 \u5e03\u5c14\u578b optional true podDefaultRouteNic Pod \u591a\u7f51\u5361\u65f6\uff0c\u914d\u7f6e Pod \u7684\u9ed8\u8ba4\u8def\u7531\u7f51\u5361\u3002\u9ed8\u8ba4\u4e3a \"\", \u5176 value \u5b9e\u9645\u4e3a Pod \u7b2c\u4e00\u5f20\u62e5\u6709\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361 \u5b57\u7b26\u4e32 optional \"\" podDefaultCniNic K8s \u4e2d Pod \u9ed8\u8ba4\u7684\u7b2c\u4e00\u5f20\u7f51\u5361 \u5e03\u5c14\u578b optional eth0 detectGateway \u9057\u5f03\uff0c\u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u5e03\u5c14\u578b optional false detectIPConflict \u9057\u5f03\uff0c\u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5 Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u5e03\u5c14\u578b optional false podMACPrefix \u662f\u5426\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00, \u524d\u7f00\u957f\u5ea6\u4e3a\u4e24\u4e2a\u5b57\u8282, \u7531\":\"\u62fc\u63a5\u3002\u6ce8\u610f\uff1a\u9996\u5b57\u8282\u7684\u6700\u4f4e\u4f4d\u5fc5\u987b\u662f \"0\"\u3002\u6bd4\u5982 \"0a:1b\"\u3002 \u5b57\u7b26\u4e32 optional \"\" overlayPodCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Pod \u7684\u5b50\u7f51\uff0c\u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 serviceCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Service \u5b50\u7f51\uff0c \u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 hijackCIDR \u989d\u5916\u7684\u9700\u8981\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\u3002\u6bd4\u5982nodelocaldns \u7684\u5730\u5740: 169.254.20.10/32 []stirng optional \u7a7a hostRuleTable \u7b56\u7565\u8def\u7531\u8868\u53f7\uff0c\u540c\u4e3b\u673a\u4e0e Pod \u901a\u4fe1\u7684\u8def\u7531\u5c06\u4f1a\u5b58\u653e\u4e8e\u8fd9\u4e2a\u8868\u53f7 \u6574\u6570\u578b optional 500 podRPFilter \u8bbe\u7f6e Pod \u7684 sysctl \u53c2\u6570 rp_filter \u6574\u6570\u578b optional 0 txQueueLen \u8bbe\u7f6e Pod \u7684\u7f51\u5361\u4f20\u8f93\u961f\u5217 \u6574\u6570\u578b optional 0 detectOptions \u68c0\u6d4b\u5730\u5740\u51b2\u7a81\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u7684\u9ad8\u7ea7\u914d\u7f6e\u9879: \u5305\u62ec\u53d1\u9001\u63a2\u6d4b\u62a5\u6587\u6b21\u6570(retries: \u9ed8\u8ba4\u4e3a 3 \u6b21), \u548c\u54cd\u5e94\u7684\u8d85\u65f6\u65f6\u95f4(timeout: \u9ed8\u8ba4\u4e3a 100ms)\uff0c\u8fd8\u6709\u53d1\u9001\u62a5\u6587\u7684\u95f4\u9694(interval:\u9ed8\u8ba4\u4e3a 10ms, \u5c06\u4f1a\u5728\u672a\u6765\u7248\u672c\u4e2d\u79fb\u9664) \u5bf9\u8c61\u7c7b\u578b optional \u7a7a logOptions \u65e5\u5fd7\u914d\u7f6e\uff0c\u5305\u62ec logLevel(\u9ed8\u8ba4\u4e3a debug) \u548c logFile(\u9ed8\u8ba4\u4e3a /var/log/spidernet/coordinator.log) \u5bf9\u8c61\u7c7b\u578b optional - \u5982\u679c\u60a8\u901a\u8fc7 SpinderMultusConfig CR \u5e2e\u52a9\u521b\u5efa NetworkAttachmentDefinition CR\uff0c\u60a8\u53ef\u4ee5\u5728 SpinderMultusConfig \u4e2d\u914d\u7f6e coordinator (\u6240\u6709\u5b57\u6bb5)\u3002\u53c2\u8003: SpinderMultusConfig \u3002 Spidercoordinators CR \u4f5c\u4e3a coordinator \u63d2\u4ef6\u7684\u5168\u5c40\u7f3a\u7701\u914d\u7f6e(\u6240\u6709\u5b57\u6bb5)\uff0c\u5176\u4f18\u5148\u7ea7\u4f4e\u4e8e NetworkAttachmentDefinition CR \u4e2d\u7684\u914d\u7f6e\u3002 \u5982\u679c\u5728 NetworkAttachmentDefinition CR \u672a\u914d\u7f6e, \u5c06\u4f7f\u7528 Spidercoordinator CR \u4f5c\u4e3a\u7f3a\u7701\u503c\u3002\u66f4\u591a\u8be6\u60c5\u53c2\u8003: Spidercoordinator \u3002","title":"CNI \u914d\u7f6e\u5b57\u6bb5\u8bf4\u660e"},{"location":"concepts/coordinator-zh_CN/#underlay-pod-clusterip","text":"\u6211\u4eec\u5728\u4f7f\u7528\u4e00\u4e9b\u5982 Macvlan\u3001IPvlan\u3001SR-IOV \u7b49 Underlay CNI\u65f6\uff0c\u4f1a\u9047\u5230\u5176 Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8fd9\u5e38\u5e38\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee CLusterIP \u9700\u8981\u7ecf\u8fc7\u5728\u4ea4\u6362\u673a\u7684\u7f51\u5173\uff0c\u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 ClusterIP \u7684\u8def\u7531\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bbf\u95ee\u3002 \u5173\u4e8e Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8bf7\u53c2\u8003 Underlay-CNI\u8bbf\u95ee Service","title":"\u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898"},{"location":"concepts/coordinator-zh_CN/#pod-ip-alpha","text":"\u5bf9\u4e8e Underlay \u7f51\u7edc\uff0cIP \u51b2\u7a81\u662f\u65e0\u6cd5\u63a5\u53d7\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u95ee\u9898\u3002\u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff0c\u652f\u6301\u540c\u65f6\u68c0\u6d4b IPv4 \u548c IPv6 \u5730\u5740\u3002\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\uff0c \u5982\u679c\u53d1\u73b0\u56de\u590d\u62a5\u6587\u7684 Mac \u5730\u5740\u4e0d\u662f\u6765\u81ea Pod \u672c\u8eab\u7684\u7f51\u5361\uff0c\u90a3\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e2a IP \u662f\u51b2\u7a81\u7684\uff0c\u5e76\u62d2\u7edd IP \u51b2\u7a81\u7684 Pod \u88ab\u521b\u5efa\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u9ed8\u8ba4\u8fd8\u4f1a\u5bf9\u53d1\u751f IP \u51b2\u7a81\u7684**\u65e0\u72b6\u6001**\u7684 Pod \u91ca\u653e\u6240\u6709\u7684\u5df2\u5206\u914d\u7684 IP \u4f7f\u5176\u91cd\u65b0\u5206\u914d\uff0c\u4f7f\u5f97 Pod \u5728\u4e0b\u4e00\u6b21\u91cd\u65b0\u8c03\u7528 CNI \u65f6\u80fd\u591f\u5c1d\u8bd5\u5206\u914d\u5230\u5176\u5b83\u975e\u51b2\u7a81\u7684 IP\u3002\u5bf9\u4e8e\u53d1\u751f IP \u51b2\u7a81\u7684**\u6709\u72b6\u6001**\u7684 Pod\uff0c\u4e3a\u4e86\u4fdd\u6301 IP \u5730\u5740\u4e5f\u662f\u6709\u72b6\u6001\u8bbe\u8ba1\uff0c\u6211\u4eec\u4e0d\u4f1a\u5bf9\u5176\u91ca\u653e\u3002\u60a8\u53ef\u901a\u8fc7 spiderpool-agent \u73af\u5883\u53d8\u91cf SPIDERPOOL_ENABLED_RELEASE_CONFLICT_IPS \u6765\u63a7\u5236\u6b64\u529f\u80fd\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-ip namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] coordinator : detectIPConflict : true # Enable detectIPConflict \u82e5 IP \u51b2\u7a81\u68c0\u67e5\u53d1\u73b0\u67d0 IP \u5df2\u88ab\u5360\u7528\uff0c\u8bf7\u68c0\u67e5\u662f\u5426\u88ab\u96c6\u7fa4\u4e2d\u5176\u4ed6\u5904\u4e8e Terminating \u9636\u6bb5\u7684 \u65e0\u72b6\u6001 Pod \u6240\u5360\u7528\uff0c\u5e76\u914d\u5408 IP \u56de\u6536\u673a\u5236 \u76f8\u5173\u53c2\u6570\u8fdb\u884c\u914d\u7f6e\u3002","title":"\u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81( alpha \u9636\u6bb5)"},{"location":"concepts/coordinator-zh_CN/#pod-mac-alpha","text":"\u6709\u4e00\u4e9b\u4f20\u7edf\u5e94\u7528\u53ef\u80fd\u9700\u8981\u901a\u8fc7\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216\u8005 IP \u5730\u5740\u6765\u8026\u5408\u5e94\u7528\u7684\u884c\u4e3a\u3002\u6bd4\u5982 License Server \u53ef\u80fd\u9700\u8981\u5e94\u7528\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216 IP \u5730\u5740\u4e3a\u5e94\u7528\u9881\u53d1 License\u3002\u5982\u679c Pod \u7684 Mac \u5730\u5740\u53d1\u751f\u6539\u53d8\uff0c\u5df2\u9881\u53d1\u7684 License \u53ef\u80fd\u65e0\u6548\u3002 \u6240\u4ee5\u9700\u8981\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u3002 Spiderpool \u53ef\u901a\u8fc7 coordinator \u56fa\u5b9a\u5e94\u7528\u7684 Mac \u5730\u5740\uff0c\u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002 \u6ce8\u610f: \u76ee\u524d\u652f\u6301\u4fee\u6539 Macvlan \u548c SR-IOV \u4f5c\u4e3a CNI \u7684 Pod\u3002 IPVlan L2 \u6a21\u5f0f\u4e0b\u4e3b\u63a5\u53e3\u4e0e\u5b50\u63a5\u53e3 Mac \u5730\u5740\u4e00\u81f4\uff0c\u4e0d\u652f\u6301\u4fee\u6539 \u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002\u4e00\u4e2a IPv4 \u5730\u5740\u957f\u5ea6 4 \u5b57\u8282\uff0c\u53ef\u4ee5\u5b8c\u5168\u8f6c\u6362\u4e3a2 \u4e2a 16 \u8fdb\u5236\u6570\u3002\u5bf9\u4e8e IPv6 \u5730\u5740\uff0c\u53ea\u53d6\u6700\u540e 4 \u4e2a\u5b57\u8282\u3002 \u56fa\u5b9a Mac \u5730\u5740\u540e\uff0c\u4e3a\u907f\u514d\u8fc7\u65f6\u7684 ARP \u7f13\u5b58\u8868\u5bfc\u81f4\u8bbf\u95ee\u5931\u8d25\uff0cCoordinator \u63d2\u4ef6\u4f1a\u53d1\u9001\u4e00\u4e2a\u514d\u8d39 ARP\uff0c\u901a\u544a\u65b0\u7684 Mac \u5730\u5740\u5230\u5c40\u57df\u7f51\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u6211\u4eec\u53ef\u4ee5\u68c0\u6d4b Pod \u7684 Mac \u5730\u5740\u7684\u524d\u7f00\u662f\u5426\u662f \"0a:1b\"","title":"\u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00(alpha)"},{"location":"concepts/coordinator-zh_CN/#txqueuelen","text":"\u4f20\u8f93\u961f\u5217\u957f\u5ea6\uff08txqueuelen\uff09\u662fTCP/IP\u534f\u8bae\u6808\u7f51\u7edc\u63a5\u53e3\u7684\u4e00\u4e2a\u503c\uff0c\u5b83\u8bbe\u7f6e\u4e86\u7f51\u7edc\u63a5\u53e3\u8bbe\u5907\u5185\u6838\u4f20\u8f93\u961f\u5217\u4e2d\u5141\u8bb8\u7684\u6570\u636e\u5305\u6570\u91cf\u3002\u5982\u679ctxqueuelen\u503c\u8fc7\u5c0f\uff0c\u53ef\u80fd\u5bfc\u81f4\u5728Pod\u4e4b\u95f4\u7684\u901a\u4fe1\u4e2d\u4e22\u5931\u6570\u636e\u5305\u3002\u5982\u679c\u9700\u8981\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c\u914d\u7f6e: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : txqueue-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : txQueueLen : 2000","title":"\u914d\u7f6e\u7f51\u5361\u4f20\u8f93\u961f\u5217(txQueueLen)"},{"location":"concepts/coordinator-zh_CN/#pod-veth0","text":"\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cCoordinator \u4e0d\u4f1a\u4e3a veth0 \u7f51\u5361\u914d\u7f6e\u672c\u5730\u94fe\u8def\u5730\u5740\u3002\u4f46\u6709\u4e9b\u573a\u666f\u4e0b(\u6bd4\u5982\u670d\u52a1\u7f51\u683c)\uff0c\u7ecf\u8fc7 veth0 \u7f51\u5361\u6d41\u5165\u7684\u7f51\u683c\u6d41\u91cf\u4f1a\u968f istio \u8bbe\u7f6e\u7684 iptables \u89c4\u5219\u91cd\u5b9a\u5411\uff0c\u5982\u679c veth0 \u6ca1\u6709 IP \u5730\u5740\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8fd9\u90e8\u5206\u6d41\u91cf\u88ab\u4e22\u5f03(\u89c1 #Issue3568 )\u3002\u6240\u4ee5\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u4e3a veth0 \u914d\u7f6e\u4e00\u4e2a\u672c\u5730\u94fe\u8def\u5730\u5740\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : istio-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : vethLinkAddress : \"169.254.200.1\" vethLinkAddress \u9ed8\u8ba4\u4e3a\u7a7a\uff0c\u8868\u793a\u4e0d\u914d\u7f6e\u3002\u4e0d\u4e3a\u7a7a\u5219\u5fc5\u987b\u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u672c\u5730\u94fe\u8def\u5730\u5740\u3002","title":"\u4e3a Pod \u7684 veth0 \u7f51\u5361\u914d\u7f6e\u672c\u5730\u94fe\u8def\u5730\u5740\uff0c\u652f\u6301\u670d\u52a1\u7f51\u683c\u573a\u666f"},{"location":"concepts/coordinator-zh_CN/#service-cidr","text":"Kubernetes 1.29 \u5f00\u59cb\u652f\u6301\u4ee5 ServiceCIDR \u8d44\u6e90\u7684\u65b9\u5f0f\u914d\u7f6e\u96c6\u7fa4 Service \u7684 CIDR\uff0c\u66f4\u591a\u4fe1\u606f\u53c2\u8003 KEP 1880 \u3002\u5982\u679c\u60a8\u7684\u96c6\u7fa4\u652f\u6301 ServiceCIDR\uff0cSpiderpool-controller \u7ec4\u4ef6 \u81ea\u52a8\u76d1\u542c ServiceCIDR \u8d44\u6e90\u7684\u53d8\u5316\uff0c\u5c06\u8bfb\u53d6\u5230\u7684 Service \u5b50\u7f51\u4fe1\u606f\u81ea\u52a8\u66f4\u65b0\u5230 Spidercoordinator \u7684 Status \u4e2d\u3002 ~# kubectl get servicecidr kubernetes -o yaml apiVersion: networking.k8s.io/v1alpha1 kind: ServiceCIDR metadata: creationTimestamp: \"2024-01-25T08:36:00Z\" finalizers: - networking.k8s.io/service-cidr-finalizer name: kubernetes resourceVersion: \"504422\" uid: 72461b7d-fddd-409d-bdf2-83d1a2c067ca spec: cidrs: - 10 .233.0.0/18 - fd00:10:233::/116 status: conditions: - lastTransitionTime: \"2024-01-28T06:38:55Z\" message: Kubernetes Service CIDR is ready reason: \"\" status: \"True\" type: Ready ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-01-25T08:41:50Z\" finalizers: - spiderpool.spidernet.io generation: 1 name: default resourceVersion: \"41645\" uid: d1e095db-d6e8-4413-b60e-fcf31ad2bf5e spec: detectGateway: false detectIPConflict: false hijackCIDR: - 10 .244.64.0/18 - fd00:10:244::/112 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: auto podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true txQueueLen: 0 status: phase: Synced serviceCIDR: - 10 .233.0.0/18 - fd00:10:233::/116","title":"\u81ea\u52a8\u83b7\u53d6\u96c6\u7fa4 Service \u7684 CIDR"},{"location":"concepts/coordinator-zh_CN/#_1","text":"underlay \u6a21\u5f0f\u4e0b\uff0cunderlay Pod \u4e0e Overlay Pod(calico or cilium) \u8fdb\u884c TCP \u901a\u4fe1\u5931\u8d25 \u6b64\u95ee\u9898\u662f\u56e0\u4e3a\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\uff0c\u53d1\u51fa\u7684\u8bf7\u6c42\u62a5\u6587\u5339\u914d\u6e90Pod \u4fa7\u7684\u8def\u7531\uff0c\u4f1a\u901a\u8fc7 veth0 \u8f6c\u53d1\u5230\u4e3b\u673a\u4fa7\uff0c\u518d\u7531\u4e3b\u673a\u4fa7\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u76ee\u6807 Pod \u770b\u89c1\u6570\u636e\u5305\u7684\u6e90 IP \u4e3a \u6e90 Pod \u7684 Underlay IP\uff0c\u76f4\u63a5\u8d70 Underlay \u7f51\u7edc\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u6e90 Pod \u6240\u5728\u4e3b\u673a\u3002 \u5728\u8be5\u4e3b\u673a\u770b\u6765\u8fd9\u662f\u4e00\u4e2a\u975e\u6cd5\u7684\u6570\u636e\u5305(\u610f\u5916\u7684\u6536\u5230 TCP \u7684\u7b2c\u4e8c\u6b21\u63e1\u624b\u62a5\u6587\uff0c\u8ba4\u4e3a\u662f conntrack table invalid), \u6240\u4ee5\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u663e\u5f0f\u7684 drop \u3002 \u76ee\u524d\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 kube-proxy \u7684\u6a21\u5f0f\u4e3a ipvs \u89c4\u907f\u3002\u8fd9\u4e2a\u95ee\u9898\u9884\u8ba1\u5728 k8s 1.29 \u4fee\u590d\u3002 \u5f53 sysctl nf_conntrack_tcp_be_liberal \u8bbe\u7f6e\u4e3a 1 \u65f6\uff0ckube-proxy \u5c06\u4e0d\u4f1a\u4e0b\u53d1\u8fd9\u6761 DROP \u89c4\u5219\u3002 overlay \u6a21\u5f0f\u4e0b, \u5f53 Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\u3002\u5982\u679c\u96c6\u7fa4\u7684\u7f3a\u7701CNI \u4e3a Cilium, Pod \u7684 underlay \u7f51\u5361 \u65e0\u6cd5\u4e0e\u8282\u70b9\u901a\u4fe1\u3002 \u6211\u4eec\u501f\u52a9\u7f3a\u7701CNI\u521b\u5efa Veth \u8bbe\u5907\uff0c\u5b9e\u73b0 Pod \u7684 underlay IP \u4e0e\u8282\u70b9\u901a\u4fe1(\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0cmacvlan \u5728 bridge \u6a21\u5f0f\u4e0b\uff0c \u5176\u7236\u5b50\u63a5\u53e3\u65e0\u6cd5\u76f4\u63a5)\uff0c\u4f46 Cilium \u4e0d\u5141\u8bb8\u975e Cilium \u5b50\u7f51\u7684 IP \u4ece Veth \u8bbe\u5907\u8f6c\u53d1\u3002","title":"\u5df2\u77e5\u95ee\u9898"},{"location":"concepts/coordinator/","text":"Coordinator English | \u7b80\u4f53\u4e2d\u6587 Spiderpool incorporates a CNI meta-plugin called coordinator that works after the Main CNI is invoked. It mainly offers the following features: Resolve the problem of underlay Pods unable to access ClusterIP Coordinate the routing for Pods with multiple NICs, ensuring consistent packet paths Detect IP conflicts within Pods Check the reachability of Pod gateways Support fixed Mac address prefixes for Pods Note: If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with veth interfaces created through coordinator : ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager Let's delve into how coordinator implements these features. CNI fields description Field Description Schema Validation Default type The name of this Spidercoordinators resource string required coordinator mode the mode in which the coordinator run. \"auto\": Automatically determine if it's overlay or underlay; \"underlay\": All NICs for pods are underlay NICs, and in this case the coordinator will create veth-pairs device to solve the problem of underlay pods accessing services; \"overlay\": The coordinator does not create veth-pair devices, but the first NIC of the pod cannot be an underlay NIC, which is created by overlay CNI (e.g. calico, cilium). Solve the problem of pod access to service through the first NIC; \"disable\": The coordinator does nothing and exits directly string optional auto tunePodRoutes Tune the pod's routing tables while a pod is in multi-NIC mode bool optional true podDefaultRouteNic Configure the default routed NIC for the pod while a pod is in multi-NIC mode, The default value is 0, indicate that the first network interface of the pod has the default route. string optional \"\" podDefaultCniNic The name of the pod's first NIC defaults to eth0 in kubernetes bool optional eth0 detectGateway DEPRECATED: Enable gateway detection while creating pods, which prevent pod creation if the gateway is unreachable bool optional false detectIPConflict DEPRECATED: Enable IP conflicting checking for pods, which prevent pod creation if the pod's ip is conflicting bool optional false podMACPrefix Enable fixing MAC address prefixes for pods. empty value is mean to disable. the length of prefix is two bytes. and the lowest bit of the first byte must be 0, example: \"0a:1b\". string optional \"\" overlayPodCIDR The default cluster CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} serviceCIDR The default service CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} hijackCIDR The CIDR that need to be forwarded via the host network, For example, the address of nodelocaldns(169.254.20.10/32 by default) []stirng optional []string{} hostRuleTable The routes on the host that communicates with the pod's underlay IPs will belong to this routing table number int optional 500 podRPFilter Set the rp_filter sysctl parameter on the pod, which is recommended to be set to 0 int optional 0 txQueueLen set txqueuelen(Transmit Queue Length) of the pod's interface int optional 0 detectOptions The advanced configuration of detectGateway and detectIPConflict, including the number of the send packets(retries: default is 3) and the response timeout(timeout: default is 100ms) and the packet sending interval(interval: default is 10ms, which will be removed in the future version). obejct optional nil logOptions The configuration of logging, including logLevel(default is debug) and logFile(default is /var/log/spidernet/coordinator.log) obejct optional nil You can configure coordinator by specifying all the relevant fields in SpinderMultusConfig if a NetworkAttachmentDefinition CR is created via SpinderMultusConfig CR . For more information, please refer to SpinderMultusConfig . Spidercoordinators CR serves as the global default configuration (all fields) for coordinator . However, this configuration has a lower priority compared to the settings in the NetworkAttachmentDefinition CR. In cases where no configuration is provided in the NetworkAttachmentDefinition CR, the values from Spidercoordinators CR serve as the defaults. For detailed information, please refer to Spidercoordinator . Resolve the problem of underlay Pods unable to access ClusterIP(beta) When using underlay CNIs like Macvlan, IPvlan, SR-IOV, and others, a common challenge arises where underlay pods are unable to access ClusterIP. This occurs because accessing ClusterIP from underlay pods requires routing through the gateway on the switch. However, in many instances, the gateway is not configured with the proper routes to reach the ClusterIP, leading to restricted access. For more information about the Underlay Pod not being able to access the ClusterIP, please refer to Underlay CNI Access Service Fix MAC address prefix for Pods(alpha) Some traditional applications may require a fixed MAC address or IP address to couple the behavior of the application. For example, the License Server may need to apply a fixed Mac address or IP address to issue a license for the app. If the MAC address of a pod changes, the issued license may be invalid. Therefore, you need to fix the MAC address of the pod. Spiderpool can fix the MAC address of the application through coordinator , and the fixed rule is to configure the MAC address prefix (2 bytes) + convert the IP of the pod (4 bytes). Note: currently supports updating Macvlan and SR-IOV as pods for CNI. In IPVlan L2 mode, the MAC addresses of the primary interface and the sub-interface are the same and cannot be modified. The fixed rule is to configure the MAC address prefix (2 bytes) + the IP of the converted pod (4 bytes). An IPv4 address is 4 bytes long and can be fully converted to 2 hexadecimal numbers. For IPv6 addresses, only the last 4 bytes are taken. After fixing the MAC address, to prevent access failure due to outdated ARP cache tables, the Coordinator plugin will send a gratuitous ARP to announce the new MAC address to the local area network. We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway You can check if the MAC address prefix of the Pod starts with \"0a:1b\" after a Pod is created. Configure the transmit queue length(txQueueLen) The Transmit Queue Length (txqueuelen) is a TCP/IP stack network interface value that sets the number of packets allowed per kernel transmit queue of a network interface device. If the txqueuelen is too small, it may cause packet loss in pod communication. we can configure it if we needs. We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : txqueue-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : txQueueLen : 2000 Configure a link-local address for the Pod's veth0 interface to support service mesh scenarios By default, Coordinator does not configure a link-local address for the veth0 interface. However, in some scenarios (such as service mesh), mesh traffic flowing through the veth0 interface will be redirected according to iptables rules set by Istio. If veth0 does not have an IP address, this can cause that traffic to be dropped (see #Issue3568). Therefore, in this scenario, we need to configure a link-local address for veth0. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : istio-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : vethLinkAddress : \"169.254.100.1\" vethLinkAddress default to \"\", It means that we don't configure an address for veth0. It must an valid link-local address if it isn't empty. Automatically get the CIDR of a clustered Service Kubernetes 1.29 starts to support configuring the CIDR of a clustered Service as a ServiceCIDR resource, for more information refer to KEP 1880 . If your cluster supports ServiceCIDR, the Spiderpool-controller component automatically listens for changes to the ServiceCIDR resource and automatically updates the Service subnet information it reads into the Status of the Spidercoordinator. ~# kubectl get servicecidr kubernetes -o yaml apiVersion: networking.k8s.io/v1alpha1 kind: ServiceCIDR metadata: creationTimestamp: \"2024-01-25T08:36:00Z\" finalizers: - networking.k8s.io/service-cidr-finalizer name: kubernetes resourceVersion: \"504422\" uid: 72461b7d-fddd-409d-bdf2-83d1a2c067ca spec: cidrs: - 10 .233.0.0/18 - fd00:10:233::/116 status: conditions: - lastTransitionTime: \"2024-01-28T06:38:55Z\" message: Kubernetes Service CIDR is ready reason: \"\" status: \"True\" type: Ready ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-01-25T08:41:50Z\" finalizers: - spiderpool.spidernet.io generation: 1 name: default resourceVersion: \"41645\" uid: d1e095db-d6e8-4413-b60e-fcf31ad2bf5e spec: detectGateway: false detectIPConflict: false hijackCIDR: - 10 .244.64.0/18 - fd00:10:244::/112 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: auto podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true txQueueLen: 0 status: phase: Synced serviceCIDR: - 10 .233.0.0/18 - fd00:10:233::/116 Known issues Underlay mode: TCP communication between underlay Pods and overlay Pods (Calico or Cilium) fails This issue arises from inconsistent packet routing paths. Request packets are matched with the routing on the source Pod side and forwarded through veth0 to the host side. And then the packets are further forwarded to the target Pod. The target Pod perceives the source IP of the packet as the underlay IP of the source Pod, allowing it to bypass the source Pod's host and directly route through the underlay network. However, on the host, this is considered an invalid packet (as it receives unexpected TCP SYN-ACK packets that are conntrack table invalid), explicitly dropping it using an iptables rule in kube-proxy. Switching the kube-proxy mode to ipvs can address this issue. This issue is expected to be fixed in K8s 1.29. if the sysctl nf_conntrack_tcp_be_liberal is set to 1, kube-proxy will not deliver the DROP rule. Overlay mode: with Cilium as the default CNI and multiple NICs for the Pod, the underlay interface of the Pod cannot communicate with the node. Macvlan interfaces do not allow direct communication between parent and child interfaces in bridge mode in most cases. To facilitate communication between the underlay IP of the Pod and the node, we rely on the default CNI to create Veth devices. However, Cilium restricts the forwarding of IPs from non-Cilium subnets through these Veth devices.","title":"Plugin coordinator"},{"location":"concepts/coordinator/#coordinator","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool incorporates a CNI meta-plugin called coordinator that works after the Main CNI is invoked. It mainly offers the following features: Resolve the problem of underlay Pods unable to access ClusterIP Coordinate the routing for Pods with multiple NICs, ensuring consistent packet paths Detect IP conflicts within Pods Check the reachability of Pod gateways Support fixed Mac address prefixes for Pods Note: If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with veth interfaces created through coordinator : ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager Let's delve into how coordinator implements these features.","title":"Coordinator"},{"location":"concepts/coordinator/#cni-fields-description","text":"Field Description Schema Validation Default type The name of this Spidercoordinators resource string required coordinator mode the mode in which the coordinator run. \"auto\": Automatically determine if it's overlay or underlay; \"underlay\": All NICs for pods are underlay NICs, and in this case the coordinator will create veth-pairs device to solve the problem of underlay pods accessing services; \"overlay\": The coordinator does not create veth-pair devices, but the first NIC of the pod cannot be an underlay NIC, which is created by overlay CNI (e.g. calico, cilium). Solve the problem of pod access to service through the first NIC; \"disable\": The coordinator does nothing and exits directly string optional auto tunePodRoutes Tune the pod's routing tables while a pod is in multi-NIC mode bool optional true podDefaultRouteNic Configure the default routed NIC for the pod while a pod is in multi-NIC mode, The default value is 0, indicate that the first network interface of the pod has the default route. string optional \"\" podDefaultCniNic The name of the pod's first NIC defaults to eth0 in kubernetes bool optional eth0 detectGateway DEPRECATED: Enable gateway detection while creating pods, which prevent pod creation if the gateway is unreachable bool optional false detectIPConflict DEPRECATED: Enable IP conflicting checking for pods, which prevent pod creation if the pod's ip is conflicting bool optional false podMACPrefix Enable fixing MAC address prefixes for pods. empty value is mean to disable. the length of prefix is two bytes. and the lowest bit of the first byte must be 0, example: \"0a:1b\". string optional \"\" overlayPodCIDR The default cluster CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} serviceCIDR The default service CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} hijackCIDR The CIDR that need to be forwarded via the host network, For example, the address of nodelocaldns(169.254.20.10/32 by default) []stirng optional []string{} hostRuleTable The routes on the host that communicates with the pod's underlay IPs will belong to this routing table number int optional 500 podRPFilter Set the rp_filter sysctl parameter on the pod, which is recommended to be set to 0 int optional 0 txQueueLen set txqueuelen(Transmit Queue Length) of the pod's interface int optional 0 detectOptions The advanced configuration of detectGateway and detectIPConflict, including the number of the send packets(retries: default is 3) and the response timeout(timeout: default is 100ms) and the packet sending interval(interval: default is 10ms, which will be removed in the future version). obejct optional nil logOptions The configuration of logging, including logLevel(default is debug) and logFile(default is /var/log/spidernet/coordinator.log) obejct optional nil You can configure coordinator by specifying all the relevant fields in SpinderMultusConfig if a NetworkAttachmentDefinition CR is created via SpinderMultusConfig CR . For more information, please refer to SpinderMultusConfig . Spidercoordinators CR serves as the global default configuration (all fields) for coordinator . However, this configuration has a lower priority compared to the settings in the NetworkAttachmentDefinition CR. In cases where no configuration is provided in the NetworkAttachmentDefinition CR, the values from Spidercoordinators CR serve as the defaults. For detailed information, please refer to Spidercoordinator .","title":"CNI fields description"},{"location":"concepts/coordinator/#resolve-the-problem-of-underlay-pods-unable-to-access-clusteripbeta","text":"When using underlay CNIs like Macvlan, IPvlan, SR-IOV, and others, a common challenge arises where underlay pods are unable to access ClusterIP. This occurs because accessing ClusterIP from underlay pods requires routing through the gateway on the switch. However, in many instances, the gateway is not configured with the proper routes to reach the ClusterIP, leading to restricted access. For more information about the Underlay Pod not being able to access the ClusterIP, please refer to Underlay CNI Access Service","title":"Resolve the problem of underlay Pods unable to access ClusterIP(beta)"},{"location":"concepts/coordinator/#fix-mac-address-prefix-for-podsalpha","text":"Some traditional applications may require a fixed MAC address or IP address to couple the behavior of the application. For example, the License Server may need to apply a fixed Mac address or IP address to issue a license for the app. If the MAC address of a pod changes, the issued license may be invalid. Therefore, you need to fix the MAC address of the pod. Spiderpool can fix the MAC address of the application through coordinator , and the fixed rule is to configure the MAC address prefix (2 bytes) + convert the IP of the pod (4 bytes). Note: currently supports updating Macvlan and SR-IOV as pods for CNI. In IPVlan L2 mode, the MAC addresses of the primary interface and the sub-interface are the same and cannot be modified. The fixed rule is to configure the MAC address prefix (2 bytes) + the IP of the converted pod (4 bytes). An IPv4 address is 4 bytes long and can be fully converted to 2 hexadecimal numbers. For IPv6 addresses, only the last 4 bytes are taken. After fixing the MAC address, to prevent access failure due to outdated ARP cache tables, the Coordinator plugin will send a gratuitous ARP to announce the new MAC address to the local area network. We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway You can check if the MAC address prefix of the Pod starts with \"0a:1b\" after a Pod is created.","title":"Fix MAC address prefix for Pods(alpha)"},{"location":"concepts/coordinator/#configure-the-transmit-queue-lengthtxqueuelen","text":"The Transmit Queue Length (txqueuelen) is a TCP/IP stack network interface value that sets the number of packets allowed per kernel transmit queue of a network interface device. If the txqueuelen is too small, it may cause packet loss in pod communication. we can configure it if we needs. We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : txqueue-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : txQueueLen : 2000","title":"Configure the transmit queue length(txQueueLen)"},{"location":"concepts/coordinator/#configure-a-link-local-address-for-the-pods-veth0-interface-to-support-service-mesh-scenarios","text":"By default, Coordinator does not configure a link-local address for the veth0 interface. However, in some scenarios (such as service mesh), mesh traffic flowing through the veth0 interface will be redirected according to iptables rules set by Istio. If veth0 does not have an IP address, this can cause that traffic to be dropped (see #Issue3568). Therefore, in this scenario, we need to configure a link-local address for veth0. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : istio-demo namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : vethLinkAddress : \"169.254.100.1\" vethLinkAddress default to \"\", It means that we don't configure an address for veth0. It must an valid link-local address if it isn't empty.","title":"Configure a link-local address for the Pod's veth0 interface to support service mesh scenarios"},{"location":"concepts/coordinator/#automatically-get-the-cidr-of-a-clustered-service","text":"Kubernetes 1.29 starts to support configuring the CIDR of a clustered Service as a ServiceCIDR resource, for more information refer to KEP 1880 . If your cluster supports ServiceCIDR, the Spiderpool-controller component automatically listens for changes to the ServiceCIDR resource and automatically updates the Service subnet information it reads into the Status of the Spidercoordinator. ~# kubectl get servicecidr kubernetes -o yaml apiVersion: networking.k8s.io/v1alpha1 kind: ServiceCIDR metadata: creationTimestamp: \"2024-01-25T08:36:00Z\" finalizers: - networking.k8s.io/service-cidr-finalizer name: kubernetes resourceVersion: \"504422\" uid: 72461b7d-fddd-409d-bdf2-83d1a2c067ca spec: cidrs: - 10 .233.0.0/18 - fd00:10:233::/116 status: conditions: - lastTransitionTime: \"2024-01-28T06:38:55Z\" message: Kubernetes Service CIDR is ready reason: \"\" status: \"True\" type: Ready ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-01-25T08:41:50Z\" finalizers: - spiderpool.spidernet.io generation: 1 name: default resourceVersion: \"41645\" uid: d1e095db-d6e8-4413-b60e-fcf31ad2bf5e spec: detectGateway: false detectIPConflict: false hijackCIDR: - 10 .244.64.0/18 - fd00:10:244::/112 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: auto podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true txQueueLen: 0 status: phase: Synced serviceCIDR: - 10 .233.0.0/18 - fd00:10:233::/116","title":"Automatically get the CIDR of a clustered Service"},{"location":"concepts/coordinator/#known-issues","text":"Underlay mode: TCP communication between underlay Pods and overlay Pods (Calico or Cilium) fails This issue arises from inconsistent packet routing paths. Request packets are matched with the routing on the source Pod side and forwarded through veth0 to the host side. And then the packets are further forwarded to the target Pod. The target Pod perceives the source IP of the packet as the underlay IP of the source Pod, allowing it to bypass the source Pod's host and directly route through the underlay network. However, on the host, this is considered an invalid packet (as it receives unexpected TCP SYN-ACK packets that are conntrack table invalid), explicitly dropping it using an iptables rule in kube-proxy. Switching the kube-proxy mode to ipvs can address this issue. This issue is expected to be fixed in K8s 1.29. if the sysctl nf_conntrack_tcp_be_liberal is set to 1, kube-proxy will not deliver the DROP rule. Overlay mode: with Cilium as the default CNI and multiple NICs for the Pod, the underlay interface of the Pod cannot communicate with the node. Macvlan interfaces do not allow direct communication between parent and child interfaces in bridge mode in most cases. To facilitate communication between the underlay IP of the Pod and the node, we rely on the default CNI to create Veth devices. However, Cilium restricts the forwarding of IPs from non-Cilium subnets through these Veth devices.","title":"Known issues"},{"location":"concepts/io-performance-zh_CN/","text":"\u7f51\u7edc IO \u6027\u80fd \u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u642d\u914d Macvlan\u3001SR-IOV\u3001IPvlan \u53ef\u4ee5\u5b9e\u73b0\u4e00\u5957\u5b8c\u6574\u7684\u7f51\u7edc\u65b9\u6848\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684\u7f51\u7edc CNI \u63d2\u4ef6\uff08\u5982 cilium \uff0c calico \uff09 \u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u7f51\u7edc \u5ef6\u65f6 \u548c \u541e\u5410\u91cf \u73af\u5883 \u672c\u6b21\u6d4b\u8bd5\u5305\u542b\u5404\u79cd\u573a\u666f\u7684\u6027\u80fd\u57fa\u51c6\u6570\u636e\u3002\u6240\u6709\u6d4b\u8bd5\u5747\u901a\u8fc7\u5728 10 Gbit/s \u7f51\u7edc\u63a5\u53e3\u7684\u4e24\u4e2a\u4e0d\u540c\u88f8\u673a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u4e4b\u95f4\u6267\u884c\u3002 Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi \u6d4b\u8bd5\u5bf9\u8c61 \u672c\u6b21\u6d4b\u8bd5\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684 Calico \u3001 Cilium \u4e24\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u5982\u4e0b\u662f\u76f8\u5173\u7684\u7248\u672c\u7b49\u4fe1\u606f\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u8bf4\u660e Spiderpool based macvlan datapath Spiderpool \u7248\u672c v0.8.0 Calico Calico \u7248\u672c v3.26.1\uff0c\u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053 Cilium Cilium \u7248\u672c v1.14.3\uff0c\u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053 sockperf \u7f51\u7edc\u5ef6\u65f6\u6d4b\u8bd5 Sockperf \u662f\u4e00\u4e2a\u7f51\u7edc\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cf\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5b83\u5141\u8bb8\u60a8\u901a\u8fc7\u6d4b\u8bd5\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u5ef6\u8fdf\u6765\u8bc4\u4f30\u7f51\u7edc\u7684\u6027\u80fd\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b83\u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.3 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 29.1 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 24.3 usec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 26.2 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.9 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 30.2 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 36.8 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 27.7 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec netperf \u6027\u80fd\u6d4b\u8bd5 netperf \u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u7f51\u7edc\u6027\u80fd\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4f8b\u5982\u541e\u5410\u91cf\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 netperf \u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service \u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9985.7 \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17571.3 \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 19793.9 \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 19215.2 \u8282\u70b9\u5230\u8282\u70b9 47560.5 \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9782.2 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17236.5 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 16002.3 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 18992.9 rps \u8282\u70b9\u5230\u8282\u70b9 47560.5 rps iperf \u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5 iperf \u662f\u4e00\u79cd\u6d41\u884c\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u7f51\u7edc\u5e26\u5bbd\u3002\u5b83\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u8fde\u63a5\u7684\u5e26\u5bbd\u548c\u6027\u80fd\u3002\u5728\u672c\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5b83\u5206\u522b\u6765\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <Pod IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <cluster IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec redis-benchmark \u6027\u80fd\u6d4b\u8bd5 redis-benchmark \u65e8\u5728\u901a\u8fc7\u6a21\u62df\u591a\u4e2a\u5ba2\u6237\u7aef\u5e76\u6267\u884c\u5404\u79cd Redis \u547d\u4ee4\u6765\u6d4b\u91cf Redis \u670d\u52a1\u5668\u7684\u6027\u80fd\u548c\u541e\u5410\u91cf\u3002\u6211\u4eec\u901a\u8fc7 redis-benchmark \u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee\u90e8\u7f72\u4e86 Redis \u670d\u52a1\u7684 Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 45682.96 rps 46992.48 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 59737.16 rps 59988.00 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 66357.00 rps 66800.27 rps \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 67444.45 rps 67783.67 rps \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 46082.95 rps 46728.97 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 60496.07 rps 58927.52 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 45578.85 rps 46274.87 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 63211.12 rps 64061.50 rps \u540c\u8282\u70b9 eBPF \u52a0\u901f\u6d4b\u8bd5 Spiderpool \u501f\u52a9\u4e8e istio-tcpip-bypass \u9879\u76ee\uff0c\u53ef\u4ee5\u5b9e\u73b0\u540c\u8282\u70b9\u901a\u4fe1\u52a0\u901f\u3002\u5728\u96c6\u7fa4\u7684\u4e00\u4e2a\u8282\u70b9\u8fd0\u884c\u8be5\u670d\u52a1\uff0c\u53e6\u4e00\u4e2a\u8282\u70b9\u4e0d\u8fd0\u884c\uff0c\u5728\u540c\u8282\u70b9 Pod \u4e4b\u95f4\u901a\u8fc7 Sockperf \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u8282\u70b9\u542f\u7528 eBPF \u52a0\u901f 7.643 usec \u8282\u70b9\u672a\u542f\u7528 eBPF \u52a0\u901f 17.335 usec \u603b\u7ed3 Spiderpool \u505a\u4e3a Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u5176 IO \u6027\u80fd\u5728\u5927\u90e8\u5206\u573a\u666f\u4e0b\u90fd\u9886\u5148\u4e8e Calico\u3001Cilium\u3002","title":"\u7f51\u7edc IO \u6027\u80fd"},{"location":"concepts/io-performance-zh_CN/#io","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u642d\u914d Macvlan\u3001SR-IOV\u3001IPvlan \u53ef\u4ee5\u5b9e\u73b0\u4e00\u5957\u5b8c\u6574\u7684\u7f51\u7edc\u65b9\u6848\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684\u7f51\u7edc CNI \u63d2\u4ef6\uff08\u5982 cilium \uff0c calico \uff09 \u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u7f51\u7edc \u5ef6\u65f6 \u548c \u541e\u5410\u91cf","title":"\u7f51\u7edc IO \u6027\u80fd"},{"location":"concepts/io-performance-zh_CN/#_1","text":"\u672c\u6b21\u6d4b\u8bd5\u5305\u542b\u5404\u79cd\u573a\u666f\u7684\u6027\u80fd\u57fa\u51c6\u6570\u636e\u3002\u6240\u6709\u6d4b\u8bd5\u5747\u901a\u8fc7\u5728 10 Gbit/s \u7f51\u7edc\u63a5\u53e3\u7684\u4e24\u4e2a\u4e0d\u540c\u88f8\u673a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u4e4b\u95f4\u6267\u884c\u3002 Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi","title":"\u73af\u5883"},{"location":"concepts/io-performance-zh_CN/#_2","text":"\u672c\u6b21\u6d4b\u8bd5\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684 Calico \u3001 Cilium \u4e24\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u5982\u4e0b\u662f\u76f8\u5173\u7684\u7248\u672c\u7b49\u4fe1\u606f\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u8bf4\u660e Spiderpool based macvlan datapath Spiderpool \u7248\u672c v0.8.0 Calico Calico \u7248\u672c v3.26.1\uff0c\u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053 Cilium Cilium \u7248\u672c v1.14.3\uff0c\u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053","title":"\u6d4b\u8bd5\u5bf9\u8c61"},{"location":"concepts/io-performance-zh_CN/#sockperf","text":"Sockperf \u662f\u4e00\u4e2a\u7f51\u7edc\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cf\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5b83\u5141\u8bb8\u60a8\u901a\u8fc7\u6d4b\u8bd5\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u5ef6\u8fdf\u6765\u8bc4\u4f30\u7f51\u7edc\u7684\u6027\u80fd\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b83\u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.3 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 29.1 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 24.3 usec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 26.2 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.9 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 30.2 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 36.8 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 27.7 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec","title":"sockperf \u7f51\u7edc\u5ef6\u65f6\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#netperf","text":"netperf \u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u7f51\u7edc\u6027\u80fd\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4f8b\u5982\u541e\u5410\u91cf\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 netperf \u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service \u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9985.7 \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17571.3 \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 19793.9 \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 19215.2 \u8282\u70b9\u5230\u8282\u70b9 47560.5 \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9782.2 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17236.5 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 16002.3 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 18992.9 rps \u8282\u70b9\u5230\u8282\u70b9 47560.5 rps","title":"netperf \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#iperf","text":"iperf \u662f\u4e00\u79cd\u6d41\u884c\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u7f51\u7edc\u5e26\u5bbd\u3002\u5b83\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u8fde\u63a5\u7684\u5e26\u5bbd\u548c\u6027\u80fd\u3002\u5728\u672c\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5b83\u5206\u522b\u6765\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <Pod IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <cluster IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec","title":"iperf \u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#redis-benchmark","text":"redis-benchmark \u65e8\u5728\u901a\u8fc7\u6a21\u62df\u591a\u4e2a\u5ba2\u6237\u7aef\u5e76\u6267\u884c\u5404\u79cd Redis \u547d\u4ee4\u6765\u6d4b\u91cf Redis \u670d\u52a1\u5668\u7684\u6027\u80fd\u548c\u541e\u5410\u91cf\u3002\u6211\u4eec\u901a\u8fc7 redis-benchmark \u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee\u90e8\u7f72\u4e86 Redis \u670d\u52a1\u7684 Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 45682.96 rps 46992.48 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 59737.16 rps 59988.00 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 66357.00 rps 66800.27 rps \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 67444.45 rps 67783.67 rps \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 46082.95 rps 46728.97 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 60496.07 rps 58927.52 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 45578.85 rps 46274.87 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 63211.12 rps 64061.50 rps","title":"redis-benchmark \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#ebpf","text":"Spiderpool \u501f\u52a9\u4e8e istio-tcpip-bypass \u9879\u76ee\uff0c\u53ef\u4ee5\u5b9e\u73b0\u540c\u8282\u70b9\u901a\u4fe1\u52a0\u901f\u3002\u5728\u96c6\u7fa4\u7684\u4e00\u4e2a\u8282\u70b9\u8fd0\u884c\u8be5\u670d\u52a1\uff0c\u53e6\u4e00\u4e2a\u8282\u70b9\u4e0d\u8fd0\u884c\uff0c\u5728\u540c\u8282\u70b9 Pod \u4e4b\u95f4\u901a\u8fc7 Sockperf \u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u8282\u70b9\u542f\u7528 eBPF \u52a0\u901f 7.643 usec \u8282\u70b9\u672a\u542f\u7528 eBPF \u52a0\u901f 17.335 usec","title":"\u540c\u8282\u70b9 eBPF \u52a0\u901f\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#_3","text":"Spiderpool \u505a\u4e3a Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u5176 IO \u6027\u80fd\u5728\u5927\u90e8\u5206\u573a\u666f\u4e0b\u90fd\u9886\u5148\u4e8e Calico\u3001Cilium\u3002","title":"\u603b\u7ed3"},{"location":"concepts/io-performance/","text":"Network I/O Performance English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used with Macvlan, SR-IOV, and IPvlan to implement a complete network solution. This article will compare it with the mainstream network CNI plug-ins on the market ( Such as cilium , calico ) Network Latency and Throughput in various scenarios ENV This test contains performance benchmark data for various scenarios. All tests were performed between containers running on two different bare metal nodes with 10 Gbit/s network interfaces. Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi Test object This test uses macvlan with Spiderpool as the test solution, and selected Calico , Cilium For comparison, two common network solutions are as follows. The following is the relevant version and other information: Test object illustrate Spiderpool based macvlan datapath Spiderpool version v0.8.0 Calico Calico version v3.26.1, based on iptables datapath and no tunnels Cilium Cilium version v1.14.3, based on full eBPF acceleration and no tunneling sockperf network latency test Sockperf is a network benchmarking tool that can be used to measure network latency. It allows you to evaluate the performance of your network by testing the latency between two endpoints. We can use it to separately test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod latency testing for Pod IP purposes. Use sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the Pod IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.3 usec Cilium based on full eBPF acceleration and no tunneling 29.1 usec Spiderpool Pod on the same subnet based on macvlan 24.3 usec Spiderpool Pod across subnets based on macvlan 26.2 usec node to node 32.2 usec Cross-node Pod latency test for cluster IP purpose. Use sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the cluster IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.9 usec Cilium based on full eBPF acceleration and no tunneling 30.2 usec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 36.8 usec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 27.7 usec node to node 32.2 usec netperf performance test netperf is a widely used network performance testing tool that allows you to measure various aspects of network performance, such as throughput. We can use netperf to test Pod's cross-node access to Pod and Service respectively. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Netperf testing of cross-node Pods for Pod IP purposes. Use netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pod access to Pod IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9985.7 Cilium based on full eBPF acceleration and no tunneling 17571.3 Spiderpool Pod on the same subnet based on macvlan 19793.9 Spiderpool Pod across subnets based on macvlan 19215.2 node to node 47560.5 Netperf testing across node Pods for cluster IP purposes. Use netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pods accessing the cluster IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9782.2 Cilium based on full eBPF acceleration and no tunneling 17236.5 Spiderpool Pod based on macvlan on the same subnet and kube-proxy 16002.3 Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 18992.9 node to node 47560.5 iperf network performance test iperf is a popular network performance testing tool that allows you to measure network bandwidth between two endpoints. It is widely used to evaluate the bandwidth and performance of network connections. In this chapter, we use it to test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . iperf testing of cross-node Pods for Pod IP purposes. Use iperf3 -c <Pod IP> -d -P 1 to test the performance of cross-node Pod access to Pod IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec Spiderpool Pod on the same subnet based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec Spiderpool Pod across subnets based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec iperf testing of cross-node Pods for cluster IP purposes. Use iperf3 -c <cluster IP> -d -P 1 to test the performance of cross-node Pod access to cluster IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec redis-benchmark performance test redis-benchmark is designed to measure the performance and throughput of a Redis server by simulating multiple clients and executing various Redis commands. We used redis-benchmark to test Pod's cross-node access to the Pod and Service where the Redis service is deployed. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod redis-benchmark testing based on Pod IP. Use redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to Pod IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 45682.96 rps 46992.48 rps Cilium based on full eBPF acceleration and no tunneling 59737.16 rps 59988.00 rps Spiderpool Pod on the same subnet based on macvlan 66357.00 rps 66800.27 rps Spiderpool Pod across subnets based on macvlan 67444.45 rps 67783.67 rps Cross-node Pod redis-benchmark testing for cluster IP purposes. Use redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to cluster IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 46082.95 rps 46728.97 rps Cilium based on full eBPF acceleration and no tunneling 60496.07 rps 58927.52 rps Spiderpool Pod based on macvlan on the same subnet and kube-proxy 45578.85 rps 46274.87 rps Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 63211.12 rps 64061.50 rps Same node eBPF acceleration test Spiderpool can achieve same-node communication acceleration with the help of the istio-tcpip-bypass project. Run the service on one node of the cluster and not on the other node. Conduct a performance test through Sockperf between Pods on the same node. The data is as follows. Test object latency Node enables eBPF acceleration 7.643 usec Node is not enabled for eBPF acceleration 17.335 usec Summary When Spiderpool is used as an underlay network solution, its IO performance is ahead of Calico and Cilium in most scenarios.","title":"I/O Performance"},{"location":"concepts/io-performance/#network-io-performance","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used with Macvlan, SR-IOV, and IPvlan to implement a complete network solution. This article will compare it with the mainstream network CNI plug-ins on the market ( Such as cilium , calico ) Network Latency and Throughput in various scenarios","title":"Network I/O Performance"},{"location":"concepts/io-performance/#env","text":"This test contains performance benchmark data for various scenarios. All tests were performed between containers running on two different bare metal nodes with 10 Gbit/s network interfaces. Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi","title":"ENV"},{"location":"concepts/io-performance/#test-object","text":"This test uses macvlan with Spiderpool as the test solution, and selected Calico , Cilium For comparison, two common network solutions are as follows. The following is the relevant version and other information: Test object illustrate Spiderpool based macvlan datapath Spiderpool version v0.8.0 Calico Calico version v3.26.1, based on iptables datapath and no tunnels Cilium Cilium version v1.14.3, based on full eBPF acceleration and no tunneling","title":"Test object"},{"location":"concepts/io-performance/#sockperf-network-latency-test","text":"Sockperf is a network benchmarking tool that can be used to measure network latency. It allows you to evaluate the performance of your network by testing the latency between two endpoints. We can use it to separately test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod latency testing for Pod IP purposes. Use sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the Pod IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.3 usec Cilium based on full eBPF acceleration and no tunneling 29.1 usec Spiderpool Pod on the same subnet based on macvlan 24.3 usec Spiderpool Pod across subnets based on macvlan 26.2 usec node to node 32.2 usec Cross-node Pod latency test for cluster IP purpose. Use sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the cluster IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.9 usec Cilium based on full eBPF acceleration and no tunneling 30.2 usec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 36.8 usec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 27.7 usec node to node 32.2 usec","title":"sockperf network latency test"},{"location":"concepts/io-performance/#netperf-performance-test","text":"netperf is a widely used network performance testing tool that allows you to measure various aspects of network performance, such as throughput. We can use netperf to test Pod's cross-node access to Pod and Service respectively. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Netperf testing of cross-node Pods for Pod IP purposes. Use netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pod access to Pod IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9985.7 Cilium based on full eBPF acceleration and no tunneling 17571.3 Spiderpool Pod on the same subnet based on macvlan 19793.9 Spiderpool Pod across subnets based on macvlan 19215.2 node to node 47560.5 Netperf testing across node Pods for cluster IP purposes. Use netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pods accessing the cluster IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9782.2 Cilium based on full eBPF acceleration and no tunneling 17236.5 Spiderpool Pod based on macvlan on the same subnet and kube-proxy 16002.3 Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 18992.9 node to node 47560.5","title":"netperf performance test"},{"location":"concepts/io-performance/#iperf-network-performance-test","text":"iperf is a popular network performance testing tool that allows you to measure network bandwidth between two endpoints. It is widely used to evaluate the bandwidth and performance of network connections. In this chapter, we use it to test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . iperf testing of cross-node Pods for Pod IP purposes. Use iperf3 -c <Pod IP> -d -P 1 to test the performance of cross-node Pod access to Pod IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec Spiderpool Pod on the same subnet based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec Spiderpool Pod across subnets based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec iperf testing of cross-node Pods for cluster IP purposes. Use iperf3 -c <cluster IP> -d -P 1 to test the performance of cross-node Pod access to cluster IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec","title":"iperf network performance test"},{"location":"concepts/io-performance/#redis-benchmark-performance-test","text":"redis-benchmark is designed to measure the performance and throughput of a Redis server by simulating multiple clients and executing various Redis commands. We used redis-benchmark to test Pod's cross-node access to the Pod and Service where the Redis service is deployed. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod redis-benchmark testing based on Pod IP. Use redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to Pod IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 45682.96 rps 46992.48 rps Cilium based on full eBPF acceleration and no tunneling 59737.16 rps 59988.00 rps Spiderpool Pod on the same subnet based on macvlan 66357.00 rps 66800.27 rps Spiderpool Pod across subnets based on macvlan 67444.45 rps 67783.67 rps Cross-node Pod redis-benchmark testing for cluster IP purposes. Use redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to cluster IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 46082.95 rps 46728.97 rps Cilium based on full eBPF acceleration and no tunneling 60496.07 rps 58927.52 rps Spiderpool Pod based on macvlan on the same subnet and kube-proxy 45578.85 rps 46274.87 rps Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 63211.12 rps 64061.50 rps","title":"redis-benchmark performance test"},{"location":"concepts/io-performance/#same-node-ebpf-acceleration-test","text":"Spiderpool can achieve same-node communication acceleration with the help of the istio-tcpip-bypass project. Run the service on one node of the cluster and not on the other node. Conduct a performance test through Sockperf between Pods on the same node. The data is as follows. Test object latency Node enables eBPF acceleration 7.643 usec Node is not enabled for eBPF acceleration 17.335 usec","title":"Same node eBPF acceleration test"},{"location":"concepts/io-performance/#summary","text":"When Spiderpool is used as an underlay network solution, its IO performance is ahead of Calico and Cilium in most scenarios.","title":"Summary"},{"location":"concepts/ipam-des-zh_CN/","text":"IPAM \u7b80\u4f53\u4e2d\u6587 | English Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u7684 IPAM \u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff1a\"Overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"Underlay \u7f51\u7edc\u65b9\u6848\"\u3002\u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 Spiderpool \u662f\u4e3a Underlay \u7f51\u7edc\u7279\u70b9\u800c\u8bbe\u8ba1\uff0c\u4ee5\u4e0b\u5bf9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 Overlay \u7f51\u7edc\u65b9\u6848 IPAM \u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u7f51\u7edc\u540c\u5bbf\u4e3b\u673a\u7f51\u7edc\u7684\u89e3\u8026\uff0c\u4f8b\u5982 Calico \u3001 Cilium \u7b49 CNI \u63d2\u4ef6\uff0c\u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u8fd9\u7c7b\u6280\u672f\u6d41\u6d3e\u7684 IPAM \u5206\u914d\u7279\u70b9\u662f\uff1a Pod \u5b50\u7f51\u4e2d\u7684 IP \u5730\u5740\u6309\u7167\u8282\u70b9\u8fdb\u884c\u4e86\u5206\u5272 \u4ee5\u4e00\u4e2a\u66f4\u5c0f\u5b50\u7f51\u63a9\u7801\u957f\u5ea6\u4e3a\u5355\u4f4d\uff0c\u628a Pod subnet \u5206\u5272\u51fa\u66f4\u5c0f\u7684 IP block \u96c6\u5408\uff0c\u4f9d\u636e IP \u4f7f\u7528\u7684\u7528\u91cf\u60c5\u51b5\uff0c\u6bcf\u4e2a node \u90fd\u4f1a\u83b7\u53d6\u5230\u4e00\u4e2a\u6216\u8005\u591a\u4e2a IP block\u3002 \u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u7279\u70b9\uff1a\u7b2c\u4e00\uff0c\u6bcf\u4e2a node \u4e0a\u7684 IPAM \u63d2\u4ef6\u53ea\u9700\u8981\u5728\u672c\u5730\u7684 IP block \u4e2d\u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u65f6\uff0c\u4e0e\u5176\u5b83 node \u4e0a\u7684 IPAM \u65e0 IP \u5206\u914d\u51b2\u7a81\uff0cIPAM \u5206\u914d\u6548\u7387\u66f4\u9ad8\u3002\u7b2c\u4e8c\uff0c\u67d0\u4e2a\u5177\u4f53\u7684 IP \u5730\u5740\u8ddf\u968f IP block \u96c6\u5408\uff0c\u4f1a\u76f8\u5bf9\u56fa\u5b9a\u7684\u4e00\u76f4\u5728\u67d0\u4e2a node \u4e0a\u88ab\u5206\u914d\uff0c\u6ca1\u6cd5\u968f\u540c Pod \u4e00\u8d77\u88ab\u8c03\u5ea6\u6f02\u79fb\u3002 IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b \u53ea\u8981 Pod \u5b50\u7f51\u4e0d\u4e0e\u76f8\u5173\u7f51\u7edc\u91cd\u53e0\uff0c\u518d\u80fd\u591f\u5408\u7406\u5229\u7528 NAT \u6280\u672f\uff0cKubernetes \u5355\u4e2a\u96c6\u7fa4\u53ef\u4ee5\u62e5\u6709\u5145\u6c9b\u7684 IP \u5730\u5740\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u5e94\u7528\u4e0d\u4f1a\u56e0\u4e3a IP \u4e0d\u591f\u800c\u542f\u52a8\u5931\u8d25\uff0cIPAM \u7ec4\u4ef6\u9762\u4e34\u7684\u5f02\u5e38 IP \u56de\u6536\u538b\u529b\u8f83\u5c0f\u3002 \u6ca1\u6709\u5e94\u7528 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42 \u5bf9\u4e8e\u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6709\u65e0\u72b6\u6001\u5e94\u7528\u548c\u6709\u72b6\u6001\u5e94\u7528\u7684\u533a\u522b\uff1a\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\" \u7684\u9700\u6c42\uff0c\u53ea\u80fd\u8ba9\u6240\u6709 Pod \u526f\u672c\u56fa\u5b9a\u5728\u4e00\u4e2a IP \u5730\u5740\u7684\u96c6\u5408\u5185\uff1b\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod name \u7b49\u4fe1\u606f\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u6709\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\uff0c\u8981\u5b9e\u73b0\u5355\u4e2a Pod \u548c\u5177\u4f53 IP \u5730\u5740\u7684\u5f3a\u7ed1\u5b9a\u3002 \u5728 \"Overlay \u7f51\u7edc\u65b9\u6848\"\u65b9\u6848\u4e0b\uff0c\u591a\u662f\u501f\u52a9\u4e86 NAT \u6280\u672f\u5411\u96c6\u7fa4\u5916\u90e8\u66b4\u9732\u670d\u52a1\u7684\u5165\u53e3\u548c\u6e90\u5730\u5740\uff0c\u501f\u52a9 DNS\u3001clusterIP \u7b49\u6280\u672f\u6765\u5b9e\u73b0\u96c6\u7fa4\u4e1c\u897f\u5411\u901a\u4fe1\u3002\u5176\u6b21\uff0cIPAM \u7684 IP block \u65b9\u5f0f\u628a IP \u76f8\u5bf9\u56fa\u5b9a\u5230\u67d0\u4e2a\u8282\u70b9\u4e0a\uff0c\u800c\u4e0d\u80fd\u4fdd\u8bc1\u5e94\u7528\u526f\u672c\u7684\u8ddf\u968f\u8c03\u5ea6\u3002\u56e0\u6b64\uff0c\u5e94\u7528\u7684 \"IP \u5730\u5740\u56fa\u5b9a\"\u80fd\u529b\u65e0\u7528\u6b66\u4e4b\u5730\uff0c\u5f53\u524d\u793e\u533a\u7684\u4e3b\u6d41 CNI \u591a\u6570\u4e0d\u652f\u6301 \"IP \u5730\u5740\u56fa\u5b9a\"\uff0c\u6216\u8005\u652f\u6301\u65b9\u6cd5\u8f83\u4e3a\u7b80\u964b\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u70b9\u662f\uff0c\u65e0\u8bba\u96c6\u7fa4\u90e8\u7f72\u5728\u4ec0\u4e48\u6837\u7684\u5e95\u5c42\u7f51\u7edc\u73af\u5883\u4e0a\uff0cCNI \u63d2\u4ef6\u7684\u517c\u5bb9\u6027\u90fd\u975e\u5e38\u597d\uff0c\u4e14\u90fd\u80fd\u591f\u4e3a Pod \u63d0\u4f9b\u5b50\u7f51\u72ec\u7acb\u3001IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b\u7684\u7f51\u7edc\u3002 Underlay \u7f51\u7edc\u65b9\u6848 IPAM \u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u5171\u4eab\u5bbf\u4e3b\u673a\u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5373 Pod \u76f4\u63a5\u83b7\u53d6\u5bbf\u4e3b\u673a\u7f51\u7edc\u4e2d\u7684 IP \u5730\u5740\u3002\u8fd9\u6837\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 Underlay \u7f51\u7edc\u65b9\u6848\u7684\u5b9e\u65bd\uff0c\u6709\u4e24\u79cd\u5178\u578b\u7684\u573a\u666f\uff1a\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5b9e\u65bd\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u4e0a\uff1b\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5728 IAAS \u73af\u5883\u4e0a\uff0c\u4f8b\u5982\u516c\u6709\u4e91\u3002\u4ee5\u4e0b\u603b\u7ed3\u4e86\"\u4f20\u7edf\u7f51\u7edc\u573a\u666f\"\u7684 IPAM \u7279\u70b9\uff1a \u5355\u4e2a IP \u5730\u5740\u5e94\u8be5\u80fd\u591f\u5728\u4efb\u4e00\u8282\u70b9\u4e0a\u88ab\u5206\u914d \u8fd9\u4e2a\u9700\u6c42\u6709\u591a\u65b9\u9762\u7684\u539f\u56e0\uff1a\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u8bbe\u5907\u589e\u52a0\u3001\u591a\u96c6\u7fa4\u6280\u672f\u7684\u53d1\u5c55\uff0cIPv4 \u5730\u5740\u8d44\u6e90\u7a00\u7f3a\uff0c\u8981\u6c42 IPAM \u63d0\u9ad8 IP \u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\uff1b\u5bf9\u4e8e\u6709 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5176 Pod \u526f\u672c\u53ef\u80fd\u4f1a\u8c03\u5ea6\u5230\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u8fd8\u4f1a\u53d1\u751f\u8282\u70b9\u95f4\u7684\u6f02\u79fb\uff0c\u8981\u6c42 IP \u5730\u5740\u4e00\u8d77\u6f02\u79fb\u3002 \u56e0\u6b64\uff0c\u5728\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4e00\u4e2a IP \u5730\u5740\u5e94\u8be5\u5177\u5907\u80fd\u591f\u88ab\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684\u53ef\u80fd\u3002 \u540c\u4e00\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\uff0c\u80fd\u5b9e\u73b0\u8de8\u5b50\u7f51\u83b7\u53d6 IP \u5730\u5740 \u4f8b\u5982\uff0c\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u5bbf\u4e3b\u673a1\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.1.0/24\uff0c\u800c\u5bbf\u4e3b\u673a2\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.2.0/24\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\u3002 \u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a \u5f88\u591a\u4f20\u7edf\u5e94\u7528\u5728\u4e91\u5316\u6539\u9020\u524d\uff0c\u662f\u90e8\u7f72\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u7684\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u7f51\u7edc\u672a\u5f15\u5165 NAT \u5730\u5740\u8f6c\u6362\uff0c\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u9700\u8981\u611f\u77e5\u5bf9\u65b9\u7684\u6e90 IP \u6216\u76ee\u7684 IP\uff0c\u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u4e5f\u4e60\u60ef\u4e86\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u6765\u7cbe\u7ec6\u7ba1\u63a7\u7f51\u7edc\u5b89\u5168\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0a\u4e91\u540e\uff0c\u65e0\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u8303\u56f4\u7684\u56fa\u5b9a\uff0c\u6709\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u5730\u5740\u7684\u552f\u4e00\u5bf9\u5e94\uff0c\u8fd9\u6837\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u6539\u9020\u5de5\u4f5c\u3002 \u4e00\u4e2a Pod \u7684\u591a\u7f51\u5361\u83b7\u53d6\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740 \u65e2\u7136\u662f\u5bf9\u63a5 Underlay \u7f51\u7edc\uff0cPod \u5c31\u4f1a\u6709\u591a\u7f51\u5361\u9700\u6c42\uff0c\u4ee5\u4f7f\u5176\u901a\u8fbe\u4e0d\u540c\u7684 Underlay \u5b50\u7f51\uff0c\u8fd9\u8981\u6c42 IPAM \u80fd\u591f\u7ed9\u5e94\u7528\u7684\u4e0d\u540c\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u4e0b\u7684 IP \u5730\u5740\u3002 IP \u5730\u5740\u51b2\u7a81 \u5728 Underlay \u7f51\u7edc\u4e2d\uff0c\u66f4\u52a0\u5bb9\u6613\u51fa\u73b0 IP \u51b2\u7a81\uff0c\u4f8b\u5982\uff0cPod \u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u4e3b\u673a IP \u53d1\u751f\u4e86\u51b2\u7a81\uff0c\u4e0e\u5176\u5b83\u5bf9\u63a5\u4e86\u76f8\u540c\u5b50\u7f51\u7684\u96c6\u7fa4\u51b2\u7a81\uff0c\u800c IPAM \u7ec4\u4ef6\u5f88\u96be\u611f\u77e5\u5916\u90e8\u8fd9\u4e9b\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u591a\u9700\u8981\u501f\u52a9 CNI \u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3002 \u5df2\u7528 IP \u5730\u5740\u7684\u91ca\u653e\u56de\u6536 \u56e0\u4e3a Underlay \u7f51\u7edc IP \u5730\u5740\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5e94\u7528\u6709 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6240\u4ee5\uff0c\"\u5e94\u5f53\"\u88ab\u91ca\u653e\u7684 IP \u5730\u5740\u82e5\u672a\u88ab IPAM \u7ec4\u4ef6\u56de\u6536\uff0c\u65b0\u542f\u52a8\u7684 Pod \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7f3a\u5c11 IP \u5730\u5740\u800c\u5931\u8d25\u3002\u8fd9\u5c31\u8981\u6c42 IPAM \u7ec4\u4ef6\u62e5\u6709\u66f4\u52a0\u7cbe\u51c6\u3001\u9ad8\u6548\u3001\u53ca\u65f6\u7684 IP \u56de\u6536\u673a\u5236\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u52bf\u6709\uff1a\u65e0\u9700\u7f51\u7edc NAT \u6620\u5c04\u7684\u5f15\u5165\uff0c\u5bf9\u5e94\u7528\u7684\u4e91\u5316\u7f51\u7edc\u6539\u9020\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u7684\u4fbf\u5229\uff1b\u5e95\u5c42\u7f51\u7edc\u7684\u9632\u706b\u5899\u7b49\u8bbe\u5907\uff0c\u53ef\u5bf9 Pod \u901a\u4fe1\u5b9e\u73b0\u76f8\u5bf9\u8f83\u4e3a\u7cbe\u7ec6\u7684\u7ba1\u63a7\uff1b\u65e0\u9700\u96a7\u9053\u6280\u672f\uff0c\u7f51\u7edc\u901a\u4fe1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u65f6\u6027\u80fd\u4e5f\u76f8\u5bf9\u7684\u63d0\u9ad8\u4e86\u3002 Spiderpool IPAM \u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool IPAM \u63d2\u4ef6\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI , calico CNI , weave CNI Spiderpool IPAM \u4e3b\u8981\u5206\u914d\u4e24\u4e2a\u5927\u6a21\u5757\uff1a\u5206\u914d\u4e0e\u56de\u6536\uff1b\u6574\u4f53\u6d41\u7a0b\u5982\u4e0b\uff1a Pod \u542f\u52a8\uff0cSpiderpool \u4f1a\u68c0\u67e5\u5bf9\u5e94 Pod \u7684 SpiderEndpoint \u5bf9\u8c61\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0cPod \u540d\u79f0\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u4e14\u5e94\u7528\u6709\u56fa\u5b9a IP \u7684\u9700\u6c42\uff0cSpiderpool \u4f1a\u4ecd\u4f7f\u7528 SpiderEndpoint \u4e2d\u7684\u8bb0\u5f55\u7684 IP \u5730\u5740\u4e3a\u5176\u5206\u914d IP\uff1b\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c\u53ea\u80fd\u5c06 IP \u56fa\u5b9a\u5728\u4e00\u5b9a\u8303\u56f4\u5185\uff0cSpiderpool \u4f1a\u901a\u8fc7\u5206\u914d\u7b97\u6cd5\u91cd\u65b0\u5206\u914d IP \u5730\u5740\u7ed9 Pod\uff0c\u540c\u6b65\u53d8\u66f4\u540e\u7684 Pod \u4fe1\u606f\u5230 IP \u6c60\u548c SpiderEndpoint \u4e2d\u3002\u5982\u679c\u4e0d\u5b58\u5728\uff0cSpiderpool \u901a\u8fc7\u5206\u914d\u7b97\u6cd5\uff08\u83b7\u53d6\u5019\u9009\u6c60 -> \u8fc7\u6ee4\u5019\u9009\u6c60 -> \u5019\u9009\u6c60\u6392\u5e8f\uff09\uff0c\u7ed9 Pod \u5206\u914d IP \u5730\u5740\uff0c \u4f1a\u540c\u65f6\u521b\u5efa\u4e00\u4e2a\u5bf9\u5e94 Pod \u7684 SpiderEndpoint \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u4e2d\u8bb0\u5f55\u7740 Pod \u6240\u4f7f\u7528\u7684 IP \u5730\u5740\u3001UID \u7b49\u4fe1\u606f\uff0c\u5b83\u4f1a\u5e26\u4e00\u4e2a finalizer\uff0c\u5e76\u968f Pod \u4e00\u8d77\u8bbe\u7f6e\u4e3a OwnerReference \u3002 Spiderpool \u57fa\u4e8e\u4e24\u79cd\u65b9\u5f0f\u786e\u4fdd IP \u5730\u5740\u7684\u5065\u58ee\u6027\uff0c\u7b2c\u4e00\uff0cSpiderpool \u7684 Informer \u673a\u5236\u8ffd\u8e2a Pod \u7684\u751f\u547d\u5468\u671f\uff0c\u7b2c\u4e8c\uff0c\u5468\u671f\u6027\u7684\u5728\u5168\u5c40\u9ed8\u8ba4\u95f4\u9694\u65f6\u95f4\u5185\u626b\u63cf IPPool \u7684 IP \u72b6\u6001\u3002\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u7531\u4e8e\u5176\u56fa\u5b9a IP \u5730\u5740\u7684\u9700\u6c42\uff0c\u4e14\u53ea\u8981\u5b83\u7684 Pod \u526f\u672c\u662f\u6709\u6548\u7684\uff0cSpiderpool \u4e0d\u4f1a\u56de\u6536\u5176 IP \u5730\u5740\u548c SpiderEndpoint \u5bf9\u8c61\uff0c\u4ee5\u4fbf\u4e8e\u5b83\u80fd\u6301\u7eed\u7684\u83b7\u5f97\u76f8\u540c\u7684 IP \u5730\u5740\u3002\u5bf9\u4e8e\u65e0\u6548\u7684 StatefulSet \u5e94\u7528\u548c Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u5c06\u56de\u6536\u5176 IP \u5730\u5740\u548c SpiderEndpoint \u5bf9\u8c61\u3002 Spiderpool IP \u5206\u914d\u7b97\u6cd5 \u5f53 Pod \u521b\u5efa\u65f6\uff0c\u5b83\u5c06\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u83b7\u53d6 IP \u5206\u914d\uff1bIP \u5206\u914d\u751f\u547d\u5468\u671f\u5c06\u7ecf\u5386 \u83b7\u53d6\u5019\u9009\u6c60 \u3001 \u8fc7\u6ee4\u5019\u9009\u6c60 \u3001 \u5019\u9009\u6c60\u6392\u5e8f \u4e09\u4e2a\u5927\u9636\u6bb5\u3002 \u83b7\u53d6\u5019\u9009\u6c60 \uff1aSpiderpool \u6709\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u53d6 \u9ad8\u4f18\u5148\u7ea7\u89c4\u5219 \u547d\u4e2d\u7684\u6240\u6709\u6c60\uff0c\u5c06\u5b83\u4eec\u6807\u8bb0\u4e3a\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4ee5\u6709\u8d44\u683c\u88ab\u8fdb\u4e00\u6b65\u8003\u8651\u3002 \u8fc7\u6ee4\u5019\u9009\u6c60 \uff1aSpiderpool \u901a\u8fc7\u4eb2\u548c\u6027\u7b49\u8fc7\u6ee4\u673a\u5236\uff0c\u66f4\u7cbe\u786e\u5730\u4ece\u6240\u6709\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5019\u9009\u6c60\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u9700\u6c42\u6216\u590d\u6742\u7684\u4f7f\u7528\u573a\u666f\u3002 \u5019\u9009\u6c60\u6392\u5e8f \uff1a\u5bf9\u4e8e\u591a\u5019\u9009\u6c60\uff0cSpiderpool \u6839\u636e SpiderIPPool \u5bf9\u8c61\u4e2d\u7684\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002 \u83b7\u53d6\u5019\u9009\u6c60 Spiderpool \u63d0\u4f9b\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u5728\u4e3a Pod \u5206\u914d IP \u5730\u5740\u65f6\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\u3002\u4ee5\u4e0b\u89c4\u5219\u6309\u7167\u4ece \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u987a\u5e8f\u5217\u51fa\uff0c\u5982\u679c\u540c\u65f6\u5b58\u5728\u4e0b\u9762\u7684\u591a\u4e2a\u89c4\u5219\uff0c\u524d\u4e00\u4e2a\u89c4\u5219\u5c06 \u8986\u76d6 \u540e\u4e00\u4e2a\u89c4\u5219\u3002 \u4f18\u5148\u7ea7 1 \uff1aSpiderSubnet \u6ce8\u89e3\u3002 SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 IPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002\u521b\u5efa\u5e94\u7528\u65f6\u53ef\u4ee5\u4f7f\u7528 ipam.spidernet.io/subnets \u6216 ipam.spidernet.io/subnet \u6ce8\u89e3\u6307\u5b9a Subnet\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u53d6 IP \u5730\u5740\u81ea\u52a8\u521b\u5efa IP \u6c60\uff0c\u5e76\u4ece\u6c60\u4e2d\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\u7ed9\u5e94\u7528\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderSubnet \u3002 \u4f18\u5148\u7ea7 2 \uff1aSpiderIPPool \u6ce8\u89e3\u3002 \u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u7ed3\u5408 SpiderIPPool \u6ce8\u89e3 ipam.spidernet.io/ippools \u6216 ipam.spidernet.io/ippool \u80fd\u7ed1\u5b9a\u4e0d\u540c\u7684 IPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 IPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderIPPool \u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 3 \uff1a\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 IP \u6c60\u3002 \u901a\u8fc7\u5728\u547d\u540d\u7a7a\u95f4\u4e2d\u8bbe\u7f6e\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u6216 ipam.spidernet.io/default-ipv6-ippool \u6307\u5b9a\u9ed8\u8ba4\u7684 IP \u6c60\u3002\u5728\u8be5\u79df\u6237\u4e2d\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u5982\u679c\u6ca1\u6709\u5176\u4ed6\u9ad8\u4f18\u5148\u7ea7\u7684\u6c60\u89c4\u5219\uff0c\u90a3\u4e48\u5c06\u4ece\u8be5\u79df\u6237\u53ef\u7528\u7684\u5019\u9009\u6c60\u4e2d\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 \u547d\u540d\u7a7a\u95f4\u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 4 \uff1aCNI \u914d\u7f6e\u6587\u4ef6\u3002 \u901a\u8fc7\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 default_ipv4_ippool \u548c default_ipv6_ippool \u5b57\u6bb5\u8bbe\u7f6e\u5168\u5c40\u7684 CNI \u9ed8\u8ba4\u6c60\uff0c\u5176\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0c\u5f53\u5e94\u7528\u4f7f\u7528\u8be5 CNI \u914d\u7f6e\u7f51\u7edc\u65f6\u5e76\u8c03\u7528 Spiderpool \uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5e94\u7528\u526f\u672c\uff0cSpiderpool \u90fd\u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u6bcf\u4e2a\u8282\u70b9\u5206\u5c5e\u4e0d\u540c\u7684\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\u7684\u573a\u666f\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684\u8282\u70b9\u4eb2\u548c\u89c4\u5219\uff0cPod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 CNI \u914d\u7f6e \u3002 \u4f18\u5148\u7ea7 5 \uff1a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\u3002 \u5728 SpiderIPPool CR \u5bf9\u8c61\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c06 spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true \uff0c\u5c06\u6c60\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\uff0c\u9ed8\u8ba4\u4e3a false \u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 \u96c6\u7fa4\u9ed8\u8ba4 IPPool \u8fc7\u6ee4\u5019\u9009\u6c60 \u901a\u8fc7\u4e0a\u8ff0\u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u5f97 IPv4 \u548c IPv6 \u7684 IPPool \u5019\u9009\u540e\uff0cSpiderpool \u4f1a\u6839\u636e\u4ee5\u4e0b\u89c4\u5219\u8fdb\u884c\u8fc7\u6ee4\uff0c\u4e86\u89e3\u54ea\u4e2a\u5019\u9009 IPPool \u53ef\u7528\u3002 IP \u6c60\u5904\u4e8e\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4f46\u5176\u5904\u4e8e terminating \u72b6\u6001\u7684\uff0cSpiderpool \u5c06\u4f1a\u8fc7\u6ee4\u8be5\u6c60\u3002 IP \u6c60\u7684 spec.disable \u5b57\u6bb5\u7528\u4e8e\u8bbe\u7f6e IPPool \u662f\u5426\u53ef\u7528\uff0c\u5f53\u8be5\u503c\u4e3a true \u65f6\uff0c\u610f\u5473\u7740 IPPool \u4e0d\u53ef\u4f7f\u7528\u3002 \u68c0\u67e5 IPPool.Spec.NodeName \u548c IPPool.Spec.NodeAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u8c03\u5ea6\u8282\u70b9\u5339\u914d\u3002 \u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.NamespaceName \u548c IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u547d\u540d\u7a7a\u95f4\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.PodAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684 matchLabels \u6240\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.MultusName \u5c5e\u6027\u662f\u5426\u4e0e Pod \u5f53\u524d NIC Multus \u914d\u7f6e\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab IPPool \u5b9e\u4f8b\u7684 exclude_ips \u5b57\u6bb5\u6240\u5305\u542b\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab ReservedIP \u5b9e\u4f8b\u6240\u4fdd\u7559\u4e86\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 IPPool \u7684\u53ef\u7528 IP \u8d44\u6e90\u88ab\u8017\u5c3d\uff0c\u5219\u8be5 IPPool \u4e5f\u5c06\u88ab\u8fc7\u6ee4\u3002 \u5019\u9009\u6c60\u6392\u5e8f \u8fc7\u6ee4\u5019\u9009\u6c60\u540e\uff0c\u53ef\u80fd\u4ecd\u5b58\u5728\u591a\u4e2a\u5019\u9009\u6c60\uff0cSpiderpool \u4f1a\u8fdb\u4e00\u6b65\u4f7f\u7528\u81ea\u5b9a\u4e49\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002 \u5177\u6709 IPPool.Spec.PodAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u9ad8\u4f18\u5148\u7ea7\u3002 \u5177\u6709 IPPool.Spec.NodeName \u6216 IPPool.Spec.NodeAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e8c\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NodeName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NodeAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.NamespaceName \u6216 IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e09\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NamespaceName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NamespaceAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.MultusName \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u4f4e\u4f18\u5148\u7ea7\u3002 \u6ce8\u610f\uff1a\u8fd9\u91cc\u6709\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u63cf\u8ff0\u8fd9\u4e2a\u89c4\u5219\u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5355\u4e00\u5173\u8054\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolB \u3002 \u5177\u6709\u5355\u4e2a\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.NodeName \u548c IPPool.Spec.NamespaceName \u7684 IPPoolB \u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u3001 IPPool.Spec.NamespaceName \u548c IPPool.Spec.MultusName \u7684 IPPoolB \u3002 NOTE\uff1a \u5982\u679c Pod \u5c5e\u4e8e StatefulSet\uff0c\u5219\u4f1a\u4f18\u5148\u5206\u914d\u7b26\u5408\u4e0a\u9762\u89c4\u5219\u7684 IP \u5730\u5740\u3002 \u4e00\u65e6 Pod \u91cd\u65b0\u542f\u52a8\uff0c\u5b83\u5c06\u5c1d\u8bd5\u91cd\u7528\u6700\u540e\u5206\u914d\u7684 IP \u5730\u5740\u3002 IP \u56de\u6536\u7b97\u6cd5 \u5728 Kubernetes \u4e2d\uff0c\u5783\u573e\u56de\u6536\uff08Garbage Collection\uff0c\u7b80\u79f0GC\uff09\u5bf9\u4e8e IP \u5730\u5740\u7684\u56de\u6536\u81f3\u5173\u91cd\u8981\u3002IP \u5730\u5740\u7684\u53ef\u7528\u6027\u76f4\u63a5\u5f71\u54cd Pod \u662f\u5426\u80fd\u591f\u6210\u529f\u542f\u52a8\u3002\u540c\u65f6 GC \u673a\u5236\u4e5f\u53ef\u4ee5\u81ea\u52a8\u56de\u6536\u4e0d\u518d\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u548c IP \u5730\u5740\u8017\u5c3d\u3002 \u5728 IPAM \u4e2d\u4f1a\u8bb0\u5f55\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684 IP \u4fe1\u606f\uff0c\u4f46\u5f53\u8fd9\u4e9b Pod \u5728 Kubernetes \u96c6\u7fa4\u4e2d\u5df2\u7ecf\u4e0d\u590d\u5b58\u5728\uff0c\u800c\u8fd9\u4e9b\u4ecd\u88ab\u8bb0\u5f55\u5728 IPAM \u4e2d\u7684 IP \u53ef\u79f0\u4e3a \u50f5\u5c38 IP \u3002Spiderpool \u9488\u5bf9 \u50f5\u5c38 IP \u5177\u6709\u5982\u4e0b\u4e24\u79cd\u56de\u6536\u65b9\u5f0f\uff1a \u5b9e\u65f6\u8ffd\u8e2a Pod \u4e8b\u4ef6\uff0c\u5224\u65ad\u662f\u5426\u9700\u8981\u56de\u6536 IP \u5730\u5740\u548c\u5176\u5bf9\u5e94\u7684 SpiderEndpoint \u5bf9\u8c61\u3002 \u57fa\u4e8e\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION \u5b9a\u4e49\u7684\u95f4\u9694\u65f6\u95f4\uff08\u9ed8\u8ba4\u4e3a 10 \u5206\u949f\uff09\u5468\u671f\u6027\u626b\u63cf IP \u6c60\u7684\u5065\u58ee\u6027\u3002 \u4e0a\u8ff0\u5b8c\u5907\u7684 IP \u56de\u6536\u7b97\u6cd5\uff0c\u80fd\u591f\u786e\u4fdd\u6240\u6709\u573a\u666f\u4e0b IP \u5730\u5740\u7684\u6b63\u786e\u56de\u6536\uff0c\u5305\u62ec\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u573a\u666f\uff1a \u5728\u96c6\u7fa4\u4e2d delete Pod \u65f6\uff0c\u7531\u4e8e \u7f51\u7edc\u5f02\u5e38 \u6216 cni \u4e8c\u8fdb\u5236 crash \u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8c03\u7528 cni delete \u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4 IP \u5730\u5740\u65e0\u6cd5\u88ab cni \u56de\u6536\u3002 \u5728 cni delete \u5931\u8d25 \u7b49\u6545\u969c\u573a\u666f\u4e0b\uff0c\u5982\u679c\u4e00\u4e2a\u66fe\u7ecf\u5206\u914d\u4e86 IP \u7684 Pod \u88ab\u9500\u6bc1\uff0c\u4f46\u5728 IPAM \u4e2d\u4ecd\u8bb0\u5f55\u7740 IP \u5730\u5740\uff0c\u5f62\u6210\u50f5\u5c38 IP\u3002Spiderpool \u57fa\u4e8e\u5468\u671f\u548c\u4e8b\u4ef6\u626b\u63cf\u673a\u5236\uff0c\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u50f5\u5c38 IP \u5730\u5740\u3002 \u56e0\u5176\u4ed6\u610f\u5916\u5bfc\u81f4 \u65e0\u72b6\u6001 Pod \u4e00\u76f4\u5904\u4e8e Terminating \u9636\u6bb5\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond + spiderpool-controller ENV SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY \u65f6\u95f4\u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_READY_NODE_ENABLED \u63a7\u5236\uff0c\u89e3\u51b3 \u8282\u70b9\u6b63\u5e38\u4f46 Pod \u5220\u9664\u5931\u8d25 \u7684\u6545\u969c\u573a\u666f\u3002 \u8282\u70b9\u610f\u5916\u5b95\u673a\u540e\uff0c\u96c6\u7fa4\u4e2d\u7684 Pod \u6c38\u4e45\u5904\u4e8e Terminating \u9636\u6bb5\uff0cPod \u5360\u7528\u7684 IP \u5730\u5740\u65e0\u6cd5\u88ab\u91ca\u653e\u3002 \u5bf9\u4e8e\u5904\u4e8e Terminating \u9636\u6bb5\u7684 \u65e0\u72b6\u6001 Pod\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond + spiderpool-controller ENV SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY \u65f6\u95f4\u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_NOT_READY_NODE_ENABLED \u63a7\u5236\uff0c\u89e3\u51b3 \u8282\u70b9\u610f\u5916\u5b95\u673a \u7684\u6545\u969c\u573a\u666f\u3002 IP \u51b2\u7a81\u68c0\u6d4b\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b \u5bf9\u4e8e Underlay \u7f51\u7edc\uff0cIP \u51b2\u7a81\u662f\u65e0\u6cd5\u63a5\u53d7\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u95ee\u9898\u3002Spiderpool \u652f\u6301 IP \u51b2\u7a81\u68c0\u6d4b\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u8be5\u529f\u80fd\u4ee5\u524d\u7531 coordinator \u63d2\u4ef6\u5b9e\u73b0\uff0c\u7531\u4e8e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e00\u4e9b\u6f5c\u5728\u7684\u901a\u4fe1\u95ee\u9898\u3002\u73b0\u5728\u7531 IPAM \u5b8c\u6210\u3002 \u53ef\u901a\u8fc7 spiderpool-conf configMap \u5f00\u542f\u6216\u5173\u95ed: apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: spiderpool data: conf.yml: | ... enableIPConflictDetection: true enableGatewayDetection: true ... \u5f00\u542f IP \u51b2\u7a81\u68c0\u6d4b\u540e\uff0cSpiderpool \u5c06\u4f1a\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u62a5\u6587\u68c0\u6d4b\u5206\u914d\u7684 IP \u5730\u5740\u662f\u5426\u51b2\u7a81\u3002\u5f53\u68c0\u6d4b\u5230\u8be5 IP \u4e0e\u7f51\u6bb5\u5185\u5176\u4ed6 IP \u51b2\u7a81\uff0c\u5c06\u4f1a\u963b\u6b62 Pod \u521b\u5efa\u3002\u652f\u6301 IPv4 \u548c IPv6 \u5f53\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\u5931\u8d25\uff0c\u5c06\u4f1a\u91cd\u8bd5 3 \u6b21\uff0c\u5982\u679c\u90fd\u5931\u8d25\uff0c\u5219\u8fd4\u56de\u9519\u8bef\u3002 \u5f53\u6210\u529f\u53d1\u9001\u63a2\u6d4b\u62a5\u6587\uff0c\u5982\u679c\u5728 100ms \u5185\u6536\u5230\u7b54\u590d\uff0c\u8bf4\u660e\u5b58\u5728 IP \u51b2\u7a81\u3002\u5982\u679c\u63a5\u6536\u9519\u8bef\u5e76\u4e14\u4e3a Network Timeout \u7c7b\u7684\u9519\u8bef\uff0c\u5219\u5224\u65ad\u4e3a\u4e0d\u51b2\u7a81\u3002 \u5f00\u542f\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u540e\uff0cSpiderpool \u5c06\u4f1a\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u62a5\u6587\u68c0\u6d4b Pod \u7684 \u7f51\u5173\u5730\u5740\u662f\u5426\u53ef\u8fbe\u3002\u5982\u679c\u53d1\u73b0\u7f51\u5173\u5730\u5740\u4e0d\u53ef\u8fbe\uff0c\u5c06\u4f1a\u963b\u6b62 Pod \u521b\u5efa\u3002 \u5f53\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\u5931\u8d25\uff0c\u5c06\u4f1a\u91cd\u8bd5 3 \u6b21\uff0c\u5982\u679c\u90fd\u5931\u8d25\uff0c\u5219\u8fd4\u56de\u9519\u8bef\u3002 \u5f53\u6210\u529f\u53d1\u9001\u63a2\u6d4b\u62a5\u6587\uff0c\u5982\u679c\u5728 100ms \u5185\u6536\u5230\u7b54\u590d\uff0c\u8bf4\u660e\u7f51\u5173\u5730\u5740\u53ef\u8fbe\u3002\u5982\u679c\u672a\u6536\u5230\u7b54\u590d\uff0c\u5219\u8bf4\u660e\u7f51\u5173\u5730\u5740\u4e0d\u53ef\u8fbe\u3002 \u6ce8\u610f: \u6709\u4e00\u4e9b\u4ea4\u6362\u673a\u4e0d\u5141\u8bb8\u88ab arp \u63a2\u6d4b\uff0c\u5426\u5219\u4f1a\u53d1\u51fa\u544a\u8b66\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e enableGatewayDetection \u4e3a false\u3002","title":"IPAM"},{"location":"concepts/ipam-des-zh_CN/#ipam","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"IPAM"},{"location":"concepts/ipam-des-zh_CN/#underlay-overlay-ipam","text":"\u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff1a\"Overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"Underlay \u7f51\u7edc\u65b9\u6848\"\u3002\u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 Spiderpool \u662f\u4e3a Underlay \u7f51\u7edc\u7279\u70b9\u800c\u8bbe\u8ba1\uff0c\u4ee5\u4e0b\u5bf9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002","title":"Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u7684 IPAM"},{"location":"concepts/ipam-des-zh_CN/#overlay-ipam","text":"\u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u7f51\u7edc\u540c\u5bbf\u4e3b\u673a\u7f51\u7edc\u7684\u89e3\u8026\uff0c\u4f8b\u5982 Calico \u3001 Cilium \u7b49 CNI \u63d2\u4ef6\uff0c\u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u8fd9\u7c7b\u6280\u672f\u6d41\u6d3e\u7684 IPAM \u5206\u914d\u7279\u70b9\u662f\uff1a Pod \u5b50\u7f51\u4e2d\u7684 IP \u5730\u5740\u6309\u7167\u8282\u70b9\u8fdb\u884c\u4e86\u5206\u5272 \u4ee5\u4e00\u4e2a\u66f4\u5c0f\u5b50\u7f51\u63a9\u7801\u957f\u5ea6\u4e3a\u5355\u4f4d\uff0c\u628a Pod subnet \u5206\u5272\u51fa\u66f4\u5c0f\u7684 IP block \u96c6\u5408\uff0c\u4f9d\u636e IP \u4f7f\u7528\u7684\u7528\u91cf\u60c5\u51b5\uff0c\u6bcf\u4e2a node \u90fd\u4f1a\u83b7\u53d6\u5230\u4e00\u4e2a\u6216\u8005\u591a\u4e2a IP block\u3002 \u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u7279\u70b9\uff1a\u7b2c\u4e00\uff0c\u6bcf\u4e2a node \u4e0a\u7684 IPAM \u63d2\u4ef6\u53ea\u9700\u8981\u5728\u672c\u5730\u7684 IP block \u4e2d\u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u65f6\uff0c\u4e0e\u5176\u5b83 node \u4e0a\u7684 IPAM \u65e0 IP \u5206\u914d\u51b2\u7a81\uff0cIPAM \u5206\u914d\u6548\u7387\u66f4\u9ad8\u3002\u7b2c\u4e8c\uff0c\u67d0\u4e2a\u5177\u4f53\u7684 IP \u5730\u5740\u8ddf\u968f IP block \u96c6\u5408\uff0c\u4f1a\u76f8\u5bf9\u56fa\u5b9a\u7684\u4e00\u76f4\u5728\u67d0\u4e2a node \u4e0a\u88ab\u5206\u914d\uff0c\u6ca1\u6cd5\u968f\u540c Pod \u4e00\u8d77\u88ab\u8c03\u5ea6\u6f02\u79fb\u3002 IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b \u53ea\u8981 Pod \u5b50\u7f51\u4e0d\u4e0e\u76f8\u5173\u7f51\u7edc\u91cd\u53e0\uff0c\u518d\u80fd\u591f\u5408\u7406\u5229\u7528 NAT \u6280\u672f\uff0cKubernetes \u5355\u4e2a\u96c6\u7fa4\u53ef\u4ee5\u62e5\u6709\u5145\u6c9b\u7684 IP \u5730\u5740\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u5e94\u7528\u4e0d\u4f1a\u56e0\u4e3a IP \u4e0d\u591f\u800c\u542f\u52a8\u5931\u8d25\uff0cIPAM \u7ec4\u4ef6\u9762\u4e34\u7684\u5f02\u5e38 IP \u56de\u6536\u538b\u529b\u8f83\u5c0f\u3002 \u6ca1\u6709\u5e94\u7528 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42 \u5bf9\u4e8e\u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6709\u65e0\u72b6\u6001\u5e94\u7528\u548c\u6709\u72b6\u6001\u5e94\u7528\u7684\u533a\u522b\uff1a\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\" \u7684\u9700\u6c42\uff0c\u53ea\u80fd\u8ba9\u6240\u6709 Pod \u526f\u672c\u56fa\u5b9a\u5728\u4e00\u4e2a IP \u5730\u5740\u7684\u96c6\u5408\u5185\uff1b\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod name \u7b49\u4fe1\u606f\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u6709\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\uff0c\u8981\u5b9e\u73b0\u5355\u4e2a Pod \u548c\u5177\u4f53 IP \u5730\u5740\u7684\u5f3a\u7ed1\u5b9a\u3002 \u5728 \"Overlay \u7f51\u7edc\u65b9\u6848\"\u65b9\u6848\u4e0b\uff0c\u591a\u662f\u501f\u52a9\u4e86 NAT \u6280\u672f\u5411\u96c6\u7fa4\u5916\u90e8\u66b4\u9732\u670d\u52a1\u7684\u5165\u53e3\u548c\u6e90\u5730\u5740\uff0c\u501f\u52a9 DNS\u3001clusterIP \u7b49\u6280\u672f\u6765\u5b9e\u73b0\u96c6\u7fa4\u4e1c\u897f\u5411\u901a\u4fe1\u3002\u5176\u6b21\uff0cIPAM \u7684 IP block \u65b9\u5f0f\u628a IP \u76f8\u5bf9\u56fa\u5b9a\u5230\u67d0\u4e2a\u8282\u70b9\u4e0a\uff0c\u800c\u4e0d\u80fd\u4fdd\u8bc1\u5e94\u7528\u526f\u672c\u7684\u8ddf\u968f\u8c03\u5ea6\u3002\u56e0\u6b64\uff0c\u5e94\u7528\u7684 \"IP \u5730\u5740\u56fa\u5b9a\"\u80fd\u529b\u65e0\u7528\u6b66\u4e4b\u5730\uff0c\u5f53\u524d\u793e\u533a\u7684\u4e3b\u6d41 CNI \u591a\u6570\u4e0d\u652f\u6301 \"IP \u5730\u5740\u56fa\u5b9a\"\uff0c\u6216\u8005\u652f\u6301\u65b9\u6cd5\u8f83\u4e3a\u7b80\u964b\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u70b9\u662f\uff0c\u65e0\u8bba\u96c6\u7fa4\u90e8\u7f72\u5728\u4ec0\u4e48\u6837\u7684\u5e95\u5c42\u7f51\u7edc\u73af\u5883\u4e0a\uff0cCNI \u63d2\u4ef6\u7684\u517c\u5bb9\u6027\u90fd\u975e\u5e38\u597d\uff0c\u4e14\u90fd\u80fd\u591f\u4e3a Pod \u63d0\u4f9b\u5b50\u7f51\u72ec\u7acb\u3001IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b\u7684\u7f51\u7edc\u3002","title":"Overlay \u7f51\u7edc\u65b9\u6848 IPAM"},{"location":"concepts/ipam-des-zh_CN/#underlay-ipam","text":"\u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u5171\u4eab\u5bbf\u4e3b\u673a\u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5373 Pod \u76f4\u63a5\u83b7\u53d6\u5bbf\u4e3b\u673a\u7f51\u7edc\u4e2d\u7684 IP \u5730\u5740\u3002\u8fd9\u6837\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 Underlay \u7f51\u7edc\u65b9\u6848\u7684\u5b9e\u65bd\uff0c\u6709\u4e24\u79cd\u5178\u578b\u7684\u573a\u666f\uff1a\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5b9e\u65bd\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u4e0a\uff1b\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5728 IAAS \u73af\u5883\u4e0a\uff0c\u4f8b\u5982\u516c\u6709\u4e91\u3002\u4ee5\u4e0b\u603b\u7ed3\u4e86\"\u4f20\u7edf\u7f51\u7edc\u573a\u666f\"\u7684 IPAM \u7279\u70b9\uff1a \u5355\u4e2a IP \u5730\u5740\u5e94\u8be5\u80fd\u591f\u5728\u4efb\u4e00\u8282\u70b9\u4e0a\u88ab\u5206\u914d \u8fd9\u4e2a\u9700\u6c42\u6709\u591a\u65b9\u9762\u7684\u539f\u56e0\uff1a\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u8bbe\u5907\u589e\u52a0\u3001\u591a\u96c6\u7fa4\u6280\u672f\u7684\u53d1\u5c55\uff0cIPv4 \u5730\u5740\u8d44\u6e90\u7a00\u7f3a\uff0c\u8981\u6c42 IPAM \u63d0\u9ad8 IP \u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\uff1b\u5bf9\u4e8e\u6709 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5176 Pod \u526f\u672c\u53ef\u80fd\u4f1a\u8c03\u5ea6\u5230\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u8fd8\u4f1a\u53d1\u751f\u8282\u70b9\u95f4\u7684\u6f02\u79fb\uff0c\u8981\u6c42 IP \u5730\u5740\u4e00\u8d77\u6f02\u79fb\u3002 \u56e0\u6b64\uff0c\u5728\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4e00\u4e2a IP \u5730\u5740\u5e94\u8be5\u5177\u5907\u80fd\u591f\u88ab\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684\u53ef\u80fd\u3002 \u540c\u4e00\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\uff0c\u80fd\u5b9e\u73b0\u8de8\u5b50\u7f51\u83b7\u53d6 IP \u5730\u5740 \u4f8b\u5982\uff0c\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u5bbf\u4e3b\u673a1\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.1.0/24\uff0c\u800c\u5bbf\u4e3b\u673a2\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.2.0/24\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\u3002 \u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a \u5f88\u591a\u4f20\u7edf\u5e94\u7528\u5728\u4e91\u5316\u6539\u9020\u524d\uff0c\u662f\u90e8\u7f72\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u7684\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u7f51\u7edc\u672a\u5f15\u5165 NAT \u5730\u5740\u8f6c\u6362\uff0c\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u9700\u8981\u611f\u77e5\u5bf9\u65b9\u7684\u6e90 IP \u6216\u76ee\u7684 IP\uff0c\u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u4e5f\u4e60\u60ef\u4e86\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u6765\u7cbe\u7ec6\u7ba1\u63a7\u7f51\u7edc\u5b89\u5168\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0a\u4e91\u540e\uff0c\u65e0\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u8303\u56f4\u7684\u56fa\u5b9a\uff0c\u6709\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u5730\u5740\u7684\u552f\u4e00\u5bf9\u5e94\uff0c\u8fd9\u6837\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u6539\u9020\u5de5\u4f5c\u3002 \u4e00\u4e2a Pod \u7684\u591a\u7f51\u5361\u83b7\u53d6\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740 \u65e2\u7136\u662f\u5bf9\u63a5 Underlay \u7f51\u7edc\uff0cPod \u5c31\u4f1a\u6709\u591a\u7f51\u5361\u9700\u6c42\uff0c\u4ee5\u4f7f\u5176\u901a\u8fbe\u4e0d\u540c\u7684 Underlay \u5b50\u7f51\uff0c\u8fd9\u8981\u6c42 IPAM \u80fd\u591f\u7ed9\u5e94\u7528\u7684\u4e0d\u540c\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u4e0b\u7684 IP \u5730\u5740\u3002 IP \u5730\u5740\u51b2\u7a81 \u5728 Underlay \u7f51\u7edc\u4e2d\uff0c\u66f4\u52a0\u5bb9\u6613\u51fa\u73b0 IP \u51b2\u7a81\uff0c\u4f8b\u5982\uff0cPod \u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u4e3b\u673a IP \u53d1\u751f\u4e86\u51b2\u7a81\uff0c\u4e0e\u5176\u5b83\u5bf9\u63a5\u4e86\u76f8\u540c\u5b50\u7f51\u7684\u96c6\u7fa4\u51b2\u7a81\uff0c\u800c IPAM \u7ec4\u4ef6\u5f88\u96be\u611f\u77e5\u5916\u90e8\u8fd9\u4e9b\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u591a\u9700\u8981\u501f\u52a9 CNI \u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3002 \u5df2\u7528 IP \u5730\u5740\u7684\u91ca\u653e\u56de\u6536 \u56e0\u4e3a Underlay \u7f51\u7edc IP \u5730\u5740\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5e94\u7528\u6709 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6240\u4ee5\uff0c\"\u5e94\u5f53\"\u88ab\u91ca\u653e\u7684 IP \u5730\u5740\u82e5\u672a\u88ab IPAM \u7ec4\u4ef6\u56de\u6536\uff0c\u65b0\u542f\u52a8\u7684 Pod \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7f3a\u5c11 IP \u5730\u5740\u800c\u5931\u8d25\u3002\u8fd9\u5c31\u8981\u6c42 IPAM \u7ec4\u4ef6\u62e5\u6709\u66f4\u52a0\u7cbe\u51c6\u3001\u9ad8\u6548\u3001\u53ca\u65f6\u7684 IP \u56de\u6536\u673a\u5236\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u52bf\u6709\uff1a\u65e0\u9700\u7f51\u7edc NAT \u6620\u5c04\u7684\u5f15\u5165\uff0c\u5bf9\u5e94\u7528\u7684\u4e91\u5316\u7f51\u7edc\u6539\u9020\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u7684\u4fbf\u5229\uff1b\u5e95\u5c42\u7f51\u7edc\u7684\u9632\u706b\u5899\u7b49\u8bbe\u5907\uff0c\u53ef\u5bf9 Pod \u901a\u4fe1\u5b9e\u73b0\u76f8\u5bf9\u8f83\u4e3a\u7cbe\u7ec6\u7684\u7ba1\u63a7\uff1b\u65e0\u9700\u96a7\u9053\u6280\u672f\uff0c\u7f51\u7edc\u901a\u4fe1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u65f6\u6027\u80fd\u4e5f\u76f8\u5bf9\u7684\u63d0\u9ad8\u4e86\u3002","title":"Underlay \u7f51\u7edc\u65b9\u6848 IPAM"},{"location":"concepts/ipam-des-zh_CN/#spiderpool-ipam","text":"\u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool IPAM \u63d2\u4ef6\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI , calico CNI , weave CNI Spiderpool IPAM \u4e3b\u8981\u5206\u914d\u4e24\u4e2a\u5927\u6a21\u5757\uff1a\u5206\u914d\u4e0e\u56de\u6536\uff1b\u6574\u4f53\u6d41\u7a0b\u5982\u4e0b\uff1a Pod \u542f\u52a8\uff0cSpiderpool \u4f1a\u68c0\u67e5\u5bf9\u5e94 Pod \u7684 SpiderEndpoint \u5bf9\u8c61\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u5b58\u5728\uff0c\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0cPod \u540d\u79f0\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u4e14\u5e94\u7528\u6709\u56fa\u5b9a IP \u7684\u9700\u6c42\uff0cSpiderpool \u4f1a\u4ecd\u4f7f\u7528 SpiderEndpoint \u4e2d\u7684\u8bb0\u5f55\u7684 IP \u5730\u5740\u4e3a\u5176\u5206\u914d IP\uff1b\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c\u53ea\u80fd\u5c06 IP \u56fa\u5b9a\u5728\u4e00\u5b9a\u8303\u56f4\u5185\uff0cSpiderpool \u4f1a\u901a\u8fc7\u5206\u914d\u7b97\u6cd5\u91cd\u65b0\u5206\u914d IP \u5730\u5740\u7ed9 Pod\uff0c\u540c\u6b65\u53d8\u66f4\u540e\u7684 Pod \u4fe1\u606f\u5230 IP \u6c60\u548c SpiderEndpoint \u4e2d\u3002\u5982\u679c\u4e0d\u5b58\u5728\uff0cSpiderpool \u901a\u8fc7\u5206\u914d\u7b97\u6cd5\uff08\u83b7\u53d6\u5019\u9009\u6c60 -> \u8fc7\u6ee4\u5019\u9009\u6c60 -> \u5019\u9009\u6c60\u6392\u5e8f\uff09\uff0c\u7ed9 Pod \u5206\u914d IP \u5730\u5740\uff0c \u4f1a\u540c\u65f6\u521b\u5efa\u4e00\u4e2a\u5bf9\u5e94 Pod \u7684 SpiderEndpoint \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u4e2d\u8bb0\u5f55\u7740 Pod \u6240\u4f7f\u7528\u7684 IP \u5730\u5740\u3001UID \u7b49\u4fe1\u606f\uff0c\u5b83\u4f1a\u5e26\u4e00\u4e2a finalizer\uff0c\u5e76\u968f Pod \u4e00\u8d77\u8bbe\u7f6e\u4e3a OwnerReference \u3002 Spiderpool \u57fa\u4e8e\u4e24\u79cd\u65b9\u5f0f\u786e\u4fdd IP \u5730\u5740\u7684\u5065\u58ee\u6027\uff0c\u7b2c\u4e00\uff0cSpiderpool \u7684 Informer \u673a\u5236\u8ffd\u8e2a Pod \u7684\u751f\u547d\u5468\u671f\uff0c\u7b2c\u4e8c\uff0c\u5468\u671f\u6027\u7684\u5728\u5168\u5c40\u9ed8\u8ba4\u95f4\u9694\u65f6\u95f4\u5185\u626b\u63cf IPPool \u7684 IP \u72b6\u6001\u3002\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u7531\u4e8e\u5176\u56fa\u5b9a IP \u5730\u5740\u7684\u9700\u6c42\uff0c\u4e14\u53ea\u8981\u5b83\u7684 Pod \u526f\u672c\u662f\u6709\u6548\u7684\uff0cSpiderpool \u4e0d\u4f1a\u56de\u6536\u5176 IP \u5730\u5740\u548c SpiderEndpoint \u5bf9\u8c61\uff0c\u4ee5\u4fbf\u4e8e\u5b83\u80fd\u6301\u7eed\u7684\u83b7\u5f97\u76f8\u540c\u7684 IP \u5730\u5740\u3002\u5bf9\u4e8e\u65e0\u6548\u7684 StatefulSet \u5e94\u7528\u548c Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u5c06\u56de\u6536\u5176 IP \u5730\u5740\u548c SpiderEndpoint \u5bf9\u8c61\u3002","title":"Spiderpool IPAM"},{"location":"concepts/ipam-des-zh_CN/#spiderpool-ip","text":"\u5f53 Pod \u521b\u5efa\u65f6\uff0c\u5b83\u5c06\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u83b7\u53d6 IP \u5206\u914d\uff1bIP \u5206\u914d\u751f\u547d\u5468\u671f\u5c06\u7ecf\u5386 \u83b7\u53d6\u5019\u9009\u6c60 \u3001 \u8fc7\u6ee4\u5019\u9009\u6c60 \u3001 \u5019\u9009\u6c60\u6392\u5e8f \u4e09\u4e2a\u5927\u9636\u6bb5\u3002 \u83b7\u53d6\u5019\u9009\u6c60 \uff1aSpiderpool \u6709\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u53d6 \u9ad8\u4f18\u5148\u7ea7\u89c4\u5219 \u547d\u4e2d\u7684\u6240\u6709\u6c60\uff0c\u5c06\u5b83\u4eec\u6807\u8bb0\u4e3a\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4ee5\u6709\u8d44\u683c\u88ab\u8fdb\u4e00\u6b65\u8003\u8651\u3002 \u8fc7\u6ee4\u5019\u9009\u6c60 \uff1aSpiderpool \u901a\u8fc7\u4eb2\u548c\u6027\u7b49\u8fc7\u6ee4\u673a\u5236\uff0c\u66f4\u7cbe\u786e\u5730\u4ece\u6240\u6709\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5019\u9009\u6c60\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u9700\u6c42\u6216\u590d\u6742\u7684\u4f7f\u7528\u573a\u666f\u3002 \u5019\u9009\u6c60\u6392\u5e8f \uff1a\u5bf9\u4e8e\u591a\u5019\u9009\u6c60\uff0cSpiderpool \u6839\u636e SpiderIPPool \u5bf9\u8c61\u4e2d\u7684\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002","title":"Spiderpool IP \u5206\u914d\u7b97\u6cd5"},{"location":"concepts/ipam-des-zh_CN/#_1","text":"Spiderpool \u63d0\u4f9b\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u5728\u4e3a Pod \u5206\u914d IP \u5730\u5740\u65f6\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\u3002\u4ee5\u4e0b\u89c4\u5219\u6309\u7167\u4ece \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u987a\u5e8f\u5217\u51fa\uff0c\u5982\u679c\u540c\u65f6\u5b58\u5728\u4e0b\u9762\u7684\u591a\u4e2a\u89c4\u5219\uff0c\u524d\u4e00\u4e2a\u89c4\u5219\u5c06 \u8986\u76d6 \u540e\u4e00\u4e2a\u89c4\u5219\u3002 \u4f18\u5148\u7ea7 1 \uff1aSpiderSubnet \u6ce8\u89e3\u3002 SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 IPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002\u521b\u5efa\u5e94\u7528\u65f6\u53ef\u4ee5\u4f7f\u7528 ipam.spidernet.io/subnets \u6216 ipam.spidernet.io/subnet \u6ce8\u89e3\u6307\u5b9a Subnet\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u53d6 IP \u5730\u5740\u81ea\u52a8\u521b\u5efa IP \u6c60\uff0c\u5e76\u4ece\u6c60\u4e2d\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\u7ed9\u5e94\u7528\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderSubnet \u3002 \u4f18\u5148\u7ea7 2 \uff1aSpiderIPPool \u6ce8\u89e3\u3002 \u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u7ed3\u5408 SpiderIPPool \u6ce8\u89e3 ipam.spidernet.io/ippools \u6216 ipam.spidernet.io/ippool \u80fd\u7ed1\u5b9a\u4e0d\u540c\u7684 IPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 IPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderIPPool \u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 3 \uff1a\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 IP \u6c60\u3002 \u901a\u8fc7\u5728\u547d\u540d\u7a7a\u95f4\u4e2d\u8bbe\u7f6e\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u6216 ipam.spidernet.io/default-ipv6-ippool \u6307\u5b9a\u9ed8\u8ba4\u7684 IP \u6c60\u3002\u5728\u8be5\u79df\u6237\u4e2d\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u5982\u679c\u6ca1\u6709\u5176\u4ed6\u9ad8\u4f18\u5148\u7ea7\u7684\u6c60\u89c4\u5219\uff0c\u90a3\u4e48\u5c06\u4ece\u8be5\u79df\u6237\u53ef\u7528\u7684\u5019\u9009\u6c60\u4e2d\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 \u547d\u540d\u7a7a\u95f4\u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 4 \uff1aCNI \u914d\u7f6e\u6587\u4ef6\u3002 \u901a\u8fc7\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 default_ipv4_ippool \u548c default_ipv6_ippool \u5b57\u6bb5\u8bbe\u7f6e\u5168\u5c40\u7684 CNI \u9ed8\u8ba4\u6c60\uff0c\u5176\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0c\u5f53\u5e94\u7528\u4f7f\u7528\u8be5 CNI \u914d\u7f6e\u7f51\u7edc\u65f6\u5e76\u8c03\u7528 Spiderpool \uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5e94\u7528\u526f\u672c\uff0cSpiderpool \u90fd\u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u6bcf\u4e2a\u8282\u70b9\u5206\u5c5e\u4e0d\u540c\u7684\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\u7684\u573a\u666f\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684\u8282\u70b9\u4eb2\u548c\u89c4\u5219\uff0cPod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 CNI \u914d\u7f6e \u3002 \u4f18\u5148\u7ea7 5 \uff1a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\u3002 \u5728 SpiderIPPool CR \u5bf9\u8c61\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c06 spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true \uff0c\u5c06\u6c60\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\uff0c\u9ed8\u8ba4\u4e3a false \u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 \u96c6\u7fa4\u9ed8\u8ba4 IPPool","title":"\u83b7\u53d6\u5019\u9009\u6c60"},{"location":"concepts/ipam-des-zh_CN/#_2","text":"\u901a\u8fc7\u4e0a\u8ff0\u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u5f97 IPv4 \u548c IPv6 \u7684 IPPool \u5019\u9009\u540e\uff0cSpiderpool \u4f1a\u6839\u636e\u4ee5\u4e0b\u89c4\u5219\u8fdb\u884c\u8fc7\u6ee4\uff0c\u4e86\u89e3\u54ea\u4e2a\u5019\u9009 IPPool \u53ef\u7528\u3002 IP \u6c60\u5904\u4e8e\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4f46\u5176\u5904\u4e8e terminating \u72b6\u6001\u7684\uff0cSpiderpool \u5c06\u4f1a\u8fc7\u6ee4\u8be5\u6c60\u3002 IP \u6c60\u7684 spec.disable \u5b57\u6bb5\u7528\u4e8e\u8bbe\u7f6e IPPool \u662f\u5426\u53ef\u7528\uff0c\u5f53\u8be5\u503c\u4e3a true \u65f6\uff0c\u610f\u5473\u7740 IPPool \u4e0d\u53ef\u4f7f\u7528\u3002 \u68c0\u67e5 IPPool.Spec.NodeName \u548c IPPool.Spec.NodeAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u8c03\u5ea6\u8282\u70b9\u5339\u914d\u3002 \u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.NamespaceName \u548c IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u547d\u540d\u7a7a\u95f4\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.PodAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684 matchLabels \u6240\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.MultusName \u5c5e\u6027\u662f\u5426\u4e0e Pod \u5f53\u524d NIC Multus \u914d\u7f6e\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab IPPool \u5b9e\u4f8b\u7684 exclude_ips \u5b57\u6bb5\u6240\u5305\u542b\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab ReservedIP \u5b9e\u4f8b\u6240\u4fdd\u7559\u4e86\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 IPPool \u7684\u53ef\u7528 IP \u8d44\u6e90\u88ab\u8017\u5c3d\uff0c\u5219\u8be5 IPPool \u4e5f\u5c06\u88ab\u8fc7\u6ee4\u3002","title":"\u8fc7\u6ee4\u5019\u9009\u6c60"},{"location":"concepts/ipam-des-zh_CN/#_3","text":"\u8fc7\u6ee4\u5019\u9009\u6c60\u540e\uff0c\u53ef\u80fd\u4ecd\u5b58\u5728\u591a\u4e2a\u5019\u9009\u6c60\uff0cSpiderpool \u4f1a\u8fdb\u4e00\u6b65\u4f7f\u7528\u81ea\u5b9a\u4e49\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002 \u5177\u6709 IPPool.Spec.PodAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u9ad8\u4f18\u5148\u7ea7\u3002 \u5177\u6709 IPPool.Spec.NodeName \u6216 IPPool.Spec.NodeAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e8c\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NodeName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NodeAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.NamespaceName \u6216 IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e09\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NamespaceName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NamespaceAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.MultusName \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u4f4e\u4f18\u5148\u7ea7\u3002 \u6ce8\u610f\uff1a\u8fd9\u91cc\u6709\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u63cf\u8ff0\u8fd9\u4e2a\u89c4\u5219\u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5355\u4e00\u5173\u8054\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolB \u3002 \u5177\u6709\u5355\u4e2a\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.NodeName \u548c IPPool.Spec.NamespaceName \u7684 IPPoolB \u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u3001 IPPool.Spec.NamespaceName \u548c IPPool.Spec.MultusName \u7684 IPPoolB \u3002 NOTE\uff1a \u5982\u679c Pod \u5c5e\u4e8e StatefulSet\uff0c\u5219\u4f1a\u4f18\u5148\u5206\u914d\u7b26\u5408\u4e0a\u9762\u89c4\u5219\u7684 IP \u5730\u5740\u3002 \u4e00\u65e6 Pod \u91cd\u65b0\u542f\u52a8\uff0c\u5b83\u5c06\u5c1d\u8bd5\u91cd\u7528\u6700\u540e\u5206\u914d\u7684 IP \u5730\u5740\u3002","title":"\u5019\u9009\u6c60\u6392\u5e8f"},{"location":"concepts/ipam-des-zh_CN/#ip","text":"\u5728 Kubernetes \u4e2d\uff0c\u5783\u573e\u56de\u6536\uff08Garbage Collection\uff0c\u7b80\u79f0GC\uff09\u5bf9\u4e8e IP \u5730\u5740\u7684\u56de\u6536\u81f3\u5173\u91cd\u8981\u3002IP \u5730\u5740\u7684\u53ef\u7528\u6027\u76f4\u63a5\u5f71\u54cd Pod \u662f\u5426\u80fd\u591f\u6210\u529f\u542f\u52a8\u3002\u540c\u65f6 GC \u673a\u5236\u4e5f\u53ef\u4ee5\u81ea\u52a8\u56de\u6536\u4e0d\u518d\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u548c IP \u5730\u5740\u8017\u5c3d\u3002 \u5728 IPAM \u4e2d\u4f1a\u8bb0\u5f55\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684 IP \u4fe1\u606f\uff0c\u4f46\u5f53\u8fd9\u4e9b Pod \u5728 Kubernetes \u96c6\u7fa4\u4e2d\u5df2\u7ecf\u4e0d\u590d\u5b58\u5728\uff0c\u800c\u8fd9\u4e9b\u4ecd\u88ab\u8bb0\u5f55\u5728 IPAM \u4e2d\u7684 IP \u53ef\u79f0\u4e3a \u50f5\u5c38 IP \u3002Spiderpool \u9488\u5bf9 \u50f5\u5c38 IP \u5177\u6709\u5982\u4e0b\u4e24\u79cd\u56de\u6536\u65b9\u5f0f\uff1a \u5b9e\u65f6\u8ffd\u8e2a Pod \u4e8b\u4ef6\uff0c\u5224\u65ad\u662f\u5426\u9700\u8981\u56de\u6536 IP \u5730\u5740\u548c\u5176\u5bf9\u5e94\u7684 SpiderEndpoint \u5bf9\u8c61\u3002 \u57fa\u4e8e\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION \u5b9a\u4e49\u7684\u95f4\u9694\u65f6\u95f4\uff08\u9ed8\u8ba4\u4e3a 10 \u5206\u949f\uff09\u5468\u671f\u6027\u626b\u63cf IP \u6c60\u7684\u5065\u58ee\u6027\u3002 \u4e0a\u8ff0\u5b8c\u5907\u7684 IP \u56de\u6536\u7b97\u6cd5\uff0c\u80fd\u591f\u786e\u4fdd\u6240\u6709\u573a\u666f\u4e0b IP \u5730\u5740\u7684\u6b63\u786e\u56de\u6536\uff0c\u5305\u62ec\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u573a\u666f\uff1a \u5728\u96c6\u7fa4\u4e2d delete Pod \u65f6\uff0c\u7531\u4e8e \u7f51\u7edc\u5f02\u5e38 \u6216 cni \u4e8c\u8fdb\u5236 crash \u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8c03\u7528 cni delete \u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4 IP \u5730\u5740\u65e0\u6cd5\u88ab cni \u56de\u6536\u3002 \u5728 cni delete \u5931\u8d25 \u7b49\u6545\u969c\u573a\u666f\u4e0b\uff0c\u5982\u679c\u4e00\u4e2a\u66fe\u7ecf\u5206\u914d\u4e86 IP \u7684 Pod \u88ab\u9500\u6bc1\uff0c\u4f46\u5728 IPAM \u4e2d\u4ecd\u8bb0\u5f55\u7740 IP \u5730\u5740\uff0c\u5f62\u6210\u50f5\u5c38 IP\u3002Spiderpool \u57fa\u4e8e\u5468\u671f\u548c\u4e8b\u4ef6\u626b\u63cf\u673a\u5236\uff0c\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u50f5\u5c38 IP \u5730\u5740\u3002 \u56e0\u5176\u4ed6\u610f\u5916\u5bfc\u81f4 \u65e0\u72b6\u6001 Pod \u4e00\u76f4\u5904\u4e8e Terminating \u9636\u6bb5\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond + spiderpool-controller ENV SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY \u65f6\u95f4\u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_READY_NODE_ENABLED \u63a7\u5236\uff0c\u89e3\u51b3 \u8282\u70b9\u6b63\u5e38\u4f46 Pod \u5220\u9664\u5931\u8d25 \u7684\u6545\u969c\u573a\u666f\u3002 \u8282\u70b9\u610f\u5916\u5b95\u673a\u540e\uff0c\u96c6\u7fa4\u4e2d\u7684 Pod \u6c38\u4e45\u5904\u4e8e Terminating \u9636\u6bb5\uff0cPod \u5360\u7528\u7684 IP \u5730\u5740\u65e0\u6cd5\u88ab\u91ca\u653e\u3002 \u5bf9\u4e8e\u5904\u4e8e Terminating \u9636\u6bb5\u7684 \u65e0\u72b6\u6001 Pod\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond + spiderpool-controller ENV SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY \u65f6\u95f4\u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_NOT_READY_NODE_ENABLED \u63a7\u5236\uff0c\u89e3\u51b3 \u8282\u70b9\u610f\u5916\u5b95\u673a \u7684\u6545\u969c\u573a\u666f\u3002","title":"IP \u56de\u6536\u7b97\u6cd5"},{"location":"concepts/ipam-des-zh_CN/#ip_1","text":"\u5bf9\u4e8e Underlay \u7f51\u7edc\uff0cIP \u51b2\u7a81\u662f\u65e0\u6cd5\u63a5\u53d7\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u95ee\u9898\u3002Spiderpool \u652f\u6301 IP \u51b2\u7a81\u68c0\u6d4b\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u8be5\u529f\u80fd\u4ee5\u524d\u7531 coordinator \u63d2\u4ef6\u5b9e\u73b0\uff0c\u7531\u4e8e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e00\u4e9b\u6f5c\u5728\u7684\u901a\u4fe1\u95ee\u9898\u3002\u73b0\u5728\u7531 IPAM \u5b8c\u6210\u3002 \u53ef\u901a\u8fc7 spiderpool-conf configMap \u5f00\u542f\u6216\u5173\u95ed: apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: spiderpool data: conf.yml: | ... enableIPConflictDetection: true enableGatewayDetection: true ... \u5f00\u542f IP \u51b2\u7a81\u68c0\u6d4b\u540e\uff0cSpiderpool \u5c06\u4f1a\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u62a5\u6587\u68c0\u6d4b\u5206\u914d\u7684 IP \u5730\u5740\u662f\u5426\u51b2\u7a81\u3002\u5f53\u68c0\u6d4b\u5230\u8be5 IP \u4e0e\u7f51\u6bb5\u5185\u5176\u4ed6 IP \u51b2\u7a81\uff0c\u5c06\u4f1a\u963b\u6b62 Pod \u521b\u5efa\u3002\u652f\u6301 IPv4 \u548c IPv6 \u5f53\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\u5931\u8d25\uff0c\u5c06\u4f1a\u91cd\u8bd5 3 \u6b21\uff0c\u5982\u679c\u90fd\u5931\u8d25\uff0c\u5219\u8fd4\u56de\u9519\u8bef\u3002 \u5f53\u6210\u529f\u53d1\u9001\u63a2\u6d4b\u62a5\u6587\uff0c\u5982\u679c\u5728 100ms \u5185\u6536\u5230\u7b54\u590d\uff0c\u8bf4\u660e\u5b58\u5728 IP \u51b2\u7a81\u3002\u5982\u679c\u63a5\u6536\u9519\u8bef\u5e76\u4e14\u4e3a Network Timeout \u7c7b\u7684\u9519\u8bef\uff0c\u5219\u5224\u65ad\u4e3a\u4e0d\u51b2\u7a81\u3002 \u5f00\u542f\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u540e\uff0cSpiderpool \u5c06\u4f1a\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u62a5\u6587\u68c0\u6d4b Pod \u7684 \u7f51\u5173\u5730\u5740\u662f\u5426\u53ef\u8fbe\u3002\u5982\u679c\u53d1\u73b0\u7f51\u5173\u5730\u5740\u4e0d\u53ef\u8fbe\uff0c\u5c06\u4f1a\u963b\u6b62 Pod \u521b\u5efa\u3002 \u5f53\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\u5931\u8d25\uff0c\u5c06\u4f1a\u91cd\u8bd5 3 \u6b21\uff0c\u5982\u679c\u90fd\u5931\u8d25\uff0c\u5219\u8fd4\u56de\u9519\u8bef\u3002 \u5f53\u6210\u529f\u53d1\u9001\u63a2\u6d4b\u62a5\u6587\uff0c\u5982\u679c\u5728 100ms \u5185\u6536\u5230\u7b54\u590d\uff0c\u8bf4\u660e\u7f51\u5173\u5730\u5740\u53ef\u8fbe\u3002\u5982\u679c\u672a\u6536\u5230\u7b54\u590d\uff0c\u5219\u8bf4\u660e\u7f51\u5173\u5730\u5740\u4e0d\u53ef\u8fbe\u3002 \u6ce8\u610f: \u6709\u4e00\u4e9b\u4ea4\u6362\u673a\u4e0d\u5141\u8bb8\u88ab arp \u63a2\u6d4b\uff0c\u5426\u5219\u4f1a\u53d1\u51fa\u544a\u8b66\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e enableGatewayDetection \u4e3a false\u3002","title":"IP \u51b2\u7a81\u68c0\u6d4b\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b"},{"location":"concepts/ipam-des/","text":"IPAM English | \u7b80\u4f53\u4e2d\u6587 IPAM for Underlay and Overlay Network Solutions There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool. IPAM for Overlay Networks These solutions implement the decoupling of the Pod network and host network, such as Calico , Cilium , and other CNI plugins. Typically, they use tunnel technology such as vxlan to build an overlay network plane and use NAT technology for north-south traffic. These IPAM solutions have the following characteristics: Divide Pod subnets into node-based IP blocks In terms of a smaller subnet mask, the Pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account. Since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, achieving more efficient allocation. A specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound Pod. Sufficient IP address resources Subnets not overlapping with any CIDR could be used by the cluster, so the cluster has enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP addresses. No requirement for static IP addresses For the static IP address requirement, there is a difference between a stateless application and a stateful application. Regarding stateless applications like deployment, the Pod's name will change when the Pod restarts, and the business logic of the application itself is stateless. Thus, static IP addresses mean that all the Pod replicas are fixed in a set of IP addresses. For stateful applications such as statefulset, considering both the fixed information including Pod's names and stateful business logic, the strong binding of one Pod and one specific IP address needs to be implemented for static IP addresses. The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology and realizes the east-west communication through DNS, clusterIP, and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas follow the scheduling. Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addresses\" or support it in a rough way. The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment and can provide independent subnets with sufficient IP addresses for Pods. IPAM for Underlay Networks These solutions share the node's network for Pods, which means Pods can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications. There are two typical scenarios for underlay network solutions: clusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\": An IP address able to be assigned to any node As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the Pod replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together. Therefore, an IP address should be able to be allocated to a Pod on any node. Different replicas within one application could obtain IP addresses across subnets Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need to be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes. Static IP addresses For some traditional applications, the source IPs or destination IPs need to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means. Therefore, in order to reduce the transformation chores after the applications move to Kubernetes, applications need static IP addresses. Pods with Multiple NICs need IP addresses of different underlay subnets Since the Pod is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets. IP conflict Underlay networks are more prone to IP conflicts. For instance, Pods conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection. Release and recover IP addresses Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched Pod may fail due to the lack of IP addresses owing to some IP addresses not being released by abnormal Pods. This requires IPAMs to have a more accurate, efficient, and timely IP recovery mechanism. The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of Pod communication; no tunneling technology contributes to improved throughput and latency performance of network communications. Spiderpool IPAM Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool IPAM, such as: Macvlan CNI Vlan CNI Ipvlan CNI SR-IOV CNI OVS CNI Multus CNI Calico CNI Weave CNI Spiderpool IPAM is primarily divided into two major modules: allocation and recovery; the overall process is as follows: When a Pod starts, Spiderpool checks if the corresponding Pod's SpiderEndpoint object exists. If it exists, for StatefulSet applications that have a fixed Pod name and require a fixed IP, Spiderpool will continue to use the IP address recorded in SpiderEndpoint for allocation. For Deployment applications that are stateless, because the Pod name changes with each Pod restart, Spiderpool can only fix the IP within a certain range and will reassign an IP address to the Pod through an allocation algorithm, synchronizing the updated Pod information to the IP pool and SpiderEndpoint. If it does not exist, Spiderpool will allocate an IP address to the Pod through an allocation algorithm (getting candidate pools -> filtering candidate pools -> sorting candidate pools), and simultaneously create a SpiderEndpoint object corresponding to the Pod, which records the IP address, UID, and other information used by the Pod. It will also carry a finalizer and be set as an OwnerReference along with the Pod. Spiderpool ensures the robustness of IP addresses through two methods. First, Spiderpool's Informer mechanism tracks the lifecycle of Pods. Second, it periodically scans the IP status of IPPool within the global default interval. For StatefulSet applications that require fixed IP addresses, as long as their Pod replicas are valid, Spiderpool will not reclaim their IP addresses and SpiderEndpoint objects, allowing them to consistently obtain the same IP address. For invalid StatefulSet applications and Deployment applications that are stateless, their IP addresses and SpiderEndpoint objects will be reclaimed. IP Allocation Algorithm When creating a Pod, it will follow the steps below to get IP allocations. The lifecycle of IP allocation involves three major stages: candidate pool acquisition , candidate pool filtering , and candidate pool sorting . Candidate pool acquisition : Spiderpool follows a strict rule of selecting pools from high to low priority . It identifies all pools that match the high priority rules and marks them as candidates for further consideration. Candidate pool filtering : Spiderpool applies filtering mechanisms such as affinity to carefully select the appropriate candidate pools from the available options. This ensures that specific requirements or complex usage scenarios are satisfied. Candidate pool sorting : In cases where multiple candidate pools exist, Spiderpool sorts them based on the priority rules defined in the SpiderIPPool object. IP addresses are then allocated sequentially, starting from the pool with available addresses. Candidate Pool Acquisition Spiderpool offers a variety of pool selection rules when assigning IP addresses to Pods. The selection process strictly adheres to a high to low priority order. The following rules are listed in descending order of priority , and if multiple rules apply at the same time, the preceding rule will overwrite the subsequent one. The 1st priority: SpiderSubnet annotation The SpiderSubnet resource represents a collection of IP addresses. When an application requires a fixed IP address, the application administrator needs to inform their platform counterparts about the available IP addresses and routing attributes. However, as they belong to different operational departments, this process becomes cumbersome, resulting in complex workflows for creating each application. To simplify this, Spiderpool's SpiderSubnet feature automatically allocates IP addresses from subnets to IPPool and assigns fixed IP addresses to applications. This greatly reduces operational costs. When creating an application, you can use the ipam.spidernet.io/subnets or ipam.spidernet.io/subnet annotation to specify the Subnet. This allows for the automatic creation of an IP pool by randomly selecting IP addresses from the subnet, which can then be allocated as fixed IPs for the application. For more details, please refer to SpiderSubnet . The 2nd priority: SpiderIPPool annotation Different IP addresses within a Subnet can be stored in separate instances of IPPool (Spiderpool ensures that there is no overlap between the address sets of IPPools). The size of the IP collection in SpiderIPPool can vary based on requirements. This design feature is particularly beneficial when dealing with limited IP address resources in the Underlay network. When creating an application, the SpiderIPPool annotation ipam.spidernet.io/ippools or ipam.spidernet.io/ippool can be used to bind different IPPools or share the same IPPool. This allows all applications to share the same Subnet while maintaining \"micro-isolation\". For more details, please refer to SpiderIPPool annotation . The 3rd priority: Namespace default IP pool By setting the annotation ipam.spidernet.io/default-ipv4-ippool or ipam.spidernet.io/default-ipv6-ippool in the namespace, you can specify the default IP pool. When creating an application within that tenant, if there are no other higher-priority pool rules, it will attempt to allocate an IP address from the available candidate pools for that tenant. For more details, please refer to Namespace Annotation . The 4th priority: CNI configuration file The global CNI default pool can be set by configuring the default_ipv4_ippool and default_ipv6_ippool fields in the CNI configuration file. Multiple IP pools can be defined as alternative pools. When an application uses this CNI configuration network and invokes Spiderpool, each application replica is sequentially assigned an IP address according to the order of elements in the \"IP pool array\". In scenarios where nodes belong to different regions or data centers, if the node where an application replica is scheduled matches the node affinity rule of the first IP pool, the Pod obtains an IP from that pool. If it doesn't meet the criteria, Spiderpool attempts to assign an IP from the alternative pools until all options have been exhausted. For more information, please refer to CNI Configuration . The 5th priority: Cluster's default IP pool Within the SpiderIPPool CR object, setting the spec.default field to true designates the pool as the cluster's default IP pool (default value is false ). For more information, please refer to Cluster's Default IP Pool . Candidate Pool Filtering To determine the availability of candidate IP pools for IPv4 and IPv6, Spiderpool filters them using the following rules: IP pools in the terminating state are filtered out. The spec.disable field of an IP pool indicates its availability. A value of true means the IP pool is not usable. Check if the IPPool.Spec.NodeName and IPPool.Spec.NodeAffinity match the Pod's scheduling node. Mismatching values result in filtering out the IP pool. Check if the IPPool.Spec.NamespaceName and IPPool.Spec.NamespaceAffinity match the Pod's namespace. Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.NamespaceName matches the Pod's matchLabels . Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.MultusName matches the current NIC Multus configuration of the Pod. If there is no match, the IP pool is filtered out. Check if all IPs within the IP pool are included in the IPPool instance's exclude_ips field. If it is, the IP pool is filtered out. Check if all IPs in the pool are reserved in the ReservedIP instance. If it is, the IP pool is filtered out. An IP pool will be filtered out if its available IP resources are exhausted. Candidate Pool Sorting After filtering the candidate pools, Spiderpool may have multiple pools remaining. To determine the order of IP address allocation, Spiderpool applies custom priority rules to sort these candidates. IP addresses are then selected from the pools with available IPs in the following manner: IP pool resources with the IPPool.Spec.PodAffinity property are given the highest priority. IPPool resources with either the IPPool.Spec.NodeName or IPPool.Spec.NodeAffinity property are given the secondary priority. The NodeName takes precedence over NodeAffinity . Following that, IP pool resources with either the IPPool.Spec.NamespaceName or IPPool.Spec.NamespaceAffinity property maintain the third-highest priority. The NamespaceName takes precedence over NamespaceAffinity . IP pool resources with the IPPool.Spec.MultusName property receive the lowest priority. Here are some simple instances to describe this rule. IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with a single affinity property IPPool.Spec.PodAffinity . IPPoolA with a single property IPPool.Spec.PodAffinity has higher priority than IPPoolB with properties IPPool.Spec.NodeName and IPPool.Spec.NamespaceName . IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with properties IPPool.Spec.PodAffinity , IPPool.Spec.NamespaceName , and IPPool.Spec.MultusName . If a Pod belongs to StatefulSet, IP addresses that meet the aforementioned rules will be allocated with priority. When a Pod is restarted, it will attempt to reuse the previously assigned IP address. IP Garbage Collection Context When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on the IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But in some cases, it may go wrong and the IP of the IPAM database is still marked as used by a nonexistent pod. When some errors happen, the CNI plugin is not called correctly when pod deletion. This could happen in cases like: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call the CNI plugin. A node breaks down and then always cannot recover, the api-server makes pods of the breakdown node to be in deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: The new pod may fail to run because the expected IP is still occupied. The IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue. Solution For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by the administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time. SpiderIPPool Garbage Collection To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP is still taken by pods, Spiderpool uses webhook to reject the deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then the spiderpool controller will remove the Current data of the SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up) SpiderIPPool Garbage Collection Algorithm In Kubernetes, garbage collection (GC for short) is crucial for recycling IP addresses. The availability of IP addresses is critical to whether a Pod can start successfully. The GC mechanism can automatically reclaim these unused IP addresses, avoiding waste of resources and exhaustion of IP addresses. The IP information assigned to the Pod will be recorded in IPAM, but when these Pods no longer exist in the Kubernetes cluster, these IPs that are still recorded in IPAM can be called zombie IPs . Spiderpool has the following two recycling methods for zombie IPs : Real-time tracking of Pod events to determine whether the IP address and its corresponding SpiderEndpoint object need to be recycled. Periodically scan the robustness of the IP pool based on the interval defined by the environment variable SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION (the default is 10 minutes). The above complete IP recovery algorithm can ensure the correct recovery of IP addresses in all scenarios, including the following special scenarios: When deleting Pod in the cluster, but due to problems such as network exception or cni binary crash , the call to cni delete fails, resulting in the IP address not being reclaimed by cni. In failure scenarios such as cni delete failure , if a Pod that has been assigned an IP is destroyed, but the IP address is still recorded in the IPAM, a phenomenon of zombie IP is formed. For this kind of problem, Spiderpool will automatically recycle these zombie IP addresses based on the cycle and event scanning mechanism. In some accidents, the stateless Pod is in a constant Terminating phase, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond + spiderpool-controller ENV SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY periods. This feature can be controlled by the environment variable SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_READY_NODE_ENABLED . This capability can be used to solve the failure scenario of unexpected Pod downtime with Node ready . After a node goes down unexpectedly, the Pod in the cluster is permanently in the Terminating phase, and the IP address occupied by the Pod cannot be released. For the stateless Pod in the Terminating phase, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond . This feature can be controlled by the environment variable SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_NOT_READY_NODE_ENABLED . This capability can be used to solve the failure scenario of unexpected node downtime . IP Conflict Detection and Gateway Reachability Detection For Underlay networks, IP conflicts are unacceptable as they can cause serious issues. Spiderpool supports IP conflict detection and gateway reachability detection, which were previously implemented by the coordinator plugin but could cause some potential communication problems. Now, this is handled by IPAM. You can enable or disable this feature through the spiderpool-conf ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: spiderpool data: conf.yml: | ... enableIPConflictDetection: true enableGatewayDetection: true ... When IP conflict detection is enabled, Spiderpool will detect if the assigned IP address conflicts with others in the subnet by sending ARP or NDP packets. If a conflict is detected, Pod creation will be blocked. This supports both IPv4 and IPv6. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates an IP conflict. If a network timeout error is received, it is considered non-conflicting. When gateway reachability detection is enabled, Spiderpool will detect if the Pod's gateway address is reachable by sending ARP or NDP packets. If the gateway address is unreachable, Pod creation will be blocked. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates the gateway address is reachable. If no response is received, it indicates the gateway address is unreachable. Note: Some switches do not allow ARP probing and will issue alerts. In such cases, you need to set enableGatewayDetection to false.","title":"IPAM"},{"location":"concepts/ipam-des/#ipam","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"IPAM"},{"location":"concepts/ipam-des/#ipam-for-underlay-and-overlay-network-solutions","text":"There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool.","title":"IPAM for Underlay and Overlay Network Solutions"},{"location":"concepts/ipam-des/#ipam-for-overlay-networks","text":"These solutions implement the decoupling of the Pod network and host network, such as Calico , Cilium , and other CNI plugins. Typically, they use tunnel technology such as vxlan to build an overlay network plane and use NAT technology for north-south traffic. These IPAM solutions have the following characteristics: Divide Pod subnets into node-based IP blocks In terms of a smaller subnet mask, the Pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account. Since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, achieving more efficient allocation. A specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound Pod. Sufficient IP address resources Subnets not overlapping with any CIDR could be used by the cluster, so the cluster has enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP addresses. No requirement for static IP addresses For the static IP address requirement, there is a difference between a stateless application and a stateful application. Regarding stateless applications like deployment, the Pod's name will change when the Pod restarts, and the business logic of the application itself is stateless. Thus, static IP addresses mean that all the Pod replicas are fixed in a set of IP addresses. For stateful applications such as statefulset, considering both the fixed information including Pod's names and stateful business logic, the strong binding of one Pod and one specific IP address needs to be implemented for static IP addresses. The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology and realizes the east-west communication through DNS, clusterIP, and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas follow the scheduling. Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addresses\" or support it in a rough way. The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment and can provide independent subnets with sufficient IP addresses for Pods.","title":"IPAM for Overlay Networks"},{"location":"concepts/ipam-des/#ipam-for-underlay-networks","text":"These solutions share the node's network for Pods, which means Pods can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications. There are two typical scenarios for underlay network solutions: clusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\": An IP address able to be assigned to any node As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the Pod replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together. Therefore, an IP address should be able to be allocated to a Pod on any node. Different replicas within one application could obtain IP addresses across subnets Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need to be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes. Static IP addresses For some traditional applications, the source IPs or destination IPs need to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means. Therefore, in order to reduce the transformation chores after the applications move to Kubernetes, applications need static IP addresses. Pods with Multiple NICs need IP addresses of different underlay subnets Since the Pod is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets. IP conflict Underlay networks are more prone to IP conflicts. For instance, Pods conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection. Release and recover IP addresses Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched Pod may fail due to the lack of IP addresses owing to some IP addresses not being released by abnormal Pods. This requires IPAMs to have a more accurate, efficient, and timely IP recovery mechanism. The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of Pod communication; no tunneling technology contributes to improved throughput and latency performance of network communications.","title":"IPAM for Underlay Networks"},{"location":"concepts/ipam-des/#spiderpool-ipam","text":"Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool IPAM, such as: Macvlan CNI Vlan CNI Ipvlan CNI SR-IOV CNI OVS CNI Multus CNI Calico CNI Weave CNI Spiderpool IPAM is primarily divided into two major modules: allocation and recovery; the overall process is as follows: When a Pod starts, Spiderpool checks if the corresponding Pod's SpiderEndpoint object exists. If it exists, for StatefulSet applications that have a fixed Pod name and require a fixed IP, Spiderpool will continue to use the IP address recorded in SpiderEndpoint for allocation. For Deployment applications that are stateless, because the Pod name changes with each Pod restart, Spiderpool can only fix the IP within a certain range and will reassign an IP address to the Pod through an allocation algorithm, synchronizing the updated Pod information to the IP pool and SpiderEndpoint. If it does not exist, Spiderpool will allocate an IP address to the Pod through an allocation algorithm (getting candidate pools -> filtering candidate pools -> sorting candidate pools), and simultaneously create a SpiderEndpoint object corresponding to the Pod, which records the IP address, UID, and other information used by the Pod. It will also carry a finalizer and be set as an OwnerReference along with the Pod. Spiderpool ensures the robustness of IP addresses through two methods. First, Spiderpool's Informer mechanism tracks the lifecycle of Pods. Second, it periodically scans the IP status of IPPool within the global default interval. For StatefulSet applications that require fixed IP addresses, as long as their Pod replicas are valid, Spiderpool will not reclaim their IP addresses and SpiderEndpoint objects, allowing them to consistently obtain the same IP address. For invalid StatefulSet applications and Deployment applications that are stateless, their IP addresses and SpiderEndpoint objects will be reclaimed.","title":"Spiderpool IPAM"},{"location":"concepts/ipam-des/#ip-allocation-algorithm","text":"When creating a Pod, it will follow the steps below to get IP allocations. The lifecycle of IP allocation involves three major stages: candidate pool acquisition , candidate pool filtering , and candidate pool sorting . Candidate pool acquisition : Spiderpool follows a strict rule of selecting pools from high to low priority . It identifies all pools that match the high priority rules and marks them as candidates for further consideration. Candidate pool filtering : Spiderpool applies filtering mechanisms such as affinity to carefully select the appropriate candidate pools from the available options. This ensures that specific requirements or complex usage scenarios are satisfied. Candidate pool sorting : In cases where multiple candidate pools exist, Spiderpool sorts them based on the priority rules defined in the SpiderIPPool object. IP addresses are then allocated sequentially, starting from the pool with available addresses.","title":"IP Allocation Algorithm"},{"location":"concepts/ipam-des/#candidate-pool-acquisition","text":"Spiderpool offers a variety of pool selection rules when assigning IP addresses to Pods. The selection process strictly adheres to a high to low priority order. The following rules are listed in descending order of priority , and if multiple rules apply at the same time, the preceding rule will overwrite the subsequent one. The 1st priority: SpiderSubnet annotation The SpiderSubnet resource represents a collection of IP addresses. When an application requires a fixed IP address, the application administrator needs to inform their platform counterparts about the available IP addresses and routing attributes. However, as they belong to different operational departments, this process becomes cumbersome, resulting in complex workflows for creating each application. To simplify this, Spiderpool's SpiderSubnet feature automatically allocates IP addresses from subnets to IPPool and assigns fixed IP addresses to applications. This greatly reduces operational costs. When creating an application, you can use the ipam.spidernet.io/subnets or ipam.spidernet.io/subnet annotation to specify the Subnet. This allows for the automatic creation of an IP pool by randomly selecting IP addresses from the subnet, which can then be allocated as fixed IPs for the application. For more details, please refer to SpiderSubnet . The 2nd priority: SpiderIPPool annotation Different IP addresses within a Subnet can be stored in separate instances of IPPool (Spiderpool ensures that there is no overlap between the address sets of IPPools). The size of the IP collection in SpiderIPPool can vary based on requirements. This design feature is particularly beneficial when dealing with limited IP address resources in the Underlay network. When creating an application, the SpiderIPPool annotation ipam.spidernet.io/ippools or ipam.spidernet.io/ippool can be used to bind different IPPools or share the same IPPool. This allows all applications to share the same Subnet while maintaining \"micro-isolation\". For more details, please refer to SpiderIPPool annotation . The 3rd priority: Namespace default IP pool By setting the annotation ipam.spidernet.io/default-ipv4-ippool or ipam.spidernet.io/default-ipv6-ippool in the namespace, you can specify the default IP pool. When creating an application within that tenant, if there are no other higher-priority pool rules, it will attempt to allocate an IP address from the available candidate pools for that tenant. For more details, please refer to Namespace Annotation . The 4th priority: CNI configuration file The global CNI default pool can be set by configuring the default_ipv4_ippool and default_ipv6_ippool fields in the CNI configuration file. Multiple IP pools can be defined as alternative pools. When an application uses this CNI configuration network and invokes Spiderpool, each application replica is sequentially assigned an IP address according to the order of elements in the \"IP pool array\". In scenarios where nodes belong to different regions or data centers, if the node where an application replica is scheduled matches the node affinity rule of the first IP pool, the Pod obtains an IP from that pool. If it doesn't meet the criteria, Spiderpool attempts to assign an IP from the alternative pools until all options have been exhausted. For more information, please refer to CNI Configuration . The 5th priority: Cluster's default IP pool Within the SpiderIPPool CR object, setting the spec.default field to true designates the pool as the cluster's default IP pool (default value is false ). For more information, please refer to Cluster's Default IP Pool .","title":"Candidate Pool Acquisition"},{"location":"concepts/ipam-des/#candidate-pool-filtering","text":"To determine the availability of candidate IP pools for IPv4 and IPv6, Spiderpool filters them using the following rules: IP pools in the terminating state are filtered out. The spec.disable field of an IP pool indicates its availability. A value of true means the IP pool is not usable. Check if the IPPool.Spec.NodeName and IPPool.Spec.NodeAffinity match the Pod's scheduling node. Mismatching values result in filtering out the IP pool. Check if the IPPool.Spec.NamespaceName and IPPool.Spec.NamespaceAffinity match the Pod's namespace. Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.NamespaceName matches the Pod's matchLabels . Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.MultusName matches the current NIC Multus configuration of the Pod. If there is no match, the IP pool is filtered out. Check if all IPs within the IP pool are included in the IPPool instance's exclude_ips field. If it is, the IP pool is filtered out. Check if all IPs in the pool are reserved in the ReservedIP instance. If it is, the IP pool is filtered out. An IP pool will be filtered out if its available IP resources are exhausted.","title":"Candidate Pool Filtering"},{"location":"concepts/ipam-des/#candidate-pool-sorting","text":"After filtering the candidate pools, Spiderpool may have multiple pools remaining. To determine the order of IP address allocation, Spiderpool applies custom priority rules to sort these candidates. IP addresses are then selected from the pools with available IPs in the following manner: IP pool resources with the IPPool.Spec.PodAffinity property are given the highest priority. IPPool resources with either the IPPool.Spec.NodeName or IPPool.Spec.NodeAffinity property are given the secondary priority. The NodeName takes precedence over NodeAffinity . Following that, IP pool resources with either the IPPool.Spec.NamespaceName or IPPool.Spec.NamespaceAffinity property maintain the third-highest priority. The NamespaceName takes precedence over NamespaceAffinity . IP pool resources with the IPPool.Spec.MultusName property receive the lowest priority. Here are some simple instances to describe this rule. IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with a single affinity property IPPool.Spec.PodAffinity . IPPoolA with a single property IPPool.Spec.PodAffinity has higher priority than IPPoolB with properties IPPool.Spec.NodeName and IPPool.Spec.NamespaceName . IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with properties IPPool.Spec.PodAffinity , IPPool.Spec.NamespaceName , and IPPool.Spec.MultusName . If a Pod belongs to StatefulSet, IP addresses that meet the aforementioned rules will be allocated with priority. When a Pod is restarted, it will attempt to reuse the previously assigned IP address.","title":"Candidate Pool Sorting"},{"location":"concepts/ipam-des/#ip-garbage-collection","text":"","title":"IP Garbage Collection"},{"location":"concepts/ipam-des/#context","text":"When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on the IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But in some cases, it may go wrong and the IP of the IPAM database is still marked as used by a nonexistent pod. When some errors happen, the CNI plugin is not called correctly when pod deletion. This could happen in cases like: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call the CNI plugin. A node breaks down and then always cannot recover, the api-server makes pods of the breakdown node to be in deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: The new pod may fail to run because the expected IP is still occupied. The IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue.","title":"Context"},{"location":"concepts/ipam-des/#solution","text":"For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by the administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time.","title":"Solution"},{"location":"concepts/ipam-des/#spiderippool-garbage-collection","text":"To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP is still taken by pods, Spiderpool uses webhook to reject the deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then the spiderpool controller will remove the Current data of the SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up)","title":"SpiderIPPool Garbage Collection"},{"location":"concepts/ipam-des/#spiderippool-garbage-collection-algorithm","text":"In Kubernetes, garbage collection (GC for short) is crucial for recycling IP addresses. The availability of IP addresses is critical to whether a Pod can start successfully. The GC mechanism can automatically reclaim these unused IP addresses, avoiding waste of resources and exhaustion of IP addresses. The IP information assigned to the Pod will be recorded in IPAM, but when these Pods no longer exist in the Kubernetes cluster, these IPs that are still recorded in IPAM can be called zombie IPs . Spiderpool has the following two recycling methods for zombie IPs : Real-time tracking of Pod events to determine whether the IP address and its corresponding SpiderEndpoint object need to be recycled. Periodically scan the robustness of the IP pool based on the interval defined by the environment variable SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION (the default is 10 minutes). The above complete IP recovery algorithm can ensure the correct recovery of IP addresses in all scenarios, including the following special scenarios: When deleting Pod in the cluster, but due to problems such as network exception or cni binary crash , the call to cni delete fails, resulting in the IP address not being reclaimed by cni. In failure scenarios such as cni delete failure , if a Pod that has been assigned an IP is destroyed, but the IP address is still recorded in the IPAM, a phenomenon of zombie IP is formed. For this kind of problem, Spiderpool will automatically recycle these zombie IP addresses based on the cycle and event scanning mechanism. In some accidents, the stateless Pod is in a constant Terminating phase, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond + spiderpool-controller ENV SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY periods. This feature can be controlled by the environment variable SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_READY_NODE_ENABLED . This capability can be used to solve the failure scenario of unexpected Pod downtime with Node ready . After a node goes down unexpectedly, the Pod in the cluster is permanently in the Terminating phase, and the IP address occupied by the Pod cannot be released. For the stateless Pod in the Terminating phase, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond . This feature can be controlled by the environment variable SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_NOT_READY_NODE_ENABLED . This capability can be used to solve the failure scenario of unexpected node downtime .","title":"SpiderIPPool Garbage Collection Algorithm"},{"location":"concepts/ipam-des/#ip-conflict-detection-and-gateway-reachability-detection","text":"For Underlay networks, IP conflicts are unacceptable as they can cause serious issues. Spiderpool supports IP conflict detection and gateway reachability detection, which were previously implemented by the coordinator plugin but could cause some potential communication problems. Now, this is handled by IPAM. You can enable or disable this feature through the spiderpool-conf ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: spiderpool data: conf.yml: | ... enableIPConflictDetection: true enableGatewayDetection: true ... When IP conflict detection is enabled, Spiderpool will detect if the assigned IP address conflicts with others in the subnet by sending ARP or NDP packets. If a conflict is detected, Pod creation will be blocked. This supports both IPv4 and IPv6. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates an IP conflict. If a network timeout error is received, it is considered non-conflicting. When gateway reachability detection is enabled, Spiderpool will detect if the Pod's gateway address is reachable by sending ARP or NDP packets. If the gateway address is unreachable, Pod creation will be blocked. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates the gateway address is reachable. If no response is received, it indicates the gateway address is unreachable. Note: Some switches do not allow ARP probing and will issue alerts. In such cases, you need to set enableGatewayDetection to false.","title":"IP Conflict Detection and Gateway Reachability Detection"},{"location":"concepts/ipam-performance-zh_CN/","text":"IPAM \u6027\u80fd\u6d4b\u8bd5 \u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u662f\u4e00\u4e2a underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684 IPAM \u548c CNI \u6574\u5408\u80fd\u529b\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u8fd0\u884c\u5728 underlay \u573a\u666f\u4e0b\u7684 IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts \uff0c Kube-OVN \uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u3001 cilium \u5728 1000 Pod \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002 \u80cc\u666f \u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002 \u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002 underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002 \u73af\u5883 Kubernetes: v1.26.7 Container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS Kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi \u6d4b\u8bd5\u5bf9\u8c61 \u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e 0.3.1 \u7248\u672c\u7684 CNI Specification \uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u7248\u672c Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3 \u65b9\u6848 \u6d4b\u8bd5\u601d\u8def\u4e3b\u8981\u662f\uff1a Underlay IP \u8d44\u6e90\u6709\u9650\uff0cIP \u7684\u6cc4\u9732\u548c\u5206\u914d\u91cd\u590d\u5bb9\u6613\u9020\u6210\u5e72\u6270\uff0c\u56e0\u6b64 IP \u5206\u914d\u7684\u51c6\u786e\u6027\u975e\u5e38\u91cd\u8981\u3002 \u5728\u5927\u91cf Pod \u542f\u52a8\u65f6\u7ade\u4e89\u5206\u914d IP\uff0cIPAM \u7684\u5206\u914d\u7b97\u6cd5\u8981\u9ad8\u6548\uff0c\u624d\u80fd\u4fdd\u969c Pod \u5feb\u901f\u53d1\u5e03\u6210\u529f\u3002 \u56e0\u6b64\uff0c\u8bbe\u8ba1\u4e86 IP \u8d44\u6e90\u548c Pod \u8d44\u6e90\u6570\u91cf\u76f8\u540c\u7684\u6781\u9650\u6d4b\u8bd5\uff0c\u8ba1\u65f6 Pod \u4ece\u521b\u5efa\u5230 Running \u7684\u65f6\u95f4\uff0c\u6765\u53d8\u76f8\u6d4b\u8bd5 IPAM \u7684\u7cbe\u786e\u6027\u548c\u5065\u58ee\u6027\u3002\u6d4b\u8bd5\u7684\u6761\u4ef6\u5982\u4e0b\uff1a IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002 \u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002 \u6d4b\u8bd5\u7ed3\u679c \u5982\u4e0b\u5c55\u793a\u4e86 IPAM \u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5176\u4e2d\uff0c\u5305\u542b\u4e86 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u548c \u4e0d\u9650\u5236 IP \u6570\u91cf \u4e24\u79cd\u573a\u666f\uff0c\u6765\u5206\u522b\u6d4b\u8bd5\u6bcf\u4e2a CNI\u3002Calico \u548c Cilium \u7b49\u662f\u57fa\u4e8e IP block \u9884\u5206\u914d\u673a\u5236\u5206\u914d IP\uff0c\u56e0\u6b64\u6ca1\u6cd5\u76f8\u5bf9 \"\u516c\u5e73\" \u5730\u8fdb\u884c \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u6d4b\u8bd5\uff0c\u53ea\u8fdb\u884c \u4e0d\u9650\u5236 IP \u6570\u91cf \u573a\u666f\u6d4b\u8bd5\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u4e0d\u9650\u5236 IP \u6570\u91cf Spiderpool based on macvlan 207s 182s Whereabouts based on macvlan \u5931\u8d25 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s \u5206\u6790 Spiderpool \u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6574\u4e2a\u96c6\u7fa4\u8282\u70b9\u7684\u6240\u6709 Pod \u90fd\u4ece\u540c\u4e00\u4e2a CIDR \u4e2d\u5206\u914d IP\uff0c\u6240\u4ee5 IP \u5206\u914d\u548c\u91ca\u653e\u9700\u8981\u9762\u4e34\u6fc0\u70c8\u7684\u7ade\u4e89\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u66f4\u5927\uff1bWhereabouts \u548c Calico\u3001Cilium \u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u5c0f\u7684 IP \u96c6\u5408\uff0c\u6240\u4ee5 IP \u5206\u914d\u7684\u7ade\u4e89\u6bd4\u8f83\u5c0f\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u5c0f\u3002\u4f46\u4ece\u4e0a\u8ff0\u5b9e\u9a8c\u6570\u636e\u4e0a\u770b\uff0c\u867d\u7136 Spiderpool \u7684 IPAM \u539f\u7406\u662f \"\u5403\u4e8f\" \u7684\uff0c\u4f46\u662f\u5206\u914d IP \u7684\u6027\u80fd\u5374\u662f\u5f88\u597d\u7684\u3002 \u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u9047\u5230\u5982\u4e0b\u73b0\u8c61\uff1a Whereabouts based on macvlan\uff1a\u5728 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u573a\u666f\u4e0b\uff0c\u5728 300s \u5185 261 \u4e2a Pod \u4ee5\u8f83\u4e3a\u5300\u901f\u7684\u72b6\u6001\u8fbe\u5230\u4e86 Running \u72b6\u6001\uff0c\u5728 1080s \u65f6\uff0c\u5206\u914d 768 \u4e2a IP \u5730\u5740\u3002\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u5728 2280s \u65f6\u8fbe\u5230 845 \u4e2a\uff0c\u540e\u7eed Whereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002\u7531\u4e8e IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u7b49\u91cf\uff0c\u5982\u679c IPAM \u7ec4\u4ef6\u672a\u80fd\u6b63\u786e\u56de\u6536 IP\uff0c\u65b0 Pod \u5c06\u56e0\u4e3a\u7f3a\u5c11 IP \u8d44\u6e90\uff0c\u4e14\u65e0\u6cd5\u83b7\u53d6\u5230\u53ef\u7528\u7684 IP\uff0c\u4ece\u800c\u65e0\u6cd5\u542f\u52a8\u3002\u5e76\u4e14\u89c2\u5bdf\u5230\u5728\u542f\u52a8\u5931\u8d25\u7684 Pod \u4e2d\uff0c\u51fa\u73b0\u4e86\u5982\u4e0b\u7684\u4e00\u4e9b\u9519\u8bef\uff1a [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\" \u603b\u7ed3 \u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e Underlay \u7f51\u7edc\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684 IPAM \u80fd\u529b\uff0c\u5176 IP \u5206\u914d\u4ee5\u53ca\u56de\u6536\u7684\u7279\u70b9\u76f8\u8f83\u4e8e\u4e3b\u6d41 Overlay CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u3001\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u7684\u6027\u80fd\u8868\u73b0\u9886\u5148\u4e8e\u540e\u8005\u3002","title":"IPAM \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/ipam-performance-zh_CN/#ipam","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u662f\u4e00\u4e2a underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684 IPAM \u548c CNI \u6574\u5408\u80fd\u529b\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u8fd0\u884c\u5728 underlay \u573a\u666f\u4e0b\u7684 IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts \uff0c Kube-OVN \uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u3001 cilium \u5728 1000 Pod \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002","title":"IPAM \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/ipam-performance-zh_CN/#_1","text":"\u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002 \u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002 underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002","title":"\u80cc\u666f"},{"location":"concepts/ipam-performance-zh_CN/#_2","text":"Kubernetes: v1.26.7 Container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS Kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi","title":"\u73af\u5883"},{"location":"concepts/ipam-performance-zh_CN/#_3","text":"\u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e 0.3.1 \u7248\u672c\u7684 CNI Specification \uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u7248\u672c Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3","title":"\u6d4b\u8bd5\u5bf9\u8c61"},{"location":"concepts/ipam-performance-zh_CN/#_4","text":"\u6d4b\u8bd5\u601d\u8def\u4e3b\u8981\u662f\uff1a Underlay IP \u8d44\u6e90\u6709\u9650\uff0cIP \u7684\u6cc4\u9732\u548c\u5206\u914d\u91cd\u590d\u5bb9\u6613\u9020\u6210\u5e72\u6270\uff0c\u56e0\u6b64 IP \u5206\u914d\u7684\u51c6\u786e\u6027\u975e\u5e38\u91cd\u8981\u3002 \u5728\u5927\u91cf Pod \u542f\u52a8\u65f6\u7ade\u4e89\u5206\u914d IP\uff0cIPAM \u7684\u5206\u914d\u7b97\u6cd5\u8981\u9ad8\u6548\uff0c\u624d\u80fd\u4fdd\u969c Pod \u5feb\u901f\u53d1\u5e03\u6210\u529f\u3002 \u56e0\u6b64\uff0c\u8bbe\u8ba1\u4e86 IP \u8d44\u6e90\u548c Pod \u8d44\u6e90\u6570\u91cf\u76f8\u540c\u7684\u6781\u9650\u6d4b\u8bd5\uff0c\u8ba1\u65f6 Pod \u4ece\u521b\u5efa\u5230 Running \u7684\u65f6\u95f4\uff0c\u6765\u53d8\u76f8\u6d4b\u8bd5 IPAM \u7684\u7cbe\u786e\u6027\u548c\u5065\u58ee\u6027\u3002\u6d4b\u8bd5\u7684\u6761\u4ef6\u5982\u4e0b\uff1a IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002 \u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002","title":"\u65b9\u6848"},{"location":"concepts/ipam-performance-zh_CN/#_5","text":"\u5982\u4e0b\u5c55\u793a\u4e86 IPAM \u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5176\u4e2d\uff0c\u5305\u542b\u4e86 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u548c \u4e0d\u9650\u5236 IP \u6570\u91cf \u4e24\u79cd\u573a\u666f\uff0c\u6765\u5206\u522b\u6d4b\u8bd5\u6bcf\u4e2a CNI\u3002Calico \u548c Cilium \u7b49\u662f\u57fa\u4e8e IP block \u9884\u5206\u914d\u673a\u5236\u5206\u914d IP\uff0c\u56e0\u6b64\u6ca1\u6cd5\u76f8\u5bf9 \"\u516c\u5e73\" \u5730\u8fdb\u884c \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u6d4b\u8bd5\uff0c\u53ea\u8fdb\u884c \u4e0d\u9650\u5236 IP \u6570\u91cf \u573a\u666f\u6d4b\u8bd5\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u4e0d\u9650\u5236 IP \u6570\u91cf Spiderpool based on macvlan 207s 182s Whereabouts based on macvlan \u5931\u8d25 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s","title":"\u6d4b\u8bd5\u7ed3\u679c"},{"location":"concepts/ipam-performance-zh_CN/#_6","text":"Spiderpool \u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6574\u4e2a\u96c6\u7fa4\u8282\u70b9\u7684\u6240\u6709 Pod \u90fd\u4ece\u540c\u4e00\u4e2a CIDR \u4e2d\u5206\u914d IP\uff0c\u6240\u4ee5 IP \u5206\u914d\u548c\u91ca\u653e\u9700\u8981\u9762\u4e34\u6fc0\u70c8\u7684\u7ade\u4e89\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u66f4\u5927\uff1bWhereabouts \u548c Calico\u3001Cilium \u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u5c0f\u7684 IP \u96c6\u5408\uff0c\u6240\u4ee5 IP \u5206\u914d\u7684\u7ade\u4e89\u6bd4\u8f83\u5c0f\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u5c0f\u3002\u4f46\u4ece\u4e0a\u8ff0\u5b9e\u9a8c\u6570\u636e\u4e0a\u770b\uff0c\u867d\u7136 Spiderpool \u7684 IPAM \u539f\u7406\u662f \"\u5403\u4e8f\" \u7684\uff0c\u4f46\u662f\u5206\u914d IP \u7684\u6027\u80fd\u5374\u662f\u5f88\u597d\u7684\u3002 \u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u9047\u5230\u5982\u4e0b\u73b0\u8c61\uff1a Whereabouts based on macvlan\uff1a\u5728 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u573a\u666f\u4e0b\uff0c\u5728 300s \u5185 261 \u4e2a Pod \u4ee5\u8f83\u4e3a\u5300\u901f\u7684\u72b6\u6001\u8fbe\u5230\u4e86 Running \u72b6\u6001\uff0c\u5728 1080s \u65f6\uff0c\u5206\u914d 768 \u4e2a IP \u5730\u5740\u3002\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u5728 2280s \u65f6\u8fbe\u5230 845 \u4e2a\uff0c\u540e\u7eed Whereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002\u7531\u4e8e IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u7b49\u91cf\uff0c\u5982\u679c IPAM \u7ec4\u4ef6\u672a\u80fd\u6b63\u786e\u56de\u6536 IP\uff0c\u65b0 Pod \u5c06\u56e0\u4e3a\u7f3a\u5c11 IP \u8d44\u6e90\uff0c\u4e14\u65e0\u6cd5\u83b7\u53d6\u5230\u53ef\u7528\u7684 IP\uff0c\u4ece\u800c\u65e0\u6cd5\u542f\u52a8\u3002\u5e76\u4e14\u89c2\u5bdf\u5230\u5728\u542f\u52a8\u5931\u8d25\u7684 Pod \u4e2d\uff0c\u51fa\u73b0\u4e86\u5982\u4e0b\u7684\u4e00\u4e9b\u9519\u8bef\uff1a [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\"","title":"\u5206\u6790"},{"location":"concepts/ipam-performance-zh_CN/#_7","text":"\u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e Underlay \u7f51\u7edc\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684 IPAM \u80fd\u529b\uff0c\u5176 IP \u5206\u914d\u4ee5\u53ca\u56de\u6536\u7684\u7279\u70b9\u76f8\u8f83\u4e8e\u4e3b\u6d41 Overlay CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u3001\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u7684\u6027\u80fd\u8868\u73b0\u9886\u5148\u4e8e\u540e\u8005\u3002","title":"\u603b\u7ed3"},{"location":"concepts/ipam-performance/","text":"IPAM Performance Testing English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is an underlay networking solution that provides rich IPAM and CNI integration capabilities. This article will compare it with the mainstream IPAM CNI plug-ins (e.g., Whereabouts , Kube-OVN ) and the widely-used overlay IPAM CNI plugins ( calico-ipam , cilium ) in 1000 Pod scenarios. Background Why do we need to do performance testing on the underlay IPAM CNI plugin? The speed at which IPAM allocates IP addresses largely determines the speed of application publishing. Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures. Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources. ENV Kubernetes: v1.26.7 Container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS Kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi Objects This test is based on the 0.3.1 version of CNI Specification , with macvlan and Spiderpool as the test object, and selected several other common network solutions in the open source community as a comparison: Test Object Version Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3 Plan The test ideas are mainly: Underlay IP resources are limited, IP leakage and duplication of IP allocation can easily cause interference, so the accuracy of IP allocation is very important. When a large number of Pods start up and compete for IP allocation, the IPAM allocation algorithm should be efficient in order to ensure that the Pods are released quickly and successfully. Therefore, we designed a limit test with the same number of IP resources and Pod resources, and timed the time from Pod creation to Running to test the accuracy and robustness of IPAM in disguise. The test conditions are as follows: IPv4 single-stack and IPv4/IPv6 dual-stack scenarios. Create 100 Deployments, each with 10 replicas. Result The following shows the results of the IPAM performance test, which includes two scenarios, The number of IPs is equal to the number of Pods and IP sufficient , to test each CNI. Calico and Cilium, for example, are based on the IP block pre-allocation mechanism to allocate IPs, and therefore can't perform the The number of IPs is equal to the number of Pods test in a relatively fair way, and only perform the IP sufficient scenario. We can only test unlimited IPs scenarios. Test Object Limit IP to Pod Equivalents IP Sufficient Spiderpool based on macvlan 207s 182s Whereabouts based on macvlan Failure 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s Analysis Spiderpool allocates IP addresses from the same CIDR range to all Pods in the whole cluster. Consequently, IP allocation and release face intense competition, presenting larger challenges in terms of IP allocation performance. By comparison, Whereabouts, Calico, and Cilium adopt an IPAM allocation principle where each node has a small IP address pool. This reduces the competition for IP allocation and mitigates the associated performance challenges. However, experimental data shows that despite Spiderpool's \"lossy\" IPAM principle, its IP allocation performance is actually quite good. During testing, the following phenomenon was encountered: Whereabouts based on macvlan: We tested the combination of macvlan and Whereabouts in a scenario where the available number of IP addresses matches the number of Pods in a 1:1 ratio. Within 300 seconds, 261 Pods reached the \"Running\" state at a relatively steady pace. By the 1080-second mark, 768 IP addresses were allocated. Afterward, the growth rate of Pods significantly slowed down, reaching 845 Pods by 2280 seconds. Subsequently, Whereabouts essentially stopped working, resulting in a positively near-infinite amount of time needed for further allocation. In our testing scenario, where the number of IP addresses matches the number of Pods in a 1:1 ratio, if the IPAM component fails to properly reclaim IP addresses, new Pods will fail to start due to a lack of available IP resources. We observed some of the following errors in the Pod that failed to start: [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\" Summary Although Spiderpool is primarily designed for underlay networks, it provides powerful IPAM capabilities. Its IP allocation and reclamation features face more intricate challenges, including IP address contention and conflicts, compared to the popular Overlay CNI IPAM plugins. However, Spiderpool's performance is ahead of the latter.","title":"IPAM Performance"},{"location":"concepts/ipam-performance/#ipam-performance-testing","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is an underlay networking solution that provides rich IPAM and CNI integration capabilities. This article will compare it with the mainstream IPAM CNI plug-ins (e.g., Whereabouts , Kube-OVN ) and the widely-used overlay IPAM CNI plugins ( calico-ipam , cilium ) in 1000 Pod scenarios.","title":"IPAM Performance Testing"},{"location":"concepts/ipam-performance/#background","text":"Why do we need to do performance testing on the underlay IPAM CNI plugin? The speed at which IPAM allocates IP addresses largely determines the speed of application publishing. Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures. Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources.","title":"Background"},{"location":"concepts/ipam-performance/#env","text":"Kubernetes: v1.26.7 Container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS Kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi","title":"ENV"},{"location":"concepts/ipam-performance/#objects","text":"This test is based on the 0.3.1 version of CNI Specification , with macvlan and Spiderpool as the test object, and selected several other common network solutions in the open source community as a comparison: Test Object Version Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3","title":"Objects"},{"location":"concepts/ipam-performance/#plan","text":"The test ideas are mainly: Underlay IP resources are limited, IP leakage and duplication of IP allocation can easily cause interference, so the accuracy of IP allocation is very important. When a large number of Pods start up and compete for IP allocation, the IPAM allocation algorithm should be efficient in order to ensure that the Pods are released quickly and successfully. Therefore, we designed a limit test with the same number of IP resources and Pod resources, and timed the time from Pod creation to Running to test the accuracy and robustness of IPAM in disguise. The test conditions are as follows: IPv4 single-stack and IPv4/IPv6 dual-stack scenarios. Create 100 Deployments, each with 10 replicas.","title":"Plan"},{"location":"concepts/ipam-performance/#result","text":"The following shows the results of the IPAM performance test, which includes two scenarios, The number of IPs is equal to the number of Pods and IP sufficient , to test each CNI. Calico and Cilium, for example, are based on the IP block pre-allocation mechanism to allocate IPs, and therefore can't perform the The number of IPs is equal to the number of Pods test in a relatively fair way, and only perform the IP sufficient scenario. We can only test unlimited IPs scenarios. Test Object Limit IP to Pod Equivalents IP Sufficient Spiderpool based on macvlan 207s 182s Whereabouts based on macvlan Failure 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s","title":"Result"},{"location":"concepts/ipam-performance/#analysis","text":"Spiderpool allocates IP addresses from the same CIDR range to all Pods in the whole cluster. Consequently, IP allocation and release face intense competition, presenting larger challenges in terms of IP allocation performance. By comparison, Whereabouts, Calico, and Cilium adopt an IPAM allocation principle where each node has a small IP address pool. This reduces the competition for IP allocation and mitigates the associated performance challenges. However, experimental data shows that despite Spiderpool's \"lossy\" IPAM principle, its IP allocation performance is actually quite good. During testing, the following phenomenon was encountered: Whereabouts based on macvlan: We tested the combination of macvlan and Whereabouts in a scenario where the available number of IP addresses matches the number of Pods in a 1:1 ratio. Within 300 seconds, 261 Pods reached the \"Running\" state at a relatively steady pace. By the 1080-second mark, 768 IP addresses were allocated. Afterward, the growth rate of Pods significantly slowed down, reaching 845 Pods by 2280 seconds. Subsequently, Whereabouts essentially stopped working, resulting in a positively near-infinite amount of time needed for further allocation. In our testing scenario, where the number of IP addresses matches the number of Pods in a 1:1 ratio, if the IPAM component fails to properly reclaim IP addresses, new Pods will fail to start due to a lack of available IP resources. We observed some of the following errors in the Pod that failed to start: [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\"","title":"Analysis"},{"location":"concepts/ipam-performance/#summary","text":"Although Spiderpool is primarily designed for underlay networks, it provides powerful IPAM capabilities. Its IP allocation and reclamation features face more intricate challenges, including IP address contention and conflicts, compared to the popular Overlay CNI IPAM plugins. However, Spiderpool's performance is ahead of the latter.","title":"Summary"},{"location":"concepts/multi_cni_coexist-zh_CN/","text":"Calico + Macvlan \u591a CNI \u6570\u636e\u8f6c\u53d1\u6d41\u7a0b \u7b80\u4f53\u4e2d\u6587 | English \u80cc\u666f CNI \u4f5c\u4e3a Kubernetes \u7684\u96c6\u7fa4\u4e2d\u91cd\u8981\u7684\u7ec4\u4ef6\u3002\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u90fd\u4f1a\u90e8\u7f72\u4e00\u4e2a CNI(\u6bd4\u5982 Calico)\uff0c\u7531\u5b83\u8d1f\u8d23\u96c6\u7fa4\u7f51\u7edc\u7684\u8fde\u901a\u6027\u3002\u6709\u4e9b\u65f6\u5019\u4e00\u4e9b\u7ec8\u7aef\u7528\u6237\u57fa\u4e8e\u6027\u80fd\u3001\u5b89\u5168\u7b49\u7684\u8003\u8651\uff0c\u4f1a\u5728\u96c6\u7fa4\u4e2d\u4f7f\u7528\u591a\u79cd\u7c7b\u578b\u7684 CNI\uff0c\u6bd4\u5982 Underlay \u7c7b\u578b\u7684 Macvlan CNI\u3002\u6b64\u65f6\u4e00\u4e2a\u96c6\u7fa4\u5c31\u53ef\u80fd\u5b58\u5728\u591a\u79cd CNI \u7c7b\u578b\u7684 Pod\uff0c\u4e0d\u540c\u7c7b\u578b\u7684 Pod \u5206\u522b\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u573a\u666f\uff1a \u5355 Calico \u7f51\u5361\u7684 Pod: \u5982 CoreDNS \u7b49\u7cfb\u7edf\u7ec4\u4ef6\uff0c\u6ca1\u6709\u56fa\u5b9a IP \u7684\u9700\u6c42\uff0c\u4e5f\u4e0d\u9700\u8981\u5357\u5317\u5411\u6d41\u91cf\u901a\u4fe1\uff0c\u53ea\u5b58\u5728\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u901a\u4fe1\u7684\u9700\u6c42\u3002 \u5355 Macvlan \u7f51\u5361\u7684 Pod: \u9002\u7528\u4e8e\u5bf9\u6027\u80fd\uff0c\u5b89\u5168\u6709\u7279\u6b8a\u8981\u6c42\u7684\u5e94\u7528\uff0c\u6216\u9700\u8981\u4ee5 Pod IP \u76f4\u63a5\u5357\u5317\u5411\u6d41\u91cf\u901a\u4fe1\u7684\u4f20\u7edf\u4e0a\u4e91\u5e94\u7528\u3002 Calico \u548c Macvlan \u7f51\u5361\u7684\u591a\u7f51\u5361 Pod\uff1a\u540c\u65f6\u517c\u987e\u4e0a\u9762\u4e8c\u8005\u7684\u9700\u6c42\u3002\u65e2\u9700\u8981\u4ee5\u56fa\u5b9a\u7684 Pod IP \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\uff0c\u53c8\u9700\u8981\u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf(\u6bd4\u5982\u548c Calico Pod \u6216 Service)\u3002 \u53e6\u5916\uff0c\u5f53\u591a CNI \u7684 Pod \u5b58\u5728\u4e8e\u4e00\u4e2a\u96c6\u7fa4\uff0c\u5b9e\u9645\u4e0a\u8fd9\u4e2a\u96c6\u7fa4\u5b58\u5728\u4e24\u79cd\u4e0d\u540c\u7684\u6570\u636e\u8f6c\u53d1\u65b9\u6848: Underlay \u548c Overlay\u3002\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e00\u4e9b\u5176\u4ed6\u95ee\u9898: \u4f7f\u7528 Underlay \u7f51\u7edc\u7684 Pod \u65e0\u6cd5\u4e0e\u96c6\u7fa4\u4e2d\u4f7f\u7528 Overlay \u7f51\u7edc\u7684 Pod \u76f4\u63a5\u901a\u4fe1: \u7531\u4e8e\u8f6c\u53d1\u8def\u5f84\u4e0d\u4e00\u81f4\uff0cOverlay \u7f51\u7edc\u5e38\u5e38\u9700\u8981\u7ecf\u8fc7\u8282\u70b9\u4f5c\u4e8c\u6b21\u8f6c\u53d1\uff0c\u4f46 Underlay \u4e00\u822c\u76f4\u63a5\u901a\u8fc7\u5e95\u5c42\u7f51\u5173\u8f6c\u53d1\u3002\u6240\u4ee5\u5f53\u4e8c\u8005\u4e92\u76f8\u8bbf\u95ee\u65f6\uff0c\u53ef\u80fd\u7531\u4e8e\u5e95\u5c42\u4ea4\u6362\u673a\u672a\u540c\u6b65\u96c6\u7fa4\u5b50\u7f51\u7684\u8def\u7531\u5bfc\u81f4\u4e22\u5305 \u53cc CNI \u53ef\u80fd\u4f1a\u589e\u52a0\u4f7f\u7528\u548c\u8fd0\u7ef4\u590d\u6742\u5ea6\uff0c\u6bd4\u5982 IP \u5730\u5740\u7ba1\u7406\u7b49 Spiderpool \u8fd9\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u89e3\u51b3\u5f53\u96c6\u7fa4\u5b58\u5728\u591a\u79cd CNI \u65f6\u7684\u4e92\u8054\u4e92\u901a\u95ee\u9898\uff0c\u53c8\u53ef\u4ee5\u51cf\u8f7b IP \u5730\u5740\u8fd0\u7ef4\u8d1f\u62c5\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4ee5 Calico \u548c Macvlan \u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5b83\u4eec\u4e4b\u95f4\u7684\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b\u3002 \u5feb\u901f\u5f00\u59cb Calico + Macvlan \u591a\u7f51\u5361\u5feb\u901f\u5f00\u59cb\u53ef\u53c2\u8003 get-stared-calico \u5355 Macvlan \u7f51\u5361\u53ef\u53c2\u8003 get-started-macvlan Underlay CNI \u8bbf\u95ee Service \u53ef\u53c2\u8003 underlay_cni_service \u4e0d\u540c\u573a\u666f\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b Macvlan \u8bbf\u95ee Macvlan Pod Macvlan Pod \u8bbf\u95ee Service Macvlan \u8bbf\u95ee Calico \u7684 Pod Macvlan Pod \u8bbf\u95ee Calico Pod \u7684 Service Calico+Macvlan \u591a\u7f51\u5361 Pod \u5206\u522b\u8bbf\u95ee Calico \u548c Macvlan Pod Calico+macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Service \u8bbf\u95ee Calico-Macvlan \u591a\u7f51\u5361 Pod \u7684 NodePort \u4e0b\u9762\u4ecb\u7ecd\u51e0\u79cd\u5178\u578b\u7684\u901a\u4fe1\u573a\u666f: Macvlan Pod \u4e4b\u95f4\u4e92\u76f8\u8bbf\u95ee\u6570\u636e\u8f6c\u53d1\u8fc7\u7a0b Macvlan \u8bbf\u95ee\u540c\u5b50\u7f51 Pod \u5982 \u56fe1 \u6807\u53f72 \uff0cMacvlan Pod1(172.16.1.3) \u901a\u8fc7 Macvlan Bridge \u6a21\u5f0f\u673a\u5236\u76f4\u63a5\u8bbf\u95ee Macvlan Pod2(172.16.1.2), \u4e0d\u7ba1\u662f\u5426\u4e3a\u540c\u8282\u70b9\u8fd8\u662f\u4e0d\u540c\u8282\u70b9\uff0c\u90fd\u4e0d\u9700\u8981\u7ecf\u8fc7\u7f51\u5173\u8f6c\u53d1\u3002 Macvlan \u8bbf\u95ee\u4e0d\u540c\u5b50\u7f51\u7684 Pod \u5982 \u56fe1 \u6807\u53f71 , Macvlan Pod1(172.16.1.2) \u8bbf\u95ee\u8de8\u7f51\u6bb5\u8de8\u8282\u70b9\u7684 Macvlan Pod2(172.16.2.100) \u6216\u8bbf\u95ee\u8de8\u7f51\u6bb5\u4f46\u540c\u8282\u70b9\u7684 Macvlan Pod3(172.17.1.100)\u3002\u7531\u4e8e\u662f\u4e0d\u540c\u7f51\u6bb5\uff0c\u90fd\u9700\u8981\u7ecf\u8fc7\u7f51\u5173\u8f6c\u53d1\u3002 Macvlan Pod \u8bbf\u95ee \u672c\u5730\u8282\u70b9 \u5982 \u56fe1 \u6807\u53f73 : \u7531\u4e8e Macvlan bridge \u673a\u5236\uff0cMacvlan \u5b50\u63a5\u53e3\u4e0d\u80fd\u76f4\u63a5\u8ddf Master \u63a5\u53e3\u901a\u4fe1\u3002\u6240\u4ee5\u6211\u4eec\u5728 Macvlan Pod(\u5982 172.17.1.100) \u521b\u5efa\u4e86\u4e00\u4e2a veth \u7f51\u5361\uff0c\u5e76\u5728 Pod \u548c\u8282\u70b9\u5206\u522b\u8bbe\u7f6e\u8def\u7531\uff0c\u89e3\u51b3\u4e0d\u80fd\u901a\u4fe1\u7684\u95ee\u9898\u3002\u8282\u70b9(172.17.1.1)\u8bbf\u95ee Macvlan Pod\uff08172.17.1.100\uff09 \u6839\u636e\u8282\u70b9 table500 \u4e2d\u7684\u8def\u7531\uff0c\u4ece vethxxx \u53d1\u8f6c\u53d1\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u3002 # pod network namespace ~# ip r 172.17.1.1 dev veth0 # host network namespace ~# ip r show table 500 172.17.1.100 dev vethxxx Macvlan Pod \u8bbf\u95ee Service \u8bbf\u95ee ClusterIP(\u5982 \u56fe2\u9ed1\u8272\u7ebf\u6240\u793a ) : Macvlan Pod\uff08172.17.1.100\uff09\u8bbf\u95ee ClusterIP\uff0810.233.0.100\uff09\u7684\u6570\u636e\u5305\u5339\u914d Pod \u5185\u90e8\u8def\u7531\uff0c\u901a\u8fc7 veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9 # pod network namespace ~# ip r 10.233.0.0/18 dev veth0 \u7ecf\u8fc7\u8282\u70b9 Kube-proxy \u8bbe\u7f6e\u7684 Service DNAT \u89c4\u5219\uff0c\u5c06 ClusterIP\uff0810.233.0.100\uff09\u89e3\u6790\u4e3a\u76ee\u6807 Macvlan Pod\uff08172.17.1.200\uff09, \u7ecf\u8fc7\u8282\u70b9\u95f4\u7684\u7f51\u7edc\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230 172.17.1.200\u3002\u6b64\u65f6\u6570\u636e\u5305\u7684\u6e90\u5730\u5740\u5df2\u88ab SNAT \u4e3a\u8282\u70b9IP\uff1a172.17.1.1. \u5230\u8fbe\u8282\u70b9 Node2\uff08172.17.1.2\uff09\uff0c\u901a\u8fc7\u4e3b\u673a\u4e0a table500 \u7684\u8def\u7531\u8868\uff0c\u5c06\u6570\u636e\u5305\u901a\u8fc7 vethxx \u8f6c\u53d1\u5230\u76ee\u6807 Pod\uff08172.17.1.100\uff09\u3002 \u76ee\u6807 Pod\uff08172.17.1.100\uff09\u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6\uff0c\u5176\u76ee\u6807\u5730\u5740\u662f\u8282\u70b9 Node1 \u7684\u5730\u5740\uff1a172.17.1.1, \u6240\u4ee5\u54cd\u5e94\u62a5\u6587\u4ece veth0 \u53d1\u51fa\uff0c\u7ecf\u8fc7\u8282\u70b9\u7f51\u7edc\u53d1\u9001\u56de Node1 \u8282\u70b9\u3002\u7ecf\u8fc7 Node1 \u8282\u70b9\u4e0a Kube-proxy \u7684 iptables \u89c4\u5219\uff0c\u5c06\u6e90\u5730\u5740\u8fd8\u539f\u4e3a ClusterIP (10.233.0.100), \u76ee\u6807\u5730\u5740\u8fd8\u539f\u4e3a Macvlan Pod\uff08172.17.1.100\uff09 \u7ecf\u8fc7\u4e3b\u673a\u4e0a\u8bbe\u7f6e\u7684 table500 \u8def\u7531\u8868\uff0c\u5c06\u54cd\u5e94\u62a5\u6587\u901a\u8fc7 vethxx \u7f51\u5361\u8f6c\u53d1\u5230\u6e90 Pod\uff08172.17.1.100\uff09\uff0c\u6574\u4e2a\u8bbf\u95ee\u8fc7\u7a0b\u7ed3\u675f\u3002 \u8bbf\u95ee NodePort Service(\u5982 \u56fe2 \u7ea2\u8272\u7ebf\u6240\u793a) \u91cd\u70b9\u4ecb\u7ecd NodePort Service \u914d\u7f6e ExternalTrafficPolicy=Local \u7684\u573a\u666f. \u96c6\u7fa4\u5916\u4e00\u4e2a\u5ba2\u6237\u7aef\uff1a1.1.1.1 \u8bbf\u95ee NodePort: 172.17.1.1:32456\u3002 \u6570\u636e\u5305\u9996\u5148\u901a\u8fc7\u5916\u90e8\u8def\u7531\u5230\u8fbe\u8282\u70b9 Node1(172.17.1.1) \u7ecf\u8fc7\u4e3b\u673a\u4e0a Kube-proxy \u8bbe\u7f6e\u7684 iptables DNAT \u89c4\u5219, \u5c06\u76ee\u6807\u5730\u5740\u6539\u5199\u4e3a Macvlan Pod(172.17.1.100), \u7ecf\u8fc7\u4e3b\u673a\u4e0a table500 \u7684\u8def\u7531\u8868\uff0c\u5c06\u6570\u636e\u5305\u901a\u8fc7 vethxx \u8f6c\u53d1\u5230\u76ee\u6807 Pod\u3002 \u5f53 Pod \u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6\uff0c\u770b\u5230\u7684\u76ee\u6807\u5730\u5740\u662f 1.1.1.1\uff0c\u5c06\u4f1a\u5339\u914d\u5230 Pod \u7684\u9ed8\u8ba4\u8def\u7531\uff0c\u5219\u4ece eth0 \u8f6c\u53d1\u51fa\u53bb\uff0c\u5bfc\u81f4\u8bbf\u95ee\u4e0d\u901a\u3002Spiderpool \u901a\u8fc7\u5728 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u8bbe\u7f6e\u4ee5\u4e0b\u7684 iptables \u89c4\u5219\u548c\u7b56\u7565\u8def\u7531\uff0c\u4f7f veth0 \u63a5\u6536\u7684 NodePort \u6d41\u91cf\u4ecd\u7136\u4ece veth0 \u8f6c\u53d1\u5230\u8282\u70b9\u4e0a\u3002 # pod network namespace iptables -i veth0 --set-xmark 0x1 ... ~# ip rule from all fwmark 0x1 lookup 500 ~# ip r show table 500 default dev veth0 \u7ecf\u8fc7\u8282\u70b9\u4e0a\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff0c\u5c06\u6570\u636e\u5305\u7684\u6e90\u5730\u5740\u6539\u4e3a\u8282\u70b9\u7684 IP(172.17.1.1)\uff0c\u518d\u901a\u8fc7\u96c6\u7fa4\u5916\u7684\u8def\u7531\uff0c\u5c06\u6570\u636e\u5305\u53d1\u9001\u81f3\u5ba2\u6237\u7aef(1.1.1.1),\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Macvlan \u8bbf\u95ee Calico \u7684 Pod \u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u4e09\u79cd\u60c5\u51b5\uff1a Macvlan Pod \u8bbf\u95ee\u540c\u8282\u70b9\u7684 Calico Pod(\u5982 \u56fe3\u7684\u7ebf\u8def1 ) : Macvlan Pod\uff08172.16.100.2\uff09\u8bbf\u95ee Calico Pod\uff0810.233.100.3/18\uff09\uff0c\u6570\u636e\u5305\u5339\u914d Pod \u4e2d\u8def\u7531( 10.233.64.0/18 via veth0 ), \u5c06\u6570\u636e\u5305\u4ece veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9\u4e0a\u3002 \u6570\u636e\u5305\u5339\u914d\u8282\u70b9\u4e0a\uff1a Calico \u7684\u865a\u62df\u8def\u7531( 10.233.100.3 dev calixxx ) \u5c06\u6570\u636e\u5305\u901a\u8fc7 Calico \u865a\u62df\u7f51\u5361\u8f6c\u53d1\u5230 Calico Pod(10.233.100.3)\u3002 \u54cd\u5e94\u62a5\u6587\u901a\u8fc7 Calico \u865a\u62df\u7f51\u5361(calixxx)\u53d1\u9001\u5230\u8282\u70b9\u4e0a\uff0c\u7531\u4e8e\u76ee\u6807\u5730\u5740\u4e3a Macvlan Pod(172.16.100.2)\uff0c\u5339\u914d\u8def\u7531( 172.16.100.2 dev vethxxx table 500 ) \u8f6c\u53d1\u5230 Macvlan Pod\uff0c\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Macvlan Pod \u8de8\u8282\u70b9\u8bbf\u95ee Calico Pod(\u8282\u70b9\u540c\u7f51\u6bb5\uff0c\u5982\u56fe\u4e09\u4e2d\uff1a2->2.1): Macvlan Pod\uff08172.16.100.2\uff09\u8bbf\u95ee Calico Pod\uff0810.233.200.2/18\uff09\uff0c\u6570\u636e\u5305\u5339\u914d Pod \u4e2d\u8def\u7531( 10.233.64.0/18 via veth0 ), \u5c06\u6570\u636e\u5305\u4ece veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9\u4e0a\u3002 \u7ecf\u8fc7\u8282\u70b9\u4e4b\u95f4\u7684 Calico \u8def\u7531\uff0c\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230\u5bf9\u7aef\u8282\u70b9(172.16.1.3)\u3002\u7531\u4e8e\u76ee\u7684\u5730\u5740\u662f: 10.233.200.2, \u5339\u914d\u5230\u8282\u70b9\u7684 Calico \u8def\u7531( 10.233.100.2 dev calixxx ) \u5c06\u6570\u636e\u5305\u901a\u8fc7 Calico \u865a\u62df\u7f51\u5361\u8f6c\u53d1\u5230 Calico Pod(10.233.100.2)\u3002 \u7531\u4e8e\u6570\u636e\u5305\u6700\u539f\u59cb\u7684\u6e90\u5730\u5740\u4e3a Macvlan Pod(172.16.100.2), \u6240\u4ee5 Calico Pod \u4f1a\u5c06\u54cd\u5e94\u62a5\u6587\u5148\u8f6c\u53d1\u5230\u672c\u8282\u70b9(172.16.1.3)\u4e0a\uff0c\u7136\u540e\u5c06\u54cd\u5e94\u62a5\u6587\u76f4\u63a5\u8f6c\u53d1\u5230 Macvlan Pod(172.16.100.2),\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u8282\u70b9\u8f6c\u53d1\uff0c\u5bfc\u81f4\u4e86\u6570\u636e\u5305\u6765\u56de\u8f6c\u53d1\u8def\u5f84\u4e0d\u4e00\u81f4\uff0c\u53ef\u80fd\u4f1a\u88ab\u5185\u6838\u8ba4\u4e3a\u5176\u6570\u636e\u5305\u7684 conntrack \u7684 state \u4e3a invalid\uff0c\u4f1a\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u4e22\u5f03: ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' iptables -A FORWARD -m conntrack --ctstate INVALID -j DROP \u8be5\u89c4\u5219\u539f\u662f\u4e3a\u4e86\u89e3\u51b3 #Issue74839 \u63d0\u51fa\u7684\u95ee\u9898\uff0c\u56e0\u4e3a \u67d0\u4e9b tcp \u62a5\u6587\u5927\u5c0f\u8d85\u51fa\u7a97\u53e3\u9650\u5236\uff0c\u5bfc\u81f4\u88ab\u5185\u6838\u6807\u8bb0\u5176 conntrack \u7684 state \u4e3a invalid\uff0c\u4ece\u800c\u5bfc\u81f4\u6574\u4e2a tcp \u94fe\u63a5\u88ab reset\u3002\u4e8e\u662f k8s \u793e\u533a\u901a\u8fc7\u4e0b\u53d1\u8fd9\u6761\u89c4\u5219\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u8fd9\u6761\u89c4\u5219\u53ef\u80fd\u4f1a\u5f71\u54cd\u6b64\u573a\u666f\u4e2d\u6570\u636e\u5305\u6765\u56de\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002\u5982\u793e\u533a\u6709\u76f8\u5173\u7684 issue \u62a5\u544a\uff1a #Issue117924 , #Issue94861 , #Issue177 \u7b49\u3002 \u6211\u4eec\u901a\u8fc7\u63a8\u52a8\u793e\u533a\u4fee\u590d\u6b64\u4e86\u95ee\u9898\uff0c\u6700\u7ec8\u5728 only drop invalid cstate packets if non liberal \u5f97\u5230\u89e3\u51b3\uff0ckubernetes \u7248\u672c\u4e3a v1.29\u3002\u6211\u4eec\u9700\u8981\u786e\u4fdd\u8bbe\u7f6e\u6bcf\u4e2a\u8282\u70b9\u7684 sysctl \u53c2\u6570: sysctl -w net.netfilter.nf_conntrack_tcp_be_liberal=1 \uff0c\u5e76\u4e14\u91cd\u542f Kube-proxy\uff0c\u8fd9\u6837 kube-proxy \u5c31\u4e0d\u4f1a\u4e0b\u53d1\u8fd9\u6761 drop \u89c4\u5219\uff0c\u4e5f\u5c31\u4e0d\u4f1a\u5f71\u54cd\u5230\u5355 Macvlan pod \u4e0e\u5355 Calico pod \u4e4b\u95f4\u7684\u901a\u4fe1\u3002 \u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u68c0\u67e5\u8282\u70b9\u662f\u5426\u8fd8\u5b58\u5728\u8fd9\u6761 drop \u89c4\u5219\uff0c\u5982\u679c\u6ca1\u6709\u8f93\u51fa\u8bf4\u660e\u6b63\u5e38\u3002\u5426\u5219\u8bf7\u68c0\u67e5 sysctl \u662f\u5426\u6b63\u786e\u8bbe\u7f6e\u4ee5\u53ca\u662f\u5426\u91cd\u542f kube-proxy\u3002 ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' \u6ce8\u610f\uff1a \u5fc5\u987b\u786e\u4fdd k8s \u7248\u672c\u5927\u4e8e v1.29\u3002\u5982\u679c\u60a8\u7684 k8s \u7248\u672c\u5c0f\u4e8e v1.29, \u8be5\u95ee\u9898\u65e0\u6cd5\u89c4\u907f\u3002 Macvlan Pod \u8bbf\u95ee Calico Pod \u7684 Service Macvlan Pod(172.16.100.2) \u63d2\u5165\u4e00\u6761 \u52ab\u6301 service \u6d41\u91cf\u7684\u8def\u7531, \u4f7f Pod \u8bbf\u95ee ClusterIP(10.233.0.100) \u7684\u65f6\u5019\uff0c\u6570\u636e\u5305\u901a\u8fc7 veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9 Node1(172.16.1.2)\u3002 \uff5e# ip r 10.233.0.0/18 via 10.7.168.71 dev eth0 src 10.233.100.2 \u5f53\u6570\u636e\u5305\u8f6c\u53d1\u5230\u8282\u70b9 Node1(172.16.1.2) \u4e4b\u540e\uff0c\u7ecf\u8fc7\u4e3b\u673a\u7f51\u7edc\u534f\u8bae\u6808\u7684 kube-proxy \u5c06 clusterip \u8f6c\u6362\u4e3a Calico Pod \u7684 IP\uff1a10.233.200.2\u3002 \u968f\u540e\u901a\u8fc7 Calico \u8bbe\u7f6e\u7684\u96a7\u9053\u8def\u7531\u8f6c\u53d1\u5230\u8282\u70b9 Node2(172.16.1.3)\u3002\u6ce8\u610f\u5f53\u8bf7\u6c42\u6570\u636e\u5305\u4ece\u8282\u70b9 node3 \u53d1\u51fa\u65f6\uff0c\u5176\u6e90\u5730\u5740\u4f1a\u88ab SNAT \u4e3a\u7684\u8282\u70b9 Node1 \u7684 IP(172.16.1.2)\u3002\u8fd9\u786e\u4fdd\u56de\u590d\u6570\u636e\u5305\u80fd\u591f\u539f\u8def\u8fd4\u56de\uff0c\u800c\u4e0d\u4f1a\u51fa\u73b0 Macvlan Pod \u8bbf\u95ee Calico Pod \u53ef\u80fd\u51fa\u73b0\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u8fd9\u6837\u8bf7\u6c42\u6570\u636e\u5305\u8f6c\u53d1\u5230\u4e3b\u673a Node2 \u540e\uff0c\u901a\u8fc7\u8282\u70b9\u4e0a calixxx \u865a\u62df\u7f51\u5361\u8f6c\u53d1\u5230\u4e86 Calico Pod\uff0810.233.200.2\uff09\u3002 Calico Pod\uff0810.233.200.2\uff09\u5c06\u56de\u590d\u62a5\u6587\u8f6c\u53d1\u5230\u8282\u70b9 Node2\u3002\u6b64\u65f6\u56de\u590d\u6570\u636e\u5305\u7684\u76ee\u6807\u5730\u5740\u4e3a\u8282\u70b9 Node1(172.16.1.2)\uff0c\u6240\u4ee5\u901a\u8fc7\u8282\u70b9\u8def\u7531\u8f6c\u53d1\u5230 Node1\u3002\u968f\u540e\u901a\u8fc7 Kube-proxy \u5c06\u76ee\u6807\u5730\u5740\u6539\u5199\u4e3a Macvlan Pod \u7684 IP\uff1a 172.16.100.2\uff0c\u6e90\u5730\u5740\u6539\u4e3a ClusterIP(10.233.0.100)\u3002\u968f\u540e\u5339\u914d\u4e3b\u673a\u4e0a\u8bbe\u7f6e\u7684\u8def\u7531 table500( 172.16.100.2 dev vethxxx table 500 )\uff0c\u5c06\u6570\u636e\u5305\u53d1\u9001\u5230 Macvlan Pod, \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Calico+Macvlan \u591a\u7f51\u5361\u7684 Pod \u5206\u522b\u8bbf\u95ee Calico \u548c Macvlan Pod \u5982 \u56fe4 \u6240\u793a\uff1a\u8282\u70b9 Node1 \u7684 Calico-Macvlan Pod (\u540c\u65f6\u5177\u5907 calico \u548c macvlan \u7f51\u5361\u7684 pod\uff0c\u8282\u70b9 Node2 \u8fd0\u884c\u4e86 Macvlan Pod\uff08\u53ea\u5177\u5907 macvlan \u7684\u7f51\u5361\uff09\u548c Calico Pod\uff08\u53ea\u5177\u5907 calico \u7684\u7f51\u5361\uff09\u3002 Calico + Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Calico Pod(\u5982\u56fe4\u7ea2\u8272\u7ebf\u6bb5\u6240\u793a): Calico-Macvlan Pod \u5177\u5907\u4e24\u5f20\u7f51\u5361\uff0cSpiderpool \u534f\u8c03\u591a\u5f20\u7f51\u5361\u7684\u8def\u7531\uff0c\u4f7f Pod \u63a5\u5165 Overlay \u548c Underlay \u4e24\u79cd\u7f51\u7edc\u3002 ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 172.16.0.0/16 dev net1 default via 169.254.1.1 dev eth0 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 \u5f53 Calico-Macvlan Pod \u8bbf\u95ee Calico Pod\uff0810.233.100.4\uff09\uff0c\u5339\u914d Pod \u7684\u8def\u7531( 10.233.64.0/18 dev eth0 ) \u901a\u8fc7 eth0 \u5c06\u6570\u636e\u5305\u53d1\u9001\u5230\u8282\u70b9 Node1(172.16.1.1)\uff0c\u6e90\u5730\u5740\u4e3a Calico \u7f51\u5361\u7684 IP(10.233.100.3)\u3002\u7531\u4e8e\u76ee\u6807\u5730\u5740\u4e3a 10.233.100.4\uff0c\u6240\u4ee5\u4f1a\u901a\u8fc7\u8282\u70b9\u95f4\u7684 Calico \u7f51\u7edc\uff0c\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230\u8282\u70b9 Node2(172.16.1.2)\uff0c\u6700\u7ec8\u8f6c\u53d1\u5230 Calico Pod \u4e2d\u3002 Calico Pod \u5728\u53d1\u51fa\u56de\u590d\u62a5\u6587\u65f6\uff0c\u6b64\u65f6\u7684\u76ee\u6807\u5730\u5740\u662f Calico-Macvlan Pod \u7684 calico \u7f51\u5361 ip \uff0810.233.100.3\uff09\uff0c\u6b64\u8fc7\u7a0b\u5982 Calico Pod \u4e4b\u95f4\u7684\u4e92\u76f8\u8bbf\u95ee\u4e00\u6837\uff0c\u901a\u8fc7 Calico \u8282\u70b9\u8def\u7531\u5c06\u56de\u590d\u62a5\u6587\u8f6c\u53d1\u5230\u8282\u70b9 Node1, \u7136\u540e\u901a\u8fc7 Calico \u7684\u865a\u62df\u8def\u7531\u8f6c\u53d1\u5230 Calico-Macvlan Pod \u4e2d\uff0c\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Calico + Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Macvlan Pod(\u5982\u56fe4\u9ed1\u8272\u7ebf\u6bb5\u6240\u793a): Calico-Macvlan Pod \u5177\u5907\u4e24\u5f20\u7f51\u5361\uff0cSpiderpool \u534f\u8c03\u591a\u5f20\u7f51\u5361\u7684\u8def\u7531\uff0c\u4f7f Pod \u540c\u65f6\u63a5\u5165 Overlay \u548c Underlay \u4e24\u79cd\u7f51\u7edc\u3002 ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 default via 169.254.1.1 dev eth0 172.16.0.0/16 dev net1 \uff5e# ip route show table 101 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 \u5f53 Calico-Macvlan Pod \u8bbf\u95ee Macvlan Pod\uff08172.16.100.3\uff09\uff0c\u6570\u636e\u5305\u5339\u914d\u8def\u7531( 172.16.0.0/16 dev net1 )\uff0c \u901a\u8fc7 Pod \u7684 macvlan \u7f51\u5361 net1 \u53d1\u9001\u51fa\u53bb\u3002 \u901a\u8fc7 Underlay \u7f51\u7edc\u76f4\u63a5\u8f6c\u53d1\u5230\u8282\u70b9 Node2 \u7684 Macvlan Pod\uff08172.16.100.3\uff09 Macvlan Pod\uff08172.16.100.3\uff09\u53d1\u51fa\u56de\u590d\u62a5\u6587\u65f6\uff0c\u7531\u4e8e\u76ee\u6807\u5730\u5740\u662f\uff1a172.16.100.2\uff0c\u518d\u6b21\u901a\u8fc7 Underlay \u7f51\u7edc\u5c06\u6570\u636e\u5305\u53d1\u9001\u5230 Calico-Macvlan Pod\u4e2d\uff0c\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Calico+macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Service \u8282\u70b9 Node1 \u5206\u522b\u8fd0\u884c Macvlan Pod\uff08\u53ea\u5177\u5907\u4e00\u5f20 Macvlan \u7f51\u5361\uff1a172.16.100.3\uff09\u548c Calico Pod\uff08\u53ea\u5177\u5907\u4e00\u5f20 Calico \u7f51\u5361\uff1a10.233.100.4\uff09\u3002 \u8282\u70b9 Node2 \u8fd0\u884c\u4e00\u4e2a Macvlan-Calico \u591a\u7f51\u5361 Pod\uff08\u540c\u65f6\u5177\u5907\u4e00\u5f20 Calico \u7f51\u5361\uff1a10.233.100.3 \u548c \u4e00\u5f20 Macvlan \u7f51\u5361\uff1a172.16.100.2\uff09 Calico-Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee ClusterIP\uff08endpoint \u4f7f\u7528 Calico \u7f51\u7edc): \u5982 \u56fe5 \u7684\u84dd\u8272\u7bad\u5934\u6240\u793a\uff1a \u6839\u636e Pod \u4e2d\u8def\u7531\uff0c\u8bbf\u95ee ClusterIP \u7684\u6d41\u91cf\u901a\u8fc7 eth0 \u8f6c\u53d1\u5230\u8282\u70b9 Node2 \u4e0a\u3002\u968f\u540e\u7ecf\u8fc7\u8282\u70b9\u4e0a\u7684 kube-proxy \u89e3\u6790\u5176\u76ee\u6807 clusterip \u5730\u5740\u4e3a Calico Pod \u7684 IP: 10.233.100.4, \u968f\u540e\u901a\u8fc7 calico \u8bbe\u7f6e\u7684\u8282\u70b9\u8def\u7531\u8f6c\u53d1\u5230\u76ee\u6807\u4e3b\u673a Node1\uff0c\u6700\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\uff0c\u8f6c\u53d1\u5230 Calico Pod \u4e2d\u3002 \u56de\u590d\u62a5\u6587\u901a\u8fc7 eth0 \u8f6c\u53d1\u5230\u8282\u70b9 Node1 \u4e0a\uff0c\u76ee\u6807\u5730\u5740\u4e3a Calico-Macvlan Pod \u7684 Calico \u7f51\u5361 IP\uff1a 10.233.100.3\uff0c\u518d\u6b21\u901a\u8fc7\u8282\u70b9\u95f4\u7684 Calico \u8def\u7531\uff0c\u8f6c\u53d1\u5230 Node2\u3002\u968f\u540e Node2 \u7684 kube-proxy \u5c06\u6e90\u5730\u5740\u6539\u4e3a clusterip \u7684\u5730\u5740\uff0c\u968f\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\u53d1\u9001\u5230 Calico-Macvlan Pod \u4e2d, \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Calico-Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Macvlan Pod \u7684 ClusterIP\uff1a \u5982 \u56fe5 \u7684\u7ea2\u8272\u7bad\u5934\u6240\u793a\uff1a \u6839\u636e Pod \u4e2d\u8def\u7531\uff0c\u8bbf\u95ee ClusterIP \u7684\u6d41\u91cf\u901a\u8fc7 eth0 \u8f6c\u53d1\u5230\u8282\u70b9 Node2 \u4e0a\u3002\u968f\u540e\u7ecf\u8fc7\u8282\u70b9\u4e0a\u7684 kube-proxy \u89e3\u6790\u5176\u76ee\u6807 clusterip \u5730\u5740\u4e3a Macvlan Pod \u7684 IP: 172.16.100.3, \u968f\u540e\u901a\u8fc7\u8282\u70b9\u95f4\u8def\u7531\uff0c\u8f6c\u53d1\u5230\u76ee\u6807\u4e3b\u673a Node1\u3002\u6ce8\u610f\uff1a\u6570\u636e\u5305\u4ece\u8282\u70b9 Node2 \u53d1\u51fa\u65f6\uff0c\u6e90\u5730\u5740\u5df2\u7ecf\u88ab SNAT \u4e3a\uff1a 172.16.1.3\u3002 \u62a5\u6587\u5230\u8fbe Node1 \u540e\uff0c\u901a\u8fc7\u8282\u70b9\u4e0a vethxxx \u865a\u62df\u7f51\u5361\uff0c\u6700\u7ec8\u8f6c\u53d1\u5230 Macvlan Pod \u4e2d\u3002 \u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6\uff0c\u7531\u4e8e\u76ee\u6807\u5730\u5740\u4e3a Node2 \u7684 IP\uff0c\u62a5\u6587\u901a\u8fc7 eth0 \u76f4\u63a5\u8f6c\u53d1\u5230 Node2 \u4e0a\u3002\u7ecf\u8fc7 Node2 \u7684 kube-proxy \u5c06\u6e90\u5730\u5740\u6539\u4e3a clusterip \u7684\u5730\u5740\uff0c\u76ee\u6807\u5730\u5740\u6539\u5199 Calico-Macvlan Pod \u7684 calico ip\uff0810.233.100.3\uff09\u3002\u6700\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\u53d1\u9001\u5230 Calico-Macvlan Pod \u4e2d, \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 \u8bbf\u95ee Calico+Macvlan \u591a\u7f51\u5361 Pod \u7684 NodePort \u5982 \u56fe5 \u9ed1\u8272\u7bad\u5934\u6240\u793a: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef(172.16.1.100)\u8bbf\u95ee NodePort Service\uff08172.16.1.3:32567\uff09\uff0c\u901a\u8fc7\u96c6\u7fa4\u5916\u8def\u7531\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230 Node2(172.1.6.1.3), \u901a\u8fc7 Kube-proxy \u8bbe\u7f6e\u7684 iptables DNAT \u89c4\u5219\uff0c\u5c06\u76ee\u6807\u5730\u5740\u6539\u4e3a\uff1a 10.233.100.3( Pod \u7684 Calico \u7f51\u5361 IP)\uff0c\u968f\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\uff0c\u8f6c\u53d1\u5230 Pod \u4e2d\u3002 Pod \u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6, \u6b64\u65f6\u76ee\u6807\u5730\u5740\u4e3a\uff1a 172.16.1.100\u3002\u76f4\u63a5\u5c06\u54cd\u5e94\u62a5\u6587\u901a\u8fc7 net1\uff08 Pod \u7684 macvlan \u7f51\u5361\u7684 IP\uff09\u53d1\u9001\u5230 172.16.1.100\u3002\u5ba2\u6237\u7aef\u6536\u5230\u62a5\u6587\u53d1\u73b0\u4e94\u5143\u7ec4\u5e76\u4e0d\u5339\u914d\uff0c\u6240\u4ee5\u4f1a\u5c06\u8be5\u62a5\u6587\u76f4\u63a5\u4e22\u5f03\u3002 Spiderpool \u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u7531\u4e8e\u975e\u5bf9\u79f0\u8def\u7531\u5bfc\u81f4\u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\uff0c\u5728 Pod \u4e2d\u8c03\u8c10\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd\u8bbf\u95ee NodePort \u7684\u56de\u590d\u62a5\u6587\u4ece eth0 \u53d1\u51fa\u3002 ~# ip rule ... from 10.233.100.3 lookup 100 ~# ip route show table 100 default via 169.254.1.1 dev eth0 \u8c03\u8c10\u7b56\u7565\u8def\u7531\u540e\uff0c\u56de\u590d\u62a5\u6587\u4ece eth0 \u53d1\u51fa\u5230 Node2, \u7ecf\u8fc7 Kube-proxy \u5c06\u6e90\u5730\u5740\u6539\u4e3a Node2 \u7684 IP(172.16.1.3), \u76ee\u6807\u5730\u5740\u4e3a\uff1a 172.16.1.100\u3002\u7ecf\u8fc7\u96c6\u7fa4\u5916\u8def\u7531\uff0c\u56de\u590d\u62a5\u6587\u5230 172.16.1.100\uff0c \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 \u6ce8\u610f\uff1a\u975e\u5bf9\u79f0\u8def\u7531\u8bbf\u95ee NodePort \u4e0d\u540c\u95ee\u9898\uff0c\u53ea\u6709\u5f53\u591a\u7f51\u5361 Pod \u7684\u9ed8\u8ba4\u8def\u7531\u5728 eth0 \u65f6\u624d\u5b58\u5728\u3002\u5982\u679c\u9ed8\u8ba4\u8def\u7531\u5728 net1, \u4e0d\u5b58\u5728\u8be5\u95ee\u9898\u3002 \u7ed3\u8bba \u6211\u4eec\u603b\u7ed3\u4e86\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684 Pod \u5b58\u5728\u4e8e\u4e00\u4e2a\u96c6\u7fa4\u65f6\u7684\u4e00\u4e9b\u901a\u4fe1\u573a\u666f\uff0c\u5982\u4e0b: \u6e90\\\u76ee\u6807 Calico Pod Macvlan Pod Calico + Macvlan \u591a\u7f51\u5361 Pod Calico Pod \u7684 Service Macvlan Pod \u7684 Service Calico + Macvlan \u591a\u7f51\u5361 Pod \u7684 Service Calico Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Macvlan Pod \u8981\u6c42 kube-proxy \u7684\u7248\u672c\u5927\u4e8e v1.29 \u2705 \u2705 \u2705 \u2705 \u2705 Calico + Macvlan \u591a\u7f51\u5361 Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"Calico + Macvlan \u591a CNI \u6570\u636e\u8f6c\u53d1\u6d41\u7a0b"},{"location":"concepts/multi_cni_coexist-zh_CN/#calico-macvlan-cni","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Calico + Macvlan \u591a CNI \u6570\u636e\u8f6c\u53d1\u6d41\u7a0b"},{"location":"concepts/multi_cni_coexist-zh_CN/#_1","text":"CNI \u4f5c\u4e3a Kubernetes \u7684\u96c6\u7fa4\u4e2d\u91cd\u8981\u7684\u7ec4\u4ef6\u3002\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u90fd\u4f1a\u90e8\u7f72\u4e00\u4e2a CNI(\u6bd4\u5982 Calico)\uff0c\u7531\u5b83\u8d1f\u8d23\u96c6\u7fa4\u7f51\u7edc\u7684\u8fde\u901a\u6027\u3002\u6709\u4e9b\u65f6\u5019\u4e00\u4e9b\u7ec8\u7aef\u7528\u6237\u57fa\u4e8e\u6027\u80fd\u3001\u5b89\u5168\u7b49\u7684\u8003\u8651\uff0c\u4f1a\u5728\u96c6\u7fa4\u4e2d\u4f7f\u7528\u591a\u79cd\u7c7b\u578b\u7684 CNI\uff0c\u6bd4\u5982 Underlay \u7c7b\u578b\u7684 Macvlan CNI\u3002\u6b64\u65f6\u4e00\u4e2a\u96c6\u7fa4\u5c31\u53ef\u80fd\u5b58\u5728\u591a\u79cd CNI \u7c7b\u578b\u7684 Pod\uff0c\u4e0d\u540c\u7c7b\u578b\u7684 Pod \u5206\u522b\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u573a\u666f\uff1a \u5355 Calico \u7f51\u5361\u7684 Pod: \u5982 CoreDNS \u7b49\u7cfb\u7edf\u7ec4\u4ef6\uff0c\u6ca1\u6709\u56fa\u5b9a IP \u7684\u9700\u6c42\uff0c\u4e5f\u4e0d\u9700\u8981\u5357\u5317\u5411\u6d41\u91cf\u901a\u4fe1\uff0c\u53ea\u5b58\u5728\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u901a\u4fe1\u7684\u9700\u6c42\u3002 \u5355 Macvlan \u7f51\u5361\u7684 Pod: \u9002\u7528\u4e8e\u5bf9\u6027\u80fd\uff0c\u5b89\u5168\u6709\u7279\u6b8a\u8981\u6c42\u7684\u5e94\u7528\uff0c\u6216\u9700\u8981\u4ee5 Pod IP \u76f4\u63a5\u5357\u5317\u5411\u6d41\u91cf\u901a\u4fe1\u7684\u4f20\u7edf\u4e0a\u4e91\u5e94\u7528\u3002 Calico \u548c Macvlan \u7f51\u5361\u7684\u591a\u7f51\u5361 Pod\uff1a\u540c\u65f6\u517c\u987e\u4e0a\u9762\u4e8c\u8005\u7684\u9700\u6c42\u3002\u65e2\u9700\u8981\u4ee5\u56fa\u5b9a\u7684 Pod IP \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\uff0c\u53c8\u9700\u8981\u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf(\u6bd4\u5982\u548c Calico Pod \u6216 Service)\u3002 \u53e6\u5916\uff0c\u5f53\u591a CNI \u7684 Pod \u5b58\u5728\u4e8e\u4e00\u4e2a\u96c6\u7fa4\uff0c\u5b9e\u9645\u4e0a\u8fd9\u4e2a\u96c6\u7fa4\u5b58\u5728\u4e24\u79cd\u4e0d\u540c\u7684\u6570\u636e\u8f6c\u53d1\u65b9\u6848: Underlay \u548c Overlay\u3002\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e00\u4e9b\u5176\u4ed6\u95ee\u9898: \u4f7f\u7528 Underlay \u7f51\u7edc\u7684 Pod \u65e0\u6cd5\u4e0e\u96c6\u7fa4\u4e2d\u4f7f\u7528 Overlay \u7f51\u7edc\u7684 Pod \u76f4\u63a5\u901a\u4fe1: \u7531\u4e8e\u8f6c\u53d1\u8def\u5f84\u4e0d\u4e00\u81f4\uff0cOverlay \u7f51\u7edc\u5e38\u5e38\u9700\u8981\u7ecf\u8fc7\u8282\u70b9\u4f5c\u4e8c\u6b21\u8f6c\u53d1\uff0c\u4f46 Underlay \u4e00\u822c\u76f4\u63a5\u901a\u8fc7\u5e95\u5c42\u7f51\u5173\u8f6c\u53d1\u3002\u6240\u4ee5\u5f53\u4e8c\u8005\u4e92\u76f8\u8bbf\u95ee\u65f6\uff0c\u53ef\u80fd\u7531\u4e8e\u5e95\u5c42\u4ea4\u6362\u673a\u672a\u540c\u6b65\u96c6\u7fa4\u5b50\u7f51\u7684\u8def\u7531\u5bfc\u81f4\u4e22\u5305 \u53cc CNI \u53ef\u80fd\u4f1a\u589e\u52a0\u4f7f\u7528\u548c\u8fd0\u7ef4\u590d\u6742\u5ea6\uff0c\u6bd4\u5982 IP \u5730\u5740\u7ba1\u7406\u7b49 Spiderpool \u8fd9\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u89e3\u51b3\u5f53\u96c6\u7fa4\u5b58\u5728\u591a\u79cd CNI \u65f6\u7684\u4e92\u8054\u4e92\u901a\u95ee\u9898\uff0c\u53c8\u53ef\u4ee5\u51cf\u8f7b IP \u5730\u5740\u8fd0\u7ef4\u8d1f\u62c5\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4ee5 Calico \u548c Macvlan \u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5b83\u4eec\u4e4b\u95f4\u7684\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b\u3002","title":"\u80cc\u666f"},{"location":"concepts/multi_cni_coexist-zh_CN/#_2","text":"Calico + Macvlan \u591a\u7f51\u5361\u5feb\u901f\u5f00\u59cb\u53ef\u53c2\u8003 get-stared-calico \u5355 Macvlan \u7f51\u5361\u53ef\u53c2\u8003 get-started-macvlan Underlay CNI \u8bbf\u95ee Service \u53ef\u53c2\u8003 underlay_cni_service","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"concepts/multi_cni_coexist-zh_CN/#_3","text":"Macvlan \u8bbf\u95ee Macvlan Pod Macvlan Pod \u8bbf\u95ee Service Macvlan \u8bbf\u95ee Calico \u7684 Pod Macvlan Pod \u8bbf\u95ee Calico Pod \u7684 Service Calico+Macvlan \u591a\u7f51\u5361 Pod \u5206\u522b\u8bbf\u95ee Calico \u548c Macvlan Pod Calico+macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Service \u8bbf\u95ee Calico-Macvlan \u591a\u7f51\u5361 Pod \u7684 NodePort \u4e0b\u9762\u4ecb\u7ecd\u51e0\u79cd\u5178\u578b\u7684\u901a\u4fe1\u573a\u666f:","title":"\u4e0d\u540c\u573a\u666f\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-pod","text":"","title":"Macvlan Pod \u4e4b\u95f4\u4e92\u76f8\u8bbf\u95ee\u6570\u636e\u8f6c\u53d1\u8fc7\u7a0b"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-pod_1","text":"\u5982 \u56fe1 \u6807\u53f72 \uff0cMacvlan Pod1(172.16.1.3) \u901a\u8fc7 Macvlan Bridge \u6a21\u5f0f\u673a\u5236\u76f4\u63a5\u8bbf\u95ee Macvlan Pod2(172.16.1.2), \u4e0d\u7ba1\u662f\u5426\u4e3a\u540c\u8282\u70b9\u8fd8\u662f\u4e0d\u540c\u8282\u70b9\uff0c\u90fd\u4e0d\u9700\u8981\u7ecf\u8fc7\u7f51\u5173\u8f6c\u53d1\u3002","title":"Macvlan \u8bbf\u95ee\u540c\u5b50\u7f51 Pod"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-pod_2","text":"\u5982 \u56fe1 \u6807\u53f71 , Macvlan Pod1(172.16.1.2) \u8bbf\u95ee\u8de8\u7f51\u6bb5\u8de8\u8282\u70b9\u7684 Macvlan Pod2(172.16.2.100) \u6216\u8bbf\u95ee\u8de8\u7f51\u6bb5\u4f46\u540c\u8282\u70b9\u7684 Macvlan Pod3(172.17.1.100)\u3002\u7531\u4e8e\u662f\u4e0d\u540c\u7f51\u6bb5\uff0c\u90fd\u9700\u8981\u7ecf\u8fc7\u7f51\u5173\u8f6c\u53d1\u3002","title":"Macvlan \u8bbf\u95ee\u4e0d\u540c\u5b50\u7f51\u7684 Pod"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-pod_3","text":"\u5982 \u56fe1 \u6807\u53f73 : \u7531\u4e8e Macvlan bridge \u673a\u5236\uff0cMacvlan \u5b50\u63a5\u53e3\u4e0d\u80fd\u76f4\u63a5\u8ddf Master \u63a5\u53e3\u901a\u4fe1\u3002\u6240\u4ee5\u6211\u4eec\u5728 Macvlan Pod(\u5982 172.17.1.100) \u521b\u5efa\u4e86\u4e00\u4e2a veth \u7f51\u5361\uff0c\u5e76\u5728 Pod \u548c\u8282\u70b9\u5206\u522b\u8bbe\u7f6e\u8def\u7531\uff0c\u89e3\u51b3\u4e0d\u80fd\u901a\u4fe1\u7684\u95ee\u9898\u3002\u8282\u70b9(172.17.1.1)\u8bbf\u95ee Macvlan Pod\uff08172.17.1.100\uff09 \u6839\u636e\u8282\u70b9 table500 \u4e2d\u7684\u8def\u7531\uff0c\u4ece vethxxx \u53d1\u8f6c\u53d1\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u3002 # pod network namespace ~# ip r 172.17.1.1 dev veth0 # host network namespace ~# ip r show table 500 172.17.1.100 dev vethxxx","title":"Macvlan Pod \u8bbf\u95ee \u672c\u5730\u8282\u70b9"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-pod-service","text":"\u8bbf\u95ee ClusterIP(\u5982 \u56fe2\u9ed1\u8272\u7ebf\u6240\u793a ) : Macvlan Pod\uff08172.17.1.100\uff09\u8bbf\u95ee ClusterIP\uff0810.233.0.100\uff09\u7684\u6570\u636e\u5305\u5339\u914d Pod \u5185\u90e8\u8def\u7531\uff0c\u901a\u8fc7 veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9 # pod network namespace ~# ip r 10.233.0.0/18 dev veth0 \u7ecf\u8fc7\u8282\u70b9 Kube-proxy \u8bbe\u7f6e\u7684 Service DNAT \u89c4\u5219\uff0c\u5c06 ClusterIP\uff0810.233.0.100\uff09\u89e3\u6790\u4e3a\u76ee\u6807 Macvlan Pod\uff08172.17.1.200\uff09, \u7ecf\u8fc7\u8282\u70b9\u95f4\u7684\u7f51\u7edc\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230 172.17.1.200\u3002\u6b64\u65f6\u6570\u636e\u5305\u7684\u6e90\u5730\u5740\u5df2\u88ab SNAT \u4e3a\u8282\u70b9IP\uff1a172.17.1.1. \u5230\u8fbe\u8282\u70b9 Node2\uff08172.17.1.2\uff09\uff0c\u901a\u8fc7\u4e3b\u673a\u4e0a table500 \u7684\u8def\u7531\u8868\uff0c\u5c06\u6570\u636e\u5305\u901a\u8fc7 vethxx \u8f6c\u53d1\u5230\u76ee\u6807 Pod\uff08172.17.1.100\uff09\u3002 \u76ee\u6807 Pod\uff08172.17.1.100\uff09\u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6\uff0c\u5176\u76ee\u6807\u5730\u5740\u662f\u8282\u70b9 Node1 \u7684\u5730\u5740\uff1a172.17.1.1, \u6240\u4ee5\u54cd\u5e94\u62a5\u6587\u4ece veth0 \u53d1\u51fa\uff0c\u7ecf\u8fc7\u8282\u70b9\u7f51\u7edc\u53d1\u9001\u56de Node1 \u8282\u70b9\u3002\u7ecf\u8fc7 Node1 \u8282\u70b9\u4e0a Kube-proxy \u7684 iptables \u89c4\u5219\uff0c\u5c06\u6e90\u5730\u5740\u8fd8\u539f\u4e3a ClusterIP (10.233.0.100), \u76ee\u6807\u5730\u5740\u8fd8\u539f\u4e3a Macvlan Pod\uff08172.17.1.100\uff09 \u7ecf\u8fc7\u4e3b\u673a\u4e0a\u8bbe\u7f6e\u7684 table500 \u8def\u7531\u8868\uff0c\u5c06\u54cd\u5e94\u62a5\u6587\u901a\u8fc7 vethxx \u7f51\u5361\u8f6c\u53d1\u5230\u6e90 Pod\uff08172.17.1.100\uff09\uff0c\u6574\u4e2a\u8bbf\u95ee\u8fc7\u7a0b\u7ed3\u675f\u3002 \u8bbf\u95ee NodePort Service(\u5982 \u56fe2 \u7ea2\u8272\u7ebf\u6240\u793a) \u91cd\u70b9\u4ecb\u7ecd NodePort Service \u914d\u7f6e ExternalTrafficPolicy=Local \u7684\u573a\u666f. \u96c6\u7fa4\u5916\u4e00\u4e2a\u5ba2\u6237\u7aef\uff1a1.1.1.1 \u8bbf\u95ee NodePort: 172.17.1.1:32456\u3002 \u6570\u636e\u5305\u9996\u5148\u901a\u8fc7\u5916\u90e8\u8def\u7531\u5230\u8fbe\u8282\u70b9 Node1(172.17.1.1) \u7ecf\u8fc7\u4e3b\u673a\u4e0a Kube-proxy \u8bbe\u7f6e\u7684 iptables DNAT \u89c4\u5219, \u5c06\u76ee\u6807\u5730\u5740\u6539\u5199\u4e3a Macvlan Pod(172.17.1.100), \u7ecf\u8fc7\u4e3b\u673a\u4e0a table500 \u7684\u8def\u7531\u8868\uff0c\u5c06\u6570\u636e\u5305\u901a\u8fc7 vethxx \u8f6c\u53d1\u5230\u76ee\u6807 Pod\u3002 \u5f53 Pod \u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6\uff0c\u770b\u5230\u7684\u76ee\u6807\u5730\u5740\u662f 1.1.1.1\uff0c\u5c06\u4f1a\u5339\u914d\u5230 Pod \u7684\u9ed8\u8ba4\u8def\u7531\uff0c\u5219\u4ece eth0 \u8f6c\u53d1\u51fa\u53bb\uff0c\u5bfc\u81f4\u8bbf\u95ee\u4e0d\u901a\u3002Spiderpool \u901a\u8fc7\u5728 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u8bbe\u7f6e\u4ee5\u4e0b\u7684 iptables \u89c4\u5219\u548c\u7b56\u7565\u8def\u7531\uff0c\u4f7f veth0 \u63a5\u6536\u7684 NodePort \u6d41\u91cf\u4ecd\u7136\u4ece veth0 \u8f6c\u53d1\u5230\u8282\u70b9\u4e0a\u3002 # pod network namespace iptables -i veth0 --set-xmark 0x1 ... ~# ip rule from all fwmark 0x1 lookup 500 ~# ip r show table 500 default dev veth0 \u7ecf\u8fc7\u8282\u70b9\u4e0a\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff0c\u5c06\u6570\u636e\u5305\u7684\u6e90\u5730\u5740\u6539\u4e3a\u8282\u70b9\u7684 IP(172.17.1.1)\uff0c\u518d\u901a\u8fc7\u96c6\u7fa4\u5916\u7684\u8def\u7531\uff0c\u5c06\u6570\u636e\u5305\u53d1\u9001\u81f3\u5ba2\u6237\u7aef(1.1.1.1),\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002","title":"Macvlan Pod \u8bbf\u95ee Service"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-calico-pod","text":"\u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u4e09\u79cd\u60c5\u51b5\uff1a Macvlan Pod \u8bbf\u95ee\u540c\u8282\u70b9\u7684 Calico Pod(\u5982 \u56fe3\u7684\u7ebf\u8def1 ) : Macvlan Pod\uff08172.16.100.2\uff09\u8bbf\u95ee Calico Pod\uff0810.233.100.3/18\uff09\uff0c\u6570\u636e\u5305\u5339\u914d Pod \u4e2d\u8def\u7531( 10.233.64.0/18 via veth0 ), \u5c06\u6570\u636e\u5305\u4ece veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9\u4e0a\u3002 \u6570\u636e\u5305\u5339\u914d\u8282\u70b9\u4e0a\uff1a Calico \u7684\u865a\u62df\u8def\u7531( 10.233.100.3 dev calixxx ) \u5c06\u6570\u636e\u5305\u901a\u8fc7 Calico \u865a\u62df\u7f51\u5361\u8f6c\u53d1\u5230 Calico Pod(10.233.100.3)\u3002 \u54cd\u5e94\u62a5\u6587\u901a\u8fc7 Calico \u865a\u62df\u7f51\u5361(calixxx)\u53d1\u9001\u5230\u8282\u70b9\u4e0a\uff0c\u7531\u4e8e\u76ee\u6807\u5730\u5740\u4e3a Macvlan Pod(172.16.100.2)\uff0c\u5339\u914d\u8def\u7531( 172.16.100.2 dev vethxxx table 500 ) \u8f6c\u53d1\u5230 Macvlan Pod\uff0c\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Macvlan Pod \u8de8\u8282\u70b9\u8bbf\u95ee Calico Pod(\u8282\u70b9\u540c\u7f51\u6bb5\uff0c\u5982\u56fe\u4e09\u4e2d\uff1a2->2.1): Macvlan Pod\uff08172.16.100.2\uff09\u8bbf\u95ee Calico Pod\uff0810.233.200.2/18\uff09\uff0c\u6570\u636e\u5305\u5339\u914d Pod \u4e2d\u8def\u7531( 10.233.64.0/18 via veth0 ), \u5c06\u6570\u636e\u5305\u4ece veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9\u4e0a\u3002 \u7ecf\u8fc7\u8282\u70b9\u4e4b\u95f4\u7684 Calico \u8def\u7531\uff0c\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230\u5bf9\u7aef\u8282\u70b9(172.16.1.3)\u3002\u7531\u4e8e\u76ee\u7684\u5730\u5740\u662f: 10.233.200.2, \u5339\u914d\u5230\u8282\u70b9\u7684 Calico \u8def\u7531( 10.233.100.2 dev calixxx ) \u5c06\u6570\u636e\u5305\u901a\u8fc7 Calico \u865a\u62df\u7f51\u5361\u8f6c\u53d1\u5230 Calico Pod(10.233.100.2)\u3002 \u7531\u4e8e\u6570\u636e\u5305\u6700\u539f\u59cb\u7684\u6e90\u5730\u5740\u4e3a Macvlan Pod(172.16.100.2), \u6240\u4ee5 Calico Pod \u4f1a\u5c06\u54cd\u5e94\u62a5\u6587\u5148\u8f6c\u53d1\u5230\u672c\u8282\u70b9(172.16.1.3)\u4e0a\uff0c\u7136\u540e\u5c06\u54cd\u5e94\u62a5\u6587\u76f4\u63a5\u8f6c\u53d1\u5230 Macvlan Pod(172.16.100.2),\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u8282\u70b9\u8f6c\u53d1\uff0c\u5bfc\u81f4\u4e86\u6570\u636e\u5305\u6765\u56de\u8f6c\u53d1\u8def\u5f84\u4e0d\u4e00\u81f4\uff0c\u53ef\u80fd\u4f1a\u88ab\u5185\u6838\u8ba4\u4e3a\u5176\u6570\u636e\u5305\u7684 conntrack \u7684 state \u4e3a invalid\uff0c\u4f1a\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u4e22\u5f03: ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' iptables -A FORWARD -m conntrack --ctstate INVALID -j DROP \u8be5\u89c4\u5219\u539f\u662f\u4e3a\u4e86\u89e3\u51b3 #Issue74839 \u63d0\u51fa\u7684\u95ee\u9898\uff0c\u56e0\u4e3a \u67d0\u4e9b tcp \u62a5\u6587\u5927\u5c0f\u8d85\u51fa\u7a97\u53e3\u9650\u5236\uff0c\u5bfc\u81f4\u88ab\u5185\u6838\u6807\u8bb0\u5176 conntrack \u7684 state \u4e3a invalid\uff0c\u4ece\u800c\u5bfc\u81f4\u6574\u4e2a tcp \u94fe\u63a5\u88ab reset\u3002\u4e8e\u662f k8s \u793e\u533a\u901a\u8fc7\u4e0b\u53d1\u8fd9\u6761\u89c4\u5219\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u8fd9\u6761\u89c4\u5219\u53ef\u80fd\u4f1a\u5f71\u54cd\u6b64\u573a\u666f\u4e2d\u6570\u636e\u5305\u6765\u56de\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002\u5982\u793e\u533a\u6709\u76f8\u5173\u7684 issue \u62a5\u544a\uff1a #Issue117924 , #Issue94861 , #Issue177 \u7b49\u3002 \u6211\u4eec\u901a\u8fc7\u63a8\u52a8\u793e\u533a\u4fee\u590d\u6b64\u4e86\u95ee\u9898\uff0c\u6700\u7ec8\u5728 only drop invalid cstate packets if non liberal \u5f97\u5230\u89e3\u51b3\uff0ckubernetes \u7248\u672c\u4e3a v1.29\u3002\u6211\u4eec\u9700\u8981\u786e\u4fdd\u8bbe\u7f6e\u6bcf\u4e2a\u8282\u70b9\u7684 sysctl \u53c2\u6570: sysctl -w net.netfilter.nf_conntrack_tcp_be_liberal=1 \uff0c\u5e76\u4e14\u91cd\u542f Kube-proxy\uff0c\u8fd9\u6837 kube-proxy \u5c31\u4e0d\u4f1a\u4e0b\u53d1\u8fd9\u6761 drop \u89c4\u5219\uff0c\u4e5f\u5c31\u4e0d\u4f1a\u5f71\u54cd\u5230\u5355 Macvlan pod \u4e0e\u5355 Calico pod \u4e4b\u95f4\u7684\u901a\u4fe1\u3002 \u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u68c0\u67e5\u8282\u70b9\u662f\u5426\u8fd8\u5b58\u5728\u8fd9\u6761 drop \u89c4\u5219\uff0c\u5982\u679c\u6ca1\u6709\u8f93\u51fa\u8bf4\u660e\u6b63\u5e38\u3002\u5426\u5219\u8bf7\u68c0\u67e5 sysctl \u662f\u5426\u6b63\u786e\u8bbe\u7f6e\u4ee5\u53ca\u662f\u5426\u91cd\u542f kube-proxy\u3002 ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' \u6ce8\u610f\uff1a \u5fc5\u987b\u786e\u4fdd k8s \u7248\u672c\u5927\u4e8e v1.29\u3002\u5982\u679c\u60a8\u7684 k8s \u7248\u672c\u5c0f\u4e8e v1.29, \u8be5\u95ee\u9898\u65e0\u6cd5\u89c4\u907f\u3002","title":"Macvlan \u8bbf\u95ee Calico \u7684 Pod"},{"location":"concepts/multi_cni_coexist-zh_CN/#macvlan-pod-calico-pod-service","text":"Macvlan Pod(172.16.100.2) \u63d2\u5165\u4e00\u6761 \u52ab\u6301 service \u6d41\u91cf\u7684\u8def\u7531, \u4f7f Pod \u8bbf\u95ee ClusterIP(10.233.0.100) \u7684\u65f6\u5019\uff0c\u6570\u636e\u5305\u901a\u8fc7 veth0 \u7f51\u5361\u8f6c\u53d1\u5230\u8282\u70b9 Node1(172.16.1.2)\u3002 \uff5e# ip r 10.233.0.0/18 via 10.7.168.71 dev eth0 src 10.233.100.2 \u5f53\u6570\u636e\u5305\u8f6c\u53d1\u5230\u8282\u70b9 Node1(172.16.1.2) \u4e4b\u540e\uff0c\u7ecf\u8fc7\u4e3b\u673a\u7f51\u7edc\u534f\u8bae\u6808\u7684 kube-proxy \u5c06 clusterip \u8f6c\u6362\u4e3a Calico Pod \u7684 IP\uff1a10.233.200.2\u3002 \u968f\u540e\u901a\u8fc7 Calico \u8bbe\u7f6e\u7684\u96a7\u9053\u8def\u7531\u8f6c\u53d1\u5230\u8282\u70b9 Node2(172.16.1.3)\u3002\u6ce8\u610f\u5f53\u8bf7\u6c42\u6570\u636e\u5305\u4ece\u8282\u70b9 node3 \u53d1\u51fa\u65f6\uff0c\u5176\u6e90\u5730\u5740\u4f1a\u88ab SNAT \u4e3a\u7684\u8282\u70b9 Node1 \u7684 IP(172.16.1.2)\u3002\u8fd9\u786e\u4fdd\u56de\u590d\u6570\u636e\u5305\u80fd\u591f\u539f\u8def\u8fd4\u56de\uff0c\u800c\u4e0d\u4f1a\u51fa\u73b0 Macvlan Pod \u8bbf\u95ee Calico Pod \u53ef\u80fd\u51fa\u73b0\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u8fd9\u6837\u8bf7\u6c42\u6570\u636e\u5305\u8f6c\u53d1\u5230\u4e3b\u673a Node2 \u540e\uff0c\u901a\u8fc7\u8282\u70b9\u4e0a calixxx \u865a\u62df\u7f51\u5361\u8f6c\u53d1\u5230\u4e86 Calico Pod\uff0810.233.200.2\uff09\u3002 Calico Pod\uff0810.233.200.2\uff09\u5c06\u56de\u590d\u62a5\u6587\u8f6c\u53d1\u5230\u8282\u70b9 Node2\u3002\u6b64\u65f6\u56de\u590d\u6570\u636e\u5305\u7684\u76ee\u6807\u5730\u5740\u4e3a\u8282\u70b9 Node1(172.16.1.2)\uff0c\u6240\u4ee5\u901a\u8fc7\u8282\u70b9\u8def\u7531\u8f6c\u53d1\u5230 Node1\u3002\u968f\u540e\u901a\u8fc7 Kube-proxy \u5c06\u76ee\u6807\u5730\u5740\u6539\u5199\u4e3a Macvlan Pod \u7684 IP\uff1a 172.16.100.2\uff0c\u6e90\u5730\u5740\u6539\u4e3a ClusterIP(10.233.0.100)\u3002\u968f\u540e\u5339\u914d\u4e3b\u673a\u4e0a\u8bbe\u7f6e\u7684\u8def\u7531 table500( 172.16.100.2 dev vethxxx table 500 )\uff0c\u5c06\u6570\u636e\u5305\u53d1\u9001\u5230 Macvlan Pod, \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002","title":"Macvlan Pod \u8bbf\u95ee Calico Pod \u7684 Service"},{"location":"concepts/multi_cni_coexist-zh_CN/#calicomacvlan-pod-calico-macvlan-pod","text":"\u5982 \u56fe4 \u6240\u793a\uff1a\u8282\u70b9 Node1 \u7684 Calico-Macvlan Pod (\u540c\u65f6\u5177\u5907 calico \u548c macvlan \u7f51\u5361\u7684 pod\uff0c\u8282\u70b9 Node2 \u8fd0\u884c\u4e86 Macvlan Pod\uff08\u53ea\u5177\u5907 macvlan \u7684\u7f51\u5361\uff09\u548c Calico Pod\uff08\u53ea\u5177\u5907 calico \u7684\u7f51\u5361\uff09\u3002 Calico + Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Calico Pod(\u5982\u56fe4\u7ea2\u8272\u7ebf\u6bb5\u6240\u793a): Calico-Macvlan Pod \u5177\u5907\u4e24\u5f20\u7f51\u5361\uff0cSpiderpool \u534f\u8c03\u591a\u5f20\u7f51\u5361\u7684\u8def\u7531\uff0c\u4f7f Pod \u63a5\u5165 Overlay \u548c Underlay \u4e24\u79cd\u7f51\u7edc\u3002 ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 172.16.0.0/16 dev net1 default via 169.254.1.1 dev eth0 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 \u5f53 Calico-Macvlan Pod \u8bbf\u95ee Calico Pod\uff0810.233.100.4\uff09\uff0c\u5339\u914d Pod \u7684\u8def\u7531( 10.233.64.0/18 dev eth0 ) \u901a\u8fc7 eth0 \u5c06\u6570\u636e\u5305\u53d1\u9001\u5230\u8282\u70b9 Node1(172.16.1.1)\uff0c\u6e90\u5730\u5740\u4e3a Calico \u7f51\u5361\u7684 IP(10.233.100.3)\u3002\u7531\u4e8e\u76ee\u6807\u5730\u5740\u4e3a 10.233.100.4\uff0c\u6240\u4ee5\u4f1a\u901a\u8fc7\u8282\u70b9\u95f4\u7684 Calico \u7f51\u7edc\uff0c\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230\u8282\u70b9 Node2(172.16.1.2)\uff0c\u6700\u7ec8\u8f6c\u53d1\u5230 Calico Pod \u4e2d\u3002 Calico Pod \u5728\u53d1\u51fa\u56de\u590d\u62a5\u6587\u65f6\uff0c\u6b64\u65f6\u7684\u76ee\u6807\u5730\u5740\u662f Calico-Macvlan Pod \u7684 calico \u7f51\u5361 ip \uff0810.233.100.3\uff09\uff0c\u6b64\u8fc7\u7a0b\u5982 Calico Pod \u4e4b\u95f4\u7684\u4e92\u76f8\u8bbf\u95ee\u4e00\u6837\uff0c\u901a\u8fc7 Calico \u8282\u70b9\u8def\u7531\u5c06\u56de\u590d\u62a5\u6587\u8f6c\u53d1\u5230\u8282\u70b9 Node1, \u7136\u540e\u901a\u8fc7 Calico \u7684\u865a\u62df\u8def\u7531\u8f6c\u53d1\u5230 Calico-Macvlan Pod \u4e2d\uff0c\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Calico + Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Macvlan Pod(\u5982\u56fe4\u9ed1\u8272\u7ebf\u6bb5\u6240\u793a): Calico-Macvlan Pod \u5177\u5907\u4e24\u5f20\u7f51\u5361\uff0cSpiderpool \u534f\u8c03\u591a\u5f20\u7f51\u5361\u7684\u8def\u7531\uff0c\u4f7f Pod \u540c\u65f6\u63a5\u5165 Overlay \u548c Underlay \u4e24\u79cd\u7f51\u7edc\u3002 ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 default via 169.254.1.1 dev eth0 172.16.0.0/16 dev net1 \uff5e# ip route show table 101 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 \u5f53 Calico-Macvlan Pod \u8bbf\u95ee Macvlan Pod\uff08172.16.100.3\uff09\uff0c\u6570\u636e\u5305\u5339\u914d\u8def\u7531( 172.16.0.0/16 dev net1 )\uff0c \u901a\u8fc7 Pod \u7684 macvlan \u7f51\u5361 net1 \u53d1\u9001\u51fa\u53bb\u3002 \u901a\u8fc7 Underlay \u7f51\u7edc\u76f4\u63a5\u8f6c\u53d1\u5230\u8282\u70b9 Node2 \u7684 Macvlan Pod\uff08172.16.100.3\uff09 Macvlan Pod\uff08172.16.100.3\uff09\u53d1\u51fa\u56de\u590d\u62a5\u6587\u65f6\uff0c\u7531\u4e8e\u76ee\u6807\u5730\u5740\u662f\uff1a172.16.100.2\uff0c\u518d\u6b21\u901a\u8fc7 Underlay \u7f51\u7edc\u5c06\u6570\u636e\u5305\u53d1\u9001\u5230 Calico-Macvlan Pod\u4e2d\uff0c\u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002","title":"Calico+Macvlan \u591a\u7f51\u5361\u7684 Pod \u5206\u522b\u8bbf\u95ee Calico \u548c Macvlan Pod"},{"location":"concepts/multi_cni_coexist-zh_CN/#calicomacvlan-pod-service","text":"\u8282\u70b9 Node1 \u5206\u522b\u8fd0\u884c Macvlan Pod\uff08\u53ea\u5177\u5907\u4e00\u5f20 Macvlan \u7f51\u5361\uff1a172.16.100.3\uff09\u548c Calico Pod\uff08\u53ea\u5177\u5907\u4e00\u5f20 Calico \u7f51\u5361\uff1a10.233.100.4\uff09\u3002 \u8282\u70b9 Node2 \u8fd0\u884c\u4e00\u4e2a Macvlan-Calico \u591a\u7f51\u5361 Pod\uff08\u540c\u65f6\u5177\u5907\u4e00\u5f20 Calico \u7f51\u5361\uff1a10.233.100.3 \u548c \u4e00\u5f20 Macvlan \u7f51\u5361\uff1a172.16.100.2\uff09 Calico-Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee ClusterIP\uff08endpoint \u4f7f\u7528 Calico \u7f51\u7edc): \u5982 \u56fe5 \u7684\u84dd\u8272\u7bad\u5934\u6240\u793a\uff1a \u6839\u636e Pod \u4e2d\u8def\u7531\uff0c\u8bbf\u95ee ClusterIP \u7684\u6d41\u91cf\u901a\u8fc7 eth0 \u8f6c\u53d1\u5230\u8282\u70b9 Node2 \u4e0a\u3002\u968f\u540e\u7ecf\u8fc7\u8282\u70b9\u4e0a\u7684 kube-proxy \u89e3\u6790\u5176\u76ee\u6807 clusterip \u5730\u5740\u4e3a Calico Pod \u7684 IP: 10.233.100.4, \u968f\u540e\u901a\u8fc7 calico \u8bbe\u7f6e\u7684\u8282\u70b9\u8def\u7531\u8f6c\u53d1\u5230\u76ee\u6807\u4e3b\u673a Node1\uff0c\u6700\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\uff0c\u8f6c\u53d1\u5230 Calico Pod \u4e2d\u3002 \u56de\u590d\u62a5\u6587\u901a\u8fc7 eth0 \u8f6c\u53d1\u5230\u8282\u70b9 Node1 \u4e0a\uff0c\u76ee\u6807\u5730\u5740\u4e3a Calico-Macvlan Pod \u7684 Calico \u7f51\u5361 IP\uff1a 10.233.100.3\uff0c\u518d\u6b21\u901a\u8fc7\u8282\u70b9\u95f4\u7684 Calico \u8def\u7531\uff0c\u8f6c\u53d1\u5230 Node2\u3002\u968f\u540e Node2 \u7684 kube-proxy \u5c06\u6e90\u5730\u5740\u6539\u4e3a clusterip \u7684\u5730\u5740\uff0c\u968f\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\u53d1\u9001\u5230 Calico-Macvlan Pod \u4e2d, \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 Calico-Macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Macvlan Pod \u7684 ClusterIP\uff1a \u5982 \u56fe5 \u7684\u7ea2\u8272\u7bad\u5934\u6240\u793a\uff1a \u6839\u636e Pod \u4e2d\u8def\u7531\uff0c\u8bbf\u95ee ClusterIP \u7684\u6d41\u91cf\u901a\u8fc7 eth0 \u8f6c\u53d1\u5230\u8282\u70b9 Node2 \u4e0a\u3002\u968f\u540e\u7ecf\u8fc7\u8282\u70b9\u4e0a\u7684 kube-proxy \u89e3\u6790\u5176\u76ee\u6807 clusterip \u5730\u5740\u4e3a Macvlan Pod \u7684 IP: 172.16.100.3, \u968f\u540e\u901a\u8fc7\u8282\u70b9\u95f4\u8def\u7531\uff0c\u8f6c\u53d1\u5230\u76ee\u6807\u4e3b\u673a Node1\u3002\u6ce8\u610f\uff1a\u6570\u636e\u5305\u4ece\u8282\u70b9 Node2 \u53d1\u51fa\u65f6\uff0c\u6e90\u5730\u5740\u5df2\u7ecf\u88ab SNAT \u4e3a\uff1a 172.16.1.3\u3002 \u62a5\u6587\u5230\u8fbe Node1 \u540e\uff0c\u901a\u8fc7\u8282\u70b9\u4e0a vethxxx \u865a\u62df\u7f51\u5361\uff0c\u6700\u7ec8\u8f6c\u53d1\u5230 Macvlan Pod \u4e2d\u3002 \u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6\uff0c\u7531\u4e8e\u76ee\u6807\u5730\u5740\u4e3a Node2 \u7684 IP\uff0c\u62a5\u6587\u901a\u8fc7 eth0 \u76f4\u63a5\u8f6c\u53d1\u5230 Node2 \u4e0a\u3002\u7ecf\u8fc7 Node2 \u7684 kube-proxy \u5c06\u6e90\u5730\u5740\u6539\u4e3a clusterip \u7684\u5730\u5740\uff0c\u76ee\u6807\u5730\u5740\u6539\u5199 Calico-Macvlan Pod \u7684 calico ip\uff0810.233.100.3\uff09\u3002\u6700\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\u53d1\u9001\u5230 Calico-Macvlan Pod \u4e2d, \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002","title":"Calico+macvlan \u591a\u7f51\u5361 Pod \u8bbf\u95ee Service"},{"location":"concepts/multi_cni_coexist-zh_CN/#calicomacvlan-pod-nodeport","text":"\u5982 \u56fe5 \u9ed1\u8272\u7bad\u5934\u6240\u793a: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef(172.16.1.100)\u8bbf\u95ee NodePort Service\uff08172.16.1.3:32567\uff09\uff0c\u901a\u8fc7\u96c6\u7fa4\u5916\u8def\u7531\u5c06\u6570\u636e\u5305\u8f6c\u53d1\u5230 Node2(172.1.6.1.3), \u901a\u8fc7 Kube-proxy \u8bbe\u7f6e\u7684 iptables DNAT \u89c4\u5219\uff0c\u5c06\u76ee\u6807\u5730\u5740\u6539\u4e3a\uff1a 10.233.100.3( Pod \u7684 Calico \u7f51\u5361 IP)\uff0c\u968f\u540e\u901a\u8fc7 calixxx \u865a\u62df\u7f51\u5361\uff0c\u8f6c\u53d1\u5230 Pod \u4e2d\u3002 Pod \u53d1\u51fa\u54cd\u5e94\u62a5\u6587\u65f6, \u6b64\u65f6\u76ee\u6807\u5730\u5740\u4e3a\uff1a 172.16.1.100\u3002\u76f4\u63a5\u5c06\u54cd\u5e94\u62a5\u6587\u901a\u8fc7 net1\uff08 Pod \u7684 macvlan \u7f51\u5361\u7684 IP\uff09\u53d1\u9001\u5230 172.16.1.100\u3002\u5ba2\u6237\u7aef\u6536\u5230\u62a5\u6587\u53d1\u73b0\u4e94\u5143\u7ec4\u5e76\u4e0d\u5339\u914d\uff0c\u6240\u4ee5\u4f1a\u5c06\u8be5\u62a5\u6587\u76f4\u63a5\u4e22\u5f03\u3002 Spiderpool \u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u7531\u4e8e\u975e\u5bf9\u79f0\u8def\u7531\u5bfc\u81f4\u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\uff0c\u5728 Pod \u4e2d\u8c03\u8c10\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd\u8bbf\u95ee NodePort \u7684\u56de\u590d\u62a5\u6587\u4ece eth0 \u53d1\u51fa\u3002 ~# ip rule ... from 10.233.100.3 lookup 100 ~# ip route show table 100 default via 169.254.1.1 dev eth0 \u8c03\u8c10\u7b56\u7565\u8def\u7531\u540e\uff0c\u56de\u590d\u62a5\u6587\u4ece eth0 \u53d1\u51fa\u5230 Node2, \u7ecf\u8fc7 Kube-proxy \u5c06\u6e90\u5730\u5740\u6539\u4e3a Node2 \u7684 IP(172.16.1.3), \u76ee\u6807\u5730\u5740\u4e3a\uff1a 172.16.1.100\u3002\u7ecf\u8fc7\u96c6\u7fa4\u5916\u8def\u7531\uff0c\u56de\u590d\u62a5\u6587\u5230 172.16.1.100\uff0c \u6574\u4e2a\u8bbf\u95ee\u7ed3\u675f\u3002 \u6ce8\u610f\uff1a\u975e\u5bf9\u79f0\u8def\u7531\u8bbf\u95ee NodePort \u4e0d\u540c\u95ee\u9898\uff0c\u53ea\u6709\u5f53\u591a\u7f51\u5361 Pod \u7684\u9ed8\u8ba4\u8def\u7531\u5728 eth0 \u65f6\u624d\u5b58\u5728\u3002\u5982\u679c\u9ed8\u8ba4\u8def\u7531\u5728 net1, \u4e0d\u5b58\u5728\u8be5\u95ee\u9898\u3002","title":"\u8bbf\u95ee Calico+Macvlan \u591a\u7f51\u5361 Pod \u7684 NodePort"},{"location":"concepts/multi_cni_coexist-zh_CN/#_4","text":"\u6211\u4eec\u603b\u7ed3\u4e86\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684 Pod \u5b58\u5728\u4e8e\u4e00\u4e2a\u96c6\u7fa4\u65f6\u7684\u4e00\u4e9b\u901a\u4fe1\u573a\u666f\uff0c\u5982\u4e0b: \u6e90\\\u76ee\u6807 Calico Pod Macvlan Pod Calico + Macvlan \u591a\u7f51\u5361 Pod Calico Pod \u7684 Service Macvlan Pod \u7684 Service Calico + Macvlan \u591a\u7f51\u5361 Pod \u7684 Service Calico Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Macvlan Pod \u8981\u6c42 kube-proxy \u7684\u7248\u672c\u5927\u4e8e v1.29 \u2705 \u2705 \u2705 \u2705 \u2705 Calico + Macvlan \u591a\u7f51\u5361 Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"\u7ed3\u8bba"},{"location":"concepts/multi_cni_coexist/","text":"Calico + Macvlan Multi-CNI Data Forwarding Process English | \u7b80\u4f53\u4e2d\u6587 Background CNIs are important components of a Kubernetes cluster. Typically, one CNI (e.g. Calico) is deployed and is responsible for the connectivity of the cluster network. In some cases, customers may use multiple types of CNIs in the cluster based on performance, security, etc., such as Macvlan CNIs of the Underlay type, and then there may be multiple Pods of different CNI types in a cluster, and different types of Pods are suitable for different scenarios: Pod with a single Calico NIC: System components such as CoreDNS do not have the need for a fixed IP, nor do they need to communicate with north-south traffic, but only need to communicate with east-west traffic in the cluster. Pods with a single Macvlan card: For applications with special requirements for performance and security, or for traditional up-cloud applications that require direct north-south traffic with the Pod IP. Multi-Card Pod with Calico and Macvlan cards: A combination of both. Both need to access cluster north-south traffic with a fixed Pod IP and cluster east-west traffic (e.g., with a Calico Pod or Service). In addition, when multiple CNI pods exist in a cluster, there are actually two different data forwarding schemes in the cluster: Underlay and Overlay. this can lead to a number of other issues: Pods using the Underlay network cannot access the cluster's north-south traffic. Pods using Underlay networks cannot communicate directly with Pods using Overlay networks in the cluster: Due to inconsistent forwarding paths, Overlay networks often need to go through nodes for secondary forwarding, whereas Underlay networks are generally forwarded directly through the underlying gateway. Therefore, when they access each other, packet loss may occur because the underlying switch does not synchronize the routes of the cluster subnet. Using two network modes for a cluster may increase the complexity of use and operation, such as IP address management. Spiderpool is a complete Underlay network solution that solves the interoperability problem when there are multiple CNIs in a cluster and reduces the IP address operation and maintenance burden. The following section describes the data forwarding process between them. Quick start Calico + Macvlan multi-network card quickstart can be found in get-stared-calico For a single Macvlan card see get-started-macvlan . Underlay CNI Access Service See underlay_cni_service Serval Data forwarding Flow Below are several typical communication scenarios: Macvlan Accessing Macvlan Pods Macvlan Pod Accessing Service Macvlan Accessing Calico Pods Macvlan Pod Accessing Calico Pod's Service Calico+Macvlan Multi-NIC Pods Accessing Calico and Macvlan Pods Calico+Macvlan Multi-NIC Pods Accessing Service Accessing NodePort of Calico-Macvlan Multi-NIC Pods Data Forwarding Process Between Macvlan Pods Macvlan Access to Pods in the Same Subnet As shown in Figure 1 Label 2 , Macvlan Pod1(172.16.1.3) directly accesses Macvlan Pod2(172.16.1.2) through the Macvlan Bridge mode mechanism. Whether on the same node or different nodes, gateway forwarding is not required. Macvlan Access to Pods in Different Subnets As shown in Figure 1 Label 1 , Macvlan Pod1(172.16.1.2) accesses cross-segment and cross-node Macvlan Pod2(172.16.2.100) or cross-segment but same-node Macvlan Pod3(172.17.1.100). Due to different network segments, gateway forwarding is required. Macvlan Pod Access to Local Node As shown in Figure 1 Label 3 : Due to the Macvlan bridge mechanism, Macvlan sub-interfaces cannot communicate directly with the Master interface. Therefore, we created a veth network card in the Macvlan Pod (e.g., 172.17.1.100) and set up routes in both Pod and node to solve the communication issue. The node(172.17.1.1) accesses Macvlan Pod(172.17.1.100) according to the route in node table500, forwarding from vethxxx to the Pod's network namespace. # pod network namespace ~# ip r 172.17.1.1 dev veth0 # host network namespace ~# ip r show table 500 172.17.1.100 dev vethxxx Macvlan Pod Access to Service Accessing ClusterIP (as shown by the black line in Figure 2 ) : Macvlan Pod(172.17.1.100) accessing ClusterIP(10.233.0.100) matches Pod internal routing and forwards packets through veth0 network card to the node # pod network namespace ~# ip r 10.233.0.0/18 dev veth0 Through the Service DNAT rules set by node Kube-proxy, ClusterIP(10.233.0.100) is resolved to target Macvlan Pod(172.17.1.200), and packets are forwarded to 172.17.1.200 through inter-node network. At this point, the source address has been SNAT'd to node IP: 172.17.1.1. Upon reaching Node2(172.17.1.2), packets are forwarded to target Pod(172.17.1.100) through vethxx via the host's table500 routing table. When the target Pod(172.17.1.100) sends response packets, its destination address is Node1's address: 172.17.1.1, so response packets are sent from veth0 through the node network back to Node1. Through Node1's Kube-proxy iptables rules, the source address is restored to ClusterIP(10.233.0.100) and destination address to Macvlan Pod(172.17.1.100) Through the host's table500 routing table, response packets are forwarded via vethxx network card to source Pod(172.17.1.100), completing the access process. Accessing NodePort Service: as shown by the red line in Figure 2 : Focus on the scenario where NodePort Service is configured with ExternalTrafficPolicy=Local. An external client: 1.1.1.1 accesses NodePort: 172.17.1.1:32456. Packets first reach node Node1(172.17.1.1) through external routing Through the host's Kube-proxy configured iptables DNAT rules, the destination address is rewritten to Macvlan Pod(172.17.1.100), and through the host's table500 routing table, packets are forwarded via vethxx to the target Pod. When the Pod sends response packets, seeing destination address 1.1.1.1, it matches the Pod's default route and forwards from eth0, causing communication failure. Spiderpool solves this by setting the following iptables rules and policy routing in the Pod's network namespace, ensuring NodePort traffic received by veth0 is still forwarded to the node via veth0. # pod network namespace iptables -i veth0 --set-xmark 0x1 ... ~# ip rule from all fwmark 0x1 lookup 500 ~# ip r show table 500 default dev veth0 Through the node's network protocol stack, the packet's source address is changed to the node's IP(172.17.1.1), then through external cluster routing, packets are sent to the client(1.1.1.1), completing the access process. Macvlan Access to Calico Pod There are three main scenarios: Macvlan Pod accessing Calico Pod on the same node (as shown in Figure 3 Path 1 ): Macvlan Pod(172.16.100.2) accessing Calico Pod(10.233.100.3/18), packets match Pod routing( 10.233.64.0/18 via veth0 ), forwarding packets from veth0 network card to the node. Packets match on the node: Calico's virtual route( 10.233.100.3 dev calixxx ) forwards packets through Calico virtual network card to Calico Pod(10.233.100.3). Response packets are sent to the node through Calico virtual network card(calixxx), and since the destination address is Macvlan Pod(172.16.100.2), matching route( 172.16.100.2 dev vethxxx table 500 ) forwards to Macvlan Pod, completing the access. Macvlan Pod accessing Calico Pod across nodes (nodes in same subnet, as shown in Figure 3: 2->2.1): Macvlan Pod(172.16.100.2) accessing Calico Pod(10.233.200.2/18), packets match Pod routing( 10.233.64.0/18 via veth0 ), forwarding packets from veth0 network card to the node. Through inter-node Calico routing, packets are forwarded to peer node(172.16.1.3). Since the destination address is 10.233.200.2, matching node's Calico route( 10.233.100.2 dev calixxx ) forwards packets through Calico virtual network card to Calico Pod(10.233.100.2). Since the original source address is Macvlan Pod(172.16.100.2), Calico Pod will forward response packets first to its local node(172.16.1.3), then directly forward response packets to Macvlan Pod(172.16.100.2) without going through node forwarding, causing inconsistent packet forwarding paths. This may cause the kernel to mark the packet's conntrack state as invalid, leading to packet drop by a kube-proxy iptables rule: ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' iptables -A FORWARD -m conntrack --ctstate INVALID -j DROP This rule was originally implemented to solve the issue raised in #Issue 74839 , where some TCP packets exceeding window limits were marked as invalid conntrack state by the kernel, causing the entire TCP connection to reset. The k8s community addressed this by implementing this rule, but it may affect scenarios with inconsistent packet paths. Related community issues include: #Issue 117924 , #Issue 94861 , #Issue 177 . We helped resolve this issue through community collaboration, ultimately fixed in only drop invalid cstate packets if non liberal for kubernetes version v1.29. We need to ensure each node's sysctl parameter is set: sysctl -w net.netfilter.nf_conntrack_tcp_be_liberal=1 , and restart Kube-proxy. This prevents kube-proxy from implementing the drop rule, avoiding impact on communication between single Macvlan pod and single Calico pod. After execution, check if the drop rule still exists on the node. No output indicates normal operation. Otherwise, check if sysctl is correctly set and if kube-proxy has been restarted. ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' Note: Must ensure k8s version is greater than v1.29. If your k8s version is below v1.29, this issue cannot be avoided. Macvlan Pod Accessing Calico Pod's Service Macvlan Pod(172.16.100.2) inserts a route to intercept service traffic, making Pod access to ClusterIP(10.233.0.100) forward packets through veth0 network card to node Node1(172.16.1.2). \uff5e# ip r 10.233.0.0/18 via 10.7.168.71 dev eth0 src 10.233.100.2 After packets are forwarded to node Node1(172.16.1.2), the host network stack's kube-proxy converts clusterip to Calico Pod's IP: 10.233.200.2. Then through Calico's tunnel routing forwards to node Node2(172.16.1.3). Note that when request packets are sent from node3, their source address is SNAT'd to Node1's IP(172.16.1.2). This ensures response packets can return via the same path, avoiding inconsistent routing paths when Macvlan Pod accesses Calico Pod. After request packets are forwarded to host Node2, they are forwarded to Calico Pod(10.233.200.2) through the node's calixxx virtual network card. Calico Pod(10.233.200.2) forwards response packets to node Node2. At this point, the response packet's destination address is node Node1(172.16.1.2), so it's forwarded to Node1 through node routing. Then through Kube-proxy, the destination address is rewritten to Macvlan Pod's IP: 172.16.100.2, and source address to ClusterIP(10.233.0.100). Then matching the host's route table500( 172.16.100.2 dev vethxxx table 500 ), packets are sent to Macvlan Pod, completing the access. Calico+Macvlan Multi-NIC Pod Accessing Calico and Macvlan Pods As shown in Figure 4 : Node1 runs a Calico-Macvlan Pod (with both calico and macvlan network cards), while Node2 runs a Macvlan Pod (with only macvlan network card) and a Calico Pod (with only calico network card). Calico + Macvlan Multi-NIC Pod Accessing Calico Pod: as shown by red line in Figure 4: Calico-Macvlan Pod has two network cards, Spiderpool coordinates routing between multiple cards, enabling Pod to access both Overlay and Underlay networks. ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 172.16.0.0/16 dev net1 default via 169.254.1.1 dev eth0 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 When Calico-Macvlan Pod accesses Calico Pod(10.233.100.4), it matches Pod routing( 10.233.64.0/18 dev eth0 ) and sends packets through eth0 to node Node1(172.16.1.1), with source address being Calico network card's IP(10.233.100.3). Since the destination address is 10.233.100.4, packets are forwarded through inter-node Calico network to node Node2(172.16.1.2), and finally to Calico Pod. When Calico Pod sends response packets, the destination address is Calico-Macvlan Pod's calico network card IP(10.233.100.3). This process is similar to communication between Calico Pods, forwarding response packets through Calico node routing to Node1, then through Calico's virtual routing to Calico-Macvlan Pod, completing the access. Calico + Macvlan Multi-NIC Pod Accessing Macvlan Pod as shown by black line in Figure 4: Calico-Macvlan Pod has two network cards, Spiderpool coordinates routing between multiple cards, enabling Pod to access both Overlay and Underlay networks simultaneously. ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 default via 169.254.1.1 dev eth0 172.16.0.0/16 dev net1 \uff5e# ip route show table 101 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 When Calico-Macvlan Pod accesses Macvlan Pod(172.16.100.3), packets match routing( 172.16.0.0/16 dev net1 ) and are sent through Pod's macvlan network card net1 . Packets are forwarded directly through Underlay network to Node2's Macvlan Pod(172.16.100.3) When Macvlan Pod(172.16.100.3) sends response packets, since the destination address is 172.16.100.2, packets are sent back to Calico-Macvlan Pod through Underlay network, completing the access. Calico+Macvlan Multi-NIC Pod Accessing Service Node1 runs a Macvlan Pod (with only one Macvlan network card: 172.16.100.3) and a Calico Pod (with only one Calico network card: 10.233.100.4). Calico-Macvlan Multi-NIC Pod Accessing ClusterIP (endpoint using Calico network) As shown by the blue arrow in Figure 5 : According to the routing in the Pod, traffic accessing ClusterIP is forwarded through eth0 to node Node2. Then, the kube-proxy on the node resolves the target clusterip address to the Calico Pod's IP: 10.233.100.4, and forwards it through the node's routing to the target host Node1, finally forwarding it to the Calico Pod through the calixxx virtual network card. Response packets are forwarded through eth0 to node Node1, with the destination address being the Calico-Macvlan Pod's Calico network card IP: 10.233.100.3. They are then forwarded through inter-node Calico routing to Node2. Subsequently, Node2's kube-proxy rewrites the source address to the clusterip address, and then sends it to the Calico-Macvlan Pod through the calixxx virtual network card, completing the access. Calico-Macvlan Multi-NIC Pod Accessing ClusterIP (endpoint using Macvlan network) As shown by the red arrow in Figure 5 : According to the routing in the Pod, traffic accessing ClusterIP is forwarded through eth0 to node Node2. Then, the kube-proxy on the node resolves the target clusterip address to the Macvlan Pod's IP: 172.16.100.3, and forwards it through inter-node routing to the target host Node1. Note that when packets are sent from Node2, their source address has already been SNAT'd to: 172.16.1.3. Upon reaching Node1, packets are forwarded through the vethxxx virtual network card, ultimately reaching the Macvlan Pod. When response packets are sent, since the destination address is Node2's IP, the packets are directly forwarded through eth0 to Node2. After passing through Node2's kube-proxy, the source address is rewritten to the clusterip address, and the destination address is changed to the Calico-Macvlan Pod's Calico IP (10.233.100.3). Finally, the packets are sent to the Calico-Macvlan Pod through the calixxx virtual network card, completing the access. Accessing NodePort of Calico-Macvlan Multi-NIC Pod (as shown by the black arrow in Figure 5 ) An external client (172.16.1.100) accesses NodePort Service (172.16.1.3:32567). The packets are routed externally to Node2 (172.16.1.3), and through the iptables DNAT rules set by Kube-proxy, the destination address is changed to: 10.233.100.3 (the Calico network card IP of the Pod), and then forwarded to the Pod through the calixxx virtual network card. When the Pod sends response packets, the destination address is: 172.16.1.100. The response packets are sent directly through net1 (the IP of the Pod's Macvlan network card) to 172.16.1.100. The client receives the packets and finds that the five-tuple does not match, so it discards the packets. To resolve this issue caused by asymmetric routing, Spiderpool tunes the policy routing in the Pod to ensure that the response packets to NodePort are sent out from eth0. ~# ip rule ... from 10.233.100.3 lookup 100 ~# ip route show table 100 default via 169.254.1.1 dev eth0 After tuning the policy routing, response packets are sent from eth0 to Node2, and through Kube-proxy, the source address is changed to Node2's IP (172.16.1.3), with the destination address being: 172.16.1.100. The response packets are routed externally to 172.16.1.100, completing the access. Note: The issue of asymmetric routing when accessing NodePort only exists when the default route of the multi-NIC Pod is on eth0. If the default route is on net1, this issue does not occur. Conclusion We have summarized some communication scenarios when these three types of Pods exist in a cluster as follows. Source\\Target Calico Pod Macvlan Pod Calico + Macvlan Multi-NIC Pod Service for Calico Pod Service for Macvlan Pod Service for Calico + Macvlan Multi-NIC Pod Calico Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Macvlan Pod requires kube-proxy version greater than v1.29. \u2705 \u2705 \u2705 \u2705 \u2705 Calico + Macvlan Multi NIC Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"Calico/Macvlan Multi-CNI Data Forwarding Workflow"},{"location":"concepts/multi_cni_coexist/#calico-macvlan-multi-cni-data-forwarding-process","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Calico + Macvlan Multi-CNI Data Forwarding Process"},{"location":"concepts/multi_cni_coexist/#background","text":"CNIs are important components of a Kubernetes cluster. Typically, one CNI (e.g. Calico) is deployed and is responsible for the connectivity of the cluster network. In some cases, customers may use multiple types of CNIs in the cluster based on performance, security, etc., such as Macvlan CNIs of the Underlay type, and then there may be multiple Pods of different CNI types in a cluster, and different types of Pods are suitable for different scenarios: Pod with a single Calico NIC: System components such as CoreDNS do not have the need for a fixed IP, nor do they need to communicate with north-south traffic, but only need to communicate with east-west traffic in the cluster. Pods with a single Macvlan card: For applications with special requirements for performance and security, or for traditional up-cloud applications that require direct north-south traffic with the Pod IP. Multi-Card Pod with Calico and Macvlan cards: A combination of both. Both need to access cluster north-south traffic with a fixed Pod IP and cluster east-west traffic (e.g., with a Calico Pod or Service). In addition, when multiple CNI pods exist in a cluster, there are actually two different data forwarding schemes in the cluster: Underlay and Overlay. this can lead to a number of other issues: Pods using the Underlay network cannot access the cluster's north-south traffic. Pods using Underlay networks cannot communicate directly with Pods using Overlay networks in the cluster: Due to inconsistent forwarding paths, Overlay networks often need to go through nodes for secondary forwarding, whereas Underlay networks are generally forwarded directly through the underlying gateway. Therefore, when they access each other, packet loss may occur because the underlying switch does not synchronize the routes of the cluster subnet. Using two network modes for a cluster may increase the complexity of use and operation, such as IP address management. Spiderpool is a complete Underlay network solution that solves the interoperability problem when there are multiple CNIs in a cluster and reduces the IP address operation and maintenance burden. The following section describes the data forwarding process between them.","title":"Background"},{"location":"concepts/multi_cni_coexist/#quick-start","text":"Calico + Macvlan multi-network card quickstart can be found in get-stared-calico For a single Macvlan card see get-started-macvlan . Underlay CNI Access Service See underlay_cni_service","title":"Quick start"},{"location":"concepts/multi_cni_coexist/#serval-data-forwarding-flow","text":"Below are several typical communication scenarios: Macvlan Accessing Macvlan Pods Macvlan Pod Accessing Service Macvlan Accessing Calico Pods Macvlan Pod Accessing Calico Pod's Service Calico+Macvlan Multi-NIC Pods Accessing Calico and Macvlan Pods Calico+Macvlan Multi-NIC Pods Accessing Service Accessing NodePort of Calico-Macvlan Multi-NIC Pods","title":"Serval Data forwarding Flow"},{"location":"concepts/multi_cni_coexist/#data-forwarding-process-between-macvlan-pods","text":"","title":"Data Forwarding Process Between Macvlan Pods"},{"location":"concepts/multi_cni_coexist/#macvlan-access-to-pods-in-the-same-subnet","text":"As shown in Figure 1 Label 2 , Macvlan Pod1(172.16.1.3) directly accesses Macvlan Pod2(172.16.1.2) through the Macvlan Bridge mode mechanism. Whether on the same node or different nodes, gateway forwarding is not required.","title":"Macvlan Access to Pods in the Same Subnet"},{"location":"concepts/multi_cni_coexist/#macvlan-access-to-pods-in-different-subnets","text":"As shown in Figure 1 Label 1 , Macvlan Pod1(172.16.1.2) accesses cross-segment and cross-node Macvlan Pod2(172.16.2.100) or cross-segment but same-node Macvlan Pod3(172.17.1.100). Due to different network segments, gateway forwarding is required.","title":"Macvlan Access to Pods in Different Subnets"},{"location":"concepts/multi_cni_coexist/#macvlan-pod-access-to-local-node","text":"As shown in Figure 1 Label 3 : Due to the Macvlan bridge mechanism, Macvlan sub-interfaces cannot communicate directly with the Master interface. Therefore, we created a veth network card in the Macvlan Pod (e.g., 172.17.1.100) and set up routes in both Pod and node to solve the communication issue. The node(172.17.1.1) accesses Macvlan Pod(172.17.1.100) according to the route in node table500, forwarding from vethxxx to the Pod's network namespace. # pod network namespace ~# ip r 172.17.1.1 dev veth0 # host network namespace ~# ip r show table 500 172.17.1.100 dev vethxxx","title":"Macvlan Pod Access to Local Node"},{"location":"concepts/multi_cni_coexist/#macvlan-pod-access-to-service","text":"Accessing ClusterIP (as shown by the black line in Figure 2 ) : Macvlan Pod(172.17.1.100) accessing ClusterIP(10.233.0.100) matches Pod internal routing and forwards packets through veth0 network card to the node # pod network namespace ~# ip r 10.233.0.0/18 dev veth0 Through the Service DNAT rules set by node Kube-proxy, ClusterIP(10.233.0.100) is resolved to target Macvlan Pod(172.17.1.200), and packets are forwarded to 172.17.1.200 through inter-node network. At this point, the source address has been SNAT'd to node IP: 172.17.1.1. Upon reaching Node2(172.17.1.2), packets are forwarded to target Pod(172.17.1.100) through vethxx via the host's table500 routing table. When the target Pod(172.17.1.100) sends response packets, its destination address is Node1's address: 172.17.1.1, so response packets are sent from veth0 through the node network back to Node1. Through Node1's Kube-proxy iptables rules, the source address is restored to ClusterIP(10.233.0.100) and destination address to Macvlan Pod(172.17.1.100) Through the host's table500 routing table, response packets are forwarded via vethxx network card to source Pod(172.17.1.100), completing the access process. Accessing NodePort Service: as shown by the red line in Figure 2 : Focus on the scenario where NodePort Service is configured with ExternalTrafficPolicy=Local. An external client: 1.1.1.1 accesses NodePort: 172.17.1.1:32456. Packets first reach node Node1(172.17.1.1) through external routing Through the host's Kube-proxy configured iptables DNAT rules, the destination address is rewritten to Macvlan Pod(172.17.1.100), and through the host's table500 routing table, packets are forwarded via vethxx to the target Pod. When the Pod sends response packets, seeing destination address 1.1.1.1, it matches the Pod's default route and forwards from eth0, causing communication failure. Spiderpool solves this by setting the following iptables rules and policy routing in the Pod's network namespace, ensuring NodePort traffic received by veth0 is still forwarded to the node via veth0. # pod network namespace iptables -i veth0 --set-xmark 0x1 ... ~# ip rule from all fwmark 0x1 lookup 500 ~# ip r show table 500 default dev veth0 Through the node's network protocol stack, the packet's source address is changed to the node's IP(172.17.1.1), then through external cluster routing, packets are sent to the client(1.1.1.1), completing the access process.","title":"Macvlan Pod Access to Service"},{"location":"concepts/multi_cni_coexist/#macvlan-access-to-calico-pod","text":"There are three main scenarios: Macvlan Pod accessing Calico Pod on the same node (as shown in Figure 3 Path 1 ): Macvlan Pod(172.16.100.2) accessing Calico Pod(10.233.100.3/18), packets match Pod routing( 10.233.64.0/18 via veth0 ), forwarding packets from veth0 network card to the node. Packets match on the node: Calico's virtual route( 10.233.100.3 dev calixxx ) forwards packets through Calico virtual network card to Calico Pod(10.233.100.3). Response packets are sent to the node through Calico virtual network card(calixxx), and since the destination address is Macvlan Pod(172.16.100.2), matching route( 172.16.100.2 dev vethxxx table 500 ) forwards to Macvlan Pod, completing the access. Macvlan Pod accessing Calico Pod across nodes (nodes in same subnet, as shown in Figure 3: 2->2.1): Macvlan Pod(172.16.100.2) accessing Calico Pod(10.233.200.2/18), packets match Pod routing( 10.233.64.0/18 via veth0 ), forwarding packets from veth0 network card to the node. Through inter-node Calico routing, packets are forwarded to peer node(172.16.1.3). Since the destination address is 10.233.200.2, matching node's Calico route( 10.233.100.2 dev calixxx ) forwards packets through Calico virtual network card to Calico Pod(10.233.100.2). Since the original source address is Macvlan Pod(172.16.100.2), Calico Pod will forward response packets first to its local node(172.16.1.3), then directly forward response packets to Macvlan Pod(172.16.100.2) without going through node forwarding, causing inconsistent packet forwarding paths. This may cause the kernel to mark the packet's conntrack state as invalid, leading to packet drop by a kube-proxy iptables rule: ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' iptables -A FORWARD -m conntrack --ctstate INVALID -j DROP This rule was originally implemented to solve the issue raised in #Issue 74839 , where some TCP packets exceeding window limits were marked as invalid conntrack state by the kernel, causing the entire TCP connection to reset. The k8s community addressed this by implementing this rule, but it may affect scenarios with inconsistent packet paths. Related community issues include: #Issue 117924 , #Issue 94861 , #Issue 177 . We helped resolve this issue through community collaboration, ultimately fixed in only drop invalid cstate packets if non liberal for kubernetes version v1.29. We need to ensure each node's sysctl parameter is set: sysctl -w net.netfilter.nf_conntrack_tcp_be_liberal=1 , and restart Kube-proxy. This prevents kube-proxy from implementing the drop rule, avoiding impact on communication between single Macvlan pod and single Calico pod. After execution, check if the drop rule still exists on the node. No output indicates normal operation. Otherwise, check if sysctl is correctly set and if kube-proxy has been restarted. ~# iptables-save -t filter | grep '--ctstate INVALID -j DROP' Note: Must ensure k8s version is greater than v1.29. If your k8s version is below v1.29, this issue cannot be avoided.","title":"Macvlan Access to Calico Pod"},{"location":"concepts/multi_cni_coexist/#macvlan-pod-accessing-calico-pods-service","text":"Macvlan Pod(172.16.100.2) inserts a route to intercept service traffic, making Pod access to ClusterIP(10.233.0.100) forward packets through veth0 network card to node Node1(172.16.1.2). \uff5e# ip r 10.233.0.0/18 via 10.7.168.71 dev eth0 src 10.233.100.2 After packets are forwarded to node Node1(172.16.1.2), the host network stack's kube-proxy converts clusterip to Calico Pod's IP: 10.233.200.2. Then through Calico's tunnel routing forwards to node Node2(172.16.1.3). Note that when request packets are sent from node3, their source address is SNAT'd to Node1's IP(172.16.1.2). This ensures response packets can return via the same path, avoiding inconsistent routing paths when Macvlan Pod accesses Calico Pod. After request packets are forwarded to host Node2, they are forwarded to Calico Pod(10.233.200.2) through the node's calixxx virtual network card. Calico Pod(10.233.200.2) forwards response packets to node Node2. At this point, the response packet's destination address is node Node1(172.16.1.2), so it's forwarded to Node1 through node routing. Then through Kube-proxy, the destination address is rewritten to Macvlan Pod's IP: 172.16.100.2, and source address to ClusterIP(10.233.0.100). Then matching the host's route table500( 172.16.100.2 dev vethxxx table 500 ), packets are sent to Macvlan Pod, completing the access.","title":"Macvlan Pod Accessing Calico Pod's Service"},{"location":"concepts/multi_cni_coexist/#calicomacvlan-multi-nic-pod-accessing-calico-and-macvlan-pods","text":"As shown in Figure 4 : Node1 runs a Calico-Macvlan Pod (with both calico and macvlan network cards), while Node2 runs a Macvlan Pod (with only macvlan network card) and a Calico Pod (with only calico network card). Calico + Macvlan Multi-NIC Pod Accessing Calico Pod: as shown by red line in Figure 4: Calico-Macvlan Pod has two network cards, Spiderpool coordinates routing between multiple cards, enabling Pod to access both Overlay and Underlay networks. ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 172.16.0.0/16 dev net1 default via 169.254.1.1 dev eth0 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 When Calico-Macvlan Pod accesses Calico Pod(10.233.100.4), it matches Pod routing( 10.233.64.0/18 dev eth0 ) and sends packets through eth0 to node Node1(172.16.1.1), with source address being Calico network card's IP(10.233.100.3). Since the destination address is 10.233.100.4, packets are forwarded through inter-node Calico network to node Node2(172.16.1.2), and finally to Calico Pod. When Calico Pod sends response packets, the destination address is Calico-Macvlan Pod's calico network card IP(10.233.100.3). This process is similar to communication between Calico Pods, forwarding response packets through Calico node routing to Node1, then through Calico's virtual routing to Calico-Macvlan Pod, completing the access. Calico + Macvlan Multi-NIC Pod Accessing Macvlan Pod as shown by black line in Figure 4: Calico-Macvlan Pod has two network cards, Spiderpool coordinates routing between multiple cards, enabling Pod to access both Overlay and Underlay networks simultaneously. ~# # all routing-table of eth0(calico) ~# ip route show table main 172.16.1.2 dev eth0 10.233.64.0/18 dev eth0 default via 169.254.1.1 dev eth0 172.16.0.0/16 dev net1 \uff5e# ip route show table 101 ~# # all routing-table of net1(macvlan) ~# ip route show table 101 default via 172.16.0.1 dev net1 172.16.0.0/16 dev net1 When Calico-Macvlan Pod accesses Macvlan Pod(172.16.100.3), packets match routing( 172.16.0.0/16 dev net1 ) and are sent through Pod's macvlan network card net1 . Packets are forwarded directly through Underlay network to Node2's Macvlan Pod(172.16.100.3) When Macvlan Pod(172.16.100.3) sends response packets, since the destination address is 172.16.100.2, packets are sent back to Calico-Macvlan Pod through Underlay network, completing the access.","title":"Calico+Macvlan Multi-NIC Pod Accessing Calico and Macvlan Pods"},{"location":"concepts/multi_cni_coexist/#calicomacvlan-multi-nic-pod-accessing-service","text":"Node1 runs a Macvlan Pod (with only one Macvlan network card: 172.16.100.3) and a Calico Pod (with only one Calico network card: 10.233.100.4). Calico-Macvlan Multi-NIC Pod Accessing ClusterIP (endpoint using Calico network) As shown by the blue arrow in Figure 5 : According to the routing in the Pod, traffic accessing ClusterIP is forwarded through eth0 to node Node2. Then, the kube-proxy on the node resolves the target clusterip address to the Calico Pod's IP: 10.233.100.4, and forwards it through the node's routing to the target host Node1, finally forwarding it to the Calico Pod through the calixxx virtual network card. Response packets are forwarded through eth0 to node Node1, with the destination address being the Calico-Macvlan Pod's Calico network card IP: 10.233.100.3. They are then forwarded through inter-node Calico routing to Node2. Subsequently, Node2's kube-proxy rewrites the source address to the clusterip address, and then sends it to the Calico-Macvlan Pod through the calixxx virtual network card, completing the access. Calico-Macvlan Multi-NIC Pod Accessing ClusterIP (endpoint using Macvlan network) As shown by the red arrow in Figure 5 : According to the routing in the Pod, traffic accessing ClusterIP is forwarded through eth0 to node Node2. Then, the kube-proxy on the node resolves the target clusterip address to the Macvlan Pod's IP: 172.16.100.3, and forwards it through inter-node routing to the target host Node1. Note that when packets are sent from Node2, their source address has already been SNAT'd to: 172.16.1.3. Upon reaching Node1, packets are forwarded through the vethxxx virtual network card, ultimately reaching the Macvlan Pod. When response packets are sent, since the destination address is Node2's IP, the packets are directly forwarded through eth0 to Node2. After passing through Node2's kube-proxy, the source address is rewritten to the clusterip address, and the destination address is changed to the Calico-Macvlan Pod's Calico IP (10.233.100.3). Finally, the packets are sent to the Calico-Macvlan Pod through the calixxx virtual network card, completing the access.","title":"Calico+Macvlan Multi-NIC Pod Accessing Service"},{"location":"concepts/multi_cni_coexist/#accessing-nodeport-of-calico-macvlan-multi-nic-pod-as-shown-by-the-black-arrow-in-figure-5","text":"An external client (172.16.1.100) accesses NodePort Service (172.16.1.3:32567). The packets are routed externally to Node2 (172.16.1.3), and through the iptables DNAT rules set by Kube-proxy, the destination address is changed to: 10.233.100.3 (the Calico network card IP of the Pod), and then forwarded to the Pod through the calixxx virtual network card. When the Pod sends response packets, the destination address is: 172.16.1.100. The response packets are sent directly through net1 (the IP of the Pod's Macvlan network card) to 172.16.1.100. The client receives the packets and finds that the five-tuple does not match, so it discards the packets. To resolve this issue caused by asymmetric routing, Spiderpool tunes the policy routing in the Pod to ensure that the response packets to NodePort are sent out from eth0. ~# ip rule ... from 10.233.100.3 lookup 100 ~# ip route show table 100 default via 169.254.1.1 dev eth0 After tuning the policy routing, response packets are sent from eth0 to Node2, and through Kube-proxy, the source address is changed to Node2's IP (172.16.1.3), with the destination address being: 172.16.1.100. The response packets are routed externally to 172.16.1.100, completing the access. Note: The issue of asymmetric routing when accessing NodePort only exists when the default route of the multi-NIC Pod is on eth0. If the default route is on net1, this issue does not occur.","title":"Accessing NodePort of Calico-Macvlan Multi-NIC Pod (as shown by the black arrow in Figure 5)"},{"location":"concepts/multi_cni_coexist/#conclusion","text":"We have summarized some communication scenarios when these three types of Pods exist in a cluster as follows. Source\\Target Calico Pod Macvlan Pod Calico + Macvlan Multi-NIC Pod Service for Calico Pod Service for Macvlan Pod Service for Calico + Macvlan Multi-NIC Pod Calico Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Macvlan Pod requires kube-proxy version greater than v1.29. \u2705 \u2705 \u2705 \u2705 \u2705 Calico + Macvlan Multi NIC Pod \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"Conclusion"},{"location":"develop/CODE-OF-CONDUCT/","text":"Code of Conduct This project follows the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"develop/CODE-OF-CONDUCT/#code-of-conduct","text":"This project follows the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"develop/contributing/","text":"Contribution setup cluster and run E2E test check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools build the local image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image set up the cluster and run E2E test What you should know is that there are some scenarios for different test. There are three scenarios mapping to different setup and test commands Goal Command for setup cluster Command for running E2E test test spiderpool make e2e_init_spiderpool make e2e_test_spiderpool test for dual-CNI cluster with calico and spiderpool make e2e_init_overlay_calico make e2e_test_overlay_calico test for dual-CNI cluster with calico and spiderpool make e2e_init_overlay_cilium make e2e_test_overlay_cilium if you are in China, it could add -e E2E_CHINA_IMAGE_REGISTRY=true to pull images from china image registry, add -e HTTP_PROXY=http://${ADDR} to get chart Examples for setup : # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init_spiderpool ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init_spiderpool -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init_spiderpool -e E2E_IP_FAMILY=ipv6 # for china developer not able access ghcr.io # it pulls images from another image registry and just use http proxy to pull chart $ make e2e_init_spiderpool -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with calico cni $ make e2e_init_calico -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with cilium cni $ make e2e_init_cilium_legacyservice -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} if it is expected to test a specified released images, run following commands : # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${MULTUS_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init_spiderpool -e E2E_SPIDERPOOL_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} \\ -e E2E_MULTUS_IMAGE_NAME=${MULTUS_IMAGE_NAME} # run all e2e test $ make e2e_test Example for running the e2e test: # run all e2e test on dual-stack cluster $ make e2e_test_spiderpool # run all e2e test on ipv4-only cluster $ make e2e_test_spiderpool -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test_spiderpool -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test_spiderpool -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test_spiderpool -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test_spiderpool -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_calico # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_cilium_legacyservice $ ls e2ereport.json $ make clean_e2e It could visit \" http://HostIp:4040 \" from the browser of your computer and get flame graph clean make clean_e2e Submit Pull Request A pull request will be checked by following workflow, which is required for merging. Only the PR labeled with the following is allowed to be merged, which is used to generate changelog when releasing. label name description release/bug this PR is to fix a bug release/none do not generate changelog when relasing release/feature-new this PR is to add a new feature release/feature-changed theis PR is to modify the implementation of an exsited feature When releasing, the changelog will be created automatically. The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'. Your PR should be signed off. When you commit your modification, add -s in your commit command git commit -s The CI check yaml format. If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml . Any golang or shell file should be licensed correctly. The CI check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . when markdown spell go error, you can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling . when CI failing for lint yaml file, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml . Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Contribution Guide"},{"location":"develop/contributing/#contribution","text":"","title":"Contribution"},{"location":"develop/contributing/#setup-cluster-and-run-e2e-test","text":"check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools build the local image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image set up the cluster and run E2E test What you should know is that there are some scenarios for different test. There are three scenarios mapping to different setup and test commands Goal Command for setup cluster Command for running E2E test test spiderpool make e2e_init_spiderpool make e2e_test_spiderpool test for dual-CNI cluster with calico and spiderpool make e2e_init_overlay_calico make e2e_test_overlay_calico test for dual-CNI cluster with calico and spiderpool make e2e_init_overlay_cilium make e2e_test_overlay_cilium if you are in China, it could add -e E2E_CHINA_IMAGE_REGISTRY=true to pull images from china image registry, add -e HTTP_PROXY=http://${ADDR} to get chart Examples for setup : # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init_spiderpool ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init_spiderpool -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init_spiderpool -e E2E_IP_FAMILY=ipv6 # for china developer not able access ghcr.io # it pulls images from another image registry and just use http proxy to pull chart $ make e2e_init_spiderpool -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with calico cni $ make e2e_init_calico -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with cilium cni $ make e2e_init_cilium_legacyservice -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} if it is expected to test a specified released images, run following commands : # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${MULTUS_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init_spiderpool -e E2E_SPIDERPOOL_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} \\ -e E2E_MULTUS_IMAGE_NAME=${MULTUS_IMAGE_NAME} # run all e2e test $ make e2e_test Example for running the e2e test: # run all e2e test on dual-stack cluster $ make e2e_test_spiderpool # run all e2e test on ipv4-only cluster $ make e2e_test_spiderpool -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test_spiderpool -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test_spiderpool -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test_spiderpool -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test_spiderpool -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_calico # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_cilium_legacyservice $ ls e2ereport.json $ make clean_e2e It could visit \" http://HostIp:4040 \" from the browser of your computer and get flame graph clean make clean_e2e","title":"setup cluster and run E2E test"},{"location":"develop/contributing/#submit-pull-request","text":"A pull request will be checked by following workflow, which is required for merging. Only the PR labeled with the following is allowed to be merged, which is used to generate changelog when releasing. label name description release/bug this PR is to fix a bug release/none do not generate changelog when relasing release/feature-new this PR is to add a new feature release/feature-changed theis PR is to modify the implementation of an exsited feature When releasing, the changelog will be created automatically. The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'. Your PR should be signed off. When you commit your modification, add -s in your commit command git commit -s The CI check yaml format. If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml . Any golang or shell file should be licensed correctly. The CI check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . when markdown spell go error, you can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling . when CI failing for lint yaml file, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml . Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Submit Pull Request"},{"location":"develop/release/","text":"workflow for release pre-steps update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml' update version in '/VERSION' a version tag should be set on right branch. The version should go with v0.1.0-rc0 v0.1.0-rc1 v0.1.0 v0.1.1 v0.1.2 v0.2.0-rc0 v0.2.0 update roadmap push a version tag If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"release/feature-new\" to be classified to \"New Features\" label \"release/feature-changed\" to be classified to \"Changed Features\" label \"release/bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml post Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"Release workflow"},{"location":"develop/release/#workflow-for-release","text":"","title":"workflow for release"},{"location":"develop/release/#pre-steps","text":"update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml' update version in '/VERSION' a version tag should be set on right branch. The version should go with v0.1.0-rc0 v0.1.0-rc1 v0.1.0 v0.1.1 v0.1.2 v0.2.0-rc0 v0.2.0 update roadmap","title":"pre-steps"},{"location":"develop/release/#push-a-version-tag","text":"If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"release/feature-new\" to be classified to \"New Features\" label \"release/feature-changed\" to be classified to \"Changed Features\" label \"release/bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml","title":"push a version tag"},{"location":"develop/release/#post","text":"Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"post"},{"location":"develop/roadmap/","text":"roadmap feature description Alpha release Beta release GA release SpiderIppool ip settings v0.2.0 v0.4.0 v0.6.0 namespace affinity v0.4.0 v0.6.0 application affinity v0.4.0 v0.6.0 multiple default ippool v0.6.0 multusname affinity v0.6.0 nodename affinity v0.6.0 v0.6.0 default cluster ippool v0.2.0 v0.4.0 v0.6.0 default namespace ippool v0.4.0 v0.5.0 default CNI ippool v0.4.0 v0.4.0 annotation ippool v0.2.0 v0.5.0 annotation route v0.2.0 v0.5.0 ippools for multi-interfaces without specified interface name in annotation v0.9.0 Auto inject RDMA resources for pods base on webhook v1.0.0 SpiderSubnet automatically create ippool v0.4.0 automatically scaling and deletion ip according to application v0.4.0 automatically delete ippool v0.5.0 annotation for multiple interface v0.4.0 keep ippool after deleting application v0.5.0 support deployment, statefulset, job, replicaset v0.4.0 support operator controller v0.4.0 flexible ip number v0.5.0 ippool inherit route and gateway attribute from its subnet v0.6.0 reservedIP reservedIP v0.4.0 v0.6.0 Fixed IP fixed ip for each pod of statefulset v0.5.0 fixed ip ranges for statefulset, deployment, replicaset v0.4.0 v0.6.0 fixed ip for kubevirt v0.8.0 support calico v0.5.0 v0.6.0 support weave v0.5.0 v0.6.0 Spidermultusconfig support macvlan ipvlan sriov custom v0.6.0 v0.7.0 support ovs-cni v0.7.0 support chain cni v0.9.6 CNI version cni v1.0.0 v0.4.0 v0.5.0 ifacer bond interface v0.6.0 v0.8.0 vlan interface v0.6.0 v0.8.0 SpiderCoordinator Sync podCIDR for calico v0.6.0 v0.8.0 Sync podCIDR for cilium v0.6.0 v0.8.0 sync clusterIP CIDR from serviceCIDR to support k8s 1.29 v0.8.0 Coordinator support underlay mode v0.6.0 v0.7.0 support overlay mode v0.6.0 v0.8.0 CRD spidercoordinators for multus configuration v0.6.0 v0.8.0 detect ip conflict and gateway v0.6.0 v0.6.0 specify the MAC of pod v0.6.0 v0.8.0 tune the default route of pod multiple interfaces v0.6.0 v0.8.0 Add an link-local address to veth0 for istio v0.9.0 Connectivity visit service based on kube-proxy v0.6.0 v0.7.0 visit local node to guarantee the pod health check v0.6.0 v0.7.0 visit nodePort with spec.externalTrafficPolicy=local or spec.externalTrafficPolicy=cluster v0.6.0 Observability eBPF: pod stats In plan Network Policy ipvlan v0.8.0 macvlan In plan sriov In plan Bandwidth ipvlan v0.8.0 macvlan In plan sriov In plan eBPF implement service by cgroup eBPF v0.8.0 accelerate communication of pods on a same node In plan Recycle IP recycle IP taken by deleted pod v0.4.0 v0.6.0 recycle IP taken by deleting pod v0.4.0 v0.6.0 recycle IP when detected IP conflict v0.10.0 Dual Stack dual-stack v0.2.0 v0.4.0 CLI debug and operate. check which pod an IP is taken by, check IP usage , trigger GC In plan Multi-cluster a broker cluster could synchronize ippool resource within a same subnet from all member clusters, which could help avoid IP conflict In plan support submariner v0.8.0 Dual CNI underlay cooperate with cilium v0.7.0 underlay cooperate with calico v0.7.0 RDMA support macvlan and ipvlan CNI for RoCE device v0.8.0 support sriov CNI for RoCE device v0.8.0 support ipoib CNI for infiniband device v0.9.0 support ib-sriov CNI for infiniband device v0.9.0 EgressGateway egressGateway v0.8.0 Structured Parameter DynamicResourceAllocation implement dra framework In plan allocated nics base on GPU and Nic topology In plan support for schedule pod by SpiderMultusConfig or SpiderIPPool Todo Multi-network Todo","title":"Roadmap"},{"location":"develop/roadmap/#roadmap","text":"feature description Alpha release Beta release GA release SpiderIppool ip settings v0.2.0 v0.4.0 v0.6.0 namespace affinity v0.4.0 v0.6.0 application affinity v0.4.0 v0.6.0 multiple default ippool v0.6.0 multusname affinity v0.6.0 nodename affinity v0.6.0 v0.6.0 default cluster ippool v0.2.0 v0.4.0 v0.6.0 default namespace ippool v0.4.0 v0.5.0 default CNI ippool v0.4.0 v0.4.0 annotation ippool v0.2.0 v0.5.0 annotation route v0.2.0 v0.5.0 ippools for multi-interfaces without specified interface name in annotation v0.9.0 Auto inject RDMA resources for pods base on webhook v1.0.0 SpiderSubnet automatically create ippool v0.4.0 automatically scaling and deletion ip according to application v0.4.0 automatically delete ippool v0.5.0 annotation for multiple interface v0.4.0 keep ippool after deleting application v0.5.0 support deployment, statefulset, job, replicaset v0.4.0 support operator controller v0.4.0 flexible ip number v0.5.0 ippool inherit route and gateway attribute from its subnet v0.6.0 reservedIP reservedIP v0.4.0 v0.6.0 Fixed IP fixed ip for each pod of statefulset v0.5.0 fixed ip ranges for statefulset, deployment, replicaset v0.4.0 v0.6.0 fixed ip for kubevirt v0.8.0 support calico v0.5.0 v0.6.0 support weave v0.5.0 v0.6.0 Spidermultusconfig support macvlan ipvlan sriov custom v0.6.0 v0.7.0 support ovs-cni v0.7.0 support chain cni v0.9.6 CNI version cni v1.0.0 v0.4.0 v0.5.0 ifacer bond interface v0.6.0 v0.8.0 vlan interface v0.6.0 v0.8.0 SpiderCoordinator Sync podCIDR for calico v0.6.0 v0.8.0 Sync podCIDR for cilium v0.6.0 v0.8.0 sync clusterIP CIDR from serviceCIDR to support k8s 1.29 v0.8.0 Coordinator support underlay mode v0.6.0 v0.7.0 support overlay mode v0.6.0 v0.8.0 CRD spidercoordinators for multus configuration v0.6.0 v0.8.0 detect ip conflict and gateway v0.6.0 v0.6.0 specify the MAC of pod v0.6.0 v0.8.0 tune the default route of pod multiple interfaces v0.6.0 v0.8.0 Add an link-local address to veth0 for istio v0.9.0 Connectivity visit service based on kube-proxy v0.6.0 v0.7.0 visit local node to guarantee the pod health check v0.6.0 v0.7.0 visit nodePort with spec.externalTrafficPolicy=local or spec.externalTrafficPolicy=cluster v0.6.0 Observability eBPF: pod stats In plan Network Policy ipvlan v0.8.0 macvlan In plan sriov In plan Bandwidth ipvlan v0.8.0 macvlan In plan sriov In plan eBPF implement service by cgroup eBPF v0.8.0 accelerate communication of pods on a same node In plan Recycle IP recycle IP taken by deleted pod v0.4.0 v0.6.0 recycle IP taken by deleting pod v0.4.0 v0.6.0 recycle IP when detected IP conflict v0.10.0 Dual Stack dual-stack v0.2.0 v0.4.0 CLI debug and operate. check which pod an IP is taken by, check IP usage , trigger GC In plan Multi-cluster a broker cluster could synchronize ippool resource within a same subnet from all member clusters, which could help avoid IP conflict In plan support submariner v0.8.0 Dual CNI underlay cooperate with cilium v0.7.0 underlay cooperate with calico v0.7.0 RDMA support macvlan and ipvlan CNI for RoCE device v0.8.0 support sriov CNI for RoCE device v0.8.0 support ipoib CNI for infiniband device v0.9.0 support ib-sriov CNI for infiniband device v0.9.0 EgressGateway egressGateway v0.8.0 Structured Parameter DynamicResourceAllocation implement dra framework In plan allocated nics base on GPU and Nic topology In plan support for schedule pod by SpiderMultusConfig or SpiderIPPool Todo Multi-network Todo","title":"roadmap"},{"location":"develop/swagger_openapi/","text":"SWAGGER OPENAPI Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes. Features Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs. Usages There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR . validate spec Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec generate source codes with the given spec To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen verify the spec with current source codes to make sure whether the current source codes is out of date To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify clean the generated source codes To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code Use swagger-ui To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web. Steps For Developers Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Swagger OpenAPI"},{"location":"develop/swagger_openapi/#swagger-openapi","text":"Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#features","text":"Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs.","title":"Features"},{"location":"develop/swagger_openapi/#usages","text":"There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR .","title":"Usages"},{"location":"develop/swagger_openapi/#validate-spec","text":"Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec","title":"validate spec"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","text":"To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen","title":"generate source codes with the given spec"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","text":"To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify","title":"verify the spec with current source codes to make sure whether the current source codes is out of date"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","text":"To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code","title":"clean the generated source codes"},{"location":"develop/swagger_openapi/#use-swagger-ui","text":"To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.","title":"Use swagger-ui"},{"location":"develop/swagger_openapi/#steps-for-developers","text":"Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Steps For Developers"},{"location":"reference/_example/","text":"CRD \u57fa\u672c\u63cf\u8ff0 \u672cCRD \u662f\u505a\u4ec0\u4e48\u7684 \u914d\u7f6e\u8bf4\u660e \u8868\u683c\uff08\u5b57\u6bb5\u3001\u63cf\u8ff0\u3001\u7f3a\u7701\u503c\uff09\uff0c\u5305\u62ec\u4e86 status \u7684\u4fe1\u606f\u8bf4\u660e \u4f7f\u7528\u4f8b\u5b50 \u7ed9\u51fa\u4e00\u4e9b\u573a\u666f\u573a\u666f\u4e0b\u7684 CR yaml","title":"CRD"},{"location":"reference/_example/#crd","text":"","title":"CRD"},{"location":"reference/_example/#_1","text":"\u672cCRD \u662f\u505a\u4ec0\u4e48\u7684","title":"\u57fa\u672c\u63cf\u8ff0"},{"location":"reference/_example/#_2","text":"\u8868\u683c\uff08\u5b57\u6bb5\u3001\u63cf\u8ff0\u3001\u7f3a\u7701\u503c\uff09\uff0c\u5305\u62ec\u4e86 status \u7684\u4fe1\u606f\u8bf4\u660e","title":"\u914d\u7f6e\u8bf4\u660e"},{"location":"reference/_example/#_3","text":"\u7ed9\u51fa\u4e00\u4e9b\u573a\u666f\u573a\u666f\u4e0b\u7684 CR yaml","title":"\u4f7f\u7528\u4f8b\u5b50"},{"location":"reference/annotation/","text":"Annotations Spiderpool provides annotations for configuring custom IPPools and routes. Pod annotations After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations. ipam.spidernet.io/subnet Specify the Subnets used to generate IPPools and allocate IP addresses. ipam.spidernet.io/subnet : |- { \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] } ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/subnets ipam.spidernet.io/subnets : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-subnet2\"], \"ipv6\": [\"demo-v6-subnet2\"] }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/ippool-ip-number This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1'). ipam.spidernet.io/ippool-ip-number : +1 ipam.spidernet.io/ippool-reclaim This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true'). ipam.spidernet.io/ippool-reclaim : true ipam.spidernet.io/ippool Specify the IPPools used to allocate IP addresses. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"] } ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/ippools It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. cleangateway (bool, optional): If set to true, no gateway routing will take effect on this network card, regardless of whether a gateway IP is set in the IPPool of this network card. ipam.spidernet.io/routes You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address. Namespace annotations A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace. ipam.spidernet.io/default-ipv4-ippool ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'","title":"Annotations"},{"location":"reference/annotation/#annotations","text":"Spiderpool provides annotations for configuring custom IPPools and routes.","title":"Annotations"},{"location":"reference/annotation/#pod-annotations","text":"After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations.","title":"Pod annotations"},{"location":"reference/annotation/#ipamspidernetiosubnet","text":"Specify the Subnets used to generate IPPools and allocate IP addresses. ipam.spidernet.io/subnet : |- { \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] } ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/subnet"},{"location":"reference/annotation/#ipamspidernetiosubnets","text":"ipam.spidernet.io/subnets : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-subnet2\"], \"ipv6\": [\"demo-v6-subnet2\"] }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/subnets"},{"location":"reference/annotation/#ipamspidernetioippool-ip-number","text":"This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1'). ipam.spidernet.io/ippool-ip-number : +1","title":"ipam.spidernet.io/ippool-ip-number"},{"location":"reference/annotation/#ipamspidernetioippool-reclaim","text":"This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true'). ipam.spidernet.io/ippool-reclaim : true","title":"ipam.spidernet.io/ippool-reclaim"},{"location":"reference/annotation/#ipamspidernetioippool","text":"Specify the IPPools used to allocate IP addresses. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"] } ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/ippool"},{"location":"reference/annotation/#ipamspidernetioippools","text":"It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. cleangateway (bool, optional): If set to true, no gateway routing will take effect on this network card, regardless of whether a gateway IP is set in the IPPool of this network card.","title":"ipam.spidernet.io/ippools"},{"location":"reference/annotation/#ipamspidernetioroutes","text":"You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address.","title":"ipam.spidernet.io/routes"},{"location":"reference/annotation/#namespace-annotations","text":"A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace.","title":"Namespace annotations"},{"location":"reference/annotation/#ipamspidernetiodefault-ipv4-ippool","text":"ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]'","title":"ipam.spidernet.io/default-ipv4-ippool"},{"location":"reference/annotation/#ipamspidernetiodefault-ipv6-ippool","text":"ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'","title":"ipam.spidernet.io/default-ipv6-ippool"},{"location":"reference/configmap/","text":"Configuration Instructions for global configuration and environment arguments of Spiderpool. Configmap Configuration Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : kube-system data : conf.yml : | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock enableIPv4: true enableIPv6: true enableStatefulSet: true enableKubevirtStaticIP: true enableSpiderSubnet: true enableIPConflictDetection: true enableGatewayDetection: true clusterSubnetDefaultFlexibleIPNumber: 1 tuneSysctlConfig: {{ .Values.spiderpoolAgent.tuneSysctlConfig }} podResourceInject: enabled: false namespacesExclude: - kube-system - spiderpool namespacesInclude: [] ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet static IP capability of Spiderpool. false : Disable StatefulSet static IP capability of Spiderpool. enableKubevirtStaticIP (bool): true : Enable kubevirt VM static IP capability of Spiderpool. false : Disable kubevirt VM static IP capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. enableIPConflictDetection (bool): true : Enable IP conflict detection capability of Spiderpool. false : Disable IP conflict detection capability of Spiderpool. enableGatewayDetection (bool): true : Enable gateway detection capability of Spiderpool. false : Disable gateway detection capability of Spiderpool. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster. podResourceInject (object): Pod resource inject capability of Spiderpool. enabled (bool): true : Enable pod resource inject capability of Spiderpool. false : Disable pod resource inject capability of Spiderpool. namespacesExclude (array): Exclude the namespaces of the pod resource inject. namespacesInclude (array): Include the namespaces of the pod resource inject.","title":"Configmap"},{"location":"reference/configmap/#configuration","text":"Instructions for global configuration and environment arguments of Spiderpool.","title":"Configuration"},{"location":"reference/configmap/#configmap-configuration","text":"Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : kube-system data : conf.yml : | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock enableIPv4: true enableIPv6: true enableStatefulSet: true enableKubevirtStaticIP: true enableSpiderSubnet: true enableIPConflictDetection: true enableGatewayDetection: true clusterSubnetDefaultFlexibleIPNumber: 1 tuneSysctlConfig: {{ .Values.spiderpoolAgent.tuneSysctlConfig }} podResourceInject: enabled: false namespacesExclude: - kube-system - spiderpool namespacesInclude: [] ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet static IP capability of Spiderpool. false : Disable StatefulSet static IP capability of Spiderpool. enableKubevirtStaticIP (bool): true : Enable kubevirt VM static IP capability of Spiderpool. false : Disable kubevirt VM static IP capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. enableIPConflictDetection (bool): true : Enable IP conflict detection capability of Spiderpool. false : Disable IP conflict detection capability of Spiderpool. enableGatewayDetection (bool): true : Enable gateway detection capability of Spiderpool. false : Disable gateway detection capability of Spiderpool. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster. podResourceInject (object): Pod resource inject capability of Spiderpool. enabled (bool): true : Enable pod resource inject capability of Spiderpool. false : Disable pod resource inject capability of Spiderpool. namespacesExclude (array): Exclude the namespaces of the pod resource inject. namespacesInclude (array): Include the namespaces of the pod resource inject.","title":"Configmap Configuration"},{"location":"reference/crd-spidercoordinator/","text":"Spidercoordinator A Spidercoordinator resource represents the global default configuration of the cni meta-plugin: coordinator. There is only one instance of this resource, which is automatically generated while you install Spiderpool and does not need to be created manually. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderCoordinator metadata : name : default spec : enableVethLinkLocalAddress : false podRPFilter : 0 hostRuleTable : 500 mode : underlay podCIDRType : auto podDefaultRouteNIC : eth0 podMACPrefix : \"\" tunePodRoutes : true txQueueLen : 0 status : overlayPodCIDR : - 10.233.64.0/18 - fd85:ee78:d8a6:8607::1:0000/112 phase : Synced serviceCIDR : - 10.233.0.0/18 - fd85:ee78:d8a6:8607::1000/116 Spidercoordinators definition Metadata Field Description Schema Validation name The name of this Spidercoordinators resource string required Spec This is the Spidercoordinators spec for users to configure. Field Description Schema Validation Values Default mode The mode in which the coordinator. auto: automatically determine if it's overlay or underlay. underlay: coordinator creates veth devices to solve the problem that CNIs such as macvlan cannot communicate with clusterIP. overlay: fix the problem that CNIs such as Macvlan cannot access ClusterIP through the Calico network card attached to the pod,coordinate policy route between interfaces to ensure consistence data path of request and reply packets string require auto,underlay,overlay auto podCIDRType The ways to fetch the CIDR of the cluster. auto(default), This means that it will automatically switch podCIDRType to cluster or calico or cilium. based on cluster CNI. calico: auto fetch the subnet of the pod from the ip pools of calico, This only works if the cluster CNI is calico; cilium: Auto fetch the pod's subnet from cilium's configMap or ip pools. Supported IPAM modes: [\"cluster-pool\",\"kubernetes\",\"multi-pool\"]; cluster: auto fetch the subnet of the pod from the kubeadm-config configmap, This is useful if there is only a globally unique default pod's subnet; none: don't get the subnet of the pod, which is useful for some special cases. In this case,you can manually configure the hijackCIDR field string require auto,cluster,calico,cilium,none auto tunePodRoutes tune pod's route while the pod is attached to multiple NICs bool optional true,false true podDefaultRouteNIC The NIC where the pod's default route resides string optional \"\",eth0,net1... underlay: eth0,overlay: net1 vethLinkAddress configure an link-local address for veth0 device, fix the istio case boolean optional true,false false podMACPrefix fix the pod's mac address with this prefix + 4 bytes IP string optional a invalid mac address prefix \"\" podRPFilter set rp_filter sysctl for the pod int required 0,1,2;suggest to be 0 0 hostRuleTable The directly routing table of the host accessing the pod's underlay IP will be placed in this policy routing table int required int 500 txQueueLen The Transmit Queue Length (txqueuelen) is a TCP/IP stack network interface value that sets the number of packets allowed per kernel transmit queue of a network interface device int optional >= 0, default to 0, it's mean to don't set it 0 Status (subresource) The Spidercoordinators status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation overlayPodCIDR the cluster pod cidr []string required serviceCIDR the cluster service cidr []string required phase Represents the status of synchronization string required reason the reason why the status is NotReady string optional","title":"CRD Spidercoordinator"},{"location":"reference/crd-spidercoordinator/#spidercoordinator","text":"A Spidercoordinator resource represents the global default configuration of the cni meta-plugin: coordinator. There is only one instance of this resource, which is automatically generated while you install Spiderpool and does not need to be created manually.","title":"Spidercoordinator"},{"location":"reference/crd-spidercoordinator/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderCoordinator metadata : name : default spec : enableVethLinkLocalAddress : false podRPFilter : 0 hostRuleTable : 500 mode : underlay podCIDRType : auto podDefaultRouteNIC : eth0 podMACPrefix : \"\" tunePodRoutes : true txQueueLen : 0 status : overlayPodCIDR : - 10.233.64.0/18 - fd85:ee78:d8a6:8607::1:0000/112 phase : Synced serviceCIDR : - 10.233.0.0/18 - fd85:ee78:d8a6:8607::1000/116","title":"Sample YAML"},{"location":"reference/crd-spidercoordinator/#spidercoordinators-definition","text":"","title":"Spidercoordinators definition"},{"location":"reference/crd-spidercoordinator/#metadata","text":"Field Description Schema Validation name The name of this Spidercoordinators resource string required","title":"Metadata"},{"location":"reference/crd-spidercoordinator/#spec","text":"This is the Spidercoordinators spec for users to configure. Field Description Schema Validation Values Default mode The mode in which the coordinator. auto: automatically determine if it's overlay or underlay. underlay: coordinator creates veth devices to solve the problem that CNIs such as macvlan cannot communicate with clusterIP. overlay: fix the problem that CNIs such as Macvlan cannot access ClusterIP through the Calico network card attached to the pod,coordinate policy route between interfaces to ensure consistence data path of request and reply packets string require auto,underlay,overlay auto podCIDRType The ways to fetch the CIDR of the cluster. auto(default), This means that it will automatically switch podCIDRType to cluster or calico or cilium. based on cluster CNI. calico: auto fetch the subnet of the pod from the ip pools of calico, This only works if the cluster CNI is calico; cilium: Auto fetch the pod's subnet from cilium's configMap or ip pools. Supported IPAM modes: [\"cluster-pool\",\"kubernetes\",\"multi-pool\"]; cluster: auto fetch the subnet of the pod from the kubeadm-config configmap, This is useful if there is only a globally unique default pod's subnet; none: don't get the subnet of the pod, which is useful for some special cases. In this case,you can manually configure the hijackCIDR field string require auto,cluster,calico,cilium,none auto tunePodRoutes tune pod's route while the pod is attached to multiple NICs bool optional true,false true podDefaultRouteNIC The NIC where the pod's default route resides string optional \"\",eth0,net1... underlay: eth0,overlay: net1 vethLinkAddress configure an link-local address for veth0 device, fix the istio case boolean optional true,false false podMACPrefix fix the pod's mac address with this prefix + 4 bytes IP string optional a invalid mac address prefix \"\" podRPFilter set rp_filter sysctl for the pod int required 0,1,2;suggest to be 0 0 hostRuleTable The directly routing table of the host accessing the pod's underlay IP will be placed in this policy routing table int required int 500 txQueueLen The Transmit Queue Length (txqueuelen) is a TCP/IP stack network interface value that sets the number of packets allowed per kernel transmit queue of a network interface device int optional >= 0, default to 0, it's mean to don't set it 0","title":"Spec"},{"location":"reference/crd-spidercoordinator/#status-subresource","text":"The Spidercoordinators status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation overlayPodCIDR the cluster pod cidr []string required serviceCIDR the cluster service cidr []string required phase Represents the status of synchronization string required reason the reason why the status is NotReady string optional","title":"Status (subresource)"},{"location":"reference/crd-spiderendpoint/","text":"SpiderEndpoint A SpiderEndpoint resource represents IP address allocation details for the corresponding pod. This resource one to one pod, and it will inherit the pod name and pod namespace. Notice: For kubevirt VM static IP feature, the SpiderEndpoint object would inherit the kubevirt VM/VMI resource name and namespace. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderEndpoint metadata : name : test-app-1-9dc78fb9-rs99d status : current : ips : - cleanGateway : false interface : eth0 ipv4 : 172.31.199.193/20 ipv4Gateway : 172.31.207.253 ipv4Pool : worker-172 vlan : 0 node : dc-test02 uid : e7b50a38-25c2-41d0-b332-7f619c69194e ownerControllerName : test-app-1 ownerControllerType : Deployment SpiderEndpoint definition Metadata Field Description Schema Validation name the name of this SpiderEndpoint resource string required namespace the namespace of this SpiderEndpoint resource string required Status (subresource) The IPPool status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation current the IP allocation details of the corresponding pod PodIPAllocation required ownerControllerType the corresponding pod top owner controller type string required ownerControllerName the corresponding pod top owner controller name string required PodIPAllocation This property describes the SpiderEndpoint corresponding pod details. Field Description Schema Validation uid corresponding pod uid string required node total IP counts of this pool to use string required ips current allocated IP counts list of IPAllocationDetail required IPAllocationDetail This property describes single Interface allocation details. Field Description Schema Validation Default interface single interface name string required ipv4 single IPv4 allocated IP address string optional ipv6 single IPv6 allocated IP address string optional ipv4Pool the IPv4 allocated IP address corresponding pool string optional ipv6Pool the IPv6 allocated IP address corresponding pool string optional vlan vlan ID int optional 0 ipv4Gateway the IPv4 gateway IP address string optional ipv6Gateway the IPv6 gateway IP address string optional cleanGateway a flag to choose whether need default route by the gateway boolean optional routes the allocation routes list if Route optional","title":"CRD SpiderEndpoint"},{"location":"reference/crd-spiderendpoint/#spiderendpoint","text":"A SpiderEndpoint resource represents IP address allocation details for the corresponding pod. This resource one to one pod, and it will inherit the pod name and pod namespace. Notice: For kubevirt VM static IP feature, the SpiderEndpoint object would inherit the kubevirt VM/VMI resource name and namespace.","title":"SpiderEndpoint"},{"location":"reference/crd-spiderendpoint/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderEndpoint metadata : name : test-app-1-9dc78fb9-rs99d status : current : ips : - cleanGateway : false interface : eth0 ipv4 : 172.31.199.193/20 ipv4Gateway : 172.31.207.253 ipv4Pool : worker-172 vlan : 0 node : dc-test02 uid : e7b50a38-25c2-41d0-b332-7f619c69194e ownerControllerName : test-app-1 ownerControllerType : Deployment","title":"Sample YAML"},{"location":"reference/crd-spiderendpoint/#spiderendpoint-definition","text":"","title":"SpiderEndpoint definition"},{"location":"reference/crd-spiderendpoint/#metadata","text":"Field Description Schema Validation name the name of this SpiderEndpoint resource string required namespace the namespace of this SpiderEndpoint resource string required","title":"Metadata"},{"location":"reference/crd-spiderendpoint/#status-subresource","text":"The IPPool status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation current the IP allocation details of the corresponding pod PodIPAllocation required ownerControllerType the corresponding pod top owner controller type string required ownerControllerName the corresponding pod top owner controller name string required","title":"Status (subresource)"},{"location":"reference/crd-spiderendpoint/#podipallocation","text":"This property describes the SpiderEndpoint corresponding pod details. Field Description Schema Validation uid corresponding pod uid string required node total IP counts of this pool to use string required ips current allocated IP counts list of IPAllocationDetail required","title":"PodIPAllocation"},{"location":"reference/crd-spiderendpoint/#ipallocationdetail","text":"This property describes single Interface allocation details. Field Description Schema Validation Default interface single interface name string required ipv4 single IPv4 allocated IP address string optional ipv6 single IPv6 allocated IP address string optional ipv4Pool the IPv4 allocated IP address corresponding pool string optional ipv6Pool the IPv6 allocated IP address corresponding pool string optional vlan vlan ID int optional 0 ipv4Gateway the IPv4 gateway IP address string optional ipv6Gateway the IPv6 gateway IP address string optional cleanGateway a flag to choose whether need default route by the gateway boolean optional routes the allocation routes list if Route optional","title":"IPAllocationDetail"},{"location":"reference/crd-spiderippool/","text":"SpiderIPPool A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : ipVersion : 4 subnet : 172.31.192.0/20 ips : - 172.31.199.180-172.31.199.189 - 172.31.199.205-172.31.199.209 excludeIPs : - 172.31.199.186-172.31.199.188 - 172.31.199.207 gateway : 172.31.207.253 default : true disable : false SpiderIPPool definition Metadata Field Description Schema Validation name the name of this SpiderIPPool resource string required Spec This is the IPPool spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this pool int optional 4,6 subnet subnet of this pool string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this pool to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this pool to filter list of strings optional array of IP ranges and single IP address gateway gateway for this pool string optional an IP address routes custom routes in this pool (please don't set default route 0.0.0.0/0 if property gateway exists) list of route optional podAffinity specify which pods can use this pool labelSelector optional kubernetes LabelSelector namespaceAffinity specify which namespaces pods can use this pool labelSelector optional kubernetes LabelSelector namespaceName specify which namespaces pods can use this pool (The priority is higher than property namespaceAffinity ) list of strings optional nodeAffinity specify which nodes pods can use this pool labelSelector optional kubernetes LabelSelector nodeName specify which nodes pods can use this pool (The priority is higher than property nodeAffinity ) list of strings optional multusName specify which multus net-attach-def objects can use this pool list of strings optional default configure this resource as a default pool for pods boolean optional true,false false disable configure whether the pool is usable boolean optional true,false false Status (subresource) The IPPool status is a subresource that processed automatically by the system to summarize the current state Field Description Schema allocatedIPs current IP allocations in this pool string totalIPCount total IP counts of this pool to use int allocatedIPCount current allocated IP counts int Route Field Description Schema Validation dst destination of this route string required gw gateway of this route string required Pod Affinity For details on configuring SpiderIPPool podAffinity, please read the Pod Affinity of IPPool . Namespace Affinity For details on configuring SpiderIPPool namespaceAffinity or namespaceName, please read the Namespace Affinity of IPPool . Notice: namespaceName has higher priority than namespaceAffinity . Node Affinity For details on configuring SpiderIPPool nodeAffinity or nodeName, please read the Node Affinity of IPPool and Network topology allocation . Notice: nodeName has higher priority than nodeAffinity . Multus Affinity For details on configuring SpiderIPPool multusName, please read the multus Affinity of IPPool .","title":"CRD SpiderIPPool"},{"location":"reference/crd-spiderippool/#spiderippool","text":"A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned.","title":"SpiderIPPool"},{"location":"reference/crd-spiderippool/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : ipVersion : 4 subnet : 172.31.192.0/20 ips : - 172.31.199.180-172.31.199.189 - 172.31.199.205-172.31.199.209 excludeIPs : - 172.31.199.186-172.31.199.188 - 172.31.199.207 gateway : 172.31.207.253 default : true disable : false","title":"Sample YAML"},{"location":"reference/crd-spiderippool/#spiderippool-definition","text":"","title":"SpiderIPPool definition"},{"location":"reference/crd-spiderippool/#metadata","text":"Field Description Schema Validation name the name of this SpiderIPPool resource string required","title":"Metadata"},{"location":"reference/crd-spiderippool/#spec","text":"This is the IPPool spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this pool int optional 4,6 subnet subnet of this pool string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this pool to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this pool to filter list of strings optional array of IP ranges and single IP address gateway gateway for this pool string optional an IP address routes custom routes in this pool (please don't set default route 0.0.0.0/0 if property gateway exists) list of route optional podAffinity specify which pods can use this pool labelSelector optional kubernetes LabelSelector namespaceAffinity specify which namespaces pods can use this pool labelSelector optional kubernetes LabelSelector namespaceName specify which namespaces pods can use this pool (The priority is higher than property namespaceAffinity ) list of strings optional nodeAffinity specify which nodes pods can use this pool labelSelector optional kubernetes LabelSelector nodeName specify which nodes pods can use this pool (The priority is higher than property nodeAffinity ) list of strings optional multusName specify which multus net-attach-def objects can use this pool list of strings optional default configure this resource as a default pool for pods boolean optional true,false false disable configure whether the pool is usable boolean optional true,false false","title":"Spec"},{"location":"reference/crd-spiderippool/#status-subresource","text":"The IPPool status is a subresource that processed automatically by the system to summarize the current state Field Description Schema allocatedIPs current IP allocations in this pool string totalIPCount total IP counts of this pool to use int allocatedIPCount current allocated IP counts int","title":"Status (subresource)"},{"location":"reference/crd-spiderippool/#route","text":"Field Description Schema Validation dst destination of this route string required gw gateway of this route string required","title":"Route"},{"location":"reference/crd-spiderippool/#pod-affinity","text":"For details on configuring SpiderIPPool podAffinity, please read the Pod Affinity of IPPool .","title":"Pod Affinity"},{"location":"reference/crd-spiderippool/#namespace-affinity","text":"For details on configuring SpiderIPPool namespaceAffinity or namespaceName, please read the Namespace Affinity of IPPool . Notice: namespaceName has higher priority than namespaceAffinity .","title":"Namespace Affinity"},{"location":"reference/crd-spiderippool/#node-affinity","text":"For details on configuring SpiderIPPool nodeAffinity or nodeName, please read the Node Affinity of IPPool and Network topology allocation . Notice: nodeName has higher priority than nodeAffinity .","title":"Node Affinity"},{"location":"reference/crd-spiderippool/#multus-affinity","text":"For details on configuring SpiderIPPool multusName, please read the multus Affinity of IPPool .","title":"Multus Affinity"},{"location":"reference/crd-spidermultusconfig/","text":"SpiderMultusConfig A SpiderMultusConfig resource represents a best practice to generate a multus net-attach-def CR object for spiderpool to use. For details on using this CRD, please read the SpiderMultusConfig guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : demo namespace : default annotations : multus.spidernet.io/cr-name : \"macvlan-100\" multus.spidernet.io/cni-version : 0.4.0 spec : cniType : macvlan macvlan : master : [ \"eth0\" ] vlanID : 100 ippools : ipv4 : [ \"default-pool-v4\" ] ipv6 : [ \"default-pool-v6\" ] SpiderMultusConfig definition Metadata Field Description Schema Validation name The name of this SpiderMultusConfig resource string required namespace The namespace of this SpiderMultusConfig resource string required annotations The annotations of this SpiderMultusConfig resource map optional Metadata.annotations You can also set annotations for this SpiderMultusConfig resource, then the corresponding Multus net-attach-def resource will inherit these annotations too. And you can also use special annotation multus.spidernet.io/cr-name and multus.spidernet.io/cni-version to customize the corresponding Multus net-attach-def resource name and CNI version. Field Description Schema Validation Default multus.spidernet.io/cr-name The customized Multus net-attach-def resource name string optional multus.spidernet.io/cni-version The customized Multus net-attach-def resource CNI version string optional 0.3.1 Spec This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values Default cniType expected main CNI type string require macvlan, ipvlan, sriov, ovs, ib-sriov, custom macvlan macvlan CNI configuration SpiderMacvlanCniConfig optional ipvlan ipvlan CNI configuration SpiderIPvlanCniConfig optional sriov sriov CNI configuration SpiderSRIOVCniConfig optional ibsriov infiniband ib-sriov CNI configuration SpiderIBSRIOVCniConfig optional ipoib infiniband ipoib CNI configuration SpiderIpoibCniConfig optional ovs ovs CNI configuration SpiderOvsCniConfig optional enableCoordinator enable coordinator or not boolean optional true,false true disableIPAM disable IPAM. when set to be true, any configuration of CNI's ippools field will be ignored boolean optional true,false false coordinator coordinator CNI configuration CoordinatorSpec optional customCNI a string that represents custom CNI configuration string optional chainCNIJsonData a list of string that represents chain CNI configuration, such as tune plugin. []string optional SpiderMacvlanCniConfig Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderIPvlanCniConfig Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderSRIOVCniConfig Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with SRIOV string required vlanID vlan ID int optional minTxRateMbps change the allowed minimum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting. The min_tx_rate value should be <= max_tx_rate. Support of this feature depends on NICs and drivers int optional maxTxRateMbps change the allowed maximum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting int optional rdmaIsolation enable RDMA CNI plugin is intended to be run as a chained CNI plugin. it ensures isolation of RDMA traffic from other workloads in the system by moving the associated RDMA interfaces of the provided network interface to the container'snetwork namespace path bool optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderIBSRIOVCniConfig Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with ib-sriov string required pkey InfiniBand pkey for VF, this field is used by ib-kubernetes to add pkey with guid to InfiniBand subnet manager client e.g. Mellanox UFM string optional linkState Enforces link state for the VF. Allowed values: auto, enable [default], disable string optional rdmaIsolation enable RDMA CNI plugin is intended to be run as a chained CNI plugin. it ensures isolation of RDMA traffic from other workloads in the system by moving the associated RDMA interfaces of the provided network interface to the container'snetwork namespace path bool optional ibKubernetesEnabled Enforces ib-sriov-cni to work with ib-kubernetes , default to false bool optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderIpoibCniConfig Field Description Schema Validation master master interface name string required ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderOvsCniConfig Field Description Schema Validation bridge name of the bridge to use string required vlan vlan ID of attached port. Trunk port if not specified int optional trunk List of VLAN ID's and/or ranges of accepted VLAN ID's Trunk optional deviceID PCI address of a VF in valid sysfs format string optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional BondConfig Field Description Schema Validation Values Name the expected bond interface name string required Mode bond interface mode int required [0,6] Options expected bond Interface configurations string optional Trunk Field Description Schema Validation Values minID the min value of vlan ID int optional [0,4094] maxID the max value of vlan ID int optional [0,4094] id the value of vlan ID int optional [0,4094] SpiderpoolPools Field Description Schema Validation ipv4 the default IPv4 IPPools in your CNI configurations list of strings optional ipv6 the default IPv6 IPPools in your CNI configurations list of strings optional","title":"CRD Spidermultusconfig"},{"location":"reference/crd-spidermultusconfig/#spidermultusconfig","text":"A SpiderMultusConfig resource represents a best practice to generate a multus net-attach-def CR object for spiderpool to use. For details on using this CRD, please read the SpiderMultusConfig guide .","title":"SpiderMultusConfig"},{"location":"reference/crd-spidermultusconfig/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : demo namespace : default annotations : multus.spidernet.io/cr-name : \"macvlan-100\" multus.spidernet.io/cni-version : 0.4.0 spec : cniType : macvlan macvlan : master : [ \"eth0\" ] vlanID : 100 ippools : ipv4 : [ \"default-pool-v4\" ] ipv6 : [ \"default-pool-v6\" ]","title":"Sample YAML"},{"location":"reference/crd-spidermultusconfig/#spidermultusconfig-definition","text":"","title":"SpiderMultusConfig definition"},{"location":"reference/crd-spidermultusconfig/#metadata","text":"Field Description Schema Validation name The name of this SpiderMultusConfig resource string required namespace The namespace of this SpiderMultusConfig resource string required annotations The annotations of this SpiderMultusConfig resource map optional","title":"Metadata"},{"location":"reference/crd-spidermultusconfig/#metadataannotations","text":"You can also set annotations for this SpiderMultusConfig resource, then the corresponding Multus net-attach-def resource will inherit these annotations too. And you can also use special annotation multus.spidernet.io/cr-name and multus.spidernet.io/cni-version to customize the corresponding Multus net-attach-def resource name and CNI version. Field Description Schema Validation Default multus.spidernet.io/cr-name The customized Multus net-attach-def resource name string optional multus.spidernet.io/cni-version The customized Multus net-attach-def resource CNI version string optional 0.3.1","title":"Metadata.annotations"},{"location":"reference/crd-spidermultusconfig/#spec","text":"This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values Default cniType expected main CNI type string require macvlan, ipvlan, sriov, ovs, ib-sriov, custom macvlan macvlan CNI configuration SpiderMacvlanCniConfig optional ipvlan ipvlan CNI configuration SpiderIPvlanCniConfig optional sriov sriov CNI configuration SpiderSRIOVCniConfig optional ibsriov infiniband ib-sriov CNI configuration SpiderIBSRIOVCniConfig optional ipoib infiniband ipoib CNI configuration SpiderIpoibCniConfig optional ovs ovs CNI configuration SpiderOvsCniConfig optional enableCoordinator enable coordinator or not boolean optional true,false true disableIPAM disable IPAM. when set to be true, any configuration of CNI's ippools field will be ignored boolean optional true,false false coordinator coordinator CNI configuration CoordinatorSpec optional customCNI a string that represents custom CNI configuration string optional chainCNIJsonData a list of string that represents chain CNI configuration, such as tune plugin. []string optional","title":"Spec"},{"location":"reference/crd-spidermultusconfig/#spidermacvlancniconfig","text":"Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderMacvlanCniConfig"},{"location":"reference/crd-spidermultusconfig/#spideripvlancniconfig","text":"Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderIPvlanCniConfig"},{"location":"reference/crd-spidermultusconfig/#spidersriovcniconfig","text":"Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with SRIOV string required vlanID vlan ID int optional minTxRateMbps change the allowed minimum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting. The min_tx_rate value should be <= max_tx_rate. Support of this feature depends on NICs and drivers int optional maxTxRateMbps change the allowed maximum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting int optional rdmaIsolation enable RDMA CNI plugin is intended to be run as a chained CNI plugin. it ensures isolation of RDMA traffic from other workloads in the system by moving the associated RDMA interfaces of the provided network interface to the container'snetwork namespace path bool optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderSRIOVCniConfig"},{"location":"reference/crd-spidermultusconfig/#spideribsriovcniconfig","text":"Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with ib-sriov string required pkey InfiniBand pkey for VF, this field is used by ib-kubernetes to add pkey with guid to InfiniBand subnet manager client e.g. Mellanox UFM string optional linkState Enforces link state for the VF. Allowed values: auto, enable [default], disable string optional rdmaIsolation enable RDMA CNI plugin is intended to be run as a chained CNI plugin. it ensures isolation of RDMA traffic from other workloads in the system by moving the associated RDMA interfaces of the provided network interface to the container'snetwork namespace path bool optional ibKubernetesEnabled Enforces ib-sriov-cni to work with ib-kubernetes , default to false bool optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderIBSRIOVCniConfig"},{"location":"reference/crd-spidermultusconfig/#spideripoibcniconfig","text":"Field Description Schema Validation master master interface name string required ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderIpoibCniConfig"},{"location":"reference/crd-spidermultusconfig/#spiderovscniconfig","text":"Field Description Schema Validation bridge name of the bridge to use string required vlan vlan ID of attached port. Trunk port if not specified int optional trunk List of VLAN ID's and/or ranges of accepted VLAN ID's Trunk optional deviceID PCI address of a VF in valid sysfs format string optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderOvsCniConfig"},{"location":"reference/crd-spidermultusconfig/#bondconfig","text":"Field Description Schema Validation Values Name the expected bond interface name string required Mode bond interface mode int required [0,6] Options expected bond Interface configurations string optional","title":"BondConfig"},{"location":"reference/crd-spidermultusconfig/#trunk","text":"Field Description Schema Validation Values minID the min value of vlan ID int optional [0,4094] maxID the max value of vlan ID int optional [0,4094] id the value of vlan ID int optional [0,4094]","title":"Trunk"},{"location":"reference/crd-spidermultusconfig/#spiderpoolpools","text":"Field Description Schema Validation ipv4 the default IPv4 IPPools in your CNI configurations list of strings optional ipv6 the default IPv6 IPPools in your CNI configurations list of strings optional","title":"SpiderpoolPools"},{"location":"reference/crd-spiderreservedip/","text":"SpiderReservedIP A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. For details on using this CRD, please read the SpiderReservedIP guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : exclude-ips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 SpiderReservedIP definition Metadata Field Description Schema Validation name the name of this SpiderReservedIP resource string required Spec This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values ipVersion IP version of this resource int optional 4,6 ips IP ranges for this resource that we expect not to use list of strings optional array of IP ranges and single IP address","title":"CRD SpiderReservedIP"},{"location":"reference/crd-spiderreservedip/#spiderreservedip","text":"A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. For details on using this CRD, please read the SpiderReservedIP guide .","title":"SpiderReservedIP"},{"location":"reference/crd-spiderreservedip/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : exclude-ips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50","title":"Sample YAML"},{"location":"reference/crd-spiderreservedip/#spiderreservedip-definition","text":"","title":"SpiderReservedIP definition"},{"location":"reference/crd-spiderreservedip/#metadata","text":"Field Description Schema Validation name the name of this SpiderReservedIP resource string required","title":"Metadata"},{"location":"reference/crd-spiderreservedip/#spec","text":"This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values ipVersion IP version of this resource int optional 4,6 ips IP ranges for this resource that we expect not to use list of strings optional array of IP ranges and single IP address","title":"Spec"},{"location":"reference/crd-spidersubnet/","text":"SpiderSubnet A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. For details on using this CRD, please read the SpiderSubnet guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : default-v4-subnet spec : ipVersion : 4 ips : - 172.22.40.2-172.22.40.254 subnet : 172.22.0.0/16 excludeIPs : - 172.22.40.10-172.22.40.20 gateway : 172.22.40.1 SpiderSubnet definition Metadata Field Description Schema Validation name the name of this SpiderSubnet resource string required Spec This is the SpiderSubnet spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this subnet int optional 4,6 subnet subnet of this resource string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this resource to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this resource to filter list of strings optional array of IP ranges and single IP address gateway gateway for this resource string optional an IP address routes custom routes in this resource list of Route optional Status (subresource) The Subnet status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema controlledIPPools current IP allocations in this subnet resource string totalIPCount total IP addresses counts of this subnet resource to use int allocatedIPCount current allocated IP addresses counts int","title":"CRD SpiderSubnet"},{"location":"reference/crd-spidersubnet/#spidersubnet","text":"A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. For details on using this CRD, please read the SpiderSubnet guide .","title":"SpiderSubnet"},{"location":"reference/crd-spidersubnet/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : default-v4-subnet spec : ipVersion : 4 ips : - 172.22.40.2-172.22.40.254 subnet : 172.22.0.0/16 excludeIPs : - 172.22.40.10-172.22.40.20 gateway : 172.22.40.1","title":"Sample YAML"},{"location":"reference/crd-spidersubnet/#spidersubnet-definition","text":"","title":"SpiderSubnet definition"},{"location":"reference/crd-spidersubnet/#metadata","text":"Field Description Schema Validation name the name of this SpiderSubnet resource string required","title":"Metadata"},{"location":"reference/crd-spidersubnet/#spec","text":"This is the SpiderSubnet spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this subnet int optional 4,6 subnet subnet of this resource string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this resource to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this resource to filter list of strings optional array of IP ranges and single IP address gateway gateway for this resource string optional an IP address routes custom routes in this resource list of Route optional","title":"Spec"},{"location":"reference/crd-spidersubnet/#status-subresource","text":"The Subnet status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema controlledIPPools current IP allocations in this subnet resource string totalIPCount total IP addresses counts of this subnet resource to use int allocatedIPCount current allocated IP addresses counts int","title":"Status (subresource)"},{"location":"reference/metrics/","text":"Metric Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller. spiderpool controller The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 spiderpool agent The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711 Get Started Enable Metric support Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true . Metric reference Spiderpool Agent Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric) Spiderpool Controller Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric) RDMA exporter Spiderpool also provides RDMA exporter to export RDMA metrics. The RDMA metrics include: Metric Name Type Description Remarks rdma_rx_write_requests Counter Number of received write requests rdma_rx_read_requests Counter Number of received read requests rdma_rx_atomic_requests Counter Number of received atomic requests rdma_rx_dct_connect Counter Number of received DCT connection requests rdma_out_of_buffer Counter Number of buffer insufficiency errors rdma_out_of_sequence Counter Number of out-of-sequence packets received rdma_duplicate_request Counter Number of duplicate requests rdma_rnr_nak_retry_err Counter Count of RNR NAK packets not exceeding QP retry limit rdma_packet_seq_err Counter Number of packet sequence errors rdma_implied_nak_seq_err Counter Number of implied NAK sequence errors rdma_local_ack_timeout_err Counter Number of times the sender's QP ack timer expired RC, XRC, DCT QPs only rdma_resp_local_length_error Counter Number of times a respondent detected a local length error rdma_resp_cqe_error Counter Number of response CQE errors rdma_req_cqe_error Counter Number of times a requester detected CQE completion with errors rdma_req_remote_invalid_request Counter Number of remote invalid request errors detected by requester rdma_req_remote_access_errors Counter Number of requested remote access errors rdma_resp_remote_access_errors Counter Number of response remote access errors rdma_resp_cqe_flush_error Counter Number of response CQE flush errors rdma_req_cqe_flush_error Counter Number of request CQE flush errors rdma_roce_adp_retrans Counter Number of RoCE adaptive retransmissions rdma_roce_adp_retrans_to Counter Number of RoCE adaptive retransmission timeouts rdma_roce_slow_restart Counter Number of RoCE slow restarts rdma_roce_slow_restart_cnps Counter Number of CNP packets generated during RoCE slow restart rdma_roce_slow_restart_trans Counter Number of times state transitioned to slow restart rdma_rp_cnp_ignored Counter Number of CNP packets received and ignored by Reaction Point HCA rdma_rp_cnp_handled Counter Number of CNP packets handled by Reaction Point HCA to reduce transmission rate rdma_np_ecn_marked_roce_packets Counter Number of ECN-marked RoCE packets indicating path congestion rdma_np_cnp_sent Counter Number of CNP packets sent when congestion is experienced in RoCEv2 IP header rdma_rx_icrc_encapsulated Counter Number of RoCE packets with ICRC errors rdma_rx_vport_rdma_unicast_packets Counter Number of received unicast RDMA packets rdma_tx_vport_rdma_unicast_packets Counter Number of transmitted unicast RDMA packets rdma_rx_vport_rdma_multicast_packets Counter Number of received multicast RDMA packets rdma_tx_vport_rdma_multicast_packets Counter Number of transmitted multicast RDMA packets rdma_rx_vport_rdma_unicast_bytes Counter Number of bytes received in unicast RDMA packets rdma_tx_vport_rdma_unicast_bytes Counter Number of bytes transmitted in unicast RDMA packets rdma_rx_vport_rdma_multicast_bytes Counter Number of bytes received in multicast RDMA packets rdma_tx_vport_rdma_multicast_bytes Counter Number of bytes transmitted in multicast RDMA packets rdma_vport_speed_mbps Speed Speed of the port in Mbps","title":"Metrics"},{"location":"reference/metrics/#metric","text":"Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller.","title":"Metric"},{"location":"reference/metrics/#spiderpool-controller","text":"The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721","title":"spiderpool controller"},{"location":"reference/metrics/#spiderpool-agent","text":"The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711","title":"spiderpool agent"},{"location":"reference/metrics/#get-started","text":"","title":"Get Started"},{"location":"reference/metrics/#enable-metric-support","text":"Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true .","title":"Enable Metric support"},{"location":"reference/metrics/#metric-reference","text":"","title":"Metric reference"},{"location":"reference/metrics/#spiderpool-agent_1","text":"Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric)","title":"Spiderpool Agent"},{"location":"reference/metrics/#spiderpool-controller_1","text":"Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric)","title":"Spiderpool Controller"},{"location":"reference/metrics/#rdma-exporter","text":"Spiderpool also provides RDMA exporter to export RDMA metrics. The RDMA metrics include: Metric Name Type Description Remarks rdma_rx_write_requests Counter Number of received write requests rdma_rx_read_requests Counter Number of received read requests rdma_rx_atomic_requests Counter Number of received atomic requests rdma_rx_dct_connect Counter Number of received DCT connection requests rdma_out_of_buffer Counter Number of buffer insufficiency errors rdma_out_of_sequence Counter Number of out-of-sequence packets received rdma_duplicate_request Counter Number of duplicate requests rdma_rnr_nak_retry_err Counter Count of RNR NAK packets not exceeding QP retry limit rdma_packet_seq_err Counter Number of packet sequence errors rdma_implied_nak_seq_err Counter Number of implied NAK sequence errors rdma_local_ack_timeout_err Counter Number of times the sender's QP ack timer expired RC, XRC, DCT QPs only rdma_resp_local_length_error Counter Number of times a respondent detected a local length error rdma_resp_cqe_error Counter Number of response CQE errors rdma_req_cqe_error Counter Number of times a requester detected CQE completion with errors rdma_req_remote_invalid_request Counter Number of remote invalid request errors detected by requester rdma_req_remote_access_errors Counter Number of requested remote access errors rdma_resp_remote_access_errors Counter Number of response remote access errors rdma_resp_cqe_flush_error Counter Number of response CQE flush errors rdma_req_cqe_flush_error Counter Number of request CQE flush errors rdma_roce_adp_retrans Counter Number of RoCE adaptive retransmissions rdma_roce_adp_retrans_to Counter Number of RoCE adaptive retransmission timeouts rdma_roce_slow_restart Counter Number of RoCE slow restarts rdma_roce_slow_restart_cnps Counter Number of CNP packets generated during RoCE slow restart rdma_roce_slow_restart_trans Counter Number of times state transitioned to slow restart rdma_rp_cnp_ignored Counter Number of CNP packets received and ignored by Reaction Point HCA rdma_rp_cnp_handled Counter Number of CNP packets handled by Reaction Point HCA to reduce transmission rate rdma_np_ecn_marked_roce_packets Counter Number of ECN-marked RoCE packets indicating path congestion rdma_np_cnp_sent Counter Number of CNP packets sent when congestion is experienced in RoCEv2 IP header rdma_rx_icrc_encapsulated Counter Number of RoCE packets with ICRC errors rdma_rx_vport_rdma_unicast_packets Counter Number of received unicast RDMA packets rdma_tx_vport_rdma_unicast_packets Counter Number of transmitted unicast RDMA packets rdma_rx_vport_rdma_multicast_packets Counter Number of received multicast RDMA packets rdma_tx_vport_rdma_multicast_packets Counter Number of transmitted multicast RDMA packets rdma_rx_vport_rdma_unicast_bytes Counter Number of bytes received in unicast RDMA packets rdma_tx_vport_rdma_unicast_bytes Counter Number of bytes transmitted in unicast RDMA packets rdma_rx_vport_rdma_multicast_bytes Counter Number of bytes received in multicast RDMA packets rdma_tx_vport_rdma_multicast_bytes Counter Number of bytes transmitted in multicast RDMA packets rdma_vport_speed_mbps Speed Speed of the port in Mbps","title":"RDMA exporter"},{"location":"reference/plugin-ifacer/","text":"CNI meta-plugin: ifacer Introduction When Pods use VLAN networks, network administrators may need to manually configure various VLAN or Bond interfaces on the nodes in advance. This process can be tedious and error-prone. Spiderpool provides a CNI meta-plugin called ifacer . This plugin dynamically creates VLAN sub-interfaces or Bond interfaces on the nodes during Pod creation, based on the provided ifacer configuration, greatly simplifying the configuration workload. In the following sections, we will delve into this plugin. Feature Support dynamic creation of VLAN sub-interfaces Support dynamic creation of Bond interfaces Notes The VLAN/Bond interfaces created by this plugin will be lost when the node restarts, but they will be automatically recreated upon the Pod restarts. Deleting existed VLAN/Bond interfaces is not supported. Configuring the address of VLAN/Bond interfaces during creation is not supported. If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, Highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with Vlan and Bond interfaces created by Ifacer : ~# INTERFACE = <your_interface_name> ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager Prerequisite There are no specific requirements including Kubernetes or Kernel versions for using this plugin. During the installation of Spiderpool, the plugin will be automatically installed in the /opt/cni/bin/ directory on each host. You can verify by checking for the presence of the ifacer binary in that directory on each host. How to use Examples please see Ifacer Configuration","title":"Ifacer plugin"},{"location":"reference/plugin-ifacer/#cni-meta-plugin-ifacer","text":"","title":"CNI meta-plugin: ifacer"},{"location":"reference/plugin-ifacer/#introduction","text":"When Pods use VLAN networks, network administrators may need to manually configure various VLAN or Bond interfaces on the nodes in advance. This process can be tedious and error-prone. Spiderpool provides a CNI meta-plugin called ifacer . This plugin dynamically creates VLAN sub-interfaces or Bond interfaces on the nodes during Pod creation, based on the provided ifacer configuration, greatly simplifying the configuration workload. In the following sections, we will delve into this plugin.","title":"Introduction"},{"location":"reference/plugin-ifacer/#feature","text":"Support dynamic creation of VLAN sub-interfaces Support dynamic creation of Bond interfaces","title":"Feature"},{"location":"reference/plugin-ifacer/#notes","text":"The VLAN/Bond interfaces created by this plugin will be lost when the node restarts, but they will be automatically recreated upon the Pod restarts. Deleting existed VLAN/Bond interfaces is not supported. Configuring the address of VLAN/Bond interfaces during creation is not supported. If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, Highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with Vlan and Bond interfaces created by Ifacer : ~# INTERFACE = <your_interface_name> ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"Notes"},{"location":"reference/plugin-ifacer/#prerequisite","text":"There are no specific requirements including Kubernetes or Kernel versions for using this plugin. During the installation of Spiderpool, the plugin will be automatically installed in the /opt/cni/bin/ directory on each host. You can verify by checking for the presence of the ifacer binary in that directory on each host.","title":"Prerequisite"},{"location":"reference/plugin-ifacer/#how-to-use","text":"Examples please see Ifacer Configuration","title":"How to use"},{"location":"reference/plugin-ipam/","text":"IPAM Plugin Configuration Here is an example of IPAM configuration. { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-pod-network\" , \"plugins\" :[ { \"name\" : \"macvlan-pod-network\" , \"type\" : \"macvlan\" , \"master\" : \"ens256\" , \"mode\" : \"bridge\" , \"mtu\" : 1500 , \"ipam\" :{ \"type\" : \"spiderpool\" , \"log_file_path\" : \"/var/log/spidernet/spiderpool.log\" , \"log_file_max_size\" : \"100\" , \"log_file_max_age\" : \"30\" , \"log_file_max_count\" : 7 , \"log_level\" : \"INFO\" , \"default_ipv4_ippool\" : [ \"default-ipv4-pool1\" , \"default-ipv4-pool2\" ], \"default_ipv6_ippool\" : [ \"default-ipv6-pool1\" , \"default-ipv6-pool2\" ] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM plugin"},{"location":"reference/plugin-ipam/#ipam-plugin-configuration","text":"Here is an example of IPAM configuration. { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-pod-network\" , \"plugins\" :[ { \"name\" : \"macvlan-pod-network\" , \"type\" : \"macvlan\" , \"master\" : \"ens256\" , \"mode\" : \"bridge\" , \"mtu\" : 1500 , \"ipam\" :{ \"type\" : \"spiderpool\" , \"log_file_path\" : \"/var/log/spidernet/spiderpool.log\" , \"log_file_max_size\" : \"100\" , \"log_file_max_age\" : \"30\" , \"log_file_max_count\" : 7 , \"log_level\" : \"INFO\" , \"default_ipv4_ippool\" : [ \"default-ipv4-pool1\" , \"default-ipv4-pool2\" ], \"default_ipv6_ippool\" : [ \"default-ipv6-pool1\" , \"default-ipv6-pool2\" ] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM Plugin Configuration"},{"location":"reference/spiderpool-agent/","text":"spiderpool-agent This page describes CLI options and ENV of spiderpool-agent. spiderpool-agent daemon Run the spiderpool agent daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin ENV env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide. SPIDERPOOL_ENABLED_RELEASE_CONFLICT_IPS true Enable/disable release conflict IPs. spiderpool-agent helps set sysctl configs for each node To optimize the kernel network configuration of a node, spiderpool-agent will by default configure the following kernel parameters: sysctl config value description net.ipv4.neigh.default.gc_thresh3 28160 This is the hard maximum number of entries to keep in the ARP cache. The garbage collector will always run if there are more than this number of entries in the cache. for ipv4 net.ipv6.neigh.default.gc_thresh3 28160 This is the hard maximum number of entries to keep in the ARP cache. The garbage collector will always run if there are more than this number of entries in the cache. for ipv6. Note: this is only avaliable in some low kernel version. net.ipv4.conf.all.arp_notify 1 Generate gratuitous arp requests when device is brought up or hardware address changes. net.ipv4.conf.all.forwarding 1 enable ipv4 forwarding net.ipv6.conf.all.forwarding 1 enable ipv6 forwarding net.ipv4.conf.all.rp_filter 0 no source validation for the each incoming packet Note: Some kernel parameters can only be set in certain kernel versions, so we will ignore the \"kernel parameter does not exist\" error when configure the kernel parameters. Example: net.ipv6.neigh.default.gc_thresh3 . Users can edit the spiderpoolAgent.securityContext field of values.yaml in the chart before installing spiderpool to update the kernel parameters that need additional configuration, or manually edit spiderpool-agent daemonSet after installing Spiderpool, and then restart spiderpool-agent pods: Users can disable this feature by following command when installing Spiderpool: helm install spiderpool -n kube-system --set global.tuneSysctlConfig=false Or configure the spiderpool-conf configMap, set tuneSysctlConfig to false and restart the spiderpool-agent pods. spiderpool-agent helps detect Pod's IPs if conflicts and Detect the gateway if reachable For Underlay networks, IP conflicts are unacceptable as they can cause serious issues. Spiderpool supports IP conflict detection and gateway reachability detection, which were previously implemented by the coordinator plugin but could cause some potential communication problems. Now, this is handled by IPAM. You can enable or disable this feature through the spiderpool-conf ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : spiderpool data : conf.yml : | ... enableIPConflictDetection: true enableGatewayDetection: true ... After applying the configMap, restart the spiderpool-agent pods. When IP conflict detection is enabled, Spiderpool will detect if the assigned IP address conflicts with others in the subnet by sending ARP or NDP packets. If a conflict is detected, Pod creation will be blocked. This supports both IPv4 and IPv6. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates an IP conflict. If a network timeout error is received, it is considered non-conflicting. When gateway reachability detection is enabled, Spiderpool will detect if the Pod's gateway address is reachable by sending ARP or NDP packets. If the gateway address is unreachable, Pod creation will be blocked. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates the gateway address is reachable. If no response is received, it indicates the gateway address is unreachable. Note: Some switches do not allow ARP probing and will issue alerts. In such cases, you need to set enableGatewayDetection to false. NOTE: Enabling IP conflict detection or gateway detection may increase the time required for IPAM calls and Pod startup, depending on the network. Particularly, when IPv6 Duplicate Address Detection (DAD) is enabled, the kernel will check for conflicts with local link addresses, which may consume additional time. spiderpool-agent shutdown Notify of stopping the spiderpool-agent daemon. spiderpool-agent metric Get local metrics. Options --port string http server port of local metric (default to 5711)","title":"spiderpool-agent"},{"location":"reference/spiderpool-agent/#spiderpool-agent","text":"This page describes CLI options and ENV of spiderpool-agent.","title":"spiderpool-agent"},{"location":"reference/spiderpool-agent/#spiderpool-agent-daemon","text":"Run the spiderpool agent daemon.","title":"spiderpool-agent daemon"},{"location":"reference/spiderpool-agent/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin","title":"Options"},{"location":"reference/spiderpool-agent/#env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide. SPIDERPOOL_ENABLED_RELEASE_CONFLICT_IPS true Enable/disable release conflict IPs.","title":"ENV"},{"location":"reference/spiderpool-agent/#spiderpool-agent-helps-set-sysctl-configs-for-each-node","text":"To optimize the kernel network configuration of a node, spiderpool-agent will by default configure the following kernel parameters: sysctl config value description net.ipv4.neigh.default.gc_thresh3 28160 This is the hard maximum number of entries to keep in the ARP cache. The garbage collector will always run if there are more than this number of entries in the cache. for ipv4 net.ipv6.neigh.default.gc_thresh3 28160 This is the hard maximum number of entries to keep in the ARP cache. The garbage collector will always run if there are more than this number of entries in the cache. for ipv6. Note: this is only avaliable in some low kernel version. net.ipv4.conf.all.arp_notify 1 Generate gratuitous arp requests when device is brought up or hardware address changes. net.ipv4.conf.all.forwarding 1 enable ipv4 forwarding net.ipv6.conf.all.forwarding 1 enable ipv6 forwarding net.ipv4.conf.all.rp_filter 0 no source validation for the each incoming packet Note: Some kernel parameters can only be set in certain kernel versions, so we will ignore the \"kernel parameter does not exist\" error when configure the kernel parameters. Example: net.ipv6.neigh.default.gc_thresh3 . Users can edit the spiderpoolAgent.securityContext field of values.yaml in the chart before installing spiderpool to update the kernel parameters that need additional configuration, or manually edit spiderpool-agent daemonSet after installing Spiderpool, and then restart spiderpool-agent pods: Users can disable this feature by following command when installing Spiderpool: helm install spiderpool -n kube-system --set global.tuneSysctlConfig=false Or configure the spiderpool-conf configMap, set tuneSysctlConfig to false and restart the spiderpool-agent pods.","title":"spiderpool-agent helps set sysctl configs for each node"},{"location":"reference/spiderpool-agent/#spiderpool-agent-helps-detect-pods-ips-if-conflicts-and-detect-the-gateway-if-reachable","text":"For Underlay networks, IP conflicts are unacceptable as they can cause serious issues. Spiderpool supports IP conflict detection and gateway reachability detection, which were previously implemented by the coordinator plugin but could cause some potential communication problems. Now, this is handled by IPAM. You can enable or disable this feature through the spiderpool-conf ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : spiderpool data : conf.yml : | ... enableIPConflictDetection: true enableGatewayDetection: true ... After applying the configMap, restart the spiderpool-agent pods. When IP conflict detection is enabled, Spiderpool will detect if the assigned IP address conflicts with others in the subnet by sending ARP or NDP packets. If a conflict is detected, Pod creation will be blocked. This supports both IPv4 and IPv6. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates an IP conflict. If a network timeout error is received, it is considered non-conflicting. When gateway reachability detection is enabled, Spiderpool will detect if the Pod's gateway address is reachable by sending ARP or NDP packets. If the gateway address is unreachable, Pod creation will be blocked. If sending ARP or NDP probe packets fails, it will retry 3 times, and if all attempts fail, an error will be returned. If the probe packet is successfully sent and a response is received within 100ms, it indicates the gateway address is reachable. If no response is received, it indicates the gateway address is unreachable. Note: Some switches do not allow ARP probing and will issue alerts. In such cases, you need to set enableGatewayDetection to false. NOTE: Enabling IP conflict detection or gateway detection may increase the time required for IPAM calls and Pod startup, depending on the network. Particularly, when IPv6 Duplicate Address Detection (DAD) is enabled, the kernel will check for conflicts with local link addresses, which may consume additional time.","title":"spiderpool-agent helps detect Pod's IPs if conflicts and Detect the gateway if reachable"},{"location":"reference/spiderpool-agent/#spiderpool-agent-shutdown","text":"Notify of stopping the spiderpool-agent daemon.","title":"spiderpool-agent shutdown"},{"location":"reference/spiderpool-agent/#spiderpool-agent-metric","text":"Get local metrics.","title":"spiderpool-agent metric"},{"location":"reference/spiderpool-agent/#options_1","text":"--port string http server port of local metric (default to 5711)","title":"Options"},{"location":"reference/spiderpool-controller/","text":"spiderpool-controller This page describes CLI options and ENV of spiderpool-controller. spiderpool-controller daemon Run the spiderpool controller daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) ENV env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_ENABLED_DEBUG_METRIC false Enable spiderpool agent to collect debug level metrics. SPIDERPOOL_METRIC_HTTP_PORT false The metrics port of spiderpool agent. SPIDERPOOL_GOPS_LISTEN_PORT 5724 The gops port of spiderpool Controller. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_HEALTH_PORT 5720 The http Port for spiderpoolController, for health checking and http service. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_READY_NODE_ENABLED true Enable/disable IP GC for stateless Terminating pod when the pod corresponding node is ready. SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_NOT_READY_NODE_ENABLED true Enable/disable IP GC for stateless Terminating pod when the pod corresponding node is not ready. SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY true The gc delay seconds after the pod times out of deleting graceful period. SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION true The gc all interval duration. SPIDERPOOL_MULTUS_CONFIG_ENABLED true Enable/disable SpiderMultusConfig. SPIDERPOOL_CNI_CONFIG_DIR /etc/cni/net.d The host path of the cni config directory. SPIDERPOOL_CILIUM_CONFIGMAP_NAMESPACE_NAME kube-system/cilium-config. The cilium's configMap, default is kube-system/cilium-config. SPIDERPOOL_COORDINATOR_DEFAULT_NAME default the name of default spidercoordinator CR SPIDERPOOL_CONTROLLER_DEPLOYMENT_NAME spiderpool-controller The deployment name of spiderpool-controller. spiderpool-controller shutdown Notify of stopping spiderpool-controller daemon. spiderpool-controller metric Get local metrics. Options --port string http server port of local metric (default to 5721) spiderpool-controller status Show status: Whether local is controller leader ... Options --port string http server port of local metric (default to 5720)","title":"spiderpool-controller"},{"location":"reference/spiderpool-controller/#spiderpool-controller","text":"This page describes CLI options and ENV of spiderpool-controller.","title":"spiderpool-controller"},{"location":"reference/spiderpool-controller/#spiderpool-controller-daemon","text":"Run the spiderpool controller daemon.","title":"spiderpool-controller daemon"},{"location":"reference/spiderpool-controller/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map)","title":"Options"},{"location":"reference/spiderpool-controller/#env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_ENABLED_DEBUG_METRIC false Enable spiderpool agent to collect debug level metrics. SPIDERPOOL_METRIC_HTTP_PORT false The metrics port of spiderpool agent. SPIDERPOOL_GOPS_LISTEN_PORT 5724 The gops port of spiderpool Controller. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_HEALTH_PORT 5720 The http Port for spiderpoolController, for health checking and http service. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_READY_NODE_ENABLED true Enable/disable IP GC for stateless Terminating pod when the pod corresponding node is ready. SPIDERPOOL_GC_STATELESS_TERMINATING_POD_ON_NOT_READY_NODE_ENABLED true Enable/disable IP GC for stateless Terminating pod when the pod corresponding node is not ready. SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY true The gc delay seconds after the pod times out of deleting graceful period. SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION true The gc all interval duration. SPIDERPOOL_MULTUS_CONFIG_ENABLED true Enable/disable SpiderMultusConfig. SPIDERPOOL_CNI_CONFIG_DIR /etc/cni/net.d The host path of the cni config directory. SPIDERPOOL_CILIUM_CONFIGMAP_NAMESPACE_NAME kube-system/cilium-config. The cilium's configMap, default is kube-system/cilium-config. SPIDERPOOL_COORDINATOR_DEFAULT_NAME default the name of default spidercoordinator CR SPIDERPOOL_CONTROLLER_DEPLOYMENT_NAME spiderpool-controller The deployment name of spiderpool-controller.","title":"ENV"},{"location":"reference/spiderpool-controller/#spiderpool-controller-shutdown","text":"Notify of stopping spiderpool-controller daemon.","title":"spiderpool-controller shutdown"},{"location":"reference/spiderpool-controller/#spiderpool-controller-metric","text":"Get local metrics.","title":"spiderpool-controller metric"},{"location":"reference/spiderpool-controller/#options_1","text":"--port string http server port of local metric (default to 5721)","title":"Options"},{"location":"reference/spiderpool-controller/#spiderpool-controller-status","text":"Show status: Whether local is controller leader ...","title":"spiderpool-controller status"},{"location":"reference/spiderpool-controller/#options_2","text":"--port string http server port of local metric (default to 5720)","title":"Options"},{"location":"reference/spiderpoolctl/","text":"spiderpoolctl This page describes CLI usage of spiderpoolctl for debug. spiderpoolctl gc Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address) spiderpoolctl ip show Show a pod that is taking this IP. Options --ip string [required] ip spiderpoolctl ip release Try to release an IP. Options --ip string [optional] ip --force [optional] force release ip spiderpoolctl ip set Set IP to be taken by a pod. This will update ippool and workload endpoint resource. Options --ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"spiderpoolctl"},{"location":"reference/spiderpoolctl/#spiderpoolctl","text":"This page describes CLI usage of spiderpoolctl for debug.","title":"spiderpoolctl"},{"location":"reference/spiderpoolctl/#spiderpoolctl-gc","text":"Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address)","title":"spiderpoolctl gc"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-show","text":"Show a pod that is taking this IP.","title":"spiderpoolctl ip show"},{"location":"reference/spiderpoolctl/#options","text":"--ip string [required] ip","title":"Options"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-release","text":"Try to release an IP.","title":"spiderpoolctl ip release"},{"location":"reference/spiderpoolctl/#options_1","text":"--ip string [optional] ip --force [optional] force release ip","title":"Options"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-set","text":"Set IP to be taken by a pod. This will update ippool and workload endpoint resource.","title":"spiderpoolctl ip set"},{"location":"reference/spiderpoolctl/#options_2","text":"--ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"Options"},{"location":"usage/_feature_example_zh/","text":"\u67d0\u529f\u80fd \u4ecb\u7ecd \u672c\u6587\u4e3a\u4e86\u6f14\u793a\u4ec0\u4e48\uff0c\u5b83\u7684\u5e94\u7528\u573a\u666f \u9879\u76ee\u529f\u80fd \u672c\u9879\u76ee\u6709\u4ec0\u4e48\u529f\u80fd\uff0c\u5b83\u4e3a\u4ec0\u4e48\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u529f\u80fd\u7684\u5e94\u7528\u573a\u666f\uff0c\u529f\u80fd\u5b9e\u65bd\u7684\u9650\u5236\u6709\u54ea\u4e9b \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49\uff0c\u672c\u9879\u76ee\u5b89\u88c5\u65f6\u54ea\u4e9b\u9009\u578b\u8981\u6253\u5f00\u6216\u5173\u95ed \u6b65\u9aa4 step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b(\u7528\u4e8e\u6392\u969c)\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6","title":"\u67d0\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_1","text":"","title":"\u67d0\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_2","text":"\u672c\u6587\u4e3a\u4e86\u6f14\u793a\u4ec0\u4e48\uff0c\u5b83\u7684\u5e94\u7528\u573a\u666f","title":"\u4ecb\u7ecd"},{"location":"usage/_feature_example_zh/#_3","text":"\u672c\u9879\u76ee\u6709\u4ec0\u4e48\u529f\u80fd\uff0c\u5b83\u4e3a\u4ec0\u4e48\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u529f\u80fd\u7684\u5e94\u7528\u573a\u666f\uff0c\u529f\u80fd\u5b9e\u65bd\u7684\u9650\u5236\u6709\u54ea\u4e9b","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_4","text":"\u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49\uff0c\u672c\u9879\u76ee\u5b89\u88c5\u65f6\u54ea\u4e9b\u9009\u578b\u8981\u6253\u5f00\u6216\u5173\u95ed","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/_feature_example_zh/#_5","text":"step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b(\u7528\u4e8e\u6392\u969c)\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6","title":"\u6b65\u9aa4"},{"location":"usage/_install_example_zh/","text":"\u5b89\u88c5\u6587\u6863 \u4ecb\u7ecd \u672c\u6587\u8bf4\u660e\u4e3a\u4e86\u5b89\u88c5\u51fa\u4ec0\u4e48\u6837\u7684\u4e00\u5957\u96c6\u7fa4\uff0c\u5b83\u7684\u4ef7\u503c\u662f\u4ec0\u4e48 \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49 \u6b65\u9aa4 step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6 \u53ea\u8c08\u5b89\u88c5\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u72b6\u6001","title":"\u5b89\u88c5\u6587\u6863"},{"location":"usage/_install_example_zh/#_1","text":"","title":"\u5b89\u88c5\u6587\u6863"},{"location":"usage/_install_example_zh/#_2","text":"\u672c\u6587\u8bf4\u660e\u4e3a\u4e86\u5b89\u88c5\u51fa\u4ec0\u4e48\u6837\u7684\u4e00\u5957\u96c6\u7fa4\uff0c\u5b83\u7684\u4ef7\u503c\u662f\u4ec0\u4e48","title":"\u4ecb\u7ecd"},{"location":"usage/_install_example_zh/#_3","text":"\u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/_install_example_zh/#_4","text":"step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6 \u53ea\u8c08\u5b89\u88c5\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u72b6\u6001","title":"\u6b65\u9aa4"},{"location":"usage/cilium-chaining-zh_CN/","text":"Cilium \u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u652f\u6301 English | \u7b80\u4f53\u4e2d\u6587 \u4ecb\u7ecd \u672c\u6587\u4ecb\u7ecd IPVlan \u5982\u4f55\u4e0e Cilium \u96c6\u6210\uff0c\u4e3a IPVlan CNI \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002 \u80cc\u666f \u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI \u5982 IPVlan\u3001Macvlan \u7b49, \u5e76\u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u501f\u52a9 Cilium chaining-mode \u529f\u80fd\u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002\u4f46 Cilium \u5728 1.12 \u7248\u672c\u6b63\u5f0f\u79fb\u9664\u4e86\u5bf9 IPVlan Dataplane \u7684\u652f\u6301, \u8be6\u89c1 removed-options \u3002 \u7531\u4e8e\u53d7\u5230 Terway \u7684\u542f\u53d1\uff0c cilium-chaining \u9879\u76ee\u57fa\u4e8e Cilium v1.12.7 \u7248\u672c\u4fee\u6539 IPVlan Dataplane \u90e8\u5206, \u4f7f Cilium \u80fd\u591f\u4ee5 chaining-mode \u7684\u65b9\u5f0f\u4e0e IPVlan \u4e00\u8d77\u5de5\u4f5c\u3002\u89e3\u51b3 IPVlan \u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002 \u73af\u5883\u51c6\u5907 \u8981\u6c42\u8282\u70b9\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4\uff0c\u5e76\u6ce8\u610f\u4e0d\u80fd\u5b89\u88c5 Cilium \u5df2\u5b89\u88c5 Helm \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5b89\u88c5 Cilium-chaining \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 cilium-chaining \u7ec4\u4ef6: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system \u9a8c\u8bc1\u5b89\u88c5: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s \u914d\u7f6e CNI \u521b\u5efa Multus NetworkAttachmentDefinition CR, \u5982\u4e0b\u662f\u521b\u5efa IPvlan NetworkAttachmentDefinition \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u5728\u5982\u4e0b\u7684\u914d\u7f6e\u4e2d\uff0c\u6307\u5b9a master \u4e3a ens192, ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u8282\u70b9\u4e0a \u5c06 cilium \u5d4c\u5165\u5230 CNI \u914d\u7f6e\u4e2d\uff0c\u653e\u7f6e\u4e8e ipvlan plugin \u4e4b\u540e CNI \u7684 name \u5fc5\u987b\u548c\u5b89\u88c5 cilium-chaining \u65f6\u7684 cniChainingMode \u4fdd\u6301\u4e00\u81f4\uff0c\u5426\u5219\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF \u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\u4f7f\u7528 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u6587\u4ef6: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none> \u9a8c\u8bc1\u7f51\u7edc\u7b56\u7565\u662f\u5426\u751f\u6548 \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms \u521b\u5efa\u7981\u6b62 Pod \u4e0e\u5916\u90e8\u901a\u4fe1\u7684\u7f51\u7edc\u7b56\u7565 ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all \u6839\u636e label \u5339\u914d\u6240\u6709 pod, \u8be5\u7b56\u7565\u7981\u6b62 Pod \u5bf9\u5916\u901a\u4fe1 \u518d\u6b21\u9a8c\u8bc1 Pod \u5bf9\u5916\u901a\u4fe1 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss \u603b\u7ed3 \u901a\u8fc7\u6d4b\u8bd5\u53ef\u4ee5\u770b\u51fa\uff0cPod \u8bbf\u95ee\u5916\u90e8\u7684\u6d41\u91cf\u88ab\u7981\u6b62\uff0c\u7f51\u7edc\u7b56\u7565\u751f\u6548\uff0c\u8bc1\u660e\u901a\u8fc7 Cilium-chaining \u9879\u76ee \u5e2e\u52a9 IPVlan \u5b9e\u73b0\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"Cilium \u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u652f\u6301"},{"location":"usage/cilium-chaining-zh_CN/#cilium-ipvlan","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Cilium \u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u652f\u6301"},{"location":"usage/cilium-chaining-zh_CN/#_1","text":"\u672c\u6587\u4ecb\u7ecd IPVlan \u5982\u4f55\u4e0e Cilium \u96c6\u6210\uff0c\u4e3a IPVlan CNI \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/cilium-chaining-zh_CN/#_2","text":"\u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI \u5982 IPVlan\u3001Macvlan \u7b49, \u5e76\u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u501f\u52a9 Cilium chaining-mode \u529f\u80fd\u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002\u4f46 Cilium \u5728 1.12 \u7248\u672c\u6b63\u5f0f\u79fb\u9664\u4e86\u5bf9 IPVlan Dataplane \u7684\u652f\u6301, \u8be6\u89c1 removed-options \u3002 \u7531\u4e8e\u53d7\u5230 Terway \u7684\u542f\u53d1\uff0c cilium-chaining \u9879\u76ee\u57fa\u4e8e Cilium v1.12.7 \u7248\u672c\u4fee\u6539 IPVlan Dataplane \u90e8\u5206, \u4f7f Cilium \u80fd\u591f\u4ee5 chaining-mode \u7684\u65b9\u5f0f\u4e0e IPVlan \u4e00\u8d77\u5de5\u4f5c\u3002\u89e3\u51b3 IPVlan \u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"\u80cc\u666f"},{"location":"usage/cilium-chaining-zh_CN/#_3","text":"\u8981\u6c42\u8282\u70b9\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4\uff0c\u5e76\u6ce8\u610f\u4e0d\u80fd\u5b89\u88c5 Cilium \u5df2\u5b89\u88c5 Helm","title":"\u73af\u5883\u51c6\u5907"},{"location":"usage/cilium-chaining-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/cilium-chaining-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool.","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/cilium-chaining-zh_CN/#cilium-chaining","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 cilium-chaining \u7ec4\u4ef6: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system \u9a8c\u8bc1\u5b89\u88c5: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s","title":"\u5b89\u88c5 Cilium-chaining"},{"location":"usage/cilium-chaining-zh_CN/#cni","text":"\u521b\u5efa Multus NetworkAttachmentDefinition CR, \u5982\u4e0b\u662f\u521b\u5efa IPvlan NetworkAttachmentDefinition \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u5728\u5982\u4e0b\u7684\u914d\u7f6e\u4e2d\uff0c\u6307\u5b9a master \u4e3a ens192, ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u8282\u70b9\u4e0a \u5c06 cilium \u5d4c\u5165\u5230 CNI \u914d\u7f6e\u4e2d\uff0c\u653e\u7f6e\u4e8e ipvlan plugin \u4e4b\u540e CNI \u7684 name \u5fc5\u987b\u548c\u5b89\u88c5 cilium-chaining \u65f6\u7684 cniChainingMode \u4fdd\u6301\u4e00\u81f4\uff0c\u5426\u5219\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF","title":"\u914d\u7f6e CNI"},{"location":"usage/cilium-chaining-zh_CN/#_5","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\u4f7f\u7528 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u6587\u4ef6: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none>","title":"\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528"},{"location":"usage/cilium-chaining-zh_CN/#_6","text":"\u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms \u521b\u5efa\u7981\u6b62 Pod \u4e0e\u5916\u90e8\u901a\u4fe1\u7684\u7f51\u7edc\u7b56\u7565 ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all \u6839\u636e label \u5339\u914d\u6240\u6709 pod, \u8be5\u7b56\u7565\u7981\u6b62 Pod \u5bf9\u5916\u901a\u4fe1 \u518d\u6b21\u9a8c\u8bc1 Pod \u5bf9\u5916\u901a\u4fe1 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss","title":"\u9a8c\u8bc1\u7f51\u7edc\u7b56\u7565\u662f\u5426\u751f\u6548"},{"location":"usage/cilium-chaining-zh_CN/#_7","text":"\u901a\u8fc7\u6d4b\u8bd5\u53ef\u4ee5\u770b\u51fa\uff0cPod \u8bbf\u95ee\u5916\u90e8\u7684\u6d41\u91cf\u88ab\u7981\u6b62\uff0c\u7f51\u7edc\u7b56\u7565\u751f\u6548\uff0c\u8bc1\u660e\u901a\u8fc7 Cilium-chaining \u9879\u76ee \u5e2e\u52a9 IPVlan \u5b9e\u73b0\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"\u603b\u7ed3"},{"location":"usage/cilium-chaining/","text":"Cilium provides network policy support for IPVlan English | \u7b80\u4f53\u4e2d\u6587 Introduction This article describes how IPVlan integrates with Cilium to provide network policy capabilities for IPVlan CNI. background Currently, most Underlay type CNIs in the community, such as IPVlan, Macvlan, etc., do not support Kubernetes' native network policy capabilities. We can use the Cilium chaining-mode function to provide network policy capabilities for IPVlan.However, Cilium officially removed support for IPVlan Dataplane in version 1.12. For details, see removed-options . Inspired by Terway , the cilium-chaining project is based on Cilium The v1.12.7 version modifies the IPVlan Dataplane part to enable Cilium to work with IPVlan in chaining-mode. Solve the problem that IPVlan does not support Kubernetes\u2019 native network policy capabilities. Prerequisites The node kernel version is required to be at least greater than 4.19 Prepare a Kubernetes cluster and be careful not to install Cilium Installed Helm Steps Install Spiderpool Refer to Installation to install Spiderpool. Install Cilium-chaining Install the cilium-chaining component using the following command: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system Verify installation: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s Configure CNI Create Multus NetworkAttachmentDefinition CR. The following is an example of creating an IPvlan NetworkAttachmentDefinition configuration: In the following configuration, specify master as ens192, ens192 must exist on the node Embed cilium into CNI configuration, placed after ipvlan plugin The name of CNI must be consistent with the cniChainingMode when installing cilium-chaining, otherwise it will not work properly IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF Create a test application In the following example Yaml, a set of DaemonSet applications will be created, using v1.multus-cni.io/default-network : used to specify the CNI configuration file used by the application: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF Check Pod running status: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none> Verify whether the network policy is effective Test the communication between Pods and Pods across nodes and subnets ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms Create a network policy that prohibits Pods from communicating with the outside world ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all matches all pods based on label. This policy prohibits pods from communicating with others. Verify Pod external communication again ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss Conclusion From the results, it can be seen that the Pod's access to external traffic is prohibited and the network policy takes effect, proving that the Cilium-chaining project helps IPVlan achieve network policy capabilities.","title":"Network Policy Support"},{"location":"usage/cilium-chaining/#cilium-provides-network-policy-support-for-ipvlan","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Cilium provides network policy support for IPVlan"},{"location":"usage/cilium-chaining/#introduction","text":"This article describes how IPVlan integrates with Cilium to provide network policy capabilities for IPVlan CNI.","title":"Introduction"},{"location":"usage/cilium-chaining/#background","text":"Currently, most Underlay type CNIs in the community, such as IPVlan, Macvlan, etc., do not support Kubernetes' native network policy capabilities. We can use the Cilium chaining-mode function to provide network policy capabilities for IPVlan.However, Cilium officially removed support for IPVlan Dataplane in version 1.12. For details, see removed-options . Inspired by Terway , the cilium-chaining project is based on Cilium The v1.12.7 version modifies the IPVlan Dataplane part to enable Cilium to work with IPVlan in chaining-mode. Solve the problem that IPVlan does not support Kubernetes\u2019 native network policy capabilities.","title":"background"},{"location":"usage/cilium-chaining/#prerequisites","text":"The node kernel version is required to be at least greater than 4.19 Prepare a Kubernetes cluster and be careful not to install Cilium Installed Helm","title":"Prerequisites"},{"location":"usage/cilium-chaining/#steps","text":"","title":"Steps"},{"location":"usage/cilium-chaining/#install-spiderpool","text":"Refer to Installation to install Spiderpool.","title":"Install Spiderpool"},{"location":"usage/cilium-chaining/#install-cilium-chaining","text":"Install the cilium-chaining component using the following command: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system Verify installation: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s","title":"Install Cilium-chaining"},{"location":"usage/cilium-chaining/#configure-cni","text":"Create Multus NetworkAttachmentDefinition CR. The following is an example of creating an IPvlan NetworkAttachmentDefinition configuration: In the following configuration, specify master as ens192, ens192 must exist on the node Embed cilium into CNI configuration, placed after ipvlan plugin The name of CNI must be consistent with the cniChainingMode when installing cilium-chaining, otherwise it will not work properly IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF","title":"Configure CNI"},{"location":"usage/cilium-chaining/#create-a-test-application","text":"In the following example Yaml, a set of DaemonSet applications will be created, using v1.multus-cni.io/default-network : used to specify the CNI configuration file used by the application: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF Check Pod running status: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none>","title":"Create a test application"},{"location":"usage/cilium-chaining/#verify-whether-the-network-policy-is-effective","text":"Test the communication between Pods and Pods across nodes and subnets ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms Create a network policy that prohibits Pods from communicating with the outside world ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all matches all pods based on label. This policy prohibits pods from communicating with others. Verify Pod external communication again ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss","title":"Verify whether the network policy is effective"},{"location":"usage/cilium-chaining/#conclusion","text":"From the results, it can be seen that the Pod's access to external traffic is prohibited and the network policy takes effect, proving that the Cilium-chaining project helps IPVlan achieve network policy capabilities.","title":"Conclusion"},{"location":"usage/cli/","text":"Command line tool (spiderpoolctl) TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/cli/#command-line-tool-spiderpoolctl","text":"TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/egress-zh_CN/","text":"Egress Policy English | \u7b80\u4f53\u4e2d\u6587 \u4ecb\u7ecd Spiderpool \u662f\u4e00\u4e2a Kubernetes \u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728 Kubernetes \u96c6\u7fa4\u4e2d\uff0cPod \u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u5176\u51fa\u53e3 IP \u5730\u5740\u4e0d\u662f\u56fa\u5b9a\u7684\u3002\u5728 Overlay \u7f51\u7edc\u4e2d\uff0c\u51fa\u53e3 IP \u5730\u5740\u4e3a Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u800c\u5728 Underlay \u7f51\u7edc\u4e2d\uff0cPod \u76f4\u63a5\u4f7f\u7528\u81ea\u8eab\u7684 IP \u5730\u5740\u4e0e\u5916\u90e8\u901a\u4fe1\u3002\u56e0\u6b64\uff0c\u5f53 Pod \u53d1\u751f\u65b0\u7684\u8c03\u5ea6\u65f6\uff0c\u65e0\u8bba\u54ea\u79cd\u7f51\u7edc\u6a21\u5f0f\uff0cPod \u4e0e\u5916\u90e8\u901a\u4fe1\u65f6\u7684 IP \u5730\u5740\u90fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7ed9\u7cfb\u7edf\u7ef4\u62a4\u4eba\u5458\u5e26\u6765\u4e86 IP \u5730\u5740\u7ba1\u7406\u7684\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\u4ee5\u53ca\u9700\u8981\u8fdb\u884c\u7f51\u7edc\u6545\u969c\u8bca\u65ad\u65f6\uff0c\u5728\u96c6\u7fa4\u5916\u90e8\uff0c\u57fa\u4e8e Pod \u539f\u672c\u7684\u51fa\u53e3 IP \u6765\u7ba1\u63a7\u51fa\u53e3\u6d41\u91cf\u5f88\u96be\u5b9e\u73b0\u3002\u800c Spiderpool \u53ef\u4ee5\u642d\u914d\u7ec4\u4ef6 EgressGateway \u5b8c\u7f8e\u89e3\u51b3 Underlay \u7f51\u7edc\u4e0b Pod \u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u7684\u95ee\u9898\u3002 \u9879\u76ee\u529f\u80fd EgressGateway \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 Egress \u7f51\u5173\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u4e0d\u540c CNI \u7f51\u7edc\u6a21\u5f0f\u4e0b\uff08Spiderpool\u3001Calico\u3001Flannel\u3001Weave\uff09\u51fa\u53e3 Egress IP \u5730\u5740\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7075\u6d3b\u914d\u7f6e\u548c\u7ba1\u7406\u51fa\u53e3\u7b56\u7565\uff0c\u4e3a\u79df\u6237\u7ea7\u6216\u96c6\u7fa4\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u7f6e Egress IP\uff0c\u4f7f\u5f97 Pod \u8bbf\u95ee\u5916\u90e8\u7f51\u7edc\u65f6\uff0c\u7cfb\u7edf\u4f1a\u7edf\u4e00\u4f7f\u7528\u8fd9\u4e2a\u8bbe\u7f6e\u7684 Egress IP \u4f5c\u4e3a\u51fa\u53e3\u5730\u5740\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002\u4f46 EgressGateway \u6240\u6709\u7684\u89c4\u5219\u90fd\u662f\u751f\u6548\u5728\u4e3b\u673a\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e0a\u7684\uff0c\u8981\u4f7f EgressGateway \u7b56\u7565\u751f\u6548\uff0c\u5219 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7684\u6d41\u91cf\uff0c\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u3002\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7 Spiderpool \u7684 spidercoordinators \u7684 spec.hijackCIDR \u5b57\u6bb5\u914d\u7f6e\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\uff0c\u518d\u901a\u8fc7 coordinator \u5c06\u5339\u914d\u7684\u6d41\u91cf\u4ece veth pair \u8f6c\u53d1\u5230\u4e3b\u673a\u4e0a\u3002\u4f7f\u5f97\u6240\u8bbf\u95ee\u7684\u5916\u90e8\u6d41\u91cf\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\uff0c\u501f\u6b64\u5b9e\u73b0 underlay \u7f51\u7edc\u4e0b\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u3002 Spiderpool \u642d\u914d EgressGateway \u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u529f\u80fd\uff1a \u89e3\u51b3 IPv4/IPv6 \u53cc\u6808\u8fde\u63a5\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u5728\u4e0d\u540c\u534f\u8bae\u6808\u4e0b\u7684\u65e0\u7f1d\u8fde\u63a5\u3002 \u89e3\u51b3 Egress \u8282\u70b9\u7684\u9ad8\u53ef\u7528\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u8fde\u901a\u6027\u4e0d\u53d7\u5355\u70b9\u6545\u969c\u7684\u5e72\u6270\u3002 \u5141\u8bb8\u66f4\u7cbe\u7ec6\u7684\u7b56\u7565\u63a7\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7 EgressGateway \u7075\u6d3b\u5730\u8fc7\u6ee4 Pods \u7684 Egress \u7b56\u7565\uff0c\u5305\u62ec Destination CIDR\u3002 \u5141\u8bb8\u8fc7\u6ee4 Egress \u5e94\u7528\uff08Pod\uff09\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u7ba1\u7406\u7279\u5b9a\u5e94\u7528\u7684\u51fa\u53e3\u6d41\u91cf\u3002 \u652f\u6301\u591a\u4e2a\u51fa\u53e3\u7f51\u5173\u5b9e\u4f8b\uff0c\u80fd\u591f\u5904\u7406\u591a\u4e2a\u7f51\u7edc\u5206\u533a\u6216\u96c6\u7fa4\u4e4b\u95f4\u7684\u901a\u4fe1\u3002 \u652f\u6301\u79df\u6237\u7ea7\u522b\u7684 Egress IP\u3002 \u652f\u6301\u81ea\u52a8\u68c0\u6d4b\u96c6\u7fa4\u6d41\u91cf\u7684 Egress \u7f51\u5173\u7b56\u7565\u3002 \u652f\u6301\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 Egress \u5b9e\u4f8b\u3002 \u53ef\u7528\u4e8e\u8f83\u4f4e\u5185\u6838\u7248\u672c\uff0c\u9002\u7528\u4e8e\u5404\u79cd Kubernetes \u90e8\u7f72\u73af\u5883\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool\uff0c\u5e76\u521b\u5efa SpiderMultusConfig CR \u4e0e IPPool CR. \u5728\u5b89\u88c5\u5b8c Spiderpool \u540e\uff0c\u5c06\u96c6\u7fa4\u5916\u7684\u670d\u52a1\u5730\u5740\u6dfb\u52a0\u5230 spiderpool.spidercoordinators \u7684 'default' \u5bf9\u8c61\u7684 'hijackCIDR' \u4e2d\uff0c\u4f7f Pod \u8bbf\u95ee\u8fd9\u4e9b\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u6d41\u91cf\u5148\u7ecf\u8fc7 Pod \u6240\u5728\u7684\u4e3b\u673a\uff0c\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\u3002 # \"10.6.168.63/32\" \u4e3a\u5916\u90e8\u670d\u52a1\u5730\u5740\u3002\u5bf9\u4e8e\u5df2\u7ecf\u8fd0\u884c\u7684 Pod\uff0c\u9700\u8981\u91cd\u542f Pod\uff0c\u8fd9\u4e9b\u8def\u7531\u89c4\u5219\u624d\u4f1a\u5728 Pod \u4e2d\u751f\u6548\u3002 ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}' \u5b89\u88c5 EgressGateway \u901a\u8fc7 helm \u5b89\u88c5 EgressGateway helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug \u5982\u679c\u9700\u8981\u4f7f\u7528 IPv6 \uff0c\u53ef\u4f7f\u7528\u9009\u9879 --set feature.enableIPv6=true \u5f00\u542f\uff0c\u5e76\u8bbe\u7f6e feature.tunnelIpv6Subnet , \u503c\u5f97\u6ce8\u610f\u7684\u662f\u5728\u901a\u8fc7 feature.tunnelIpv4Subnet \u4e0e feature.tunnelIpv6Subnet \u914d\u7f6e IPv4 \u6216 IPv6 \u7f51\u6bb5\u65f6\uff0c\u9700\u8981\u4fdd\u8bc1\u7f51\u6bb5\u548c\u96c6\u7fa4\u5185\u7684\u5176\u4ed6\u5730\u5740\u4e0d\u51b2\u7a81\u3002 feature.enableGatewayReplyRoute \u4e3a true \u65f6\uff0c\u5c06\u5f00\u542f\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u8fd4\u56de\u8def\u7531\u89c4\u5219\uff0c\u5728\u4e0e Spiderpool \u642d\u914d\u652f\u6301 underlay CNI \u65f6\uff0c\u5fc5\u987b\u5f00\u542f\u8be5\u9009\u9879\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u8fd8\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d EgressGateway \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u9a8c\u8bc1 EgressGateway \u5b89\u88c5 ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m \u66f4\u591a\u5b89\u88c5\u7ec6\u8282\uff0c\u53c2\u8003 EgressGateway \u5b89\u88c5 \u521b\u5efa EgressGateway \u5b9e\u4f8b EgressGateway \u5b9a\u4e49\u4e86\u4e00\u7ec4\u8282\u70b9\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u96c6\u7fa4\u5185\u7684 egress \u6d41\u91cf\u5c06\u4f1a\u901a\u8fc7\u8fd9\u7ec4\u8282\u70b9\u8f6c\u53d1\u800c\u51fa\u96c6\u7fa4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u9884\u5148\u5b9a\u4e49 EgressGateway \u5b9e\u4f8b\uff0c\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a EgressGateway \u5b9e\u4f8b\uff0c\u5176\u4e2d\uff1a spec.ippools.ipv4 \uff1a\u5b9a\u4e49\u4e86\u4e00\u7ec4 egress \u7684\u51fa\u53e3 IP \u5730\u5740\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u73af\u5883\u7684\u5b9e\u9645\u60c5\u51b5\u8c03\u6574\u3002\u5e76\u4e14 spec.ippools.ipv4 \u7684 CIDR \u5e94\u8be5\u4e0e\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u51fa\u53e3\u7f51\u5361\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\u662f\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361\uff09\u7684\u5b50\u7f51\u76f8\u540c\uff0c\u5426\u5219\uff0c\u53ef\u80fd\u5bfc\u81f4 egress \u8bbf\u95ee\u4e0d\u901a\u3002 spec.nodeSelector \uff1aEgressGateway \u63d0\u4f9b\u7684\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u5339\u914d\u65f6\uff0c\u8be5\u8282\u70b9\u5c06\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u4e0d\u5339\u914d\u65f6\uff0cEgressGateway \u4f1a\u7565\u8fc7\u8be5\u8282\u70b9\uff0c\u5b83\u5c06\u4e0d\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5b83\u652f\u6301\u9009\u62e9\u591a\u4e2a\u8282\u70b9\u6765\u5b9e\u73b0\u9ad8\u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF \u7ed9\u8282\u70b9\u6253\u4e0a\u4e0a\u8ff0\u4e2d nodeSelector.selector.matchLabels \u6240\u6307\u5b9a\u7684 Label\uff0c\u4f7f\u8282\u70b9\u80fd\u88ab EgressGateway \u9009\u4e2d\uff0c\u4f5c\u4e3a\u51fa\u53e3\u7f51\u5173\u3002 ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressGateway \u72b6\u6001\u3002\u5176\u4e2d\uff1a spec.ippools.ipv4DefaultEIP \u4ee3\u8868\u8be5\u7ec4 EgressGateway \u7684\u9ed8\u8ba4 VIP\uff0c\u5b83\u662f\u4f1a\u4ece spec.ippools.ipv4 \u4e2d\u968f\u673a\u9009\u62e9\u7684\u4e00\u4e2a IP \u5730\u5740\uff0c\u5b83\u7684\u4f5c\u7528\u662f\uff1a\u5f53\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy \u5bf9\u8c61\u65f6\uff0c\u5982\u679c\u672a\u6307\u5b9a VIP \u5730\u5740\uff0c\u5219\u4f7f\u7528\u8be5\u9ed8\u8ba4 VIP status.nodeList \u4ee3\u8868\u8bc6\u522b\u5230\u4e86\u7b26\u5408 spec.nodeSelector \u7684\u8282\u70b9\u53ca\u8be5\u8282\u70b9\u5bf9\u5e94\u7684 EgressTunnel \u5bf9\u8c61\u7684\u72b6\u6001\u3002 ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready \u521b\u5efa\u5e94\u7528\u548c\u51fa\u53e3\u7b56\u7565 \u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u5b83\u5c06\u7528\u4e8e\u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7528\u9014\uff0c\u5e76\u7ed9\u5b83\u6253\u4e0a label \u4ee5\u4fbf\u4e0e EgressPolicy \u5173\u8054\uff0c\u5982\u4e0b\u662f\u793a\u4f8b Yaml\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u8be5\u503c\u5bf9\u5e94\u7684 Multus CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b\u7684 SpiderIPPool \u8d44\u6e90, \u8be5\u503c\u5bf9\u5e94\u7684 SpiderIPPool CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF EgressPolicy \u5b9e\u4f8b\u7528\u4e8e\u5b9a\u4e49\u54ea\u4e9b Pod \u7684\u51fa\u53e3\u6d41\u91cf\u8981\u7ecf\u8fc7 EgressGateway \u8282\u70b9\u8f6c\u53d1\uff0c\u4ee5\u53ca\u5176\u5b83\u7684\u914d\u7f6e\u7ec6\u8282\u3002\u5982\u4e0b\u662f\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy CR \u5bf9\u8c61\u7684\u793a\u4f8b\uff0c\u5176\u4e2d\u3002 spec.egressGatewayName \u7528\u4e8e\u6307\u5b9a\u4e86\u4f7f\u7528\u54ea\u4e00\u7ec4 EgressGateway \u3002 spec.appliedTo.podSelector \u7528\u4e8e\u6307\u5b9a\u672c\u7b56\u7565\u5728\u96c6\u7fa4\u5185\u7684\u54ea\u4e9b Pod \u4e0a\u751f\u6548\u3002 namespace \u7528\u4e8e\u6307\u5b9a EgressPolicy \u5bf9\u8c61\u6240\u5728\u79df\u6237\uff0c\u56e0\u4e3a EgressPolicy \u662f\u79df\u6237\u7ea7\u522b\u7684\uff0c\u6240\u4ee5\u5b83\u52a1\u5fc5\u521b\u5efa\u5728\u4e0a\u8ff0\u5173\u8054\u5e94\u7528\u7684\u76f8\u540c namespace \u4e0b\uff0c\u8fd9\u6837\u5339\u914d\u7684 Pod \u8bbf\u95ee\u4efb\u610f\u96c6\u7fa4\u5916\u90e8\u7684\u5730\u5740\u65f6\uff0c\u624d\u80fd\u88ab EgressGateway Node \u8f6c\u53d1\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressPolicy \u7684\u72b6\u6001\u3002\u5176\u4e2d\uff1a status.eip \u5c55\u793a\u4e86\u8be5\u7ec4\u5e94\u7528\u51fa\u96c6\u7fa4\u65f6\u4f7f\u7528\u7684\u51fa\u53e3 IP \u5730\u5740\u3002 status.node \u5c55\u793a\u4e86\u54ea\u4e00\u4e2a EgressGateway \u7684\u8282\u70b9\u8d1f\u8d23\u8be5 EgressPolicy \u51fa\u53e3\u6d41\u91cf\u7684\u8f6c\u53d1\u3002 ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 \u521b\u5efa EgressPolicy \u5bf9\u8c61\u540e\uff0c\u4f1a\u6839\u636e EgressPolicy \u9009\u62e9\u7684\u5e94\u7528\u751f\u6210\u4e00\u4e2a\u5305\u542b\u6240\u6709\u5e94\u7528\u7684 IP \u5730\u5740\u96c6\u5408\u7684 EgressEndpointSlices \u5bf9\u8c61\uff0c\u5f53\u5e94\u7528\u65e0\u6cd5\u51fa\u53e3\u8bbf\u95ee\u65f6\uff0c\u53ef\u4ee5\u67e5\u770b EgressEndpointSlices \u5bf9\u8c61\u4e2d\u7684 IP \u5730\u5740\u662f\u5426\u6b63\u5e38\u3002 ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default \u6d4b\u8bd5 \u5728\u96c6\u7fa4\u5916\u90e8\u7f72\u5e94\u7528 nettools \uff0c\u7528\u4e8e\u6a21\u62df\u4e00\u4e2a\u96c6\u7fa4\u5916\u90e8\u7684\u670d\u52a1\uff0c\u800c nettools \u4f1a\u5728 http \u56de\u590d\u4e2d\u8fd4\u56de\u8bf7\u6c42\u8005\u7684\u6e90 IP \u5730\u5740\u3002 ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 \u5728\u96c6\u7fa4\u5185\u7684\u6d4b\u8bd5\u5e94\u7528\uff1atest-app \u4e2d\uff0c\u9a8c\u8bc1\u51fa\u53e3\u6d41\u91cf\u7684\u6548\u679c\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5728\u8be5\u5e94\u7528\u5bf9\u5e94 Pod \u4e2d\u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c nettools \u8fd4\u56de\u7684\u6e90 IP \u7b26\u5408\u4e86 EgressPolicy.status.eip \u7684\u6548\u679c\u3002 ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.1.92:8080 # \u96c6\u7fa4\u5916\u8282\u70b9\u7684 IP \u5730\u5740 + webPort Remote IP: 10 .6.168.201","title":"Egress Policy"},{"location":"usage/egress-zh_CN/#egress-policy","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Egress Policy"},{"location":"usage/egress-zh_CN/#_1","text":"Spiderpool \u662f\u4e00\u4e2a Kubernetes \u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728 Kubernetes \u96c6\u7fa4\u4e2d\uff0cPod \u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u5176\u51fa\u53e3 IP \u5730\u5740\u4e0d\u662f\u56fa\u5b9a\u7684\u3002\u5728 Overlay \u7f51\u7edc\u4e2d\uff0c\u51fa\u53e3 IP \u5730\u5740\u4e3a Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u800c\u5728 Underlay \u7f51\u7edc\u4e2d\uff0cPod \u76f4\u63a5\u4f7f\u7528\u81ea\u8eab\u7684 IP \u5730\u5740\u4e0e\u5916\u90e8\u901a\u4fe1\u3002\u56e0\u6b64\uff0c\u5f53 Pod \u53d1\u751f\u65b0\u7684\u8c03\u5ea6\u65f6\uff0c\u65e0\u8bba\u54ea\u79cd\u7f51\u7edc\u6a21\u5f0f\uff0cPod \u4e0e\u5916\u90e8\u901a\u4fe1\u65f6\u7684 IP \u5730\u5740\u90fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7ed9\u7cfb\u7edf\u7ef4\u62a4\u4eba\u5458\u5e26\u6765\u4e86 IP \u5730\u5740\u7ba1\u7406\u7684\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\u4ee5\u53ca\u9700\u8981\u8fdb\u884c\u7f51\u7edc\u6545\u969c\u8bca\u65ad\u65f6\uff0c\u5728\u96c6\u7fa4\u5916\u90e8\uff0c\u57fa\u4e8e Pod \u539f\u672c\u7684\u51fa\u53e3 IP \u6765\u7ba1\u63a7\u51fa\u53e3\u6d41\u91cf\u5f88\u96be\u5b9e\u73b0\u3002\u800c Spiderpool \u53ef\u4ee5\u642d\u914d\u7ec4\u4ef6 EgressGateway \u5b8c\u7f8e\u89e3\u51b3 Underlay \u7f51\u7edc\u4e0b Pod \u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u7684\u95ee\u9898\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/egress-zh_CN/#_2","text":"EgressGateway \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 Egress \u7f51\u5173\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u4e0d\u540c CNI \u7f51\u7edc\u6a21\u5f0f\u4e0b\uff08Spiderpool\u3001Calico\u3001Flannel\u3001Weave\uff09\u51fa\u53e3 Egress IP \u5730\u5740\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7075\u6d3b\u914d\u7f6e\u548c\u7ba1\u7406\u51fa\u53e3\u7b56\u7565\uff0c\u4e3a\u79df\u6237\u7ea7\u6216\u96c6\u7fa4\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u7f6e Egress IP\uff0c\u4f7f\u5f97 Pod \u8bbf\u95ee\u5916\u90e8\u7f51\u7edc\u65f6\uff0c\u7cfb\u7edf\u4f1a\u7edf\u4e00\u4f7f\u7528\u8fd9\u4e2a\u8bbe\u7f6e\u7684 Egress IP \u4f5c\u4e3a\u51fa\u53e3\u5730\u5740\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002\u4f46 EgressGateway \u6240\u6709\u7684\u89c4\u5219\u90fd\u662f\u751f\u6548\u5728\u4e3b\u673a\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e0a\u7684\uff0c\u8981\u4f7f EgressGateway \u7b56\u7565\u751f\u6548\uff0c\u5219 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7684\u6d41\u91cf\uff0c\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u3002\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7 Spiderpool \u7684 spidercoordinators \u7684 spec.hijackCIDR \u5b57\u6bb5\u914d\u7f6e\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\uff0c\u518d\u901a\u8fc7 coordinator \u5c06\u5339\u914d\u7684\u6d41\u91cf\u4ece veth pair \u8f6c\u53d1\u5230\u4e3b\u673a\u4e0a\u3002\u4f7f\u5f97\u6240\u8bbf\u95ee\u7684\u5916\u90e8\u6d41\u91cf\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\uff0c\u501f\u6b64\u5b9e\u73b0 underlay \u7f51\u7edc\u4e0b\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u3002 Spiderpool \u642d\u914d EgressGateway \u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u529f\u80fd\uff1a \u89e3\u51b3 IPv4/IPv6 \u53cc\u6808\u8fde\u63a5\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u5728\u4e0d\u540c\u534f\u8bae\u6808\u4e0b\u7684\u65e0\u7f1d\u8fde\u63a5\u3002 \u89e3\u51b3 Egress \u8282\u70b9\u7684\u9ad8\u53ef\u7528\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u8fde\u901a\u6027\u4e0d\u53d7\u5355\u70b9\u6545\u969c\u7684\u5e72\u6270\u3002 \u5141\u8bb8\u66f4\u7cbe\u7ec6\u7684\u7b56\u7565\u63a7\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7 EgressGateway \u7075\u6d3b\u5730\u8fc7\u6ee4 Pods \u7684 Egress \u7b56\u7565\uff0c\u5305\u62ec Destination CIDR\u3002 \u5141\u8bb8\u8fc7\u6ee4 Egress \u5e94\u7528\uff08Pod\uff09\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u7ba1\u7406\u7279\u5b9a\u5e94\u7528\u7684\u51fa\u53e3\u6d41\u91cf\u3002 \u652f\u6301\u591a\u4e2a\u51fa\u53e3\u7f51\u5173\u5b9e\u4f8b\uff0c\u80fd\u591f\u5904\u7406\u591a\u4e2a\u7f51\u7edc\u5206\u533a\u6216\u96c6\u7fa4\u4e4b\u95f4\u7684\u901a\u4fe1\u3002 \u652f\u6301\u79df\u6237\u7ea7\u522b\u7684 Egress IP\u3002 \u652f\u6301\u81ea\u52a8\u68c0\u6d4b\u96c6\u7fa4\u6d41\u91cf\u7684 Egress \u7f51\u5173\u7b56\u7565\u3002 \u652f\u6301\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 Egress \u5b9e\u4f8b\u3002 \u53ef\u7528\u4e8e\u8f83\u4f4e\u5185\u6838\u7248\u672c\uff0c\u9002\u7528\u4e8e\u5404\u79cd Kubernetes \u90e8\u7f72\u73af\u5883\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/egress-zh_CN/#_3","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/egress-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/egress-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool\uff0c\u5e76\u521b\u5efa SpiderMultusConfig CR \u4e0e IPPool CR. \u5728\u5b89\u88c5\u5b8c Spiderpool \u540e\uff0c\u5c06\u96c6\u7fa4\u5916\u7684\u670d\u52a1\u5730\u5740\u6dfb\u52a0\u5230 spiderpool.spidercoordinators \u7684 'default' \u5bf9\u8c61\u7684 'hijackCIDR' \u4e2d\uff0c\u4f7f Pod \u8bbf\u95ee\u8fd9\u4e9b\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u6d41\u91cf\u5148\u7ecf\u8fc7 Pod \u6240\u5728\u7684\u4e3b\u673a\uff0c\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\u3002 # \"10.6.168.63/32\" \u4e3a\u5916\u90e8\u670d\u52a1\u5730\u5740\u3002\u5bf9\u4e8e\u5df2\u7ecf\u8fd0\u884c\u7684 Pod\uff0c\u9700\u8981\u91cd\u542f Pod\uff0c\u8fd9\u4e9b\u8def\u7531\u89c4\u5219\u624d\u4f1a\u5728 Pod \u4e2d\u751f\u6548\u3002 ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}'","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/egress-zh_CN/#egressgateway","text":"\u901a\u8fc7 helm \u5b89\u88c5 EgressGateway helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug \u5982\u679c\u9700\u8981\u4f7f\u7528 IPv6 \uff0c\u53ef\u4f7f\u7528\u9009\u9879 --set feature.enableIPv6=true \u5f00\u542f\uff0c\u5e76\u8bbe\u7f6e feature.tunnelIpv6Subnet , \u503c\u5f97\u6ce8\u610f\u7684\u662f\u5728\u901a\u8fc7 feature.tunnelIpv4Subnet \u4e0e feature.tunnelIpv6Subnet \u914d\u7f6e IPv4 \u6216 IPv6 \u7f51\u6bb5\u65f6\uff0c\u9700\u8981\u4fdd\u8bc1\u7f51\u6bb5\u548c\u96c6\u7fa4\u5185\u7684\u5176\u4ed6\u5730\u5740\u4e0d\u51b2\u7a81\u3002 feature.enableGatewayReplyRoute \u4e3a true \u65f6\uff0c\u5c06\u5f00\u542f\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u8fd4\u56de\u8def\u7531\u89c4\u5219\uff0c\u5728\u4e0e Spiderpool \u642d\u914d\u652f\u6301 underlay CNI \u65f6\uff0c\u5fc5\u987b\u5f00\u542f\u8be5\u9009\u9879\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u8fd8\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d EgressGateway \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u9a8c\u8bc1 EgressGateway \u5b89\u88c5 ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m \u66f4\u591a\u5b89\u88c5\u7ec6\u8282\uff0c\u53c2\u8003 EgressGateway \u5b89\u88c5","title":"\u5b89\u88c5 EgressGateway"},{"location":"usage/egress-zh_CN/#egressgateway_1","text":"EgressGateway \u5b9a\u4e49\u4e86\u4e00\u7ec4\u8282\u70b9\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u96c6\u7fa4\u5185\u7684 egress \u6d41\u91cf\u5c06\u4f1a\u901a\u8fc7\u8fd9\u7ec4\u8282\u70b9\u8f6c\u53d1\u800c\u51fa\u96c6\u7fa4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u9884\u5148\u5b9a\u4e49 EgressGateway \u5b9e\u4f8b\uff0c\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a EgressGateway \u5b9e\u4f8b\uff0c\u5176\u4e2d\uff1a spec.ippools.ipv4 \uff1a\u5b9a\u4e49\u4e86\u4e00\u7ec4 egress \u7684\u51fa\u53e3 IP \u5730\u5740\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u73af\u5883\u7684\u5b9e\u9645\u60c5\u51b5\u8c03\u6574\u3002\u5e76\u4e14 spec.ippools.ipv4 \u7684 CIDR \u5e94\u8be5\u4e0e\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u51fa\u53e3\u7f51\u5361\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\u662f\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361\uff09\u7684\u5b50\u7f51\u76f8\u540c\uff0c\u5426\u5219\uff0c\u53ef\u80fd\u5bfc\u81f4 egress \u8bbf\u95ee\u4e0d\u901a\u3002 spec.nodeSelector \uff1aEgressGateway \u63d0\u4f9b\u7684\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u5339\u914d\u65f6\uff0c\u8be5\u8282\u70b9\u5c06\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u4e0d\u5339\u914d\u65f6\uff0cEgressGateway \u4f1a\u7565\u8fc7\u8be5\u8282\u70b9\uff0c\u5b83\u5c06\u4e0d\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5b83\u652f\u6301\u9009\u62e9\u591a\u4e2a\u8282\u70b9\u6765\u5b9e\u73b0\u9ad8\u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF \u7ed9\u8282\u70b9\u6253\u4e0a\u4e0a\u8ff0\u4e2d nodeSelector.selector.matchLabels \u6240\u6307\u5b9a\u7684 Label\uff0c\u4f7f\u8282\u70b9\u80fd\u88ab EgressGateway \u9009\u4e2d\uff0c\u4f5c\u4e3a\u51fa\u53e3\u7f51\u5173\u3002 ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressGateway \u72b6\u6001\u3002\u5176\u4e2d\uff1a spec.ippools.ipv4DefaultEIP \u4ee3\u8868\u8be5\u7ec4 EgressGateway \u7684\u9ed8\u8ba4 VIP\uff0c\u5b83\u662f\u4f1a\u4ece spec.ippools.ipv4 \u4e2d\u968f\u673a\u9009\u62e9\u7684\u4e00\u4e2a IP \u5730\u5740\uff0c\u5b83\u7684\u4f5c\u7528\u662f\uff1a\u5f53\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy \u5bf9\u8c61\u65f6\uff0c\u5982\u679c\u672a\u6307\u5b9a VIP \u5730\u5740\uff0c\u5219\u4f7f\u7528\u8be5\u9ed8\u8ba4 VIP status.nodeList \u4ee3\u8868\u8bc6\u522b\u5230\u4e86\u7b26\u5408 spec.nodeSelector \u7684\u8282\u70b9\u53ca\u8be5\u8282\u70b9\u5bf9\u5e94\u7684 EgressTunnel \u5bf9\u8c61\u7684\u72b6\u6001\u3002 ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready","title":"\u521b\u5efa EgressGateway \u5b9e\u4f8b"},{"location":"usage/egress-zh_CN/#_5","text":"\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u5b83\u5c06\u7528\u4e8e\u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7528\u9014\uff0c\u5e76\u7ed9\u5b83\u6253\u4e0a label \u4ee5\u4fbf\u4e0e EgressPolicy \u5173\u8054\uff0c\u5982\u4e0b\u662f\u793a\u4f8b Yaml\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u8be5\u503c\u5bf9\u5e94\u7684 Multus CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b\u7684 SpiderIPPool \u8d44\u6e90, \u8be5\u503c\u5bf9\u5e94\u7684 SpiderIPPool CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF EgressPolicy \u5b9e\u4f8b\u7528\u4e8e\u5b9a\u4e49\u54ea\u4e9b Pod \u7684\u51fa\u53e3\u6d41\u91cf\u8981\u7ecf\u8fc7 EgressGateway \u8282\u70b9\u8f6c\u53d1\uff0c\u4ee5\u53ca\u5176\u5b83\u7684\u914d\u7f6e\u7ec6\u8282\u3002\u5982\u4e0b\u662f\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy CR \u5bf9\u8c61\u7684\u793a\u4f8b\uff0c\u5176\u4e2d\u3002 spec.egressGatewayName \u7528\u4e8e\u6307\u5b9a\u4e86\u4f7f\u7528\u54ea\u4e00\u7ec4 EgressGateway \u3002 spec.appliedTo.podSelector \u7528\u4e8e\u6307\u5b9a\u672c\u7b56\u7565\u5728\u96c6\u7fa4\u5185\u7684\u54ea\u4e9b Pod \u4e0a\u751f\u6548\u3002 namespace \u7528\u4e8e\u6307\u5b9a EgressPolicy \u5bf9\u8c61\u6240\u5728\u79df\u6237\uff0c\u56e0\u4e3a EgressPolicy \u662f\u79df\u6237\u7ea7\u522b\u7684\uff0c\u6240\u4ee5\u5b83\u52a1\u5fc5\u521b\u5efa\u5728\u4e0a\u8ff0\u5173\u8054\u5e94\u7528\u7684\u76f8\u540c namespace \u4e0b\uff0c\u8fd9\u6837\u5339\u914d\u7684 Pod \u8bbf\u95ee\u4efb\u610f\u96c6\u7fa4\u5916\u90e8\u7684\u5730\u5740\u65f6\uff0c\u624d\u80fd\u88ab EgressGateway Node \u8f6c\u53d1\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressPolicy \u7684\u72b6\u6001\u3002\u5176\u4e2d\uff1a status.eip \u5c55\u793a\u4e86\u8be5\u7ec4\u5e94\u7528\u51fa\u96c6\u7fa4\u65f6\u4f7f\u7528\u7684\u51fa\u53e3 IP \u5730\u5740\u3002 status.node \u5c55\u793a\u4e86\u54ea\u4e00\u4e2a EgressGateway \u7684\u8282\u70b9\u8d1f\u8d23\u8be5 EgressPolicy \u51fa\u53e3\u6d41\u91cf\u7684\u8f6c\u53d1\u3002 ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 \u521b\u5efa EgressPolicy \u5bf9\u8c61\u540e\uff0c\u4f1a\u6839\u636e EgressPolicy \u9009\u62e9\u7684\u5e94\u7528\u751f\u6210\u4e00\u4e2a\u5305\u542b\u6240\u6709\u5e94\u7528\u7684 IP \u5730\u5740\u96c6\u5408\u7684 EgressEndpointSlices \u5bf9\u8c61\uff0c\u5f53\u5e94\u7528\u65e0\u6cd5\u51fa\u53e3\u8bbf\u95ee\u65f6\uff0c\u53ef\u4ee5\u67e5\u770b EgressEndpointSlices \u5bf9\u8c61\u4e2d\u7684 IP \u5730\u5740\u662f\u5426\u6b63\u5e38\u3002 ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default","title":"\u521b\u5efa\u5e94\u7528\u548c\u51fa\u53e3\u7b56\u7565"},{"location":"usage/egress-zh_CN/#_6","text":"\u5728\u96c6\u7fa4\u5916\u90e8\u7f72\u5e94\u7528 nettools \uff0c\u7528\u4e8e\u6a21\u62df\u4e00\u4e2a\u96c6\u7fa4\u5916\u90e8\u7684\u670d\u52a1\uff0c\u800c nettools \u4f1a\u5728 http \u56de\u590d\u4e2d\u8fd4\u56de\u8bf7\u6c42\u8005\u7684\u6e90 IP \u5730\u5740\u3002 ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 \u5728\u96c6\u7fa4\u5185\u7684\u6d4b\u8bd5\u5e94\u7528\uff1atest-app \u4e2d\uff0c\u9a8c\u8bc1\u51fa\u53e3\u6d41\u91cf\u7684\u6548\u679c\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5728\u8be5\u5e94\u7528\u5bf9\u5e94 Pod \u4e2d\u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c nettools \u8fd4\u56de\u7684\u6e90 IP \u7b26\u5408\u4e86 EgressPolicy.status.eip \u7684\u6548\u679c\u3002 ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.1.92:8080 # \u96c6\u7fa4\u5916\u8282\u70b9\u7684 IP \u5730\u5740 + webPort Remote IP: 10 .6.168.201","title":"\u6d4b\u8bd5"},{"location":"usage/egress/","text":"Egress Policy English | \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool is an Underlay networking solution for Kubernetes, but the egress IP address is not fixed in a Kubernetes cluster when a Pod accesses an external service. In an Overlay network, the egress IP address is the address of the node on which the Pod resides, whereas in an Underlay network, the Pod communicates directly with the outside world using its own IP address. Therefore, when a Pod undergoes new scheduling, the IP address of the Pod when communicating with the outside world will change regardless of the network mode. This instability creates IP address management challenges for system maintainers. Especially when the cluster scale increases and network troubleshooting is required, it is difficult to control the egress traffic based on the Pod's original egress IP outside the cluster. Spiderpool can be used with the component EgressGateway to solve the problem of Pod egress traffic management in Underlay network. Features of EgressGateway EgressGateway is an open source Egress gateway designed to solve the problem of exporting Egress IP addresses in different CNI network modes (Spiderpool, Calico, Flannel, Weave). By flexibly configuring and managing egress policies, Egress IP is set for tenant-level or cluster-level workloads, so that when a Pod accesses an external network, the system will uniformly use this set Egress IP as the egress address, thus providing a stable egress traffic management solution. However, all EgressGateway rules are effective on the host's network namespace. To make the EgressGateway policy effective, the traffic of Pods accessing the outside of the cluster has to go through the host's network namespace. Therefore, you can configure the subnet routes forwarded from the host via the spec.hijackCIDR field of spidercoordinators in Spiderpool, and then configure the subnet routes forwarded from the host via coordinator to forward matching traffic from the veth pair to the host. This enables egress traffic management on an underlay network by allowing access to external traffic to be matched by EgressGateway rules. Some of the features and benefits of Spiderpool with EgressGateway are as follows: Solve IPv4 IPv6 dual-stack connectivity, ensuring seamless communication across different protocol stacks. Solve the high availability of Egress Nodes, ensuring network connectivity remains unaffected by single-point failures. Support finer-grained policy control, allowing flexible filtering of Pods' Egress policies, including Destination CIDR. Support application-level control, allowing EgressGateway to filter Egress applications (Pods) for precise management of specific application outbound traffic. Support multiple egress gateways instance, capable of handling communication between multiple network partitions or clusters. Support namespaced egress IP. Support automatic detection of cluster traffic for egress gateways policies. Support namespace default egress instances. Can be used in low kernel version, making EgressGateway suitable for various Kubernetes deployment environments. Prerequisites A ready-to-use Kubernetes. Helm has been already installed. Steps Install Spiderpool Refer to Installation to install Spiderpool and create SpiderMultusConfig CR and IPPool CR. After installing Spiderpool, Add the service addresses outside the cluster to the 'hijackCIDR' field in the 'default' object of spiderpool.spidercoordinators. This ensures that when Pods access these external services, the traffic is routed through the host where the Pod is located, allowing the EgressGateway rules to match. # For running Pods, you need to restart them for these routing rules to take effect within the Pods. ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}' Install EgressGateway Install EgressGateway via helm: helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug If IPv6 is required, enable it with the option -set feature.enableIPv6=true and set feature.tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature. tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature.tunnelIpv6Subnet , you need to make sure that the segments don't conflict with any other addresses in the cluster. feature.enableGatewayReplyRoute is true to enable return routing rules on gateway nodes, which must be enabled when pairing with Spiderpool to support underlay CNI. If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for EgressGateway. Verify your EgressGateway installation: ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m For more installation details, refer to EgressGateway Installation . Create an instance of EgressGateway An EgressGateway defines a set of nodes that act as an egress gateway for the cluster, through which egress traffic within the cluster will be forwarded out of the cluster. Therefore, an EgressGateway instance needs to be pre-defined. The following example Yaml creates an EgressGateway instance. spec.ippools.ipv4 : defines a set of egress IP addresses, which need to be adjusted according to the actual situation of the specific environment. The CIDR of spec.ippools.ipv4 should be the same as the subnet of the egress NIC on the gateway node (usually the NIC of the default route), or else the egress access may not work. spec.nodeSelector : the node affinity method provided by EgressGateway, when selector.matchLabels matches with a node, the node will be used as the egress gateway for the cluster, when selector.matchLabels does not match with a node, the When selector.matchLabels does not match with a node, the EgressGateway skips that node and it will not be used as an egress gateway for the cluster, which supports selecting multiple nodes for high availability. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF Label the node with the Label specified in nodeSelector.selector.matchLabels above so that the node can be selected by the EgressGateway to act as an egress gateway. ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" When the creation is complete, check the EgressGateway status. spec.ippools.ipv4DefaultEIP represents the default VIP of the EgressGateway for the group, which is an IP address that will be randomly selected from spec.ippools.ipv4 , and its function is: when creating an EgressPolicy object for an application, if no VIP address is specified, the is used if no VIP address is specified when creating an EgressPolicy object for the application. status.nodeList represents the status of the nodes identified as matching the spec.nodeSelector and the corresponding EgressTunnel object for that node. ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready Create Applications and Egress Policies Create an application that will be used to test Pod access for external cluster purposes and label it to be associated with the EgressPolicy, as shown in the following example Yaml. v1.multus-cni.io/default-network : used to specify the subnet used by the application, the Multus CR corresponding to this value needs to be created in advance by referring to the installation document to create it in advance. ipam.spidernet.io/ippool : Specify which SpiderIPPool resources are used by the Pod, the corresponding SpiderIPPool CR should be created in advance by referring to the Installation . cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"], } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF The EgressPolicy instance is used to define which Pods' egress traffic is to be forwarded through the EgressGateway node, as well as other configuration details. The following is an example of creating an EgressPolicy CR object for an application. spec.egressGatewayName is used to specify which set of EgressGateways to use. spec.appliedTo.podSelector is used to specify on which Pods within the cluster this policy takes effect. namespace is used to specify the tenant where the EgressPolicy object resides. Because EgressPolicy is tenant-level, it must be created under the same namespace as the associated application, so that when the matching Pod accesses any address outside the cluster, it can be forwarded by the EgressGateway Node. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF When creation is complete, check the status of the EgressPolicy. status.eip shows the egress IP address used by the group when applying out of the cluster. status.node shows which EgressGateway node is responsible for forwarding traffic out of the EgressPolicy. ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 After creating the EgressPolicy object, an EgressEndpointSlices object containing a collection of IP addresses of all the applications will be generated according to the application selected by the EgressPolicy, so that you can check whether the IP addresses in the EgressEndpointSlices object are normal or not when the application cannot be accessed by export. ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default Test Results Deploy the application nettools outside the cluster to emulate a service outside the cluster, and nettools will return the source IP address of the requester in the http reply. ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 To verify the effect of egress traffic in a test app within the cluster: test-app, you can see that the source IP returned by nettools complies with EgressPolicy.status.eip when accessing an external service in the Pod corresponding to this app. ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.168.63:8080 # IP address of the node outside the cluster + webPort Remote IP: 10 .6.168.201","title":"Egress Policy"},{"location":"usage/egress/#egress-policy","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Egress Policy"},{"location":"usage/egress/#introduction","text":"Spiderpool is an Underlay networking solution for Kubernetes, but the egress IP address is not fixed in a Kubernetes cluster when a Pod accesses an external service. In an Overlay network, the egress IP address is the address of the node on which the Pod resides, whereas in an Underlay network, the Pod communicates directly with the outside world using its own IP address. Therefore, when a Pod undergoes new scheduling, the IP address of the Pod when communicating with the outside world will change regardless of the network mode. This instability creates IP address management challenges for system maintainers. Especially when the cluster scale increases and network troubleshooting is required, it is difficult to control the egress traffic based on the Pod's original egress IP outside the cluster. Spiderpool can be used with the component EgressGateway to solve the problem of Pod egress traffic management in Underlay network.","title":"Introduction"},{"location":"usage/egress/#features-of-egressgateway","text":"EgressGateway is an open source Egress gateway designed to solve the problem of exporting Egress IP addresses in different CNI network modes (Spiderpool, Calico, Flannel, Weave). By flexibly configuring and managing egress policies, Egress IP is set for tenant-level or cluster-level workloads, so that when a Pod accesses an external network, the system will uniformly use this set Egress IP as the egress address, thus providing a stable egress traffic management solution. However, all EgressGateway rules are effective on the host's network namespace. To make the EgressGateway policy effective, the traffic of Pods accessing the outside of the cluster has to go through the host's network namespace. Therefore, you can configure the subnet routes forwarded from the host via the spec.hijackCIDR field of spidercoordinators in Spiderpool, and then configure the subnet routes forwarded from the host via coordinator to forward matching traffic from the veth pair to the host. This enables egress traffic management on an underlay network by allowing access to external traffic to be matched by EgressGateway rules. Some of the features and benefits of Spiderpool with EgressGateway are as follows: Solve IPv4 IPv6 dual-stack connectivity, ensuring seamless communication across different protocol stacks. Solve the high availability of Egress Nodes, ensuring network connectivity remains unaffected by single-point failures. Support finer-grained policy control, allowing flexible filtering of Pods' Egress policies, including Destination CIDR. Support application-level control, allowing EgressGateway to filter Egress applications (Pods) for precise management of specific application outbound traffic. Support multiple egress gateways instance, capable of handling communication between multiple network partitions or clusters. Support namespaced egress IP. Support automatic detection of cluster traffic for egress gateways policies. Support namespace default egress instances. Can be used in low kernel version, making EgressGateway suitable for various Kubernetes deployment environments.","title":"Features of EgressGateway"},{"location":"usage/egress/#prerequisites","text":"A ready-to-use Kubernetes. Helm has been already installed.","title":"Prerequisites"},{"location":"usage/egress/#steps","text":"","title":"Steps"},{"location":"usage/egress/#install-spiderpool","text":"Refer to Installation to install Spiderpool and create SpiderMultusConfig CR and IPPool CR. After installing Spiderpool, Add the service addresses outside the cluster to the 'hijackCIDR' field in the 'default' object of spiderpool.spidercoordinators. This ensures that when Pods access these external services, the traffic is routed through the host where the Pod is located, allowing the EgressGateway rules to match. # For running Pods, you need to restart them for these routing rules to take effect within the Pods. ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}'","title":"Install Spiderpool"},{"location":"usage/egress/#install-egressgateway","text":"Install EgressGateway via helm: helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug If IPv6 is required, enable it with the option -set feature.enableIPv6=true and set feature.tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature. tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature.tunnelIpv6Subnet , you need to make sure that the segments don't conflict with any other addresses in the cluster. feature.enableGatewayReplyRoute is true to enable return routing rules on gateway nodes, which must be enabled when pairing with Spiderpool to support underlay CNI. If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for EgressGateway. Verify your EgressGateway installation: ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m For more installation details, refer to EgressGateway Installation .","title":"Install EgressGateway"},{"location":"usage/egress/#create-an-instance-of-egressgateway","text":"An EgressGateway defines a set of nodes that act as an egress gateway for the cluster, through which egress traffic within the cluster will be forwarded out of the cluster. Therefore, an EgressGateway instance needs to be pre-defined. The following example Yaml creates an EgressGateway instance. spec.ippools.ipv4 : defines a set of egress IP addresses, which need to be adjusted according to the actual situation of the specific environment. The CIDR of spec.ippools.ipv4 should be the same as the subnet of the egress NIC on the gateway node (usually the NIC of the default route), or else the egress access may not work. spec.nodeSelector : the node affinity method provided by EgressGateway, when selector.matchLabels matches with a node, the node will be used as the egress gateway for the cluster, when selector.matchLabels does not match with a node, the When selector.matchLabels does not match with a node, the EgressGateway skips that node and it will not be used as an egress gateway for the cluster, which supports selecting multiple nodes for high availability. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF Label the node with the Label specified in nodeSelector.selector.matchLabels above so that the node can be selected by the EgressGateway to act as an egress gateway. ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" When the creation is complete, check the EgressGateway status. spec.ippools.ipv4DefaultEIP represents the default VIP of the EgressGateway for the group, which is an IP address that will be randomly selected from spec.ippools.ipv4 , and its function is: when creating an EgressPolicy object for an application, if no VIP address is specified, the is used if no VIP address is specified when creating an EgressPolicy object for the application. status.nodeList represents the status of the nodes identified as matching the spec.nodeSelector and the corresponding EgressTunnel object for that node. ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready","title":"Create an instance of EgressGateway"},{"location":"usage/egress/#create-applications-and-egress-policies","text":"Create an application that will be used to test Pod access for external cluster purposes and label it to be associated with the EgressPolicy, as shown in the following example Yaml. v1.multus-cni.io/default-network : used to specify the subnet used by the application, the Multus CR corresponding to this value needs to be created in advance by referring to the installation document to create it in advance. ipam.spidernet.io/ippool : Specify which SpiderIPPool resources are used by the Pod, the corresponding SpiderIPPool CR should be created in advance by referring to the Installation . cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"], } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF The EgressPolicy instance is used to define which Pods' egress traffic is to be forwarded through the EgressGateway node, as well as other configuration details. The following is an example of creating an EgressPolicy CR object for an application. spec.egressGatewayName is used to specify which set of EgressGateways to use. spec.appliedTo.podSelector is used to specify on which Pods within the cluster this policy takes effect. namespace is used to specify the tenant where the EgressPolicy object resides. Because EgressPolicy is tenant-level, it must be created under the same namespace as the associated application, so that when the matching Pod accesses any address outside the cluster, it can be forwarded by the EgressGateway Node. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF When creation is complete, check the status of the EgressPolicy. status.eip shows the egress IP address used by the group when applying out of the cluster. status.node shows which EgressGateway node is responsible for forwarding traffic out of the EgressPolicy. ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 After creating the EgressPolicy object, an EgressEndpointSlices object containing a collection of IP addresses of all the applications will be generated according to the application selected by the EgressPolicy, so that you can check whether the IP addresses in the EgressEndpointSlices object are normal or not when the application cannot be accessed by export. ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default","title":"Create Applications and Egress Policies"},{"location":"usage/egress/#test-results","text":"Deploy the application nettools outside the cluster to emulate a service outside the cluster, and nettools will return the source IP address of the requester in the http reply. ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 To verify the effect of egress traffic in a test app within the cluster: test-app, you can see that the source IP returned by nettools complies with EgressPolicy.status.eip when accessing an external service in the Pod corresponding to this app. ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.168.63:8080 # IP address of the node outside the cluster + webPort Remote IP: 10 .6.168.201","title":"Test Results"},{"location":"usage/faq-zh_CN/","text":"FAQ \u7ecf\u5e38\u88ab\u95ee\u5230\u7684\u95ee\u9898 English | \u7b80\u4f53\u4e2d\u6587 \u6574\u4f53 \u4ec0\u4e48\u662f Spiderpool \uff1f Spiderpool \u9879\u76ee\u7531\u591a\u4e2a\u5b50\u63d2\u4ef6\u9879\u76ee\u7ec4\u6210\uff0c\u5305\u62ec\u6709\uff1a spiderpool , coordinator , ifacer . \u8fd9\u91cc\u7684 spiderpool \u63d2\u4ef6\u662f\u4e00\u6b3e\u670d\u52a1\u4e8e main CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u53ef\u4e3a\u60a8\u7684\u96c6\u7fa4\u7ba1\u7406 IP\u3002 coordinator \u63d2\u4ef6\u80fd\u4e3a\u4f60\u534f\u540c\u8def\u7531\u3002 ifacer \u63d2\u4ef6\u53ef\u4e3a\u4f60\u521b\u5efa vlan \u5b50\u63a5\u53e3\u4ee5\u53ca\u521b\u5efa bond \u7f51\u5361\u3002\u5176\u4e2d\uff0c coordinator \u548c ifacer \u63d2\u4ef6\u4ee5 CNI \u534f\u8bae\u4e2d\u7684\u94fe\u5f0f\u8c03\u7528\u65b9\u5f0f\u6765\u4f7f\u7528\uff0c\u4e14\u53ef\u9009\u5e76\u975e\u5f3a\u5236\u4f7f\u7528\u3002 \u914d\u7f6e \u4e3a\u4ec0\u4e48\u66f4\u6539 configmap \u7684\u914d\u7f6e\u540e\u5374\u65e0\u6cd5\u751f\u6548\uff1f \u4fee\u6539 configmap \u8d44\u6e90 spiderpool-conf \u914d\u7f6e\u540e\uff0c\u9700\u8981\u91cd\u542f spiderpool-agent \u548c spiderpool-controller \u7ec4\u4ef6\u3002 \u4f7f\u7528 SpiderSubnet\u529f\u80fd\u4f7f\u7528\u4e0d\u6b63\u5e38 \u5982\u679c\u9047\u5230\u62a5\u9519 Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource \uff0c\u8bf7\u68c0\u67e5 configmap spiderpool-conf \u786e\u4fdd SpiderSubnet \u529f\u80fd\u5df2\u542f\u52a8\u3002 \u82e5\u9047\u5230\u62a5\u9519 failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables \uff0c\u8bf7\u68c0\u67e5 spiderpool-controller \u7684\u65e5\u5fd7\u3002\u76ee\u524d Spiderpool \u7684 controller \u7ec4\u4ef6\u8981\u6c42\u4f7f\u7528 SpiderSubnet \u529f\u80fd\u7684\u96c6\u7fa4\u6700\u4f4e\u7248\u672c\u4e3a v1.21 , \u5982\u9047\u5230\u4ee5\u4e0b\u65e5\u5fd7\u62a5\u9519\u5373\u8868\u660e\u5f53\u524d\u96c6\u7fa4\u7248\u672c\u8fc7\u4f4e: W1220 05:44:16.129916 1 reflector.go:535] k8s.io/client-go/informers/factory.go:150: failed to list *v1.CronJob: the server could not find the requested resource E1220 05:44:16.129978 1 reflector.go:147] k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CronJob: failed to list *v1.CronJob: the server could not find the requested resource Spiderpool IPAM \u662f\u5426\u4f9d\u8d56 spiderpool-controller \u7ec4\u4ef6\uff1f spiderpool-controller \u7ec4\u4ef6\u9488\u5bf9 SpiderSubnet\u3001 SpiderIPPool \u7b49\u8d44\u6e90\u7684 Spec \u5b57\u6bb5\u5b9e\u73b0\u4e86 Webhook \u529f\u80fd\u3002\u800c spiderpool-agent \u7ec4\u4ef6\u662f IPAM \u529f\u80fd\u5b9e\u73b0\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u5728\u5206\u914d IP \u7684\u65f6\u5019\u4f1a\u5bf9 SpiderIPPool \u8d44\u6e90\u7684 Status \u5b57\u6bb5\u8fdb\u884c\u4fee\u6539\uff0c\u8be5\u5b57\u6bb5\u5c5e\u4e8e subresource \uff0c\u4e0d\u4f1a\u88ab spiderpool-controller \u6240\u6ce8\u518c\u7684 Webhook \u62e6\u622a\u5230\uff0c\u6240\u4ee5 IPAM \u4e0d\u4f1a\u4f9d\u8d56 spiderpool-controller \u7ec4\u4ef6\u3002","title":"FAQ"},{"location":"usage/faq-zh_CN/#faq","text":"\u7ecf\u5e38\u88ab\u95ee\u5230\u7684\u95ee\u9898 English | \u7b80\u4f53\u4e2d\u6587","title":"FAQ"},{"location":"usage/faq-zh_CN/#_1","text":"","title":"\u6574\u4f53"},{"location":"usage/faq-zh_CN/#spiderpool","text":"Spiderpool \u9879\u76ee\u7531\u591a\u4e2a\u5b50\u63d2\u4ef6\u9879\u76ee\u7ec4\u6210\uff0c\u5305\u62ec\u6709\uff1a spiderpool , coordinator , ifacer . \u8fd9\u91cc\u7684 spiderpool \u63d2\u4ef6\u662f\u4e00\u6b3e\u670d\u52a1\u4e8e main CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u53ef\u4e3a\u60a8\u7684\u96c6\u7fa4\u7ba1\u7406 IP\u3002 coordinator \u63d2\u4ef6\u80fd\u4e3a\u4f60\u534f\u540c\u8def\u7531\u3002 ifacer \u63d2\u4ef6\u53ef\u4e3a\u4f60\u521b\u5efa vlan \u5b50\u63a5\u53e3\u4ee5\u53ca\u521b\u5efa bond \u7f51\u5361\u3002\u5176\u4e2d\uff0c coordinator \u548c ifacer \u63d2\u4ef6\u4ee5 CNI \u534f\u8bae\u4e2d\u7684\u94fe\u5f0f\u8c03\u7528\u65b9\u5f0f\u6765\u4f7f\u7528\uff0c\u4e14\u53ef\u9009\u5e76\u975e\u5f3a\u5236\u4f7f\u7528\u3002","title":"\u4ec0\u4e48\u662f Spiderpool \uff1f"},{"location":"usage/faq-zh_CN/#_2","text":"","title":"\u914d\u7f6e"},{"location":"usage/faq-zh_CN/#configmap","text":"\u4fee\u6539 configmap \u8d44\u6e90 spiderpool-conf \u914d\u7f6e\u540e\uff0c\u9700\u8981\u91cd\u542f spiderpool-agent \u548c spiderpool-controller \u7ec4\u4ef6\u3002","title":"\u4e3a\u4ec0\u4e48\u66f4\u6539 configmap \u7684\u914d\u7f6e\u540e\u5374\u65e0\u6cd5\u751f\u6548\uff1f"},{"location":"usage/faq-zh_CN/#_3","text":"","title":"\u4f7f\u7528"},{"location":"usage/faq-zh_CN/#spidersubnet","text":"\u5982\u679c\u9047\u5230\u62a5\u9519 Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource \uff0c\u8bf7\u68c0\u67e5 configmap spiderpool-conf \u786e\u4fdd SpiderSubnet \u529f\u80fd\u5df2\u542f\u52a8\u3002 \u82e5\u9047\u5230\u62a5\u9519 failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables \uff0c\u8bf7\u68c0\u67e5 spiderpool-controller \u7684\u65e5\u5fd7\u3002\u76ee\u524d Spiderpool \u7684 controller \u7ec4\u4ef6\u8981\u6c42\u4f7f\u7528 SpiderSubnet \u529f\u80fd\u7684\u96c6\u7fa4\u6700\u4f4e\u7248\u672c\u4e3a v1.21 , \u5982\u9047\u5230\u4ee5\u4e0b\u65e5\u5fd7\u62a5\u9519\u5373\u8868\u660e\u5f53\u524d\u96c6\u7fa4\u7248\u672c\u8fc7\u4f4e: W1220 05:44:16.129916 1 reflector.go:535] k8s.io/client-go/informers/factory.go:150: failed to list *v1.CronJob: the server could not find the requested resource E1220 05:44:16.129978 1 reflector.go:147] k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CronJob: failed to list *v1.CronJob: the server could not find the requested resource","title":"SpiderSubnet\u529f\u80fd\u4f7f\u7528\u4e0d\u6b63\u5e38"},{"location":"usage/faq-zh_CN/#spiderpool-ipam-spiderpool-controller","text":"spiderpool-controller \u7ec4\u4ef6\u9488\u5bf9 SpiderSubnet\u3001 SpiderIPPool \u7b49\u8d44\u6e90\u7684 Spec \u5b57\u6bb5\u5b9e\u73b0\u4e86 Webhook \u529f\u80fd\u3002\u800c spiderpool-agent \u7ec4\u4ef6\u662f IPAM \u529f\u80fd\u5b9e\u73b0\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u5728\u5206\u914d IP \u7684\u65f6\u5019\u4f1a\u5bf9 SpiderIPPool \u8d44\u6e90\u7684 Status \u5b57\u6bb5\u8fdb\u884c\u4fee\u6539\uff0c\u8be5\u5b57\u6bb5\u5c5e\u4e8e subresource \uff0c\u4e0d\u4f1a\u88ab spiderpool-controller \u6240\u6ce8\u518c\u7684 Webhook \u62e6\u622a\u5230\uff0c\u6240\u4ee5 IPAM \u4e0d\u4f1a\u4f9d\u8d56 spiderpool-controller \u7ec4\u4ef6\u3002","title":"Spiderpool IPAM \u662f\u5426\u4f9d\u8d56 spiderpool-controller \u7ec4\u4ef6\uff1f"},{"location":"usage/faq/","text":"FAQ Frequently asked questions English | \u7b80\u4f53\u4e2d\u6587 General What is Spiderpool? Spiderpool project is consist of several plugins include: spiderpool , coordinator , ifacer . The spiderpool basically is a IPAM plugin works for CNI main plugin to manage IP addresses for the container. The coordinator is a plugin that coordinate the routes. The ifacer plugin help you to create vlan sub-interface or create bond interfaces. The coordinator and ifacer plugin are used in CNI plugin chaining and they also optional to use. Configuration Why doesn't changing configmap configuration update the behavior of Spiderpool? If you change the configmap spiderpool-conf configurations, you need to restart spiderpool-agent and spiderpool-controller components Operation Why SpiderSubnet feature not works well? For error like Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource , you need to update configmap spiderpool-conf to enable SpiderSubnet feature and restart spiderpool-agent and spiderpool-controller components. For error like failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables , you should check spiderpool-controller logs. The spiderpool-controller component requires that the kubernetes cluster has kubernetes version not lower than v1.21 once using the SpiderSubnet feature. The following error logs means your kubernetes cluster version is too low: W1220 05:44:16.129916 1 reflector.go:535] k8s.io/client-go/informers/factory.go:150: failed to list *v1.CronJob: the server could not find the requested resource E1220 05:44:16.129978 1 reflector.go:147] k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CronJob: failed to list *v1.CronJob: the server could not find the requested resource Does Spiderpool IPAM relies on spiderpool-controller component? spiderpool-controller component implements the webhook for the Spec property of SpiderSubnet, SpiderIPPool resources. And the spiderpool-agent component is the core of implementing the IPAM, once allocating the IP addresses it will update the SpiderIPPool resource Status property. The property belongs to subresource , so the request would not be intercepted by the spiderpool-controller webhook. Therefore, the IPAM doesn't rely on spiderpool-controller component.","title":"FAQ"},{"location":"usage/faq/#faq","text":"Frequently asked questions English | \u7b80\u4f53\u4e2d\u6587","title":"FAQ"},{"location":"usage/faq/#general","text":"","title":"General"},{"location":"usage/faq/#what-is-spiderpool","text":"Spiderpool project is consist of several plugins include: spiderpool , coordinator , ifacer . The spiderpool basically is a IPAM plugin works for CNI main plugin to manage IP addresses for the container. The coordinator is a plugin that coordinate the routes. The ifacer plugin help you to create vlan sub-interface or create bond interfaces. The coordinator and ifacer plugin are used in CNI plugin chaining and they also optional to use.","title":"What is Spiderpool?"},{"location":"usage/faq/#configuration","text":"","title":"Configuration"},{"location":"usage/faq/#why-doesnt-changing-configmap-configuration-update-the-behavior-of-spiderpool","text":"If you change the configmap spiderpool-conf configurations, you need to restart spiderpool-agent and spiderpool-controller components","title":"Why doesn't changing configmap configuration update the behavior of Spiderpool?"},{"location":"usage/faq/#operation","text":"","title":"Operation"},{"location":"usage/faq/#why-spidersubnet-feature-not-works-well","text":"For error like Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource , you need to update configmap spiderpool-conf to enable SpiderSubnet feature and restart spiderpool-agent and spiderpool-controller components. For error like failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables , you should check spiderpool-controller logs. The spiderpool-controller component requires that the kubernetes cluster has kubernetes version not lower than v1.21 once using the SpiderSubnet feature. The following error logs means your kubernetes cluster version is too low: W1220 05:44:16.129916 1 reflector.go:535] k8s.io/client-go/informers/factory.go:150: failed to list *v1.CronJob: the server could not find the requested resource E1220 05:44:16.129978 1 reflector.go:147] k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CronJob: failed to list *v1.CronJob: the server could not find the requested resource","title":"Why SpiderSubnet feature not works well?"},{"location":"usage/faq/#does-spiderpool-ipam-relies-on-spiderpool-controller-component","text":"spiderpool-controller component implements the webhook for the Spec property of SpiderSubnet, SpiderIPPool resources. And the spiderpool-agent component is the core of implementing the IPAM, once allocating the IP addresses it will update the SpiderIPPool resource Status property. The property belongs to subresource , so the request would not be intercepted by the spiderpool-controller webhook. Therefore, the IPAM doesn't rely on spiderpool-controller component.","title":"Does Spiderpool IPAM relies on spiderpool-controller component?"},{"location":"usage/ipoib-zh_CN/","text":"IPoIB For Infiniband \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u672c\u8282\u4ecb\u7ecd\u57fa\u4e8e\u4e3b\u673a\u4e0a\u7684 Infiniband \u7f51\u5361\uff0c\u5982\u4f55\u7ed9 POD \u5206\u914d IPoIB \u7f51\u5361\uff0c\u4f7f\u5f97\u4f20\u7edf\u7684 TCP/IP \u5e94\u7528\u80fd\u591f\u5728 Infiniband \u7f51\u7edc\u4e2d\u5de5\u4f5c\u3002 Spiderpool \u57fa\u4e8e IPoIB CNI \u7ed9 POD \u63d0\u4f9b IPoIB \u7684\u7f51\u5361\uff0c\u5b83\u5e76\u4e0d\u63d0\u4f9b RDMA \u7f51\u5361\u901a\u4fe1\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9700\u8981 TCP/IP \u901a\u4fe1\u7684\u5e38\u89c4\u5e94\u7528\u3002 \u57fa\u4e8e IPoIB \u7684\u5e38\u89c4\u7f51\u5361 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e IPoIB \u4f7f\u5f97 Pod \u63a5\u5165\u5e38\u89c4\u7684 TCP/IP \u7f51\u5361\uff0c\u4f7f\u5f97\u5e94\u7528\u80fd\u591f\u5728 Infiniband \u7f51\u7edc\u4e2d\u8fdb\u884c TCP/IP \u901a\u4fe1\uff0c\u4f46\u662f\u5e94\u7528\u4e0d\u80fd\u8fdb\u884c RDMA \u901a\u4fe1 \u5728\u5bbf\u4e3b\u673a\u4e0a\uff0c\u786e\u4fdd\u4e3b\u673a\u62e5\u6709 Infiniband \u7f51\u5361\uff0c\u4e14\u5b89\u88c5\u597d\u9a71\u52a8\u3002 \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u63a5\u5165\u4e86 mellanox ConnectX 5 VPI \u7f51\u5361\uff0c\u53ef\u6309\u7167 NVIDIA \u5b98\u65b9\u6307\u5bfc \u5b89\u88c5\u6700\u65b0\u7684 OFED \u9a71\u52a8\u3002 \u5bf9\u4e8e mellanox \u7684 VPI \u7cfb\u5217\u7f51\u5361\uff0c\u53ef\u53c2\u8003\u5b98\u65b9\u7684 \u5207\u6362 Infiniband \u6a21\u5f0f \uff0c\u786e\u4fdd\u7f51\u5361\u5de5\u4f5c\u5728 Infiniband \u6a21\u5f0f\u4e0b\u3002 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u67e5\u8be2\u4e3b\u673a\u4e0a\u662f\u5426\u5177\u5907 Infiniband \u7f51\u5361\u8bbe\u5907 \uff1a ~# lspci -nn | grep Infiniband 86:00.0 Infiniband controller [0207]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] ~# rdma link link mlx5_0/1 subnet_prefix fe80:0000:0000:0000 lid 2 sm_lid 2 lmc 0 state ACTIVE physical_state LINK_UP ~# ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand \u67e5\u770b Infiniband \u7f51\u5361\u7684 IPoIB \u63a5\u53e3 ~# ip a show ibs5f0 9: ibs5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 2044 qdisc mq state UP group default qlen 256 link/infiniband 00:00:10:49:fe:80:00:00:00:00:00:00:e8:eb:d3:03:00:93:ae:10 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff altname ibp134s0f0 inet 172.91.0.10/16 brd 172.91.255.255 scope global ibs5f0 valid_lft forever preferred_lft forever inet6 fd00:91::172:91:0:10/64 scope global valid_lft forever preferred_lft forever inet6 fe80::eaeb:d303:93:ae10/64 scope link valid_lft forever preferred_lft forever \u5b89\u88c5\u597d Spiderpool \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m \u521b\u5efa ipoib \u7684 CNI \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90\u3002\u5176\u4e2d SpiderMultusConfig \u7684 spec.ipoib.master \u6307\u5411\u4e3b\u673a\u4e0a\u7684 Infiniband \u7f51\u5361 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-91 spec: gateway: 172.91.0.1 ips: - 172.91.0.100-172.91.0.120 subnet: 172.91.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipoib namespace: kube-system spec: cniType: ipoib ipoib: master: \"ibs5f0\" ippools: ipv4: [\"v4-91\"] EOF \u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u7684\u914d\u7f6e\uff0c\u6765\u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\u8fdb\u884c\u6d4b\u8bd5 ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/ipoib\" NAME=ipoib cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4\u5e94\u7528\u4e4b\u95f4\u80fd\u6b63\u5e38 TCP/IP \u901a\u4fe1 ~# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ipoib-psf4q 1/1 Running 0 34s 172.91.0.112 10-20-1-20 <none> <none> ipoib-t9hm7 1/1 Running 0 34s 172.91.0.116 10-20-1-10 <none> <none> \u4ece\u4e00\u4e2a POD \u4e2d\u6210\u529f\u8bbf\u95ee\u53e6\u4e00\u4e2a POD ~# kubectl exec -it ipoib-psf4q bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ipoib-psf4q:/# ping 172.91.0.116 PING 172.91.0.116 (172.91.0.116) 56(84) bytes of data. 64 bytes from 172.91.0.116: icmp_seq=1 ttl=64 time=1.10 ms 64 bytes from 172.91.0.116: icmp_seq=2 ttl=64 time=0.235 ms","title":"IPoIB For Infiniband"},{"location":"usage/ipoib-zh_CN/#ipoib-for-infiniband","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"IPoIB For Infiniband"},{"location":"usage/ipoib-zh_CN/#_1","text":"\u672c\u8282\u4ecb\u7ecd\u57fa\u4e8e\u4e3b\u673a\u4e0a\u7684 Infiniband \u7f51\u5361\uff0c\u5982\u4f55\u7ed9 POD \u5206\u914d IPoIB \u7f51\u5361\uff0c\u4f7f\u5f97\u4f20\u7edf\u7684 TCP/IP \u5e94\u7528\u80fd\u591f\u5728 Infiniband \u7f51\u7edc\u4e2d\u5de5\u4f5c\u3002 Spiderpool \u57fa\u4e8e IPoIB CNI \u7ed9 POD \u63d0\u4f9b IPoIB \u7684\u7f51\u5361\uff0c\u5b83\u5e76\u4e0d\u63d0\u4f9b RDMA \u7f51\u5361\u901a\u4fe1\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9700\u8981 TCP/IP \u901a\u4fe1\u7684\u5e38\u89c4\u5e94\u7528\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/ipoib-zh_CN/#ipoib","text":"\u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e IPoIB \u4f7f\u5f97 Pod \u63a5\u5165\u5e38\u89c4\u7684 TCP/IP \u7f51\u5361\uff0c\u4f7f\u5f97\u5e94\u7528\u80fd\u591f\u5728 Infiniband \u7f51\u7edc\u4e2d\u8fdb\u884c TCP/IP \u901a\u4fe1\uff0c\u4f46\u662f\u5e94\u7528\u4e0d\u80fd\u8fdb\u884c RDMA \u901a\u4fe1 \u5728\u5bbf\u4e3b\u673a\u4e0a\uff0c\u786e\u4fdd\u4e3b\u673a\u62e5\u6709 Infiniband \u7f51\u5361\uff0c\u4e14\u5b89\u88c5\u597d\u9a71\u52a8\u3002 \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u63a5\u5165\u4e86 mellanox ConnectX 5 VPI \u7f51\u5361\uff0c\u53ef\u6309\u7167 NVIDIA \u5b98\u65b9\u6307\u5bfc \u5b89\u88c5\u6700\u65b0\u7684 OFED \u9a71\u52a8\u3002 \u5bf9\u4e8e mellanox \u7684 VPI \u7cfb\u5217\u7f51\u5361\uff0c\u53ef\u53c2\u8003\u5b98\u65b9\u7684 \u5207\u6362 Infiniband \u6a21\u5f0f \uff0c\u786e\u4fdd\u7f51\u5361\u5de5\u4f5c\u5728 Infiniband \u6a21\u5f0f\u4e0b\u3002 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u67e5\u8be2\u4e3b\u673a\u4e0a\u662f\u5426\u5177\u5907 Infiniband \u7f51\u5361\u8bbe\u5907 \uff1a ~# lspci -nn | grep Infiniband 86:00.0 Infiniband controller [0207]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] ~# rdma link link mlx5_0/1 subnet_prefix fe80:0000:0000:0000 lid 2 sm_lid 2 lmc 0 state ACTIVE physical_state LINK_UP ~# ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand \u67e5\u770b Infiniband \u7f51\u5361\u7684 IPoIB \u63a5\u53e3 ~# ip a show ibs5f0 9: ibs5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 2044 qdisc mq state UP group default qlen 256 link/infiniband 00:00:10:49:fe:80:00:00:00:00:00:00:e8:eb:d3:03:00:93:ae:10 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff altname ibp134s0f0 inet 172.91.0.10/16 brd 172.91.255.255 scope global ibs5f0 valid_lft forever preferred_lft forever inet6 fd00:91::172:91:0:10/64 scope global valid_lft forever preferred_lft forever inet6 fe80::eaeb:d303:93:ae10/64 scope link valid_lft forever preferred_lft forever \u5b89\u88c5\u597d Spiderpool \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m \u521b\u5efa ipoib \u7684 CNI \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90\u3002\u5176\u4e2d SpiderMultusConfig \u7684 spec.ipoib.master \u6307\u5411\u4e3b\u673a\u4e0a\u7684 Infiniband \u7f51\u5361 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-91 spec: gateway: 172.91.0.1 ips: - 172.91.0.100-172.91.0.120 subnet: 172.91.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipoib namespace: kube-system spec: cniType: ipoib ipoib: master: \"ibs5f0\" ippools: ipv4: [\"v4-91\"] EOF \u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u7684\u914d\u7f6e\uff0c\u6765\u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\u8fdb\u884c\u6d4b\u8bd5 ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/ipoib\" NAME=ipoib cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4\u5e94\u7528\u4e4b\u95f4\u80fd\u6b63\u5e38 TCP/IP \u901a\u4fe1 ~# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ipoib-psf4q 1/1 Running 0 34s 172.91.0.112 10-20-1-20 <none> <none> ipoib-t9hm7 1/1 Running 0 34s 172.91.0.116 10-20-1-10 <none> <none> \u4ece\u4e00\u4e2a POD \u4e2d\u6210\u529f\u8bbf\u95ee\u53e6\u4e00\u4e2a POD ~# kubectl exec -it ipoib-psf4q bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ipoib-psf4q:/# ping 172.91.0.116 PING 172.91.0.116 (172.91.0.116) 56(84) bytes of data. 64 bytes from 172.91.0.116: icmp_seq=1 ttl=64 time=1.10 ms 64 bytes from 172.91.0.116: icmp_seq=2 ttl=64 time=0.235 ms","title":"\u57fa\u4e8e IPoIB \u7684\u5e38\u89c4\u7f51\u5361"},{"location":"usage/ipoib/","text":"IPoIB For Infiniband English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction This chapter introduces how POD access network with the IPoIP interface. IPoIB CNI provides an IPoIB network card for POD, without RDMA device. It is suitable for conventional applications that require TCP/IP communication, as it does not require an SRIOV network card, allowing more PODs to run on the host IPoIB The following steps demonstrate how to use IPoIB on a cluster with 2 nodes, it enables Pod to own a regular TCP/IP network cards without RDMA device. Ensure that the host machine has an Infiniband card installed and the driver is properly installed. In our demo environment, the host machine is equipped with a Mellanox ConnectX-5 VPI NIC. Follow the official NVIDIA guide to install the latest OFED driver. For Mellanox's VPI series network cards, you can refer to the official Switching Infiniband Mode to ensure that the network card is working in Infiniband mode. To confirm the presence of Inifiniband devices, use the following command: ~# lspci -nn | grep Infiniband 86:00.0 Infiniband controller [0207]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] ~# rdma link link mlx5_0/1 subnet_prefix fe80:0000:0000:0000 lid 2 sm_lid 2 lmc 0 state ACTIVE physical_state LINK_UP ~# ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand Check the ipoib interface of the Inifiniband device ~# ip a show ibs5f0 9: ibs5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 2044 qdisc mq state UP group default qlen 256 link/infiniband 00:00:10:49:fe:80:00:00:00:00:00:00:e8:eb:d3:03:00:93:ae:10 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff altname ibp134s0f0 inet 172.91.0.10/16 brd 172.91.255.255 scope global ibs5f0 valid_lft forever preferred_lft forever inet6 fd00:91::172:91:0:10/64 scope global valid_lft forever preferred_lft forever inet6 fe80::eaeb:d303:93:ae10/64 scope link valid_lft forever preferred_lft forever Install Spiderpool If you are a user from China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to pull image from China registry. Once the installation is complete, the following components will be installed: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m Create the CNI configuration of ipoib, and the ippool. The spec.ipoib.master of SpiderMultusConfig should be set to the infiniband interface of the node. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-91 spec: gateway: 172.91.0.1 ips: - 172.91.0.100-172.91.0.120 subnet: 172.91.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipoib namespace: kube-system spec: cniType: ipoib ipoib: master: \"ibs5f0\" ippools: ipv4: [\"v4-91\"] EOF Following the configurations from the previous step, create a DaemonSet application that spans across nodes for testing ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/ipoib\" NAME=ipoib cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF Verify that the network communication is correct between the PODs across nodes. ~# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ipoib-psf4q 1/1 Running 0 34s 172.91.0.112 10-20-1-20 <none> <none> ipoib-t9hm7 1/1 Running 0 34s 172.91.0.116 10-20-1-10 <none> <none> Succeed to access each other ~# kubectl exec -it ipoib-psf4q bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ipoib-psf4q:/# ping 172.91.0.116 PING 172.91.0.116 (172.91.0.116) 56(84) bytes of data. 64 bytes from 172.91.0.116: icmp_seq=1 ttl=64 time=1.10 ms 64 bytes from 172.91.0.116: icmp_seq=2 ttl=64 time=0.235 ms","title":"IPoIB For Infiniband"},{"location":"usage/ipoib/#ipoib-for-infiniband","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"IPoIB For Infiniband"},{"location":"usage/ipoib/#introduction","text":"This chapter introduces how POD access network with the IPoIP interface. IPoIB CNI provides an IPoIB network card for POD, without RDMA device. It is suitable for conventional applications that require TCP/IP communication, as it does not require an SRIOV network card, allowing more PODs to run on the host","title":"Introduction"},{"location":"usage/ipoib/#ipoib","text":"The following steps demonstrate how to use IPoIB on a cluster with 2 nodes, it enables Pod to own a regular TCP/IP network cards without RDMA device. Ensure that the host machine has an Infiniband card installed and the driver is properly installed. In our demo environment, the host machine is equipped with a Mellanox ConnectX-5 VPI NIC. Follow the official NVIDIA guide to install the latest OFED driver. For Mellanox's VPI series network cards, you can refer to the official Switching Infiniband Mode to ensure that the network card is working in Infiniband mode. To confirm the presence of Inifiniband devices, use the following command: ~# lspci -nn | grep Infiniband 86:00.0 Infiniband controller [0207]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] ~# rdma link link mlx5_0/1 subnet_prefix fe80:0000:0000:0000 lid 2 sm_lid 2 lmc 0 state ACTIVE physical_state LINK_UP ~# ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand Check the ipoib interface of the Inifiniband device ~# ip a show ibs5f0 9: ibs5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 2044 qdisc mq state UP group default qlen 256 link/infiniband 00:00:10:49:fe:80:00:00:00:00:00:00:e8:eb:d3:03:00:93:ae:10 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff altname ibp134s0f0 inet 172.91.0.10/16 brd 172.91.255.255 scope global ibs5f0 valid_lft forever preferred_lft forever inet6 fd00:91::172:91:0:10/64 scope global valid_lft forever preferred_lft forever inet6 fe80::eaeb:d303:93:ae10/64 scope link valid_lft forever preferred_lft forever Install Spiderpool If you are a user from China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to pull image from China registry. Once the installation is complete, the following components will be installed: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m Create the CNI configuration of ipoib, and the ippool. The spec.ipoib.master of SpiderMultusConfig should be set to the infiniband interface of the node. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-91 spec: gateway: 172.91.0.1 ips: - 172.91.0.100-172.91.0.120 subnet: 172.91.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipoib namespace: kube-system spec: cniType: ipoib ipoib: master: \"ibs5f0\" ippools: ipv4: [\"v4-91\"] EOF Following the configurations from the previous step, create a DaemonSet application that spans across nodes for testing ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/ipoib\" NAME=ipoib cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF Verify that the network communication is correct between the PODs across nodes. ~# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ipoib-psf4q 1/1 Running 0 34s 172.91.0.112 10-20-1-20 <none> <none> ipoib-t9hm7 1/1 Running 0 34s 172.91.0.116 10-20-1-10 <none> <none> Succeed to access each other ~# kubectl exec -it ipoib-psf4q bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ipoib-psf4q:/# ping 172.91.0.116 PING 172.91.0.116 (172.91.0.116) 56(84) bytes of data. 64 bytes from 172.91.0.116: icmp_seq=1 ttl=64 time=1.10 ms 64 bytes from 172.91.0.116: icmp_seq=2 ttl=64 time=0.235 ms","title":"IPoIB"},{"location":"usage/ipvlan_bandwidth-zh_CN/","text":"IPVlan \u5e26\u5bbd\u7ba1\u7406 \u7b80\u4f53\u4e2d\u6587 \uff5c English \u672c\u6587\u5c06\u5c55\u793a\u5982\u4f55\u501f\u52a9 cilium-chaining \u8fd9\u4e2a\u9879\u76ee\uff0c\u5b9e\u73b0 IPVlan CNI \u7684\u5e26\u5bbd\u7ba1\u7406\u80fd\u529b\u3002 \u80cc\u666f Kubernetes \u5b98\u65b9\u652f\u6301\u5411 Pod \u4e2d\u6ce8\u5165 Annotations \u7684\u65b9\u5f0f\u8bbe\u7f6e Pod \u7684\u5165\u53e3\u51fa\u53e3\u5e26\u5bbd\uff0c\u53c2\u8003 \u5e26\u5bbd\u9650\u5236 \u5f53\u6211\u4eec\u4f7f\u7528 IPVlan \u4f5c\u4e3a CNI \u65f6\uff0c\u5b83\u672c\u8eab\u4e0d\u5177\u5907\u7ba1\u7406 Pod \u7684\u5165\u53e3/\u51fa\u53e3\u6d41\u91cf\u5e26\u5bbd\u7ba1\u7406\u80fd\u529b\u3002\u5f00\u6e90\u9879\u76ee Cilium \u652f\u6301\u4ee5 cni-chaining \u7684\u65b9\u5f0f\u4e0e IPVlan \u4e00\u8d77\u8054\u52a8\u5de5\u4f5c\uff0c\u57fa\u4e8e eBPF \u6280\u672f\u53ef\u4ee5\u5e2e\u52a9 IPVlan \u5b9e\u73b0\u52a0\u901f\u8bbf\u95ee Service, \u5e26\u5bbd\u80fd\u529b\u7ba1\u7406\u7b49\u529f\u80fd\u3002\u4f46\u76ee\u524d Cilium \u5df2\u7ecf\u79fb\u9664\u652f\u6301 IPVlan \u7684 Dataplane\u3002 cilium-chaining \u9879\u76ee\u57fa\u4e8e cilium v1.12.7 \u6784\u5efa\uff0c\u5e76\u652f\u6301 IPVlan \u7684 Dataplane, \u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u5b83\u5e2e\u52a9 IPVlan \u5b9e\u73b0 Pod \u7684\u7f51\u7edc\u5e26\u5bbd\u7ba1\u7406\u80fd\u529b\u3002 \u9884\u7f6e\u6761\u4ef6 Helm \u548c Kubectl \u4e8c\u8fdb\u5236\u5de5\u5177 \u8981\u6c42\u8282\u70b9\u5185\u6838\u81f3\u5927\u4e8e 4.19 \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003\u6587\u6863: \u5b89\u88c5 Spiderpool \u5b89\u88c5 Cilium-chaining \u9879\u76ee \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 cilium-chainging \u9879\u76ee: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/cilium-chaining/main/manifests/cilium-chaining.yaml \u5b89\u88c5\u72b6\u6001\u68c0\u67e5: ~# kubectl get po -n kube-system | grep cilium-chain cilium-chaining-gl76b 1 /1 Running 0 137m cilium-chaining-nzvrg 1 /1 Running 0 137m \u521b\u5efa CNI \u914d\u7f6e\u6587\u4ef6\u548c IP \u6c60 \u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa CNI \u914d\u7f6e: # \u521b\u5efa Multus Network-attachement-definition CR IPVLAN_MASTER_INTERFACE = ens192 IPPOOL_NAME = ens192-v4 cat << EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan namespace: kube-system spec: config: ' { \"cniVersion\": \"0.4.0\", \"name\": \"terway-chainer\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\":\"spiderpool\", \"default_ipv4_ippool\": [\"${IPPOOL_NAME}\"] } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" } ] }' EOF \u914d\u7f6e cilium-cni \u4ee5 cni-chain \u7684\u6a21\u5f0f\u642d\u914d ipvlan cni \u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa CNI \u914d\u7f6e: # \u521b\u5efa IP \u6c60 cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ${IPPOOL_NAME } spec: default: false disable: false gateway: 172 .51.0.1 ipVersion: 4 ips: - 172 .51.0.100-172.51.0.108 subnet: 172 .51.0.230/16 \u6ce8\u610f ens192 \u9700\u8981\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\uff0c\u5e76\u4e14\u914d\u7f6e IP \u6c60\u7684\u7f51\u6bb5\u9700\u8981\u548c ens192 \u6240\u5728\u7269\u7406\u7f51\u7edc\u4fdd\u6301\u4e00\u81f4 \u521b\u5efa\u5e94\u7528\u6d4b\u8bd5\u5e26\u5bbd\u9650\u5236 \u4f7f\u7528\u4e0a\u9762\u521b\u5efa\u7684 CNI \u914d\u7f6e\u4ee5\u53ca IP \u6c60\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528\uff0c\u9a8c\u8bc1 Pod \u7684\u5e26\u5bbd\u662f\u5426\u53d7\u5230\u9650\u5236: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: test spec: replicas: 2 selector: matchLabels: app: test template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/ipvlan kubernetes.io/ingress-bandwidth: 100M kubernetes.io/egress-bandwidth: 100M labels: app: test spec: containers: - env: - name: NODE _NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: http protocol: TCP resources: {} \u51e0\u4e2a annotations \u4ecb\u7ecd: v1.multus-cni.io/default-network: ipvlan : \u6307\u5b9a Pod \u7684\u7f3a\u7701 CNI \u4e3a \u4e4b\u524d\u521b\u5efa\u7684 ipvlan kubernetes.io/ingress-bandwidth: 100m : \u8bbe\u7f6e Pod \u7684\u5165\u53e3\u5e26\u5bbd\u4e3a 100M kubernetes.io/egress-bandwidth: 100m : \u8bbe\u7f6e Pod \u7684\u51fa\u53e3\u5e26\u5bbd\u4e3a 100M ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-58d785fb4c-b9cld 1 /1 Running 0 175m 172 .51.0.102 10 -20-1-230 <none> <none> test-58d785fb4c-kwh4h 1 /1 Running 0 175m 172 .51.0.100 10 -20-1-220 <none> <none> \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u5206\u522b\u8fdb\u5165\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\uff0c\u4f7f\u7528 iperf3 \u5de5\u5177\u6d4b\u8bd5\u5176\u7f51\u7edc\u5e26\u5bbd: ~# crictl ps | grep test b2b60a6e14e21 8f2213208a7f5 10 seconds ago Running nginx 0 f46848c1a713a test-58d785fb4c-kwh4h ~# crictl inspect b2b60a6e14e21 | grep pid \"pid\" : 284529 , \"pid\" : 1 \"type\" : \"pid\" ~# nsenter -t 284529 -n ~# iperf3 -s \u5728\u53e6\u5916\u4e00\u4e2a\u8282\u70b9\u7684 Pod \u4f5c\u4e3a Client \u8bbf\u95ee: root@10-20-1-230:~# crictl ps | grep test 0e3e211f83723 8f2213208a7f5 39 seconds ago Running nginx 0 3f668220e8349 test-58d785fb4c-b9cld root@10-20-1-230:~# crictl inspect 0e3e211f83723 | grep pid \"pid\" : 976027 , \"pid\" : 1 \"type\" : \"pid\" root@10-20-1-230:~# nsenter -t 976027 -n root@10-20-1-230:~# root@10-20-1-230:~# iperf3 -c 172 .51.0.100 Connecting to host 172 .51.0.100, port 5201 [ 5 ] local 172 .51.0.102 port 50504 connected to 172 .51.0.100 port 5201 [ ID ] Interval Transfer Bitrate Retr Cwnd [ 5 ] 0 .00-1.00 sec 37 .1 MBytes 311 Mbits/sec 0 35 .4 KBytes [ 5 ] 1 .00-2.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 103 KBytes [ 5 ] 2 .00-3.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 7 .07 KBytes [ 5 ] 3 .00-4.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 29 .7 KBytes [ 5 ] 4 .00-5.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 33 .9 KBytes [ 5 ] 5 .00-6.00 sec 12 .5 MBytes 105 Mbits/sec 0 29 .7 KBytes [ 5 ] 6 .00-7.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 62 .2 KBytes [ 5 ] 7 .00-8.00 sec 12 .5 MBytes 105 Mbits/sec 0 22 .6 KBytes [ 5 ] 8 .00-9.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 69 .3 KBytes [ 5 ] 9 .00-10.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 52 .3 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID ] Interval Transfer Bitrate Retr [ 5 ] 0 .00-10.00 sec 137 MBytes 115 Mbits/sec 0 sender [ 5 ] 0 .00-10.00 sec 134 MBytes 113 Mbits/sec receiver iperf Done. \u53ef\u4ee5\u770b\u5230\u7ed3\u679c\u4e3a 115 Mbits/sec \uff0c\u8bf4\u660e Pod \u7684\u5e26\u5bbd\u5df2\u7ecf\u88ab\u9650\u5236\u4e3a\u6211\u4eec\u901a\u8fc7 annotations \u4e2d\u5b9a\u4e49\u7684\u5927\u5c0f\u3002","title":"IPVlan \u5e26\u5bbd\u7ba1\u7406"},{"location":"usage/ipvlan_bandwidth-zh_CN/#ipvlan","text":"\u7b80\u4f53\u4e2d\u6587 \uff5c English \u672c\u6587\u5c06\u5c55\u793a\u5982\u4f55\u501f\u52a9 cilium-chaining \u8fd9\u4e2a\u9879\u76ee\uff0c\u5b9e\u73b0 IPVlan CNI \u7684\u5e26\u5bbd\u7ba1\u7406\u80fd\u529b\u3002","title":"IPVlan \u5e26\u5bbd\u7ba1\u7406"},{"location":"usage/ipvlan_bandwidth-zh_CN/#_1","text":"Kubernetes \u5b98\u65b9\u652f\u6301\u5411 Pod \u4e2d\u6ce8\u5165 Annotations \u7684\u65b9\u5f0f\u8bbe\u7f6e Pod \u7684\u5165\u53e3\u51fa\u53e3\u5e26\u5bbd\uff0c\u53c2\u8003 \u5e26\u5bbd\u9650\u5236 \u5f53\u6211\u4eec\u4f7f\u7528 IPVlan \u4f5c\u4e3a CNI \u65f6\uff0c\u5b83\u672c\u8eab\u4e0d\u5177\u5907\u7ba1\u7406 Pod \u7684\u5165\u53e3/\u51fa\u53e3\u6d41\u91cf\u5e26\u5bbd\u7ba1\u7406\u80fd\u529b\u3002\u5f00\u6e90\u9879\u76ee Cilium \u652f\u6301\u4ee5 cni-chaining \u7684\u65b9\u5f0f\u4e0e IPVlan \u4e00\u8d77\u8054\u52a8\u5de5\u4f5c\uff0c\u57fa\u4e8e eBPF \u6280\u672f\u53ef\u4ee5\u5e2e\u52a9 IPVlan \u5b9e\u73b0\u52a0\u901f\u8bbf\u95ee Service, \u5e26\u5bbd\u80fd\u529b\u7ba1\u7406\u7b49\u529f\u80fd\u3002\u4f46\u76ee\u524d Cilium \u5df2\u7ecf\u79fb\u9664\u652f\u6301 IPVlan \u7684 Dataplane\u3002 cilium-chaining \u9879\u76ee\u57fa\u4e8e cilium v1.12.7 \u6784\u5efa\uff0c\u5e76\u652f\u6301 IPVlan \u7684 Dataplane, \u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u5b83\u5e2e\u52a9 IPVlan \u5b9e\u73b0 Pod \u7684\u7f51\u7edc\u5e26\u5bbd\u7ba1\u7406\u80fd\u529b\u3002","title":"\u80cc\u666f"},{"location":"usage/ipvlan_bandwidth-zh_CN/#_2","text":"Helm \u548c Kubectl \u4e8c\u8fdb\u5236\u5de5\u5177 \u8981\u6c42\u8282\u70b9\u5185\u6838\u81f3\u5927\u4e8e 4.19","title":"\u9884\u7f6e\u6761\u4ef6"},{"location":"usage/ipvlan_bandwidth-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003\u6587\u6863: \u5b89\u88c5 Spiderpool","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/ipvlan_bandwidth-zh_CN/#cilium-chaining","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 cilium-chainging \u9879\u76ee: kubectl apply -f https://raw.githubusercontent.com/spidernet-io/cilium-chaining/main/manifests/cilium-chaining.yaml \u5b89\u88c5\u72b6\u6001\u68c0\u67e5: ~# kubectl get po -n kube-system | grep cilium-chain cilium-chaining-gl76b 1 /1 Running 0 137m cilium-chaining-nzvrg 1 /1 Running 0 137m","title":"\u5b89\u88c5 Cilium-chaining \u9879\u76ee"},{"location":"usage/ipvlan_bandwidth-zh_CN/#cni-ip","text":"\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa CNI \u914d\u7f6e: # \u521b\u5efa Multus Network-attachement-definition CR IPVLAN_MASTER_INTERFACE = ens192 IPPOOL_NAME = ens192-v4 cat << EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan namespace: kube-system spec: config: ' { \"cniVersion\": \"0.4.0\", \"name\": \"terway-chainer\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\":\"spiderpool\", \"default_ipv4_ippool\": [\"${IPPOOL_NAME}\"] } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" } ] }' EOF \u914d\u7f6e cilium-cni \u4ee5 cni-chain \u7684\u6a21\u5f0f\u642d\u914d ipvlan cni \u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa CNI \u914d\u7f6e: # \u521b\u5efa IP \u6c60 cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ${IPPOOL_NAME } spec: default: false disable: false gateway: 172 .51.0.1 ipVersion: 4 ips: - 172 .51.0.100-172.51.0.108 subnet: 172 .51.0.230/16 \u6ce8\u610f ens192 \u9700\u8981\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\uff0c\u5e76\u4e14\u914d\u7f6e IP \u6c60\u7684\u7f51\u6bb5\u9700\u8981\u548c ens192 \u6240\u5728\u7269\u7406\u7f51\u7edc\u4fdd\u6301\u4e00\u81f4","title":"\u521b\u5efa CNI \u914d\u7f6e\u6587\u4ef6\u548c IP \u6c60"},{"location":"usage/ipvlan_bandwidth-zh_CN/#_3","text":"\u4f7f\u7528\u4e0a\u9762\u521b\u5efa\u7684 CNI \u914d\u7f6e\u4ee5\u53ca IP \u6c60\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528\uff0c\u9a8c\u8bc1 Pod \u7684\u5e26\u5bbd\u662f\u5426\u53d7\u5230\u9650\u5236: cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: test spec: replicas: 2 selector: matchLabels: app: test template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/ipvlan kubernetes.io/ingress-bandwidth: 100M kubernetes.io/egress-bandwidth: 100M labels: app: test spec: containers: - env: - name: NODE _NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: http protocol: TCP resources: {} \u51e0\u4e2a annotations \u4ecb\u7ecd: v1.multus-cni.io/default-network: ipvlan : \u6307\u5b9a Pod \u7684\u7f3a\u7701 CNI \u4e3a \u4e4b\u524d\u521b\u5efa\u7684 ipvlan kubernetes.io/ingress-bandwidth: 100m : \u8bbe\u7f6e Pod \u7684\u5165\u53e3\u5e26\u5bbd\u4e3a 100M kubernetes.io/egress-bandwidth: 100m : \u8bbe\u7f6e Pod \u7684\u51fa\u53e3\u5e26\u5bbd\u4e3a 100M ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-58d785fb4c-b9cld 1 /1 Running 0 175m 172 .51.0.102 10 -20-1-230 <none> <none> test-58d785fb4c-kwh4h 1 /1 Running 0 175m 172 .51.0.100 10 -20-1-220 <none> <none> \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u5206\u522b\u8fdb\u5165\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\uff0c\u4f7f\u7528 iperf3 \u5de5\u5177\u6d4b\u8bd5\u5176\u7f51\u7edc\u5e26\u5bbd: ~# crictl ps | grep test b2b60a6e14e21 8f2213208a7f5 10 seconds ago Running nginx 0 f46848c1a713a test-58d785fb4c-kwh4h ~# crictl inspect b2b60a6e14e21 | grep pid \"pid\" : 284529 , \"pid\" : 1 \"type\" : \"pid\" ~# nsenter -t 284529 -n ~# iperf3 -s \u5728\u53e6\u5916\u4e00\u4e2a\u8282\u70b9\u7684 Pod \u4f5c\u4e3a Client \u8bbf\u95ee: root@10-20-1-230:~# crictl ps | grep test 0e3e211f83723 8f2213208a7f5 39 seconds ago Running nginx 0 3f668220e8349 test-58d785fb4c-b9cld root@10-20-1-230:~# crictl inspect 0e3e211f83723 | grep pid \"pid\" : 976027 , \"pid\" : 1 \"type\" : \"pid\" root@10-20-1-230:~# nsenter -t 976027 -n root@10-20-1-230:~# root@10-20-1-230:~# iperf3 -c 172 .51.0.100 Connecting to host 172 .51.0.100, port 5201 [ 5 ] local 172 .51.0.102 port 50504 connected to 172 .51.0.100 port 5201 [ ID ] Interval Transfer Bitrate Retr Cwnd [ 5 ] 0 .00-1.00 sec 37 .1 MBytes 311 Mbits/sec 0 35 .4 KBytes [ 5 ] 1 .00-2.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 103 KBytes [ 5 ] 2 .00-3.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 7 .07 KBytes [ 5 ] 3 .00-4.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 29 .7 KBytes [ 5 ] 4 .00-5.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 33 .9 KBytes [ 5 ] 5 .00-6.00 sec 12 .5 MBytes 105 Mbits/sec 0 29 .7 KBytes [ 5 ] 6 .00-7.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 62 .2 KBytes [ 5 ] 7 .00-8.00 sec 12 .5 MBytes 105 Mbits/sec 0 22 .6 KBytes [ 5 ] 8 .00-9.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 69 .3 KBytes [ 5 ] 9 .00-10.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 52 .3 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID ] Interval Transfer Bitrate Retr [ 5 ] 0 .00-10.00 sec 137 MBytes 115 Mbits/sec 0 sender [ 5 ] 0 .00-10.00 sec 134 MBytes 113 Mbits/sec receiver iperf Done. \u53ef\u4ee5\u770b\u5230\u7ed3\u679c\u4e3a 115 Mbits/sec \uff0c\u8bf4\u660e Pod \u7684\u5e26\u5bbd\u5df2\u7ecf\u88ab\u9650\u5236\u4e3a\u6211\u4eec\u901a\u8fc7 annotations \u4e2d\u5b9a\u4e49\u7684\u5927\u5c0f\u3002","title":"\u521b\u5efa\u5e94\u7528\u6d4b\u8bd5\u5e26\u5bbd\u9650\u5236"},{"location":"usage/ipvlan_bandwidth/","text":"Bandwidth manager for IPVlan CNI English \uff5c \u7b80\u4f53\u4e2d\u6587 This article will show how to implement the bandwidth management capabilities of IPVlan CNI with the help of the project cilium-chaining . Background Kubernetes supports setting the ingress/egress bandwidth of a Pod by injecting Annotations into the Pod, refer to Bandwidth Limiting When we use IPVlan as CNI, it does not have the ability to manage the ingress/egress traffic bandwidth of the Pod itself. The open source project Cilium supports cni-chaining to work with IPVlan, based on eBPF technology, it can help IPVlan realize accelerated access to services, bandwidth capacity management and other functions. However, Cilium has removed support for IPVlan Dataplane in the latest release. cilium-chaining project is built on cilium v1.12.7 and supports IPVlan Dataplane, we can use it to support IPVlan Dataplane by cilium v1.12.7. Dataplane, we can use it to help IPVlan realize the network bandwidth management capability of Pod. Pre-conditions Helm and Kubectl binary tools. Requires node kernel to be greater than 4.19. Installing Spiderpool Installation of Spiderpool can be found in the documentation: Install Spiderpool Install the cilium-chaining project Install the cilium-chaining project with the following command. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/cilium-chaining/main/manifests/cilium-chaining.yaml Check the status of the installation: ~# kubectl get po -n kube-system | grep cilium-chain cilium-chaining-gl76b 1 /1 Running 0 137m cilium-chaining-nzvrg 1 /1 Running 0 137m Creating a CNI config and IPPool Refer to the following command to create a CNI configuration file: # Create Multus Network-attachement-definition CR IPVLAN_MASTER_INTERFACE = ens192 IPPOOL_NAME = ens192-v4 cat << EOF | kubectl apply -f - - apiVersion: k8s-v4 apiVersion: k8s.cni.cncf.io/v1 Type: NetworkAttachmentDefinition Metadata: Name: ipvlan Namespace: kube-system spec: config: ' { \"cniVersion\": \"0.4.0\", \"name\": \"terway-chainer\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\", \"default_ipv4_ippool\": [\"${ippool_name}\"]}} , { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" } ] }' EOF Configure cilium-cni in cni-chain mode with ipvlan cni Create a CNI configuration by referring to the following command. # Create the IP pool cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata. name: ${IPPOOL_NAME } spec: ${ IPPOOL_NAME } default: false disable: false gateway: 172 .51.0.1 ipVersion: 4 ips: 172 .51.0.100-172.51.0.108 - 172 .51.0.100-172.51.0.108 subnet: 172 .51.0.230/16 Note that ens192 needs to exist on the host, and the network segment on which the IP pool is configured needs to be the same as the physical network on which ens192 resides. Create an application to test bandwidth limitation Create a test application using the CNI configuration and IP pool created above to verify that the Pod's bandwidth is limited. cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: test spec: replicas: 2 selector: matchLabels: app: test template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/ipvlan kubernetes.io/ingress-bandwidth: 100M kubernetes.io/egress-bandwidth: 100M labels: app: test spec: containers: - env: - name: NODE _NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: http protocol: TCP resources: {} A few annotations to introduce. v1.multus-cni.io/default-network: ipvlan : Specifies that the default CNI for the Pod is the previously created ipvlan. kubernetes.io/ingress-bandwidth: 100m : Sets the ingress bandwidth of the Pod to 100M. kubernetes.io/ingress-bandwidth: 100m : Sets the Pod's egress bandwidth to 100M. ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-58d785fb4c-b9cld 1 /1 Running 0 175m 172 .51.0.102 10 -20-1-230 <none> <none> test-58d785fb4c-kwh4h 1 /1 Running 0 175m 172 .51.0.100 10 -20-1-220 <none> <none> When the Pod is created, go to the Pod's network namespace and test its network bandwidth using the iperf3 utility. root@10-20-1-230:~# crictl ps | grep test 0e3e211f83723 8f2213208a7f5 39 seconds ago Running nginx 0 3f668220e8349 test-58d785fb4c-b9cld root@10-20-1-230:~# crictl inspect 0e3e211f83723 | grep pid \"pid\" : 976027 , \"pid\" : 1 \"type\" : \"pid\" root@10-20-1-230:~# nsenter -t 976027 -n root@10-20-1-230:~# root@10-20-1-230:~# iperf3 -c 172 .51.0.100 Connecting to host 172 .51.0.100, port 5201 [ 5 ] local 172 .51.0.102 port 50504 connected to 172 .51.0.100 port 5201 [ ID ] Interval Transfer Bitrate Retr Cwnd [ 5 ] 0 .00-1.00 sec 37 .1 MBytes 311 Mbits/sec 0 35 .4 KBytes [ 5 ] 1 .00-2.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 103 KBytes [ 5 ] 2 .00-3.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 7 .07 KBytes [ 5 ] 3 .00-4.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 29 .7 KBytes [ 5 ] 4 .00-5.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 33 .9 KBytes [ 5 ] 5 .00-6.00 sec 12 .5 MBytes 105 Mbits/sec 0 29 .7 KBytes [ 5 ] 6 .00-7.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 62 .2 KBytes [ 5 ] 7 .00-8.00 sec 12 .5 MBytes 105 Mbits/sec 0 22 .6 KBytes [ 5 ] 8 .00-9.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 69 .3 KBytes [ 5 ] 9 .00-10.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 52 .3 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID ] Interval Transfer Bitrate Retr [ 5 ] 0 .00-10.00 sec 137 MBytes 115 Mbits/sec 0 sender [ 5 ] 0 .00-10.00 sec 134 MBytes 113 Mbits/sec receiver iperf Done. You can see that the result is 115 Mbits/sec, indicating that the Pod's bandwidth has been limited to the size we defined in the annotations.","title":"Bandwidth Manage for IPVlan CNI"},{"location":"usage/ipvlan_bandwidth/#bandwidth-manager-for-ipvlan-cni","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587 This article will show how to implement the bandwidth management capabilities of IPVlan CNI with the help of the project cilium-chaining .","title":"Bandwidth manager for IPVlan CNI"},{"location":"usage/ipvlan_bandwidth/#background","text":"Kubernetes supports setting the ingress/egress bandwidth of a Pod by injecting Annotations into the Pod, refer to Bandwidth Limiting When we use IPVlan as CNI, it does not have the ability to manage the ingress/egress traffic bandwidth of the Pod itself. The open source project Cilium supports cni-chaining to work with IPVlan, based on eBPF technology, it can help IPVlan realize accelerated access to services, bandwidth capacity management and other functions. However, Cilium has removed support for IPVlan Dataplane in the latest release. cilium-chaining project is built on cilium v1.12.7 and supports IPVlan Dataplane, we can use it to support IPVlan Dataplane by cilium v1.12.7. Dataplane, we can use it to help IPVlan realize the network bandwidth management capability of Pod.","title":"Background"},{"location":"usage/ipvlan_bandwidth/#pre-conditions","text":"Helm and Kubectl binary tools. Requires node kernel to be greater than 4.19.","title":"Pre-conditions"},{"location":"usage/ipvlan_bandwidth/#installing-spiderpool","text":"Installation of Spiderpool can be found in the documentation: Install Spiderpool","title":"Installing Spiderpool"},{"location":"usage/ipvlan_bandwidth/#install-the-cilium-chaining-project","text":"Install the cilium-chaining project with the following command. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/cilium-chaining/main/manifests/cilium-chaining.yaml Check the status of the installation: ~# kubectl get po -n kube-system | grep cilium-chain cilium-chaining-gl76b 1 /1 Running 0 137m cilium-chaining-nzvrg 1 /1 Running 0 137m","title":"Install the cilium-chaining project"},{"location":"usage/ipvlan_bandwidth/#creating-a-cni-config-and-ippool","text":"Refer to the following command to create a CNI configuration file: # Create Multus Network-attachement-definition CR IPVLAN_MASTER_INTERFACE = ens192 IPPOOL_NAME = ens192-v4 cat << EOF | kubectl apply -f - - apiVersion: k8s-v4 apiVersion: k8s.cni.cncf.io/v1 Type: NetworkAttachmentDefinition Metadata: Name: ipvlan Namespace: kube-system spec: config: ' { \"cniVersion\": \"0.4.0\", \"name\": \"terway-chainer\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\", \"default_ipv4_ippool\": [\"${ippool_name}\"]}} , { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" } ] }' EOF Configure cilium-cni in cni-chain mode with ipvlan cni Create a CNI configuration by referring to the following command. # Create the IP pool cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata. name: ${IPPOOL_NAME } spec: ${ IPPOOL_NAME } default: false disable: false gateway: 172 .51.0.1 ipVersion: 4 ips: 172 .51.0.100-172.51.0.108 - 172 .51.0.100-172.51.0.108 subnet: 172 .51.0.230/16 Note that ens192 needs to exist on the host, and the network segment on which the IP pool is configured needs to be the same as the physical network on which ens192 resides.","title":"Creating a CNI config and IPPool"},{"location":"usage/ipvlan_bandwidth/#create-an-application-to-test-bandwidth-limitation","text":"Create a test application using the CNI configuration and IP pool created above to verify that the Pod's bandwidth is limited. cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: test spec: replicas: 2 selector: matchLabels: app: test template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/ipvlan kubernetes.io/ingress-bandwidth: 100M kubernetes.io/egress-bandwidth: 100M labels: app: test spec: containers: - env: - name: NODE _NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: http protocol: TCP resources: {} A few annotations to introduce. v1.multus-cni.io/default-network: ipvlan : Specifies that the default CNI for the Pod is the previously created ipvlan. kubernetes.io/ingress-bandwidth: 100m : Sets the ingress bandwidth of the Pod to 100M. kubernetes.io/ingress-bandwidth: 100m : Sets the Pod's egress bandwidth to 100M. ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-58d785fb4c-b9cld 1 /1 Running 0 175m 172 .51.0.102 10 -20-1-230 <none> <none> test-58d785fb4c-kwh4h 1 /1 Running 0 175m 172 .51.0.100 10 -20-1-220 <none> <none> When the Pod is created, go to the Pod's network namespace and test its network bandwidth using the iperf3 utility. root@10-20-1-230:~# crictl ps | grep test 0e3e211f83723 8f2213208a7f5 39 seconds ago Running nginx 0 3f668220e8349 test-58d785fb4c-b9cld root@10-20-1-230:~# crictl inspect 0e3e211f83723 | grep pid \"pid\" : 976027 , \"pid\" : 1 \"type\" : \"pid\" root@10-20-1-230:~# nsenter -t 976027 -n root@10-20-1-230:~# root@10-20-1-230:~# iperf3 -c 172 .51.0.100 Connecting to host 172 .51.0.100, port 5201 [ 5 ] local 172 .51.0.102 port 50504 connected to 172 .51.0.100 port 5201 [ ID ] Interval Transfer Bitrate Retr Cwnd [ 5 ] 0 .00-1.00 sec 37 .1 MBytes 311 Mbits/sec 0 35 .4 KBytes [ 5 ] 1 .00-2.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 103 KBytes [ 5 ] 2 .00-3.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 7 .07 KBytes [ 5 ] 3 .00-4.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 29 .7 KBytes [ 5 ] 4 .00-5.00 sec 11 .2 MBytes 94 .4 Mbits/sec 0 33 .9 KBytes [ 5 ] 5 .00-6.00 sec 12 .5 MBytes 105 Mbits/sec 0 29 .7 KBytes [ 5 ] 6 .00-7.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 62 .2 KBytes [ 5 ] 7 .00-8.00 sec 12 .5 MBytes 105 Mbits/sec 0 22 .6 KBytes [ 5 ] 8 .00-9.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 69 .3 KBytes [ 5 ] 9 .00-10.00 sec 10 .0 MBytes 83 .9 Mbits/sec 0 52 .3 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID ] Interval Transfer Bitrate Retr [ 5 ] 0 .00-10.00 sec 137 MBytes 115 Mbits/sec 0 sender [ 5 ] 0 .00-10.00 sec 134 MBytes 113 Mbits/sec receiver iperf Done. You can see that the result is 115 Mbits/sec, indicating that the Pod's bandwidth has been limited to the size we defined in the annotations.","title":"Create an application to test bandwidth limitation"},{"location":"usage/istio-zh_CN/","text":"Istio \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u5728 Istio \u573a\u666f\u4e0b\uff0c\u4f7f\u7528 Spiderpool \u914d\u7f6e\u670d\u52a1\u7f51\u683c\u5e94\u7528\u4f7f\u7528 Underlay \u7f51\u7edc\u65f6\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u6d41\u91cf\u65e0\u6cd5\u88ab istio \u52ab\u6301\u7684\u95ee\u9898\u3002\u8fd9\u662f\u56e0\u4e3a\uff1a \u8bbf\u95ee\u670d\u52a1\u7f51\u683c Pod \u7684\u6d41\u91cf\u901a\u8fc7\u5176 veth0 \u7f51\u5361(\u7531 Spiderpool \u521b\u5efa)\u8f6c\u53d1\u3002\u6d41\u91cf\u968f\u540e\u4f1a\u901a\u8fc7 istio \u8bbe\u7f6e\u7684 iptables redirect \u89c4\u5219\uff0c\u88ab\u52ab\u6301\u5230 sidecar \u5bb9\u5668\u4e2d\u3002\u4f46\u7531\u4e8e iptables redirect \u89c4\u5219\u5fc5\u987b\u8981\u6c42\u63a5\u6536\u6d41\u91cf\u7684\u7f51\u5361\u5fc5\u987b\u914d\u7f6e IP \u5730\u5740\uff0c\u5426\u5219\u8be5\u6570\u636e\u5305\u4f1a\u88ab\u5185\u6838\u6c89\u9ed8\u7684\u4e22\u5f03\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSpiderpool \u4e0d\u4f1a\u4e3a\u4f7f\u7528 Underlay \u7f51\u7edc\u7684 Pod \u7684 veth0 \u7f51\u5361\u914d\u7f6e IP \u5730\u5740, \u6240\u4ee5\u8fd9\u4f1a\u5bfc\u81f4\u8bbf\u95ee\u670d\u52a1\u7f51\u683c\u7684\u6d41\u91cf\u88ab\u4e22\u5f03\u3002 \u53c2\u8003 #Issue 3568 \u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c Spiderpool \u63d0\u4f9b\u4e00\u4e2a\u914d\u7f6e: vethLinkAddress \uff0c\u7528\u4e8e\u4e3a veth0 \u7f51\u5361\u914d\u7f6e\u4e00\u4e2a link-local \u5730\u5740\u3002 \u5982\u4f55\u914d\u7f6e \u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5f00\u542f\u8fd9\u4e2a\u529f\u80fd\uff1a helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set coordinator.vethLinkAddress = 169 .254.100.1 vethLinkAddress \u5fc5\u987b\u662f\u4e00\u4e2a\u5408\u6cd5\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u67e5\u770b Spidercoordinator \u7684\u914d\u7f6e\uff0c\u786e\u4fdd vethLinkAddress \u5df2\u914d\u7f6e\u6b63\u786e\uff1a ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-10-30T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" vethLinkAddress: 169 .254.100.1 podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 \u5982\u679c\u60a8\u5df2\u7ecf\u5b89\u88c5 Spiderpool, \u60a8\u53ef\u4ee5\u76f4\u63a5\u4fee\u6539 Spidercoordinator \u4e2d\u5173\u4e8e vethLinkAddress \u7684\u914d\u7f6e: kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"vethLinkAddress\": \"169.254.100.1\"}}' \u6b65\u9aa4 3 \u4e2d\u662f\u96c6\u7fa4\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u5982\u679c\u60a8\u4e0d\u5e0c\u671b\u6574\u4e2a\u96c6\u7fa4\u9ed8\u8ba4\u90fd\u914d\u7f6e vethLinkAddress\uff0c\u60a8\u53ef\u4ee5\u4e3a\u5355\u4e2a\u7f51\u5361\u914d\u7f6e\uff1a MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} coordinator: vethLinkAddress: 169.254.100.1 EOF \u9a8c\u8bc1 \u521b\u5efa\u5e94\u7528\u540e\uff0c\u53ef\u67e5\u770b Pod \u7684 veth0 \u7f51\u5361\u662f\u5426\u6b63\u786e\u914d\u7f6e IP \u5730\u5740\uff1a169.254.100.1 ~# kubectl exec -it <pod-name> -n <namespace> -- ip addr show veth0","title":"Istio"},{"location":"usage/istio-zh_CN/#istio","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Istio"},{"location":"usage/istio-zh_CN/#_1","text":"\u5728 Istio \u573a\u666f\u4e0b\uff0c\u4f7f\u7528 Spiderpool \u914d\u7f6e\u670d\u52a1\u7f51\u683c\u5e94\u7528\u4f7f\u7528 Underlay \u7f51\u7edc\u65f6\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u6d41\u91cf\u65e0\u6cd5\u88ab istio \u52ab\u6301\u7684\u95ee\u9898\u3002\u8fd9\u662f\u56e0\u4e3a\uff1a \u8bbf\u95ee\u670d\u52a1\u7f51\u683c Pod \u7684\u6d41\u91cf\u901a\u8fc7\u5176 veth0 \u7f51\u5361(\u7531 Spiderpool \u521b\u5efa)\u8f6c\u53d1\u3002\u6d41\u91cf\u968f\u540e\u4f1a\u901a\u8fc7 istio \u8bbe\u7f6e\u7684 iptables redirect \u89c4\u5219\uff0c\u88ab\u52ab\u6301\u5230 sidecar \u5bb9\u5668\u4e2d\u3002\u4f46\u7531\u4e8e iptables redirect \u89c4\u5219\u5fc5\u987b\u8981\u6c42\u63a5\u6536\u6d41\u91cf\u7684\u7f51\u5361\u5fc5\u987b\u914d\u7f6e IP \u5730\u5740\uff0c\u5426\u5219\u8be5\u6570\u636e\u5305\u4f1a\u88ab\u5185\u6838\u6c89\u9ed8\u7684\u4e22\u5f03\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSpiderpool \u4e0d\u4f1a\u4e3a\u4f7f\u7528 Underlay \u7f51\u7edc\u7684 Pod \u7684 veth0 \u7f51\u5361\u914d\u7f6e IP \u5730\u5740, \u6240\u4ee5\u8fd9\u4f1a\u5bfc\u81f4\u8bbf\u95ee\u670d\u52a1\u7f51\u683c\u7684\u6d41\u91cf\u88ab\u4e22\u5f03\u3002 \u53c2\u8003 #Issue 3568 \u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c Spiderpool \u63d0\u4f9b\u4e00\u4e2a\u914d\u7f6e: vethLinkAddress \uff0c\u7528\u4e8e\u4e3a veth0 \u7f51\u5361\u914d\u7f6e\u4e00\u4e2a link-local \u5730\u5740\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/istio-zh_CN/#_2","text":"\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5f00\u542f\u8fd9\u4e2a\u529f\u80fd\uff1a helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set coordinator.vethLinkAddress = 169 .254.100.1 vethLinkAddress \u5fc5\u987b\u662f\u4e00\u4e2a\u5408\u6cd5\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u67e5\u770b Spidercoordinator \u7684\u914d\u7f6e\uff0c\u786e\u4fdd vethLinkAddress \u5df2\u914d\u7f6e\u6b63\u786e\uff1a ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-10-30T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" vethLinkAddress: 169 .254.100.1 podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 \u5982\u679c\u60a8\u5df2\u7ecf\u5b89\u88c5 Spiderpool, \u60a8\u53ef\u4ee5\u76f4\u63a5\u4fee\u6539 Spidercoordinator \u4e2d\u5173\u4e8e vethLinkAddress \u7684\u914d\u7f6e: kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"vethLinkAddress\": \"169.254.100.1\"}}' \u6b65\u9aa4 3 \u4e2d\u662f\u96c6\u7fa4\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u5982\u679c\u60a8\u4e0d\u5e0c\u671b\u6574\u4e2a\u96c6\u7fa4\u9ed8\u8ba4\u90fd\u914d\u7f6e vethLinkAddress\uff0c\u60a8\u53ef\u4ee5\u4e3a\u5355\u4e2a\u7f51\u5361\u914d\u7f6e\uff1a MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} coordinator: vethLinkAddress: 169.254.100.1 EOF","title":"\u5982\u4f55\u914d\u7f6e"},{"location":"usage/istio-zh_CN/#_3","text":"\u521b\u5efa\u5e94\u7528\u540e\uff0c\u53ef\u67e5\u770b Pod \u7684 veth0 \u7f51\u5361\u662f\u5426\u6b63\u786e\u914d\u7f6e IP \u5730\u5740\uff1a169.254.100.1 ~# kubectl exec -it <pod-name> -n <namespace> -- ip addr show veth0","title":"\u9a8c\u8bc1"},{"location":"usage/istio/","text":"Istio English | \u7b80\u4f53\u4e2d\u6587 Introduction In the context of Istio, when using Spiderpool to configure the network for service mesh applications with an Underlay network, there may be issues where traffic cannot be intercepted by Istio. This is because: Traffic accessing the service mesh Pod is forwarded through its veth0 network interface (created by Spiderpool). The traffic is then intercepted to the sidecar container through the iptables redirect rules set by Istio. However, since iptables redirect rules require the receiving network interface to be configured with an IP address, otherwise the packet will be silently dropped by the kernel. By default, Spiderpool does not configure an IP address for the veth0 network interface of Pods using the Underlay network, which leads to the traffic accessing the service mesh being dropped. Refer to #Issue 3568 . To solve this problem, Spiderpool provides a configuration: vethLinkAddress , which is used to configure a link-local address for the veth0 network interface. How to Configure When installing Spiderpool using Helm, you can enable this feature with the following command: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set coordinator.vethLinkAddress = 169 .254.100.1 vethLinkAddress must be a valid IP address. If you are a user in China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to use a domestic image source. After installation, check the configuration of the Spidercoordinator to ensure that vethLinkAddress is configured correctly: ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-10-30T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" vethLinkAddress: 169 .254.100.1 podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 If you have already installed Spiderpool, you can directly modify the configuration of vethLinkAddress in the Spidercoordinator: kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"vethLinkAddress\": \"169.254.100.1\"}}' Step 3 is the default setting for the cluster. If you do not want the entire cluster to default to configuring vethLinkAddress , you can configure it for a single network interface: MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} coordinator: vethLinkAddress: 169.254.100.1 EOF Verification After creating the application, you can check whether the Pod's veth0 network interface is correctly configured with the IP address: 169.254.100.1 ~# kubectl exec -it <pod-name> -n <namespace> -- ip addr show veth0","title":"Istio"},{"location":"usage/istio/#istio","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Istio"},{"location":"usage/istio/#introduction","text":"In the context of Istio, when using Spiderpool to configure the network for service mesh applications with an Underlay network, there may be issues where traffic cannot be intercepted by Istio. This is because: Traffic accessing the service mesh Pod is forwarded through its veth0 network interface (created by Spiderpool). The traffic is then intercepted to the sidecar container through the iptables redirect rules set by Istio. However, since iptables redirect rules require the receiving network interface to be configured with an IP address, otherwise the packet will be silently dropped by the kernel. By default, Spiderpool does not configure an IP address for the veth0 network interface of Pods using the Underlay network, which leads to the traffic accessing the service mesh being dropped. Refer to #Issue 3568 . To solve this problem, Spiderpool provides a configuration: vethLinkAddress , which is used to configure a link-local address for the veth0 network interface.","title":"Introduction"},{"location":"usage/istio/#how-to-configure","text":"When installing Spiderpool using Helm, you can enable this feature with the following command: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set coordinator.vethLinkAddress = 169 .254.100.1 vethLinkAddress must be a valid IP address. If you are a user in China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to use a domestic image source. After installation, check the configuration of the Spidercoordinator to ensure that vethLinkAddress is configured correctly: ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2024-10-30T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" vethLinkAddress: 169 .254.100.1 podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 If you have already installed Spiderpool, you can directly modify the configuration of vethLinkAddress in the Spidercoordinator: kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"vethLinkAddress\": \"169.254.100.1\"}}' Step 3 is the default setting for the cluster. If you do not want the entire cluster to default to configuring vethLinkAddress , you can configure it for a single network interface: MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} coordinator: vethLinkAddress: 169.254.100.1 EOF","title":"How to Configure"},{"location":"usage/istio/#verification","text":"After creating the application, you can check whether the Pod's veth0 network interface is correctly configured with the IP address: 169.254.100.1 ~# kubectl exec -it <pod-name> -n <namespace> -- ip addr show veth0","title":"Verification"},{"location":"usage/kubevirt-zh_CN/","text":"KubeVirt \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u80fd\u4fdd\u8bc1 kubevirt VM \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 KubeVirt \u7f51\u7edc\u642d\u914d Spiderpool underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u53ef\u7ed9 KubeVirt \u8d4b\u4e88\u4ecb\u5165 underlay \u7684\u80fd\u529b: \u5bf9\u4e8e KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d Spiderpool macvlan \u96c6\u6210\u65b9\u6848\u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u652f\u6301 Service Mesh \u7684\u6240\u6709\u529f\u80fd\uff0c\u4e0d\u8fc7\u53ea\u80fd\u4f7f\u7528**\u5355\u7f51\u5361**\uff0c\u4e14\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 \u5bf9\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d OVS CNI \u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u4e0d\u652f\u6301 Service Mesh \u529f\u80fd\uff0c\u53ef\u4f7f\u7528**\u591a\u7f51\u5361**\uff0c\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 Spiderpool \u652f\u6301\u5bf9 KubeVirt Pod \u8fdb\u884c IP \u51b2\u7a81\u68c0\u6d4b\uff0c\u907f\u514d\u51fa\u73b0 IP \u51b2\u7a81\u3002\u4f46\u5bf9\u4e8e KubeVirt \u70ed\u8fc1\u79fb\u5e94\u7528\uff0c\u5f53\u5f00\u542f IP \u51b2\u7a81\u68c0\u6d4b\uff0c\u4f1a\u5bfc\u81f4\u70ed\u8fc1\u79fb\u865a\u62df\u673a\u65e0\u6cd5\u542f\u52a8\u3002\u6240\u4ee5\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u5373\u4f7f\u5f00\u542f\u4e86 IP \u51b2\u7a81\u68c0\u6d4b\u529f\u80fd\uff0cSpiderpool \u4e5f\u4e0d\u4f1a\u5bf9 KubeVirt \u8fdb\u884c IP \u51b2\u7a81\u68c0\u6d4b\u3002 KubeVirt VM \u56fa\u5b9a\u5730\u5740 KubeVirt VM \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a VM \u7684\u70ed\u8fc1\u79fb\uff0c\u671f\u671b\u8fc1\u79fb\u8fc7\u540e\u7684 VM \u4ecd\u80fd\u7ee7\u627f\u4e4b\u524d\u7684 IP \u5730\u5740\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u4e86\u91cd\u542f\u7684\u60c5\u51b5\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 VMI(VirtualMachineInstance) \u8d44\u6e90\u88ab\u5220\u9664\u7684\u60c5\u666f\u3002 \u6b64\u5916\uff0cKubeVirt VM \u56fa\u5b9a IP \u5730\u5740\u4e0e StatefulSet \u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e VM \uff0cPod \u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u7684\u540d\u5b57\u662f\u4f1a\u53d1\u751f\u53d8\u5316\u7684\uff0c\u4f46\u662f\u5176\u5bf9\u5e94\u7684 VMI \u4e0d\u8bba\u91cd\u542f\u4e0e\u5426\uff0c\u5176\u540d\u5b57\u90fd\u4e0d\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u4f1a\u4ee5 VM \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740(\u6211\u4eec\u7684 SpiderEndpoint \u8d44\u6e90\u5c06\u4f1a\u7ee7\u627f\u4f7f\u7528 VM \u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4ee5\u53ca\u540d\u5b57)\u3002 \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u6211\u4eec Spiderpool \u4f1a\u56e0\u6b64\u4ee5 Pod \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cVM \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba VM \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u5230\u76f8\u540c IP\u3002 \u82e5\u5173\u95ed\uff0cVM \u5bf9\u5e94\u7684 Pod \u5c06\u88ab\u5f53\u4f5c\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u901a\u8fc7 --set ipam.enableKubevirtStaticIP=false \u5173\u95ed\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u4ee5\u4e0b\u6d41\u7a0b\u5c06\u4f1a\u6f14\u793a KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan CNI \u4ee5\u4f7f\u5f97 VM \u83b7\u5f97 underlay \u63a5\u5165\u80fd\u529b\uff0c\u5e76\u901a\u8fc7 Spiderpool \u5b9e\u73b0\u5206\u914d\u56fa\u5b9a IP \u7684\u529f\u80fd\u3002 Notice\uff1a\u5f53\u524d macvlan \u548c ipvlan \u5e76\u4e0d\u9002\u7528\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u56e0\u4e3a\u5bf9\u4e8e bridge \u7f51\u7edc\u6a21\u5f0f\u4f1a\u5c06 Pod \u7f51\u5361\u7684 MAC \u5730\u5740\u79fb\u52a8\u5230 VM\uff0c\u4f7f\u5f97 Pod \u4f7f\u7528\u53e6\u4e00\u4e2a\u4e0d\u540c\u7684\u5730\u5740\u3002\u800c macvlan \u548c ipvlan CNI \u8981\u6c42 Pod \u7684\u7f51\u5361\u63a5\u53e3\u5177\u6709\u539f\u59cb MAC \u5730\u5740\u3002 \u5b89\u88c5 Spiderpool \u8bf7\u53c2\u8003 Macvlan Quick Start \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u53ef\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableKubevirtStaticIP=true \u521b\u5efa KubeVirt VM \u5e94\u7528 underlay \u5355\u7f51\u5361\u573a\u666f \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u9009\u62e9\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u7684 CNI \u914d\u7f6e\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-cirros labels : kubevirt.io/vm : vm-cirros spec : runStrategy : Always template : metadata : annotations : v1.multus-cni.io/default-network : kube-system/macvlan-ens192 labels : kubevirt.io/vm : vm-cirros spec : domain : devices : disks : - name : containerdisk disk : bus : virtio - name : cloudinitdisk disk : bus : virtio interfaces : - name : default passt : {} resources : requests : memory : 64M networks : - name : default pod : {} volumes : - name : containerdisk containerDisk : image : quay.io/kubevirt/cirros-container-disk-demo - name : cloudinitdisk cloudInitNoCloud : userData : | #!/bin/sh echo 'printed from cloud-init userdata' \u6700\u7ec8\uff0c\u5728 KubeVirt VM \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VM Pod, \u89c2\u5bdf\u5230\u65b0\u7684 Pod \u7684 IP \u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VMI\uff0c\u89c2\u5bdf\u5230\u540e\u7eed\u65b0\u7684 Pod \u7684IP \u4e5f\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 VM \u4e5f\u53ef\u4e0e\u5176\u4ed6 underlay Pod \u7684\u901a\u4fe1\u3002 ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms VM \u4e5f\u53ef\u8bbf\u95ee cluster IP\u3002 ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes underlay \u591a\u7f51\u5361\u573a\u666f \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt bridge \u7f51\u7edc\u6a21\u5f0f\u642d\u914d ovs-cni \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippools : \u4e3a\u5e94\u7528\u6307\u5b9a\u6bcf\u5f20\u7f51\u5361\u9009\u62e9\u4f7f\u7528\u54ea\u4e9b IP \u6c60\u3002(\u4f60\u4e5f\u53ef\u4f7f\u7528 multus \u5b9e\u4f8b\u4e2d\u6307\u5b9a\u7684 CNI \u914d\u7f6e\u6587\u4ef6\u7ea7\u522b\u9ed8\u8ba4 IP \u6c60) \u8981\u6c42 multus \u5b9e\u4f8b kube-system/ovs-vlan30 \u548c kube-system/ovs-vlan40 \u542f\u7528 coordinator \u63d2\u4ef6\u6765\u534f\u8c03\u591a\u7f51\u5361\u9ed8\u8ba4\u8def\u7531\u3002 ovs-cni \u4e0d\u652f\u6301 clusterIP \u8bbf\u95ee\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk \u603b\u7ed3 Spiderpool \u80fd\u4fdd\u8bc1 KubeVirt VM Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 KubeVirt \u865a\u62df\u673a\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002\u5e76\u53ef\u914d\u5408 macvlan \u6216 OVS CNI \u4e0e KubeVirt \u7684\u591a\u79cd\u7f51\u7edc\u6a21\u5f0f\u5b9e\u73b0 VM underlay \u63a5\u5165\u80fd\u529b\u3002","title":"KubeVirt"},{"location":"usage/kubevirt-zh_CN/#kubevirt","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"KubeVirt"},{"location":"usage/kubevirt-zh_CN/#_1","text":"Spiderpool \u80fd\u4fdd\u8bc1 kubevirt VM \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/kubevirt-zh_CN/#kubevirt_1","text":"Spiderpool underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u53ef\u7ed9 KubeVirt \u8d4b\u4e88\u4ecb\u5165 underlay \u7684\u80fd\u529b: \u5bf9\u4e8e KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d Spiderpool macvlan \u96c6\u6210\u65b9\u6848\u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u652f\u6301 Service Mesh \u7684\u6240\u6709\u529f\u80fd\uff0c\u4e0d\u8fc7\u53ea\u80fd\u4f7f\u7528**\u5355\u7f51\u5361**\uff0c\u4e14\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 \u5bf9\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d OVS CNI \u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u4e0d\u652f\u6301 Service Mesh \u529f\u80fd\uff0c\u53ef\u4f7f\u7528**\u591a\u7f51\u5361**\uff0c\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 Spiderpool \u652f\u6301\u5bf9 KubeVirt Pod \u8fdb\u884c IP \u51b2\u7a81\u68c0\u6d4b\uff0c\u907f\u514d\u51fa\u73b0 IP \u51b2\u7a81\u3002\u4f46\u5bf9\u4e8e KubeVirt \u70ed\u8fc1\u79fb\u5e94\u7528\uff0c\u5f53\u5f00\u542f IP \u51b2\u7a81\u68c0\u6d4b\uff0c\u4f1a\u5bfc\u81f4\u70ed\u8fc1\u79fb\u865a\u62df\u673a\u65e0\u6cd5\u542f\u52a8\u3002\u6240\u4ee5\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u5373\u4f7f\u5f00\u542f\u4e86 IP \u51b2\u7a81\u68c0\u6d4b\u529f\u80fd\uff0cSpiderpool \u4e5f\u4e0d\u4f1a\u5bf9 KubeVirt \u8fdb\u884c IP \u51b2\u7a81\u68c0\u6d4b\u3002","title":"KubeVirt \u7f51\u7edc\u642d\u914d"},{"location":"usage/kubevirt-zh_CN/#kubevirt-vm","text":"KubeVirt VM \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a VM \u7684\u70ed\u8fc1\u79fb\uff0c\u671f\u671b\u8fc1\u79fb\u8fc7\u540e\u7684 VM \u4ecd\u80fd\u7ee7\u627f\u4e4b\u524d\u7684 IP \u5730\u5740\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u4e86\u91cd\u542f\u7684\u60c5\u51b5\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 VMI(VirtualMachineInstance) \u8d44\u6e90\u88ab\u5220\u9664\u7684\u60c5\u666f\u3002 \u6b64\u5916\uff0cKubeVirt VM \u56fa\u5b9a IP \u5730\u5740\u4e0e StatefulSet \u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e VM \uff0cPod \u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u7684\u540d\u5b57\u662f\u4f1a\u53d1\u751f\u53d8\u5316\u7684\uff0c\u4f46\u662f\u5176\u5bf9\u5e94\u7684 VMI \u4e0d\u8bba\u91cd\u542f\u4e0e\u5426\uff0c\u5176\u540d\u5b57\u90fd\u4e0d\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u4f1a\u4ee5 VM \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740(\u6211\u4eec\u7684 SpiderEndpoint \u8d44\u6e90\u5c06\u4f1a\u7ee7\u627f\u4f7f\u7528 VM \u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4ee5\u53ca\u540d\u5b57)\u3002 \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u6211\u4eec Spiderpool \u4f1a\u56e0\u6b64\u4ee5 Pod \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cVM \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba VM \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u5230\u76f8\u540c IP\u3002 \u82e5\u5173\u95ed\uff0cVM \u5bf9\u5e94\u7684 Pod \u5c06\u88ab\u5f53\u4f5c\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u901a\u8fc7 --set ipam.enableKubevirtStaticIP=false \u5173\u95ed\u3002","title":"KubeVirt VM \u56fa\u5b9a\u5730\u5740"},{"location":"usage/kubevirt-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/kubevirt-zh_CN/#_3","text":"\u4ee5\u4e0b\u6d41\u7a0b\u5c06\u4f1a\u6f14\u793a KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan CNI \u4ee5\u4f7f\u5f97 VM \u83b7\u5f97 underlay \u63a5\u5165\u80fd\u529b\uff0c\u5e76\u901a\u8fc7 Spiderpool \u5b9e\u73b0\u5206\u914d\u56fa\u5b9a IP \u7684\u529f\u80fd\u3002 Notice\uff1a\u5f53\u524d macvlan \u548c ipvlan \u5e76\u4e0d\u9002\u7528\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u56e0\u4e3a\u5bf9\u4e8e bridge \u7f51\u7edc\u6a21\u5f0f\u4f1a\u5c06 Pod \u7f51\u5361\u7684 MAC \u5730\u5740\u79fb\u52a8\u5230 VM\uff0c\u4f7f\u5f97 Pod \u4f7f\u7528\u53e6\u4e00\u4e2a\u4e0d\u540c\u7684\u5730\u5740\u3002\u800c macvlan \u548c ipvlan CNI \u8981\u6c42 Pod \u7684\u7f51\u5361\u63a5\u53e3\u5177\u6709\u539f\u59cb MAC \u5730\u5740\u3002","title":"\u6b65\u9aa4"},{"location":"usage/kubevirt-zh_CN/#spiderpool","text":"\u8bf7\u53c2\u8003 Macvlan Quick Start \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u53ef\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableKubevirtStaticIP=true","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/kubevirt-zh_CN/#kubevirt-vm_1","text":"","title":"\u521b\u5efa KubeVirt VM \u5e94\u7528"},{"location":"usage/kubevirt-zh_CN/#underlay","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u9009\u62e9\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u7684 CNI \u914d\u7f6e\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-cirros labels : kubevirt.io/vm : vm-cirros spec : runStrategy : Always template : metadata : annotations : v1.multus-cni.io/default-network : kube-system/macvlan-ens192 labels : kubevirt.io/vm : vm-cirros spec : domain : devices : disks : - name : containerdisk disk : bus : virtio - name : cloudinitdisk disk : bus : virtio interfaces : - name : default passt : {} resources : requests : memory : 64M networks : - name : default pod : {} volumes : - name : containerdisk containerDisk : image : quay.io/kubevirt/cirros-container-disk-demo - name : cloudinitdisk cloudInitNoCloud : userData : | #!/bin/sh echo 'printed from cloud-init userdata' \u6700\u7ec8\uff0c\u5728 KubeVirt VM \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VM Pod, \u89c2\u5bdf\u5230\u65b0\u7684 Pod \u7684 IP \u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VMI\uff0c\u89c2\u5bdf\u5230\u540e\u7eed\u65b0\u7684 Pod \u7684IP \u4e5f\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 VM \u4e5f\u53ef\u4e0e\u5176\u4ed6 underlay Pod \u7684\u901a\u4fe1\u3002 ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms VM \u4e5f\u53ef\u8bbf\u95ee cluster IP\u3002 ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes","title":"underlay \u5355\u7f51\u5361\u573a\u666f"},{"location":"usage/kubevirt-zh_CN/#underlay_1","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt bridge \u7f51\u7edc\u6a21\u5f0f\u642d\u914d ovs-cni \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippools : \u4e3a\u5e94\u7528\u6307\u5b9a\u6bcf\u5f20\u7f51\u5361\u9009\u62e9\u4f7f\u7528\u54ea\u4e9b IP \u6c60\u3002(\u4f60\u4e5f\u53ef\u4f7f\u7528 multus \u5b9e\u4f8b\u4e2d\u6307\u5b9a\u7684 CNI \u914d\u7f6e\u6587\u4ef6\u7ea7\u522b\u9ed8\u8ba4 IP \u6c60) \u8981\u6c42 multus \u5b9e\u4f8b kube-system/ovs-vlan30 \u548c kube-system/ovs-vlan40 \u542f\u7528 coordinator \u63d2\u4ef6\u6765\u534f\u8c03\u591a\u7f51\u5361\u9ed8\u8ba4\u8def\u7531\u3002 ovs-cni \u4e0d\u652f\u6301 clusterIP \u8bbf\u95ee\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk","title":"underlay \u591a\u7f51\u5361\u573a\u666f"},{"location":"usage/kubevirt-zh_CN/#_4","text":"Spiderpool \u80fd\u4fdd\u8bc1 KubeVirt VM Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 KubeVirt \u865a\u62df\u673a\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002\u5e76\u53ef\u914d\u5408 macvlan \u6216 OVS CNI \u4e0e KubeVirt \u7684\u591a\u79cd\u7f51\u7edc\u6a21\u5f0f\u5b9e\u73b0 VM underlay \u63a5\u5165\u80fd\u529b\u3002","title":"\u603b\u7ed3"},{"location":"usage/kubevirt/","text":"KubeVirt English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool ensures that KubeVirt VM Pods consistently obtain the same IP addresses during restart and rebuild processes. Integrate with KubeVirt Networking The Spiderpool underlay networking solution provides the ability to integrate with KubeVirt: For KubeVirt's passt network mode, it can be used in conjunction with the Spiderpool macvlan integrated solution. In this network mode, all Service Mesh functionalities are supported . However, only single NIC is supported, and live migration is not available. For KubeVirt's bridge network mode, it can be used in conjunction with OVS CNI. In this network mode, Service Mesh functionalities are not supported , but multiple NICs can be used, and live migration is not available. Spiderpool supports IP conflict detection for KubeVirt Pods to prevent IP conflicts. However, for KubeVirt live migration applications, enabling IP conflict detection will prevent the live migration virtual machine from starting. Therefore, in this scenario, even if the IP conflict detection feature is enabled, Spiderpool will not perform IP conflict detection for KubeVirt. Fix IP Address for KubeVirt VMs KubeVirt VMs may require fixed IP addresses in the following scenarios: During VM live migration, it is expected that the VM retains its previous IP address after migration. When the Pod associated with the VM resource undergoes a restart. When the VMI (VirtualMachineInstance) resource corresponding to the VM resource is deleted. It is important to note that the pattern of fixed IP addresses for KubeVirt VMs differs from that of StatefulSets: For VMs, the Pod name changes between restarts, but the VMI name remains unchanged regardless of restarts. Therefore, the fixed IPs will be recorded based on VMs. Specifically, SpiderEndpoint resource will be associated with the VM resource's namespace and name to record its fixed IP address. For StatefulSets, the Pod name remains the same between restarts, so Spiderpool records the fixed IP addresses based on Pods. This feature is enabled by default. When enabled, there are no restrictions. VMs can use a limited set of IP addresses from an IP pool to assign fixed IPs. However, regardless of whether a VM uses a fixed IP pool, its Pod can consistently obtain the same IP address. If disabled, Pods associated with VMs will be treated as stateless. During installation of Spiderpool using Helm, you can disable it by using --set ipam.enableKubevirtStaticIP=false . Prerequisites A ready Kubernetes cluster. Helm has been installed. Steps The following steps demonstrate how to use the passt network mode of KubeVirt with macvlan CNI to enable VMs to access the underlay network and assign fixed IPs using Spiderpool. Currently, macvlan and ipvlan are not suitable for KubeVirt's bridge network mode because in bridge network mode, the MAC address of the Pod interface is moved to the VM, causing the Pod to use a different address. However, macvlan and ipvlan CNI require the Pod's network interface to have the original MAC address. Install Spiderpool Please refer to Macvlan Quick Start for installing Spiderpool. Make sure to set the Helm installation option ipam.enableKubevirtStaticIP=true . Create KubeVirt VM Applications underlay single NIC situation In the example YAML below, we create 1 KubeVirt VM application with KubeVirt passt network mode + macvlan: v1.multus-cni.io/default-network : select the default network CNI configuration for the application. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: vm-cirros labels: kubevirt.io/vm: vm-cirros spec: runStrategy: Always template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default passt: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userData: | #!/bin/sh echo 'printed from cloud-init userdata' When creating a KubeVirt VM application, Spiderpool randomly selects an IP from the specified IP pool to establish a binding with the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 Upon restarting a Kubevirt VM Pod, the new Pod retains its original IP address as expected. ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 When restarting a Kubevirt VMI, new Pods also maintain their assigned IP addresses as expected. ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 The VM can communicate with other underlay Pods ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms The VM can access cluster IP addresses. ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes underlay multiple NICs situation In the example YAML below, we create 1 KubeVirt VM application with KubeVirt bridge network mode + ovs-cni : ipam.spidernet.io/ippools : select IPPools for every NIC.(You can also use the multus resource CNI configuration level default IPPools) The multus resource kube-system/ovs-vlan30 and kube-system/ovs-vlan40 must enable the coordinator plugin to solve the multiple interfaces default route problem. ovs-cni doesn't support clusterIP network connectivity access. apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk Summary Spiderpool guarantees that KubeVirt VM Pods consistently acquire the same IP addresses during restart and rebuild processes, meeting the fixed IP address requirements for Kubevirt VMs. It seamlessly integrates with macvlan or OVS CNI to enable VMs to access underlay networks.","title":"Kubevirt"},{"location":"usage/kubevirt/#kubevirt","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"KubeVirt"},{"location":"usage/kubevirt/#introduction","text":"Spiderpool ensures that KubeVirt VM Pods consistently obtain the same IP addresses during restart and rebuild processes.","title":"Introduction"},{"location":"usage/kubevirt/#integrate-with-kubevirt-networking","text":"The Spiderpool underlay networking solution provides the ability to integrate with KubeVirt: For KubeVirt's passt network mode, it can be used in conjunction with the Spiderpool macvlan integrated solution. In this network mode, all Service Mesh functionalities are supported . However, only single NIC is supported, and live migration is not available. For KubeVirt's bridge network mode, it can be used in conjunction with OVS CNI. In this network mode, Service Mesh functionalities are not supported , but multiple NICs can be used, and live migration is not available. Spiderpool supports IP conflict detection for KubeVirt Pods to prevent IP conflicts. However, for KubeVirt live migration applications, enabling IP conflict detection will prevent the live migration virtual machine from starting. Therefore, in this scenario, even if the IP conflict detection feature is enabled, Spiderpool will not perform IP conflict detection for KubeVirt.","title":"Integrate with KubeVirt Networking"},{"location":"usage/kubevirt/#fix-ip-address-for-kubevirt-vms","text":"KubeVirt VMs may require fixed IP addresses in the following scenarios: During VM live migration, it is expected that the VM retains its previous IP address after migration. When the Pod associated with the VM resource undergoes a restart. When the VMI (VirtualMachineInstance) resource corresponding to the VM resource is deleted. It is important to note that the pattern of fixed IP addresses for KubeVirt VMs differs from that of StatefulSets: For VMs, the Pod name changes between restarts, but the VMI name remains unchanged regardless of restarts. Therefore, the fixed IPs will be recorded based on VMs. Specifically, SpiderEndpoint resource will be associated with the VM resource's namespace and name to record its fixed IP address. For StatefulSets, the Pod name remains the same between restarts, so Spiderpool records the fixed IP addresses based on Pods. This feature is enabled by default. When enabled, there are no restrictions. VMs can use a limited set of IP addresses from an IP pool to assign fixed IPs. However, regardless of whether a VM uses a fixed IP pool, its Pod can consistently obtain the same IP address. If disabled, Pods associated with VMs will be treated as stateless. During installation of Spiderpool using Helm, you can disable it by using --set ipam.enableKubevirtStaticIP=false .","title":"Fix IP Address for KubeVirt VMs"},{"location":"usage/kubevirt/#prerequisites","text":"A ready Kubernetes cluster. Helm has been installed.","title":"Prerequisites"},{"location":"usage/kubevirt/#steps","text":"The following steps demonstrate how to use the passt network mode of KubeVirt with macvlan CNI to enable VMs to access the underlay network and assign fixed IPs using Spiderpool. Currently, macvlan and ipvlan are not suitable for KubeVirt's bridge network mode because in bridge network mode, the MAC address of the Pod interface is moved to the VM, causing the Pod to use a different address. However, macvlan and ipvlan CNI require the Pod's network interface to have the original MAC address.","title":"Steps"},{"location":"usage/kubevirt/#install-spiderpool","text":"Please refer to Macvlan Quick Start for installing Spiderpool. Make sure to set the Helm installation option ipam.enableKubevirtStaticIP=true .","title":"Install Spiderpool"},{"location":"usage/kubevirt/#create-kubevirt-vm-applications","text":"","title":"Create KubeVirt VM Applications"},{"location":"usage/kubevirt/#underlay-single-nic-situation","text":"In the example YAML below, we create 1 KubeVirt VM application with KubeVirt passt network mode + macvlan: v1.multus-cni.io/default-network : select the default network CNI configuration for the application. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: vm-cirros labels: kubevirt.io/vm: vm-cirros spec: runStrategy: Always template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default passt: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userData: | #!/bin/sh echo 'printed from cloud-init userdata' When creating a KubeVirt VM application, Spiderpool randomly selects an IP from the specified IP pool to establish a binding with the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 Upon restarting a Kubevirt VM Pod, the new Pod retains its original IP address as expected. ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 When restarting a Kubevirt VMI, new Pods also maintain their assigned IP addresses as expected. ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 The VM can communicate with other underlay Pods ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms The VM can access cluster IP addresses. ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes","title":"underlay single NIC situation"},{"location":"usage/kubevirt/#underlay-multiple-nics-situation","text":"In the example YAML below, we create 1 KubeVirt VM application with KubeVirt bridge network mode + ovs-cni : ipam.spidernet.io/ippools : select IPPools for every NIC.(You can also use the multus resource CNI configuration level default IPPools) The multus resource kube-system/ovs-vlan30 and kube-system/ovs-vlan40 must enable the coordinator plugin to solve the multiple interfaces default route problem. ovs-cni doesn't support clusterIP network connectivity access. apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk","title":"underlay multiple NICs situation"},{"location":"usage/kubevirt/#summary","text":"Spiderpool guarantees that KubeVirt VM Pods consistently acquire the same IP addresses during restart and rebuild processes, meeting the fixed IP address requirements for Kubevirt VMs. It seamlessly integrates with macvlan or OVS CNI to enable VMs to access underlay networks.","title":"Summary"},{"location":"usage/multi-interfaces-annotation/","text":"Pod annotation of multi-NIC When assigning multiple NICs to a Pod with Multus CNI , Spiderpool supports to specify the IP pools for each interface. This feature supports to implement by annotation ipam.spidernet.io/subnets and ipam.spidernet.io/ippools Get Started The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a Pod with two NICs with IP in different subnets. Set up Spiderpool Follow the guide installation to install Spiderpool. Set up Multus Configuration In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed: Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use. In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service . ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf-ens192 20s macvlan-conf-ens224 22s multiple NICs by subnet Create two Subnets to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-test-ens192 4 10 .6.0.1/16 0 10 subnet-test-ens224 4 10 .7.0.1/16 0 10 In the following example Yaml, 2 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-test-app-v4-eth0-b1a361c7e9df 4 10 .6.0.1/16 2 3 false false auto-test-app-v4-net1-b1a361c7e9df 4 10 .7.0.1/16 2 3 false false ~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.6.168.171-10.6.168.173\" ] ~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.7.168.171-10.7.168.173\" ] ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f4594ff67-fkqbw 1 /1 Running 0 40s 10 .6.168.172 node2 <none> <none> test-app-6f4594ff67-gwlx8 1 /1 Running 0 40s 10 .6.168.173 node1 <none> <none> ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a 3 : eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.168.172/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 26 :6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 5 : net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .7.168.173/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.173 lookup 100 32765 : from all to 10 .7.168.173/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 proto kernel scope link src 10 .7.168.173 10 .96.0.0/12 via 10 .6.168.123 dev veth0 multiple NICs by IPPool Create two IPPools to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 0 5 false false ippool-test-ens224 4 10 .7.0.1/16 0 5 false false In the following example Yaml, 1 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 1 5 false false ippool-test-ens224 4 10 .7.0.1/16 1 5 false false ~# kubectl get po -l app = ippool-test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-65f646574c-mpr47 1 /1 Running 0 6m18s 10 .6.168.175 node2 <none> <none> ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a ... 3 : eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff inet 10 .6.168.175/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 86 :ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff 5 : net1@eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff inet 10 .7.168.177/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.177 lookup 100 32765 : from all to 10 .7.168.177/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 scope link src 10 .7.168.177 10 .96.0.0/12 via 10 .6.168.123 dev veth0 Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\ --ignore-not-found = true","title":"MultipleInterfaces"},{"location":"usage/multi-interfaces-annotation/#pod-annotation-of-multi-nic","text":"When assigning multiple NICs to a Pod with Multus CNI , Spiderpool supports to specify the IP pools for each interface. This feature supports to implement by annotation ipam.spidernet.io/subnets and ipam.spidernet.io/ippools","title":"Pod annotation of multi-NIC"},{"location":"usage/multi-interfaces-annotation/#get-started","text":"The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a Pod with two NICs with IP in different subnets.","title":"Get Started"},{"location":"usage/multi-interfaces-annotation/#set-up-spiderpool","text":"Follow the guide installation to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/multi-interfaces-annotation/#set-up-multus-configuration","text":"In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed: Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use. In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service . ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf-ens192 20s macvlan-conf-ens224 22s","title":"Set up Multus Configuration"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-subnet","text":"Create two Subnets to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-test-ens192 4 10 .6.0.1/16 0 10 subnet-test-ens224 4 10 .7.0.1/16 0 10 In the following example Yaml, 2 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-test-app-v4-eth0-b1a361c7e9df 4 10 .6.0.1/16 2 3 false false auto-test-app-v4-net1-b1a361c7e9df 4 10 .7.0.1/16 2 3 false false ~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.6.168.171-10.6.168.173\" ] ~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.7.168.171-10.7.168.173\" ] ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f4594ff67-fkqbw 1 /1 Running 0 40s 10 .6.168.172 node2 <none> <none> test-app-6f4594ff67-gwlx8 1 /1 Running 0 40s 10 .6.168.173 node1 <none> <none> ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a 3 : eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.168.172/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 26 :6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 5 : net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .7.168.173/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.173 lookup 100 32765 : from all to 10 .7.168.173/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 proto kernel scope link src 10 .7.168.173 10 .96.0.0/12 via 10 .6.168.123 dev veth0","title":"multiple NICs by subnet"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-ippool","text":"Create two IPPools to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 0 5 false false ippool-test-ens224 4 10 .7.0.1/16 0 5 false false In the following example Yaml, 1 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 1 5 false false ippool-test-ens224 4 10 .7.0.1/16 1 5 false false ~# kubectl get po -l app = ippool-test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-65f646574c-mpr47 1 /1 Running 0 6m18s 10 .6.168.175 node2 <none> <none> ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a ... 3 : eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff inet 10 .6.168.175/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 86 :ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff 5 : net1@eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff inet 10 .7.168.177/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.177 lookup 100 32765 : from all to 10 .7.168.177/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 scope link src 10 .7.168.177 10 .96.0.0/12 via 10 .6.168.123 dev veth0","title":"multiple NICs by IPPool"},{"location":"usage/multi-interfaces-annotation/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/network-topology-zh_CN/","text":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u968f\u7740\u6570\u636e\u4e2d\u5fc3\u79c1\u6709\u4e91\u7684\u4e0d\u65ad\u666e\u53ca\uff0cUnderlay \u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u4f20\u8f93\u548c\u66f4\u597d\u7684\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u80fd\u529b\u3002\u7531\u4e8e\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9760\u3001\u5b89\u5168\u7b49\u7279\u6027\uff0cUnderlay \u5728\u4e0b\u5217\u573a\u666f\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7684\u5e94\u7528\uff1a \u5ef6\u65f6\u654f\u611f\u7684\u5e94\u7528\uff1a\u67d0\u4e9b\u7279\u5b9a\u884c\u4e1a\u6216\u5e94\u7528\uff08\u5982\u91d1\u878d\u4ea4\u6613\u3001\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u7b49\uff09\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u975e\u5e38\u654f\u611f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cUnderlay \u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u7269\u7406\u548c\u94fe\u8def\u5c42\u7684\u8fde\u63a5\u6765\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u65f6\u95f4\u3002\u8fd9\u79cd\u4f4e\u5ef6\u8fdf\u7684\u7279\u6027\u4f7f\u5f97 Underlay \u7f51\u7edc\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u5e94\u7528\u9700\u6c42\u7684\u7406\u60f3\u9009\u62e9\u3002 \u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff1a\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7 Underlay \u7f51\u7edc\u7684 IPAM \u5bf9\u5e94\u7528\u56fa\u5b9a\u51fa\u53e3 IP \u5730\u5740\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u63a7\u5236\u96c6\u7fa4\u4e0e\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002 \u5728 Underlay \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5f53\u5b83\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u800c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u5b50\u7f51\u65f6\uff0c\u5c06\u5bf9 IP \u5730\u5740\u7ba1\u7406\uff08IPAM\uff09\u5e26\u6765\u6311\u6218\uff0c\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u80fd\u5b9e\u73b0\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u7684\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u3002 \u4e00\u4e2a\u96c6\u7fa4\uff0c\u4f46\u96c6\u7fa4\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 10.6.1.0/24\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.16.2.0/24\u3002 \u5728\u4e0a\u8ff0\u7684\u573a\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u4e0e\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\uff0cSpiderpool \u7684 CR\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5b9e\u73b0 IP \u6c60\u4e0e\u8282\u70b9\u4e4b\u95f4\u7684\u4eb2\u548c\u6027\uff0c\u8ba9 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e00\u8282\u70b9\u65f6\uff0c\u80fd\u4ece\u8282\u70b9\u6240\u5728\u7684 Underlay \u5b50\u7f51\u4e2d\u83b7\u5f97 IP \u5730\u5740\uff0c\u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd. \u5b9e\u65bd\u8981\u6c42 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4 \u51c6\u5907\u4e00\u5957\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4\uff0c\u5982\u8282\u70b9 1 \u4f7f\u7528 10.6.0.0/16 \uff0c\u8282\u70b9 2 \u4f7f\u7528 10.7.0.0/16 \u5b50\u7f51\uff0c\u4ee5\u4e0b\u662f\u6240\u4f7f\u7528\u7684\u96c6\u7fa4\u4fe1\u606f\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u56fe\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 eth0 \u4f5c\u4e3a master \u7684\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u5e94\u4e0e\u96c6\u7fa4\u8de8\u8d8a\u7f51\u7edc\u533a\u7684\u8282\u70b9\u4e0a\u7684\u63a5\u53e3\u540d\u79f0\u5339\u914d\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m \u521b\u5efa IPPools Spiderpool \u7684 CRD\uff1aSpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a daemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0cSpiderpool \u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u8282\u70b9\u8de8\u7f51\u7edc\u533a\u57df\u7684\u573a\u666f\u5206\u914d IP \u65f6\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684 IPPool.spec.nodeAffinity \u6ce8\u89e3\uff0c Pod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u53ef\u4ee5\u901a\u8fc7\u5907\u9009\u6c60 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u3002 v1.multus-cni.io/default-network\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5b8c\u6210\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0 Pod \u7684 IP \u5c5e\u4e8e Pod \u6240\u5728\u8282\u70b9\u7684\u5b50\u7f51\u5185\uff0c\u6240\u5bf9\u5e94\u7684 IP \u6c60\u4e3a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e86 IP \u5730\u5740\u3002 ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false \u8de8\u7f51\u7edc\u533a\u57df\u7684 Pod \u4e0e Pod \u4e4b\u95f4\u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms \u603b\u7ed3 \u4e0d\u540c\u7f51\u7edc\u533a\u57df\u7684 Pod \u80fd\u591f\u6b63\u5e38\u901a\u4fe1\uff0cSpiderpool \u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u9700\u6c42\u3002","title":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d"},{"location":"usage/network-topology-zh_CN/#ip","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d"},{"location":"usage/network-topology-zh_CN/#_1","text":"\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u79c1\u6709\u4e91\u7684\u4e0d\u65ad\u666e\u53ca\uff0cUnderlay \u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u4f20\u8f93\u548c\u66f4\u597d\u7684\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u80fd\u529b\u3002\u7531\u4e8e\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9760\u3001\u5b89\u5168\u7b49\u7279\u6027\uff0cUnderlay \u5728\u4e0b\u5217\u573a\u666f\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7684\u5e94\u7528\uff1a \u5ef6\u65f6\u654f\u611f\u7684\u5e94\u7528\uff1a\u67d0\u4e9b\u7279\u5b9a\u884c\u4e1a\u6216\u5e94\u7528\uff08\u5982\u91d1\u878d\u4ea4\u6613\u3001\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u7b49\uff09\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u975e\u5e38\u654f\u611f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cUnderlay \u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u7269\u7406\u548c\u94fe\u8def\u5c42\u7684\u8fde\u63a5\u6765\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u65f6\u95f4\u3002\u8fd9\u79cd\u4f4e\u5ef6\u8fdf\u7684\u7279\u6027\u4f7f\u5f97 Underlay \u7f51\u7edc\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u5e94\u7528\u9700\u6c42\u7684\u7406\u60f3\u9009\u62e9\u3002 \u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff1a\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7 Underlay \u7f51\u7edc\u7684 IPAM \u5bf9\u5e94\u7528\u56fa\u5b9a\u51fa\u53e3 IP \u5730\u5740\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u63a7\u5236\u96c6\u7fa4\u4e0e\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002 \u5728 Underlay \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5f53\u5b83\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u800c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u5b50\u7f51\u65f6\uff0c\u5c06\u5bf9 IP \u5730\u5740\u7ba1\u7406\uff08IPAM\uff09\u5e26\u6765\u6311\u6218\uff0c\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u80fd\u5b9e\u73b0\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u7684\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/network-topology-zh_CN/#_2","text":"Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u3002 \u4e00\u4e2a\u96c6\u7fa4\uff0c\u4f46\u96c6\u7fa4\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 10.6.1.0/24\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.16.2.0/24\u3002 \u5728\u4e0a\u8ff0\u7684\u573a\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u4e0e\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\uff0cSpiderpool \u7684 CR\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5b9e\u73b0 IP \u6c60\u4e0e\u8282\u70b9\u4e4b\u95f4\u7684\u4eb2\u548c\u6027\uff0c\u8ba9 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e00\u8282\u70b9\u65f6\uff0c\u80fd\u4ece\u8282\u70b9\u6240\u5728\u7684 Underlay \u5b50\u7f51\u4e2d\u83b7\u5f97 IP \u5730\u5740\uff0c\u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd.","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/network-topology-zh_CN/#_3","text":"\u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/network-topology-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/network-topology-zh_CN/#_5","text":"\u51c6\u5907\u4e00\u5957\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4\uff0c\u5982\u8282\u70b9 1 \u4f7f\u7528 10.6.0.0/16 \uff0c\u8282\u70b9 2 \u4f7f\u7528 10.7.0.0/16 \u5b50\u7f51\uff0c\u4ee5\u4e0b\u662f\u6240\u4f7f\u7528\u7684\u96c6\u7fa4\u4fe1\u606f\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u56fe\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none>","title":"\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4"},{"location":"usage/network-topology-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/network-topology-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 eth0 \u4f5c\u4e3a master \u7684\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u5e94\u4e0e\u96c6\u7fa4\u8de8\u8d8a\u7f51\u7edc\u533a\u7684\u8282\u70b9\u4e0a\u7684\u63a5\u53e3\u540d\u79f0\u5339\u914d\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/network-topology-zh_CN/#ippools","text":"Spiderpool \u7684 CRD\uff1aSpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF","title":"\u521b\u5efa IPPools"},{"location":"usage/network-topology-zh_CN/#_6","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a daemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0cSpiderpool \u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u8282\u70b9\u8de8\u7f51\u7edc\u533a\u57df\u7684\u573a\u666f\u5206\u914d IP \u65f6\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684 IPPool.spec.nodeAffinity \u6ce8\u89e3\uff0c Pod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u53ef\u4ee5\u901a\u8fc7\u5907\u9009\u6c60 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u3002 v1.multus-cni.io/default-network\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5b8c\u6210\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0 Pod \u7684 IP \u5c5e\u4e8e Pod \u6240\u5728\u8282\u70b9\u7684\u5b50\u7f51\u5185\uff0c\u6240\u5bf9\u5e94\u7684 IP \u6c60\u4e3a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e86 IP \u5730\u5740\u3002 ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false \u8de8\u7f51\u7edc\u533a\u57df\u7684 Pod \u4e0e Pod \u4e4b\u95f4\u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/network-topology-zh_CN/#_7","text":"\u4e0d\u540c\u7f51\u7edc\u533a\u57df\u7684 Pod \u80fd\u591f\u6b63\u5e38\u901a\u4fe1\uff0cSpiderpool \u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u9700\u6c42\u3002","title":"\u603b\u7ed3"},{"location":"usage/network-topology/","text":"Application across network zones English | \u7b80\u4f53\u4e2d\u6587 Introduce The rising popularity of private cloud data centers has made underlay networks essential components of data center network architecture by offering efficient network transmission and improved network topology management capabilities. Underlay is widely used in the following scenarios due to its low latency, reliability, and security: Latency-sensitive applications: applications in specific industries, such as financial trading and real-time video transmission, are highly sensitive to network latency. Underlay networks directly control physical and link-layer connections to reduce data transmission time, providing an ideal solution for these applications. Firewall security control and management: firewalls are often used to manage north-south traffic, namely communication between internal and external networks, by checking, filtering, and restricting communication traffic. IP address management (IPAM) solutions of underlay networks that allocate fixed egress IP addresses for applications can provide better communication management and control between the cluster and external networks, further enhancing overall network security. In the cluster of the Underlay network, when its nodes are distributed in different regions or data centers, and some node regions can only use specific subnets, it will bring challenges to IP address management (IPAM). This article will introduce a method that can realize A complete underlay network solution for IP allocation across network regions. Project Functions Spiderpool provides the function of node topology, which can help solve the problem of IP allocation across network regions. Its implementation principle is as follows. A cluster, but the nodes of the cluster are distributed in different regions or data centers, some nodes' regions can only use the subnet 10.6.1.0/24, and some nodes' regions can only use the subnet 172.16.2.0/24. In the appeal scenario, when an application deploys copies across subnets, IPAM is required to assign IP addresses that match the subnet to different Pods under the same application on different nodes. Spiderpool's CR: SpiderIPPool The nodeName field is provided to realize the affinity between the IP pool and the node, so that when the Pod is scheduled to a certain node, it can obtain the IP address from the Underlay subnet where the node is located, and realize the node topology function. Implementation Requirements Installed Helm . Steps Clusters spanning network regions Prepare a set of clusters that span the network area. For example, node 1 uses 10.6.0.0/16 and node 2 uses 10.7.0.0/16 subnet. The following is the cluster information and network topology used: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> ~# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS controller-node-1 Ready control-plane,master 1h v1.25.3 node-subnet = subnet-6, ... worker-node-1 Ready <none> 1h v1.25.3 node-subnet = subnet-7, ... Install Spiderpool Install Spiderpool via helm. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Verify the installation\uff1a ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m Install CNI configuration To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an IPvlan SpiderMultusConfig configuration: master: In this example the interface eth0 is used as the parameter for master, this parameter should match the interface name on the nodes where the cluster spans network zones. MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } In the example of this article, use the above configuration to create the following two Macvlan SpiderMultusConfig, which will be automatically generated based on the Multus NetworkAttachmentDefinition CR, which corresponds to the eth0 network card of the host. ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m Create IPPools CRD of Spiderpool: SpiderIPPool provides nodeName field. When nodeName is not empty, when Pod starts on a node and tries to allocate IP from SpiderIPPool, if the node where Pod is located matches the nodeName setting, it can be retrieved from SpiderIPPool The IP is allocated successfully, otherwise the IP cannot be allocated from the SpiderIPPool. When nodeName is empty, Spiderpool does not enforce any allocation limit on Pods. As above, using the following Yaml, create 2 SpiderIPPools that will provide IP addresses to Pods on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF Create the application The following sample yaml creates a daemonSet application where: ipam.spidernet.io/ippool: It is used to specify the IP pool of Spiderpool. Multiple IP pools can be set as alternative pools. Spiderpool will try to allocate IP addresses in sequence according to the order of elements in the \"IP pool array\". When assigning IP in the network area scenario, if the node to which the application copy is scheduled meets the IPPool.spec.nodeAffinity annotation of the first IP pool, the Pod will obtain the IP allocation from the pool. Select the IP pool in the selected pool and continue to allocate IPs for Pods until all candidate pools fail to be screened. You can learn more about usage with alternative pools. v1.multus-cni.io/default-network: Used to specify the NetworkAttachmentDefinition configuration of Multus, which will create a default network card for the application. ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF After creating an application, it can be observed that each Pod's IP address is assigned from an IP pool belonging to the same subnet as its node. ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false Communication between Pods across network zones: ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms Summarize Pods in different network areas can communicate normally, and Spiderpool can well meet the IP allocation requirements based on cross-network areas.","title":"Node-based Topology"},{"location":"usage/network-topology/#application-across-network-zones","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Application across network zones"},{"location":"usage/network-topology/#introduce","text":"The rising popularity of private cloud data centers has made underlay networks essential components of data center network architecture by offering efficient network transmission and improved network topology management capabilities. Underlay is widely used in the following scenarios due to its low latency, reliability, and security: Latency-sensitive applications: applications in specific industries, such as financial trading and real-time video transmission, are highly sensitive to network latency. Underlay networks directly control physical and link-layer connections to reduce data transmission time, providing an ideal solution for these applications. Firewall security control and management: firewalls are often used to manage north-south traffic, namely communication between internal and external networks, by checking, filtering, and restricting communication traffic. IP address management (IPAM) solutions of underlay networks that allocate fixed egress IP addresses for applications can provide better communication management and control between the cluster and external networks, further enhancing overall network security. In the cluster of the Underlay network, when its nodes are distributed in different regions or data centers, and some node regions can only use specific subnets, it will bring challenges to IP address management (IPAM). This article will introduce a method that can realize A complete underlay network solution for IP allocation across network regions.","title":"Introduce"},{"location":"usage/network-topology/#project-functions","text":"Spiderpool provides the function of node topology, which can help solve the problem of IP allocation across network regions. Its implementation principle is as follows. A cluster, but the nodes of the cluster are distributed in different regions or data centers, some nodes' regions can only use the subnet 10.6.1.0/24, and some nodes' regions can only use the subnet 172.16.2.0/24. In the appeal scenario, when an application deploys copies across subnets, IPAM is required to assign IP addresses that match the subnet to different Pods under the same application on different nodes. Spiderpool's CR: SpiderIPPool The nodeName field is provided to realize the affinity between the IP pool and the node, so that when the Pod is scheduled to a certain node, it can obtain the IP address from the Underlay subnet where the node is located, and realize the node topology function.","title":"Project Functions"},{"location":"usage/network-topology/#implementation-requirements","text":"Installed Helm .","title":"Implementation Requirements"},{"location":"usage/network-topology/#steps","text":"","title":"Steps"},{"location":"usage/network-topology/#clusters-spanning-network-regions","text":"Prepare a set of clusters that span the network area. For example, node 1 uses 10.6.0.0/16 and node 2 uses 10.7.0.0/16 subnet. The following is the cluster information and network topology used: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> ~# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS controller-node-1 Ready control-plane,master 1h v1.25.3 node-subnet = subnet-6, ... worker-node-1 Ready <none> 1h v1.25.3 node-subnet = subnet-7, ...","title":"Clusters spanning network regions"},{"location":"usage/network-topology/#install-spiderpool","text":"Install Spiderpool via helm. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Verify the installation\uff1a ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m","title":"Install Spiderpool"},{"location":"usage/network-topology/#install-cni-configuration","text":"To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an IPvlan SpiderMultusConfig configuration: master: In this example the interface eth0 is used as the parameter for master, this parameter should match the interface name on the nodes where the cluster spans network zones. MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } In the example of this article, use the above configuration to create the following two Macvlan SpiderMultusConfig, which will be automatically generated based on the Multus NetworkAttachmentDefinition CR, which corresponds to the eth0 network card of the host. ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"Install CNI configuration"},{"location":"usage/network-topology/#create-ippools","text":"CRD of Spiderpool: SpiderIPPool provides nodeName field. When nodeName is not empty, when Pod starts on a node and tries to allocate IP from SpiderIPPool, if the node where Pod is located matches the nodeName setting, it can be retrieved from SpiderIPPool The IP is allocated successfully, otherwise the IP cannot be allocated from the SpiderIPPool. When nodeName is empty, Spiderpool does not enforce any allocation limit on Pods. As above, using the following Yaml, create 2 SpiderIPPools that will provide IP addresses to Pods on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF","title":"Create IPPools"},{"location":"usage/network-topology/#create-the-application","text":"The following sample yaml creates a daemonSet application where: ipam.spidernet.io/ippool: It is used to specify the IP pool of Spiderpool. Multiple IP pools can be set as alternative pools. Spiderpool will try to allocate IP addresses in sequence according to the order of elements in the \"IP pool array\". When assigning IP in the network area scenario, if the node to which the application copy is scheduled meets the IPPool.spec.nodeAffinity annotation of the first IP pool, the Pod will obtain the IP allocation from the pool. Select the IP pool in the selected pool and continue to allocate IPs for Pods until all candidate pools fail to be screened. You can learn more about usage with alternative pools. v1.multus-cni.io/default-network: Used to specify the NetworkAttachmentDefinition configuration of Multus, which will create a default network card for the application. ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF After creating an application, it can be observed that each Pod's IP address is assigned from an IP pool belonging to the same subnet as its node. ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false Communication between Pods across network zones: ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms","title":"Create the application"},{"location":"usage/network-topology/#summarize","text":"Pods in different network areas can communicate normally, and Spiderpool can well meet the IP allocation requirements based on cross-network areas.","title":"Summarize"},{"location":"usage/operator-zh_CN/","text":"IPAM \u5bf9 operator \u652f\u6301 \u7b80\u4f53\u4e2d\u6587 \uff5c English \u63cf\u8ff0 Operator \u901a\u5e38\u7528\u4e8e\u5b9e\u73b0\u81ea\u5b9a\u4e49\u63a7\u5236\u5668\u3002Spiderpool \u652f\u6301\u4e3a\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u521b\u5efa\u7684 Pod \u5206\u914d IP\u3002\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff1a \u624b\u52a8 IPPool \u7ba1\u7406\u5458\u53ef\u4ee5\u521b\u5efa IPPool \u5bf9\u8c61\u5e76\u4e3a Pod \u5206\u914d IP\u3002 \u81ea\u52a8 IPPool Spiderpool \u652f\u6301\u4e3a\u5e94\u7528\u7a0b\u5e8f\u81ea\u52a8\u7ba1\u7406 IPPool\uff0c\u5b83\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u521b\u5efa\u3001\u5220\u9664\u3001\u6269\u5c55\u548c\u7f29\u5c0f\u4e00\u4e2a\u4e13\u7528\u7684 SpiderIPPool \u5bf9\u8c61\uff0c\u5e76\u4e3a\u5176\u5206\u914d\u9759\u6001 IP \u5730\u5740\u3002 \u6b64\u529f\u80fd\u4f7f\u7528 informer \u6280\u672f\u6765\u76d1\u89c6\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u6790\u5176\u526f\u672c\u6570\u91cf\u5e76\u7ba1\u7406 SpiderIPPool \u5bf9\u8c61\uff0c\u5b83\u4e0e Kubernetes \u539f\u751f\u63a7\u5236\u5668\uff08\u5982 Deployment\u3001ReplicaSet\u3001StatefulSet\u3001Job\u3001CronJob\u3001DaemonSet\uff09\u914d\u5408\u826f\u597d\u3002 \u6b64\u529f\u80fd\u4e5f\u652f\u6301\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\uff0c\u4f46 Spiderpool \u65e0\u6cd5\u89e3\u6790\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u7684\u5bf9\u8c61 yaml\uff0c\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff1a \u4e0d\u652f\u6301\u81ea\u52a8\u6269\u5c55\u548c\u7f29\u5c0f IP \u4e0d\u652f\u6301\u81ea\u52a8\u5220\u9664 IPPool \u672a\u6765\uff0cSpiderpool \u53ef\u80fd\u4f1a\u652f\u6301\u81ea\u52a8 IPPool \u7684\u6240\u6709\u64cd\u4f5c\u3002 \u53e6\u4e00\u4e2a\u5173\u4e8e\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u7684\u95ee\u9898\u662f\u6709\u72b6\u6001\u6216\u65e0\u72b6\u6001\u3002\u56e0\u4e3a Spiderpool \u65e0\u6cd5\u5224\u65ad\u7531\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u521b\u5efa\u7684\u5e94\u7528\u7a0b\u5e8f\u662f\u5426\u6709\u72b6\u6001\u3002 \u6240\u4ee5 Spiderpool \u5c06\u5b83\u4eec\u89c6\u4e3a \u65e0\u72b6\u6001 Pod\uff0c\u5982 Deployment \uff0c\u8fd9\u610f\u5473\u7740\u7531\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u521b\u5efa\u7684 Pod \u80fd\u591f\u50cf Deployment \u4e00\u6837\u56fa\u5b9a IP \u8303\u56f4\uff0c\u4f46\u4e0d\u80fd\u50cf Statefulset \u4e00\u6837\u5c06\u6bcf\u4e2a Pod \u7ed1\u5b9a\u5230\u7279\u5b9a\u7684 IP \u5730\u5740\u3002 \u5165\u95e8 \u5c06\u4f7f\u7528 OpenKruise \u6765\u6f14\u793a Spiderpool \u5982\u4f55\u652f\u6301 operator\u3002 \u8bbe\u7f6e Spiderpool \u8bf7\u53c2\u9605 \u5b89\u88c5 \u4ee5\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002 \u8bbe\u7f6e OpenKruise \u8bf7\u53c2\u8003 OpenKruise \u901a\u8fc7 \u624b\u52a8 IPPool \u65b9\u5f0f\u521b\u5efa Pod \u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49 IPPool\u3002 kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 \u521b\u5efa\u4e00\u4e2a\u5177\u6709 3 \u4e2a\u526f\u672c\u7684 OpenKruise CloneSet\uff0c\u5e76\u901a\u8fc7\u6ce8\u91ca ipam.spidernet.io/ippool \u6307\u5b9a IPPool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u5982\u9884\u671f\uff0cOpenKruise CloneSet custom-kruise-cloneset \u7684 Pod \u5c06\u4ece IPPool custom-ipv4-ippool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2 \u901a\u8fc7 \u81ea\u52a8 IPPool \u65b9\u5f0f\u521b\u5efa Pod \u521b\u5efa\u4e00\u4e2a\u5177\u6709 3 \u4e2a\u526f\u672c\u7684 OpenKruise CloneSet\uff0c\u5e76\u901a\u8fc7\u6ce8\u91ca ipam.spidernet.io/subnet \u6307\u5b9a\u5b50\u7f51 apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u6ce8\u610f\uff1a \u5fc5\u987b\u4e3a\u81ea\u52a8\u521b\u5efa\u7684 IPPool \u6307\u5b9a\u56fa\u5b9a\u7684 IP \u6570\u91cf\uff0c\u5982 ipam.spidernet.io/ippool-ip-number: \"5\" \u3002 \u56e0\u4e3a Spiderpool \u65e0\u6cd5\u77e5\u9053\u526f\u672c\u6570\u91cf\uff0c\u6240\u4ee5\u4e0d\u652f\u6301\u7c7b\u4f3c ipam.spidernet.io/ippool-ip-number: \"+5\" \u7684\u6ce8\u91ca\u3002 \u68c0\u67e5\u72b6\u6001 \u5982\u9884\u671f\uff0cSpiderpool \u5c06\u4ece subnet-demo-v4 \u548c subnet-demo-v6 \u5bf9\u8c61\u4e2d\u521b\u5efa\u81ea\u52a8\u521b\u5efa\u7684 IPPool\u3002 OpenKruise CloneSet custom-kruise-cloneset \u7684 Pod \u5c06\u4ece\u521b\u5efa\u7684 IPPool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"IPAM \u5bf9 operator \u652f\u6301"},{"location":"usage/operator-zh_CN/#ipam-operator","text":"\u7b80\u4f53\u4e2d\u6587 \uff5c English","title":"IPAM \u5bf9 operator \u652f\u6301"},{"location":"usage/operator-zh_CN/#_1","text":"Operator \u901a\u5e38\u7528\u4e8e\u5b9e\u73b0\u81ea\u5b9a\u4e49\u63a7\u5236\u5668\u3002Spiderpool \u652f\u6301\u4e3a\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u521b\u5efa\u7684 Pod \u5206\u914d IP\u3002\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff1a \u624b\u52a8 IPPool \u7ba1\u7406\u5458\u53ef\u4ee5\u521b\u5efa IPPool \u5bf9\u8c61\u5e76\u4e3a Pod \u5206\u914d IP\u3002 \u81ea\u52a8 IPPool Spiderpool \u652f\u6301\u4e3a\u5e94\u7528\u7a0b\u5e8f\u81ea\u52a8\u7ba1\u7406 IPPool\uff0c\u5b83\u53ef\u4ee5\u4e3a\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u521b\u5efa\u3001\u5220\u9664\u3001\u6269\u5c55\u548c\u7f29\u5c0f\u4e00\u4e2a\u4e13\u7528\u7684 SpiderIPPool \u5bf9\u8c61\uff0c\u5e76\u4e3a\u5176\u5206\u914d\u9759\u6001 IP \u5730\u5740\u3002 \u6b64\u529f\u80fd\u4f7f\u7528 informer \u6280\u672f\u6765\u76d1\u89c6\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u6790\u5176\u526f\u672c\u6570\u91cf\u5e76\u7ba1\u7406 SpiderIPPool \u5bf9\u8c61\uff0c\u5b83\u4e0e Kubernetes \u539f\u751f\u63a7\u5236\u5668\uff08\u5982 Deployment\u3001ReplicaSet\u3001StatefulSet\u3001Job\u3001CronJob\u3001DaemonSet\uff09\u914d\u5408\u826f\u597d\u3002 \u6b64\u529f\u80fd\u4e5f\u652f\u6301\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\uff0c\u4f46 Spiderpool \u65e0\u6cd5\u89e3\u6790\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u7684\u5bf9\u8c61 yaml\uff0c\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff1a \u4e0d\u652f\u6301\u81ea\u52a8\u6269\u5c55\u548c\u7f29\u5c0f IP \u4e0d\u652f\u6301\u81ea\u52a8\u5220\u9664 IPPool \u672a\u6765\uff0cSpiderpool \u53ef\u80fd\u4f1a\u652f\u6301\u81ea\u52a8 IPPool \u7684\u6240\u6709\u64cd\u4f5c\u3002 \u53e6\u4e00\u4e2a\u5173\u4e8e\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u7684\u95ee\u9898\u662f\u6709\u72b6\u6001\u6216\u65e0\u72b6\u6001\u3002\u56e0\u4e3a Spiderpool \u65e0\u6cd5\u5224\u65ad\u7531\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u521b\u5efa\u7684\u5e94\u7528\u7a0b\u5e8f\u662f\u5426\u6709\u72b6\u6001\u3002 \u6240\u4ee5 Spiderpool \u5c06\u5b83\u4eec\u89c6\u4e3a \u65e0\u72b6\u6001 Pod\uff0c\u5982 Deployment \uff0c\u8fd9\u610f\u5473\u7740\u7531\u975e Kubernetes \u539f\u751f\u63a7\u5236\u5668\u521b\u5efa\u7684 Pod \u80fd\u591f\u50cf Deployment \u4e00\u6837\u56fa\u5b9a IP \u8303\u56f4\uff0c\u4f46\u4e0d\u80fd\u50cf Statefulset \u4e00\u6837\u5c06\u6bcf\u4e2a Pod \u7ed1\u5b9a\u5230\u7279\u5b9a\u7684 IP \u5730\u5740\u3002","title":"\u63cf\u8ff0"},{"location":"usage/operator-zh_CN/#_2","text":"\u5c06\u4f7f\u7528 OpenKruise \u6765\u6f14\u793a Spiderpool \u5982\u4f55\u652f\u6301 operator\u3002","title":"\u5165\u95e8"},{"location":"usage/operator-zh_CN/#spiderpool","text":"\u8bf7\u53c2\u9605 \u5b89\u88c5 \u4ee5\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002","title":"\u8bbe\u7f6e Spiderpool"},{"location":"usage/operator-zh_CN/#openkruise","text":"\u8bf7\u53c2\u8003 OpenKruise","title":"\u8bbe\u7f6e OpenKruise"},{"location":"usage/operator-zh_CN/#ippool-pod","text":"\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49 IPPool\u3002 kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 \u521b\u5efa\u4e00\u4e2a\u5177\u6709 3 \u4e2a\u526f\u672c\u7684 OpenKruise CloneSet\uff0c\u5e76\u901a\u8fc7\u6ce8\u91ca ipam.spidernet.io/ippool \u6307\u5b9a IPPool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u5982\u9884\u671f\uff0cOpenKruise CloneSet custom-kruise-cloneset \u7684 Pod \u5c06\u4ece IPPool custom-ipv4-ippool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2","title":"\u901a\u8fc7 \u624b\u52a8 IPPool \u65b9\u5f0f\u521b\u5efa Pod"},{"location":"usage/operator-zh_CN/#ippool-pod_1","text":"\u521b\u5efa\u4e00\u4e2a\u5177\u6709 3 \u4e2a\u526f\u672c\u7684 OpenKruise CloneSet\uff0c\u5e76\u901a\u8fc7\u6ce8\u91ca ipam.spidernet.io/subnet \u6307\u5b9a\u5b50\u7f51 apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u6ce8\u610f\uff1a \u5fc5\u987b\u4e3a\u81ea\u52a8\u521b\u5efa\u7684 IPPool \u6307\u5b9a\u56fa\u5b9a\u7684 IP \u6570\u91cf\uff0c\u5982 ipam.spidernet.io/ippool-ip-number: \"5\" \u3002 \u56e0\u4e3a Spiderpool \u65e0\u6cd5\u77e5\u9053\u526f\u672c\u6570\u91cf\uff0c\u6240\u4ee5\u4e0d\u652f\u6301\u7c7b\u4f3c ipam.spidernet.io/ippool-ip-number: \"+5\" \u7684\u6ce8\u91ca\u3002 \u68c0\u67e5\u72b6\u6001 \u5982\u9884\u671f\uff0cSpiderpool \u5c06\u4ece subnet-demo-v4 \u548c subnet-demo-v6 \u5bf9\u8c61\u4e2d\u521b\u5efa\u81ea\u52a8\u521b\u5efa\u7684 IPPool\u3002 OpenKruise CloneSet custom-kruise-cloneset \u7684 Pod \u5c06\u4ece\u521b\u5efa\u7684 IPPool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"\u901a\u8fc7 \u81ea\u52a8 IPPool \u65b9\u5f0f\u521b\u5efa Pod"},{"location":"usage/operator/","text":"IPAM for operator English | \u7b80\u4f53\u4e2d\u6587 Description Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to Pods created not by kubernetes-native controller. There are two ways to do this: Manual ippool The administrator could create ippool object and assign IP to Pods. Automatical ippool Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application. This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet. This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: does not support automatically scale up and down the IP does not support automatically delete the ippool In the future, spiderpool may support all operation of automatical ippool. Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as stateless Pod like Deployment , this means Pods created by none kubernetes-native controller is able to fix the IP range like Deployment , but not able to bind each Pod to a specific IP address like Statefulset . Get Started It will use OpenKruise to demonstrate how Spiderpool supports operator. Set up Spiderpool See installation for more details. Set up OpenKruise Please refer to OpenKruise Create Pod by Manual ippool way Create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations ipam.spidernet.io/ippool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2 Create Pod by Automatical ippool way Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations ipam.spidernet.io/subnet apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] NOTICE: You must specify a fixed IP number for auto-created IPPool like ipam.spidernet.io/ippool-ip-number: \"5\" . Because Spiderpool has no idea about the replica number, so it does not support annotation like ipam.spidernet.io/ippool-ip-number: \"+5\" . Check status As expected, Spiderpool will create auto-created IPPool from subnet-demo-v4 and subnet-demo-v6 objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"IPAM for operator"},{"location":"usage/operator/#ipam-for-operator","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"IPAM for operator"},{"location":"usage/operator/#description","text":"Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to Pods created not by kubernetes-native controller. There are two ways to do this: Manual ippool The administrator could create ippool object and assign IP to Pods. Automatical ippool Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application. This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet. This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: does not support automatically scale up and down the IP does not support automatically delete the ippool In the future, spiderpool may support all operation of automatical ippool. Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as stateless Pod like Deployment , this means Pods created by none kubernetes-native controller is able to fix the IP range like Deployment , but not able to bind each Pod to a specific IP address like Statefulset .","title":"Description"},{"location":"usage/operator/#get-started","text":"It will use OpenKruise to demonstrate how Spiderpool supports operator.","title":"Get Started"},{"location":"usage/operator/#set-up-spiderpool","text":"See installation for more details.","title":"Set up Spiderpool"},{"location":"usage/operator/#set-up-openkruise","text":"Please refer to OpenKruise","title":"Set up OpenKruise"},{"location":"usage/operator/#create-pod-by-manual-ippool-way","text":"Create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations ipam.spidernet.io/ippool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2","title":"Create Pod by Manual ippool way"},{"location":"usage/operator/#create-pod-by-automatical-ippool-way","text":"Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations ipam.spidernet.io/subnet apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] NOTICE: You must specify a fixed IP number for auto-created IPPool like ipam.spidernet.io/ippool-ip-number: \"5\" . Because Spiderpool has no idea about the replica number, so it does not support annotation like ipam.spidernet.io/ippool-ip-number: \"+5\" . Check status As expected, Spiderpool will create auto-created IPPool from subnet-demo-v4 and subnet-demo-v6 objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"Create Pod by Automatical ippool way"},{"location":"usage/rdma-metrics-zh_CN/","text":"RDMA \u6307\u6807 English | \u7b80\u4f53\u4e2d\u6587 RDMA \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7f51\u7edc\u901a\u4fe1\u6280\u672f\uff0c\u5141\u8bb8\u4e00\u53f0\u8ba1\u7b97\u673a\u76f4\u63a5\u8bbf\u95ee\u53e6\u4e00\u53f0\u8ba1\u7b97\u673a\u7684\u5185\u5b58\uff0c\u65e0\u9700\u64cd\u4f5c\u7cfb\u7edf\u4ecb\u5165\uff0c\u4ece\u800c\u51cf\u5c11\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u901f\u5ea6\u548c\u6548\u7387\u3002RDMA \u652f\u6301\u9ad8\u901f\u6570\u636e\u4f20\u8f93\uff0c\u51cf\u5c11 CPU \u8d1f\u8f7d\uff0c\u975e\u5e38\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u7684\u573a\u666f\u3002 \u5728 Kubernetes \u96c6\u7fa4\u4e2d\uff0cspiderpool CNI \u652f\u6301 RoCE \u548c IB 2 \u79cd RDMA \u573a\u666f\uff0cPod \u53ef\u4ee5\u901a\u8fc7\u5171\u4eab\u548c\u72ec\u5360\u7684\u65b9\u5f0f\u4f7f\u7528 RDMA \u7f51\u5361\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u65b9\u5f0f\u6765\u4f7f\u7528 RDMA \u7f51\u5361\u3002 spiderpool \u540c\u65f6\u63d0\u4f9b\u4e86 RDMA exporter \u529f\u80fd\u548c grafana \u76d1\u63a7\u9762\u677f\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7 Pod/Node RDMA \u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5305\u62ec\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u3001\u4e22\u5305\u7387\u7b49\uff0c\u53ef\u4ee5\u65f6\u53d1\u73b0\u95ee\u9898\u5e76\u91c7\u53d6\u63aa\u65bd\u89e3\u51b3\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002 RDMA \u6307\u6807\u7684\u5e38\u89c1\u573a\u666f \u6027\u80fd\u76d1\u63a7 : \u541e\u5410\u91cf : \u6d4b\u91cf\u901a\u8fc7\u7f51\u7edc\u4f20\u8f93\u7684\u6570\u636e\u91cf\u3002 \u5ef6\u8fdf : \u6d4b\u91cf\u6570\u636e\u4ece\u6e90\u5230\u76ee\u7684\u5730\u7684\u4f20\u8f93\u65f6\u95f4\u3002 \u4e22\u5305\u7387 : \u76d1\u63a7\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u4e22\u5931\u7684\u6570\u636e\u5305\u6570\u91cf\u3002 \u9519\u8bef\u68c0\u6d4b : \u4f20\u8f93\u9519\u8bef : \u68c0\u6d4b\u6570\u636e\u4f20\u8f93\u4e2d\u7684\u9519\u8bef\u3002 \u8fde\u63a5\u5931\u8d25 : \u76d1\u63a7\u5931\u8d25\u7684\u8fde\u63a5\u5c1d\u8bd5\u548c\u65ad\u5f00\u8fde\u63a5\u3002 \u7f51\u7edc\u5065\u5eb7\u72b6\u51b5 : \u62e5\u585e : \u68c0\u6d4b\u7f51\u7edc\u62e5\u585e\u548c\u74f6\u9888\u3002 \u5982\u4f55\u5f00\u542f helm upgrade --install spiderpool spiderpool/spiderpool --reuse-values --wait --namespace spiderpool --create-namespace \\ --set sriov.install = true \\ --set spiderpoolAgent.prometheus.enabled = true \\ --set spiderpoolAgent.prometheus.enabledRdmaMetric = true \\ --set grafanaDashboard.install = true \\ --set spiderpoolAgent.prometheus.serviceMonitor.install = true \u901a\u8fc7\u8bbe\u7f6e --reuse-values \u91cd\u7528\u73b0\u6709\u7684\u914d\u7f6e \u901a\u8fc7\u8bbe\u7f6e --wait \u7b49\u5f85\u6240\u6709 Pod \u8fd0\u884c \u901a\u8fc7\u8bbe\u7f6e --namespace \u6307\u5b9a Helm \u5b89\u88c5\u7684\u547d\u540d\u7a7a\u95f4 \u901a\u8fc7\u8bbe\u7f6e --set sriov.install=true \u5f00\u542f SR-IOV, \u66f4\u591a\u53ef\u4ee5\u53c2\u8003 \u521b\u5efa\u96c6\u7fa4 - \u57fa\u4e8e SR-IOV \u6280\u672f\u7ed9\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b . \u901a\u8fc7\u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabled \u542f\u7528 Prometheus \u76d1\u63a7 \u901a\u8fc7\u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabledRdmaMetric=true \uff0c\u53ef\u4ee5\u542f\u7528 RDMA \u6307\u6807 exporter \u901a\u8fc7\u8bbe\u7f6e --set grafanaDashboard.install=true \uff0c\u53ef\u4ee5\u542f\u7528 GrafanaDashboard \u770b\u677f\uff08GrafanaDashboard \u8981\u6c42\u96c6\u7fa4\u5b89\u88c5 grafana-operator \uff0c\u5982\u679c\u60a8\u4e0d\u4f7f\u7528 grafana-operator\uff0c\u5219\u9700\u8981\u5c06 charts/spiderpool/files \u770b\u677f\u5bfc\u5165\u5230\u60a8\u7684 grafana\uff09\u3002 \u6307\u6807\u53c2\u8003 \u8bbf\u95ee Metrics \u53c2\u8003 \u67e5\u770b\u6307\u6807\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 Grafana \u76d1\u63a7\u9762\u677f \u4ee5\u4e0b\u56db\u4e2a\u76d1\u63a7\u9762\u677f\u4e2d\uff0cRDMA Pod \u76d1\u63a7\u9762\u677f\u4ec5\u5c55\u793a\u6765\u81ea RDMA \u9694\u79bb\u5b50\u7cfb\u7edf\u4e2d SR-IOV Pod \u7684\u76d1\u63a7\u6570\u636e\u3002\u800c\u5bf9\u4e8e\u91c7\u7528\u5171\u4eab\u65b9\u5f0f\u7684 macVLAN Pod\uff0c\u5176 RDMA \u7f51\u5361\u6570\u636e\u672a\u7eb3\u5165\u8be5\u9762\u677f\u3002 Grafana RDMA Cluster \u76d1\u63a7\u9762\u677f\uff0c\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 RDMA \u76d1\u63a7\u3002 Grafana RDMA Node \u76d1\u63a7\u9762\u677f\uff0c\u53ef\u4ee5\u67e5\u770b\u6bcf\u4e2a\u7269\u7406\u7f51\u5361\u7684 RDMA \u76d1\u63a7\uff0c\u4ee5\u53ca\u8be5\u7269\u7406\u7f51\u5361\u7684\u5e26\u5bbd\u5229\u7528\u7387\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u8be5\u8282\u70b9\u5bbf\u4e3b\u673a\u7684 vf \u7f51\u5361\u7edf\u8ba1\uff0c\u4ee5\u53ca\u8be5\u8282\u70b9 Pod \u4f7f\u7528 RDMA \u7f51\u5361\u7684\u76d1\u63a7\u3002 Grafana RDMA Pod \u76d1\u63a7\u9762\u677f\uff0c\u53ef\u4ee5\u67e5\u770b Pod \u91cc\u9762\u6bcf\u5f20\u7f51\u5361\u7684 RDMA \u76d1\u63a7\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7f51\u5361\u9519\u8bef\u7edf\u8ba1\u4fe1\u606f\uff0c\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u7684\u7edf\u8ba1\u53ef\u4ee5\u6392\u67e5\u95ee\u9898\u3002 Grafana RDMA Workload \u76d1\u63a7\u9762\u677f\u3002\u5728\u8fdb\u884c AI \u63a8\u7406\u548c\u8bad\u7ec3\u65f6\uff0c\u5f80\u5f80\u4f7f\u7528 Job, Deployment, KServer \u7b49\u9876\u5c42\u8d44\u6e90\u4e0b\u53d1 CR \u542f\u52a8\u4e00\u7ec4 Pod \u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u67e5\u770b\u6bcf\u4e2a\u9876\u5c42\u8d44\u6e90\u7684 RDMA \u76d1\u63a7\u3002","title":"RDMA \u6307\u6807"},{"location":"usage/rdma-metrics-zh_CN/#rdma","text":"English | \u7b80\u4f53\u4e2d\u6587 RDMA \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7f51\u7edc\u901a\u4fe1\u6280\u672f\uff0c\u5141\u8bb8\u4e00\u53f0\u8ba1\u7b97\u673a\u76f4\u63a5\u8bbf\u95ee\u53e6\u4e00\u53f0\u8ba1\u7b97\u673a\u7684\u5185\u5b58\uff0c\u65e0\u9700\u64cd\u4f5c\u7cfb\u7edf\u4ecb\u5165\uff0c\u4ece\u800c\u51cf\u5c11\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u901f\u5ea6\u548c\u6548\u7387\u3002RDMA \u652f\u6301\u9ad8\u901f\u6570\u636e\u4f20\u8f93\uff0c\u51cf\u5c11 CPU \u8d1f\u8f7d\uff0c\u975e\u5e38\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u7684\u573a\u666f\u3002 \u5728 Kubernetes \u96c6\u7fa4\u4e2d\uff0cspiderpool CNI \u652f\u6301 RoCE \u548c IB 2 \u79cd RDMA \u573a\u666f\uff0cPod \u53ef\u4ee5\u901a\u8fc7\u5171\u4eab\u548c\u72ec\u5360\u7684\u65b9\u5f0f\u4f7f\u7528 RDMA \u7f51\u5361\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u65b9\u5f0f\u6765\u4f7f\u7528 RDMA \u7f51\u5361\u3002 spiderpool \u540c\u65f6\u63d0\u4f9b\u4e86 RDMA exporter \u529f\u80fd\u548c grafana \u76d1\u63a7\u9762\u677f\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7 Pod/Node RDMA \u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5305\u62ec\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u3001\u4e22\u5305\u7387\u7b49\uff0c\u53ef\u4ee5\u65f6\u53d1\u73b0\u95ee\u9898\u5e76\u91c7\u53d6\u63aa\u65bd\u89e3\u51b3\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002","title":"RDMA \u6307\u6807"},{"location":"usage/rdma-metrics-zh_CN/#rdma_1","text":"\u6027\u80fd\u76d1\u63a7 : \u541e\u5410\u91cf : \u6d4b\u91cf\u901a\u8fc7\u7f51\u7edc\u4f20\u8f93\u7684\u6570\u636e\u91cf\u3002 \u5ef6\u8fdf : \u6d4b\u91cf\u6570\u636e\u4ece\u6e90\u5230\u76ee\u7684\u5730\u7684\u4f20\u8f93\u65f6\u95f4\u3002 \u4e22\u5305\u7387 : \u76d1\u63a7\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u4e22\u5931\u7684\u6570\u636e\u5305\u6570\u91cf\u3002 \u9519\u8bef\u68c0\u6d4b : \u4f20\u8f93\u9519\u8bef : \u68c0\u6d4b\u6570\u636e\u4f20\u8f93\u4e2d\u7684\u9519\u8bef\u3002 \u8fde\u63a5\u5931\u8d25 : \u76d1\u63a7\u5931\u8d25\u7684\u8fde\u63a5\u5c1d\u8bd5\u548c\u65ad\u5f00\u8fde\u63a5\u3002 \u7f51\u7edc\u5065\u5eb7\u72b6\u51b5 : \u62e5\u585e : \u68c0\u6d4b\u7f51\u7edc\u62e5\u585e\u548c\u74f6\u9888\u3002","title":"RDMA \u6307\u6807\u7684\u5e38\u89c1\u573a\u666f"},{"location":"usage/rdma-metrics-zh_CN/#_1","text":"helm upgrade --install spiderpool spiderpool/spiderpool --reuse-values --wait --namespace spiderpool --create-namespace \\ --set sriov.install = true \\ --set spiderpoolAgent.prometheus.enabled = true \\ --set spiderpoolAgent.prometheus.enabledRdmaMetric = true \\ --set grafanaDashboard.install = true \\ --set spiderpoolAgent.prometheus.serviceMonitor.install = true \u901a\u8fc7\u8bbe\u7f6e --reuse-values \u91cd\u7528\u73b0\u6709\u7684\u914d\u7f6e \u901a\u8fc7\u8bbe\u7f6e --wait \u7b49\u5f85\u6240\u6709 Pod \u8fd0\u884c \u901a\u8fc7\u8bbe\u7f6e --namespace \u6307\u5b9a Helm \u5b89\u88c5\u7684\u547d\u540d\u7a7a\u95f4 \u901a\u8fc7\u8bbe\u7f6e --set sriov.install=true \u5f00\u542f SR-IOV, \u66f4\u591a\u53ef\u4ee5\u53c2\u8003 \u521b\u5efa\u96c6\u7fa4 - \u57fa\u4e8e SR-IOV \u6280\u672f\u7ed9\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b . \u901a\u8fc7\u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabled \u542f\u7528 Prometheus \u76d1\u63a7 \u901a\u8fc7\u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabledRdmaMetric=true \uff0c\u53ef\u4ee5\u542f\u7528 RDMA \u6307\u6807 exporter \u901a\u8fc7\u8bbe\u7f6e --set grafanaDashboard.install=true \uff0c\u53ef\u4ee5\u542f\u7528 GrafanaDashboard \u770b\u677f\uff08GrafanaDashboard \u8981\u6c42\u96c6\u7fa4\u5b89\u88c5 grafana-operator \uff0c\u5982\u679c\u60a8\u4e0d\u4f7f\u7528 grafana-operator\uff0c\u5219\u9700\u8981\u5c06 charts/spiderpool/files \u770b\u677f\u5bfc\u5165\u5230\u60a8\u7684 grafana\uff09\u3002","title":"\u5982\u4f55\u5f00\u542f"},{"location":"usage/rdma-metrics-zh_CN/#_2","text":"\u8bbf\u95ee Metrics \u53c2\u8003 \u67e5\u770b\u6307\u6807\u7684\u8be6\u7ec6\u4fe1\u606f\u3002","title":"\u6307\u6807\u53c2\u8003"},{"location":"usage/rdma-metrics-zh_CN/#grafana","text":"\u4ee5\u4e0b\u56db\u4e2a\u76d1\u63a7\u9762\u677f\u4e2d\uff0cRDMA Pod \u76d1\u63a7\u9762\u677f\u4ec5\u5c55\u793a\u6765\u81ea RDMA \u9694\u79bb\u5b50\u7cfb\u7edf\u4e2d SR-IOV Pod \u7684\u76d1\u63a7\u6570\u636e\u3002\u800c\u5bf9\u4e8e\u91c7\u7528\u5171\u4eab\u65b9\u5f0f\u7684 macVLAN Pod\uff0c\u5176 RDMA \u7f51\u5361\u6570\u636e\u672a\u7eb3\u5165\u8be5\u9762\u677f\u3002 Grafana RDMA Cluster \u76d1\u63a7\u9762\u677f\uff0c\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 RDMA \u76d1\u63a7\u3002 Grafana RDMA Node \u76d1\u63a7\u9762\u677f\uff0c\u53ef\u4ee5\u67e5\u770b\u6bcf\u4e2a\u7269\u7406\u7f51\u5361\u7684 RDMA \u76d1\u63a7\uff0c\u4ee5\u53ca\u8be5\u7269\u7406\u7f51\u5361\u7684\u5e26\u5bbd\u5229\u7528\u7387\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u8be5\u8282\u70b9\u5bbf\u4e3b\u673a\u7684 vf \u7f51\u5361\u7edf\u8ba1\uff0c\u4ee5\u53ca\u8be5\u8282\u70b9 Pod \u4f7f\u7528 RDMA \u7f51\u5361\u7684\u76d1\u63a7\u3002 Grafana RDMA Pod \u76d1\u63a7\u9762\u677f\uff0c\u53ef\u4ee5\u67e5\u770b Pod \u91cc\u9762\u6bcf\u5f20\u7f51\u5361\u7684 RDMA \u76d1\u63a7\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7f51\u5361\u9519\u8bef\u7edf\u8ba1\u4fe1\u606f\uff0c\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u7684\u7edf\u8ba1\u53ef\u4ee5\u6392\u67e5\u95ee\u9898\u3002 Grafana RDMA Workload \u76d1\u63a7\u9762\u677f\u3002\u5728\u8fdb\u884c AI \u63a8\u7406\u548c\u8bad\u7ec3\u65f6\uff0c\u5f80\u5f80\u4f7f\u7528 Job, Deployment, KServer \u7b49\u9876\u5c42\u8d44\u6e90\u4e0b\u53d1 CR \u542f\u52a8\u4e00\u7ec4 Pod \u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u67e5\u770b\u6bcf\u4e2a\u9876\u5c42\u8d44\u6e90\u7684 RDMA \u76d1\u63a7\u3002","title":"Grafana \u76d1\u63a7\u9762\u677f"},{"location":"usage/rdma-metrics/","text":"RDMA Metrics English \uff5c \u7b80\u4f53\u4e2d\u6587 RDMA is an efficient network communication technology that allows one computer to directly access the memory of another computer without involving the operating system, thus reducing latency and improving data transfer speed and efficiency. RDMA supports high-speed data transmission and reduces CPU load, making it ideal for scenarios requiring high-speed network communication. In a Kubernetes cluster, the spiderpool CNI supports two RDMA scenarios: RoCE and IB. Pods can use the RDMA network card in either shared or exclusive modes. Users can choose the appropriate method based on their needs for utilizing RDMA network cards. Spiderpool also provides an RDMA exporter feature and a Grafana monitoring panel. By monitoring the performance of Pod/Node RDMA networks in real-time, including throughput, latency, packet loss rate, etc., issues can be detected and measures taken to improve network reliability and performance. Common Scenarios for RDMA Metrics Performance Monitoring : Throughput : Measures the amount of data transmitted over the network. Latency : Measures the time it takes for data to travel from source to destination. Packet Loss Rate : Monitors the number of data packets lost during transmission. Error Detection : Transmission Errors : Detects errors in data transmission. Connection Failures : Monitors failed connection attempts and disconnects. Network Health : Congestion : Detects network congestion and bottlenecks. How to Enable helm upgrade --install spiderpool spiderpool/spiderpool --reuse-values --wait --namespace spiderpool --create-namespace \\ --set sriov.install = true \\ --set spiderpoolAgent.prometheus.enabled = true \\ --set spiderpoolAgent.prometheus.enabledRdmaMetric = true \\ --set grafanaDashboard.install = true \\ --set spiderpoolAgent.prometheus.serviceMonitor.install = true Use --reuse-values to reuse existing configurations. Use --wait to wait for all Pods to be running. Use --namespace to specify the Helm installation namespace. Use --set sriov.install=true to enable SR-IOV, For more you can refer to Create a cluster - provide Infiniband and RoCE RDMA network with SR-IOV . Use --set spiderpoolAgent.prometheus.enabled to enable Prometheus monitoring. Use --set spiderpoolAgent.prometheus.enabledRdmaMetric=true to enable the RDMA metric exporter. Use -set grafanaDashboard.install=true to install Grafana Dashboard (GrafanaDashboard requires the cluster to install grafana-operator , or if you don't use it, you need to import the charts/spiderpool/files dashboard into your grafana). Metric Reference Visit Metrics Reference to view detailed information about the metrics. Grafana Monitoring Dashboard Among the following four monitoring dashboards, the RDMA Pod monitoring dashboard only displays monitoring data from SR-IOV Pods in the RDMA-isolated subsystem. As for macVLAN Pods, which use a shared mode, their RDMA network card data is not included in this dashboard. The Grafana RDMA Cluster monitoring dashboard provides a view of the RDMA metrics for each node in the current cluster. The Grafana RDMA Node monitoring dashboard displays RDMA metrics for each physical NIC (Network Interface Card) and the bandwidth utilization of those NICs. It also includes statistics for VF NICs on the host node and monitoring metrics for Pods using RDMA NICs on that node. The Grafana RDMA Pod monitoring dashboard provides RDMA metrics for each NIC within a Pod, along with NIC error statistics. These metrics help in troubleshooting issues. The Grafana RDMA Workload monitoring dashboard is designed for monitoring RDMA metrics for top-level resources such as Jobs, Deployments, and KServers. These resources typically initiate a set of Pods for AI inference and training tasks.","title":"Enable RDMA metrics"},{"location":"usage/rdma-metrics/#rdma-metrics","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587 RDMA is an efficient network communication technology that allows one computer to directly access the memory of another computer without involving the operating system, thus reducing latency and improving data transfer speed and efficiency. RDMA supports high-speed data transmission and reduces CPU load, making it ideal for scenarios requiring high-speed network communication. In a Kubernetes cluster, the spiderpool CNI supports two RDMA scenarios: RoCE and IB. Pods can use the RDMA network card in either shared or exclusive modes. Users can choose the appropriate method based on their needs for utilizing RDMA network cards. Spiderpool also provides an RDMA exporter feature and a Grafana monitoring panel. By monitoring the performance of Pod/Node RDMA networks in real-time, including throughput, latency, packet loss rate, etc., issues can be detected and measures taken to improve network reliability and performance.","title":"RDMA Metrics"},{"location":"usage/rdma-metrics/#common-scenarios-for-rdma-metrics","text":"Performance Monitoring : Throughput : Measures the amount of data transmitted over the network. Latency : Measures the time it takes for data to travel from source to destination. Packet Loss Rate : Monitors the number of data packets lost during transmission. Error Detection : Transmission Errors : Detects errors in data transmission. Connection Failures : Monitors failed connection attempts and disconnects. Network Health : Congestion : Detects network congestion and bottlenecks.","title":"Common Scenarios for RDMA Metrics"},{"location":"usage/rdma-metrics/#how-to-enable","text":"helm upgrade --install spiderpool spiderpool/spiderpool --reuse-values --wait --namespace spiderpool --create-namespace \\ --set sriov.install = true \\ --set spiderpoolAgent.prometheus.enabled = true \\ --set spiderpoolAgent.prometheus.enabledRdmaMetric = true \\ --set grafanaDashboard.install = true \\ --set spiderpoolAgent.prometheus.serviceMonitor.install = true Use --reuse-values to reuse existing configurations. Use --wait to wait for all Pods to be running. Use --namespace to specify the Helm installation namespace. Use --set sriov.install=true to enable SR-IOV, For more you can refer to Create a cluster - provide Infiniband and RoCE RDMA network with SR-IOV . Use --set spiderpoolAgent.prometheus.enabled to enable Prometheus monitoring. Use --set spiderpoolAgent.prometheus.enabledRdmaMetric=true to enable the RDMA metric exporter. Use -set grafanaDashboard.install=true to install Grafana Dashboard (GrafanaDashboard requires the cluster to install grafana-operator , or if you don't use it, you need to import the charts/spiderpool/files dashboard into your grafana).","title":"How to Enable"},{"location":"usage/rdma-metrics/#metric-reference","text":"Visit Metrics Reference to view detailed information about the metrics.","title":"Metric Reference"},{"location":"usage/rdma-metrics/#grafana-monitoring-dashboard","text":"Among the following four monitoring dashboards, the RDMA Pod monitoring dashboard only displays monitoring data from SR-IOV Pods in the RDMA-isolated subsystem. As for macVLAN Pods, which use a shared mode, their RDMA network card data is not included in this dashboard. The Grafana RDMA Cluster monitoring dashboard provides a view of the RDMA metrics for each node in the current cluster. The Grafana RDMA Node monitoring dashboard displays RDMA metrics for each physical NIC (Network Interface Card) and the bandwidth utilization of those NICs. It also includes statistics for VF NICs on the host node and monitoring metrics for Pods using RDMA NICs on that node. The Grafana RDMA Pod monitoring dashboard provides RDMA metrics for each NIC within a Pod, along with NIC error statistics. These metrics help in troubleshooting issues. The Grafana RDMA Workload monitoring dashboard is designed for monitoring RDMA metrics for top-level resources such as Jobs, Deployments, and KServers. These resources typically initiate a set of Pods for AI inference and training tasks.","title":"Grafana Monitoring Dashboard"},{"location":"usage/readme-zh_CN/","text":"\u4f7f\u7528\u7d22\u5f15 English | \u7b80\u4f53\u4e2d\u6587 \u5b89\u88c5 Spiderpool \u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u5b89\u88c5 Spiderpool \u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u63a5\u5165 Spiderpool \u4e00\u4e2a\u6216\u591a\u4e2a Underlay \u7f51\u7edc\u7684\u7f51\u5361\uff0c\u4ece\u800c\u8ba9 Pod \u5177\u5907\u63a5\u5165 underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 \u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e macvlan \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e SR-IOV CNI \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e ovs \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e weave CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u5728\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883\u4e0a\u5b89\u88c5 Spiderpool \u5728\u516c\u6709\u4e91\u548c\u865a\u62df\u673a\u73af\u5883\u4e0a\u8fd0\u884c Spiderpool\uff0c\u4f7f\u5f97 POD \u5bf9\u63a5 VPC \u7684\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u5728\u963f\u91cc\u4e91\u4e0a\u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1aVMware vsphere \u521b\u5efa\u96c6\u7fa4\uff1a\u5728 AWS \u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u5728 VMware vSphere \u5e73\u53f0\u4e0a\u8fd0\u884c ipvlan CNI\uff0c\u65e0\u9700\u6253\u5f00 vSwitch \u7684 \"\u6df7\u6742\"\u8f6c\u53d1\u6a21\u5f0f \uff0c\u4ece\u800c\u786e\u4fdd vSphere \u5e73\u53f0\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53c2\u8003 \u5b89\u88c5 \u57fa\u4e8e Overlay CNI \u548c Spiderpool \u7684\u53cc CNI \u96c6\u7fa4 \u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u540c\u65f6\u63a5\u5165\u4e00\u4e2a Overlay CNI \u7f51\u5361\u548c\u591a\u4e2a spiderpool \u7684 Underlay CNI \u8f85\u52a9\u7f51\u5361\uff0c\u4ece\u800c\u8ba9 Pod \u540c\u65f6\u5177\u5907\u63a5\u5165 overlay \u548c underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 underlay CNI \u548c overlay CNI \u534f\u540c \u3002\u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e kind \u96c6\u7fa4\u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e Cilium \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc \u4e3a AI \u96c6\u7fa4\u5b89\u88c5 Spiderpool AI \u96c6\u7fa4\u901a\u5e38\u4f7f\u7528\u591a\u8f68\u7684 RDMA \u7f51\u7edc\u4e3a GPU \u63d0\u4f9b\u901a\u4fe1\u3002Spiderpool \u53ef\u4e3a\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e SR-IOV \u4e3a\u5bb9\u5668\u63d0\u4f9b Infiniband \u548c RoCE RDMA \u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e Macvlan \u4e3a\u5bb9\u5668\u63d0\u4f9b RoCE RDMA \u7f51\u7edc TLS \u8bc1\u4e66 \u5b89\u88c5 spiderpool \u65f6\uff0c\u53ef\u6307\u5b9a TLS \u8bc1\u4e66\u7684\u751f\u6210\u65b9\u5f0f\uff0c\u53ef\u53c2\u8003 \u6587\u7ae0 \u5378\u8f7d Spiderpool \u53ef\u53c2\u8003 \u5378\u8f7d \u5347\u7ea7 Spiderpool \u53ef\u53c2\u8003 \u5347\u7ea7 \u4f7f\u7528 Spiderpool IPAM \u529f\u80fd \u5e94\u7528\u53ef\u4ee5\u5171\u4eab\u4e00\u4e2a IP \u6c60\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u53ef\u4ee5\u72ec\u4eab\u4e00\u4e2a IP \u5730\u5740\u6c60\uff0c\u5e76\u56fa\u5b9a\u6240\u6709 Pod \u7684 IP \u4f7f\u7528\u8303\u56f4\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u6709\u72b6\u6001\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u6bcf\u4e00\u4e2a Pod \u6301\u4e45\u5316\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\uff0c\u540c\u65f6\u5728\u6269\u7f29\u65f6\u53ef\u63a7\u5236\u6240\u6709 Pod \u6240\u4f7f\u7528\u7684 IP \u8303\u56f4\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301\u4e3a kubevirt \u63d0\u4f9b underlay \u7f51\u7edc\uff0c\u56fa\u5b9a\u865a\u62df\u673a\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u5bf9\u4e8e\u4e00\u4e2a\u8de8\u5b50\u7f51\u90e8\u7f72\u7684\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u5176\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 Subnet \u529f\u80fd\uff0c\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7ba1\u7406\u5458\u7684\u804c\u8d23\u5206\u79bb\uff0c \u53e6\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u4e3a\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\u81ea\u52a8\u7ba1\u7406 IP \u6c60\uff0c\u5305\u62ec\u81ea\u52a8\u521b\u5efa\u3001\u6269\u7f29\u5bb9 IP\u3001\u5220\u9664 \u56fa\u5b9a IP \u6c60\uff0c \u8fd9\u80fd\u591f\u51cf\u5c11\u5927\u91cf\u7684\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8be5\u529f\u80fd\u9664\u4e86\u652f\u6301 K8S \u539f\u751f\u7684\u5e94\u7528\u63a7\u5236\u5668\uff0c\u540c\u65f6\u652f\u6301\u57fa\u4e8e operator \u5b9e\u73b0\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u63a7\u5236\u5668\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u8bbe\u7f6e\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\uff0c\u4e5f\u53ef\u79df\u6237\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\u3002\u540c\u65f6\uff0cIP \u6c60\u65e2\u53ef\u4ee5\u88ab\u6574\u4e2a\u96c6\u7fa4\u5171\u4eab\uff0c \u4e5f\u53ef\u88ab\u9650\u5b9a\u4e3a\u88ab\u4e00\u4e2a\u79df\u6237\u4f7f\u7528\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u57fa\u4e8e\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u6ee1\u8db3\u6bcf\u4e2a\u8282\u70b9\u7cbe\u7ec6\u5316\u7684\u5b50\u7f51\u89c4\u5212\u9700\u6c42\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u53ef\u4ee5\u901a\u8fc7 IP \u6c60\u548c Pod annotaiton \u7b49\u591a\u79cd\u65b9\u5f0f\u5b9a\u5236\u81ea\u5b9a\u4e49\u8def\u7531\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e94\u7528\u53ef\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\uff0c\u5b9e\u73b0 IP \u8d44\u6e90\u7684\u5907\u7528\u6548\u679c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8bbe\u7f6e\u5168\u5c40\u7684\u9884\u7559 IP\uff0c\u8ba9 IPAM \u4e0d\u5206\u914d\u51fa\u8fd9\u4e9b IP \u5730\u5740\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u5df2\u7528 IP \u51b2\u7a81\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u62a5\u544a \u3002 \u5408\u7406\u7684 IP \u56de\u6536\u673a\u5236\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6216\u5e94\u7528\u5728\u6545\u969c\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u53ca\u65f6\u5206\u914d\u5230 IP \u5730\u5740\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u591a\u7f51\u5361\u529f\u80fd \u652f\u6301\u4e3a Pod \u591a\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff1b\u5e2e\u52a9\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u534f\u8c03\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u4e22\u5305\uff1b\u652f\u6301\u5b9a\u5236\u54ea\u5f20\u7f51\u5361\u7684\u7f51\u5173\u4f5c\u4e3a\u7f3a\u7701\u8def\u7531\u3002 \u5bf9\u4e8e Pod \u5177\u5907\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e Pod \u5177\u5907\u4e00\u4e2a overlay \u7f51\u5361\u548c\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8fde\u901a\u6027\u529f\u80fd \u652f\u6301 RDMA \u7f51\u5361\u7684 shared \u548c exclusive \u6a21\u5f0f\uff0c\u80fd\u57fa\u4e8e maclan\u3001ipvlan \u548c SR-IOV CNI \u4e3a\u5e94\u7528\u63d0\u4f9b RDMA \u901a\u4fe1\u8bbe\u5907\u3002\u5177\u4f53\u53ef\u53c2\u8003 SR-IOV \u4f8b\u5b50 \u548c Macvlan \u4f8b\u5b50 . coordinator \u63d2\u4ef6\u80fd\u591f\u4f9d\u636e\u7f51\u5361\u7684 IP \u5730\u5740\u6765\u91cd\u65b0\u914d\u7f6e MAC \u5730\u5740\uff0c\u4f7f\u4e24\u8005\u4e00\u4e00\u5bf9\u5e94\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u907f\u514d\u7f51\u7edc\u4e2d\u7684\u4ea4\u6362\u8def\u7531\u8bbe\u5907\u66f4\u65b0 ARP \u8f6c\u53d1\u89c4\u5219\uff0c\u907f\u514d\u4e22\u5305\u3002\u53ef\u53c2\u8003 \u6587\u7ae0 \u3002 \u5bf9 Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\uff0c \u63d0\u4f9b\u4e86\u57fa\u4e8e kube-proxy \u548c eBPF kube-proxy replacement \u8bbf\u95ee ClusterIP \u8bbf\u95ee\uff0c\u5e76\u8054\u901a Pod \u548c\u5bbf\u4e3b\u673a\u901a\u4fe1\uff0c\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u67e5\u95ee\u9898\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u80fd\u591f\u5e2e\u52a9\u5b9e\u65bd IP \u5730\u5740\u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u8bc1 Pod \u901a\u4fe1\u6b63\u5e38\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u591a\u96c6\u7fa4\u7f51\u7edc\u53ef\u57fa\u4e8e\u76f8\u540c\u7684 underlay \u7f51\u7edc\u6216\u8005 Submariner \u5b9e\u73b0\u8054\u901a\u3002 \u8fd0\u7ef4\u7ba1\u7406\u529f\u80fd \u5728 Pod \u542f\u52a8\u65f6\uff0c\u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u52a8\u6001\u521b\u5efa BOND \u63a5\u53e3\u548c VLAN \u5b50\u63a5\u53e3\uff0c\u4ee5\u5e2e\u52a9 Macvlan CNI \u548c ipvlan CNI \u51c6\u5907 master \u63a5\u53e3\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u4ee5\u6700\u4f73\u5b9e\u8df5\u7684 CNI \u914d\u7f6e\u6765\u4fbf\u6377\u5730\u751f\u6210 Multus NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5e76\u4e14\u4fdd\u8bc1\u5176\u6b63\u786e\u7684 JSON \u683c\u5f0f\u6765\u63d0\u9ad8\u4f7f\u7528\u4f53\u9a8c\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5176\u5b83\u529f\u80fd \u6307\u6807 \u652f\u6301 AMD64 \u548c ARM64 \u6240\u6709\u7684\u529f\u80fd\u90fd\u80fd\u591f\u5728 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u4e0b\u5de5\u4f5c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u4f7f\u7528\u7d22\u5f15"},{"location":"usage/readme-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"\u4f7f\u7528\u7d22\u5f15"},{"location":"usage/readme-zh_CN/#spiderpool","text":"","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#spiderpool_1","text":"\u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u63a5\u5165 Spiderpool \u4e00\u4e2a\u6216\u591a\u4e2a Underlay \u7f51\u7edc\u7684\u7f51\u5361\uff0c\u4ece\u800c\u8ba9 Pod \u5177\u5907\u63a5\u5165 underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 \u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e macvlan \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e SR-IOV CNI \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e ovs \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e weave CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4","title":"\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#spiderpool_2","text":"\u5728\u516c\u6709\u4e91\u548c\u865a\u62df\u673a\u73af\u5883\u4e0a\u8fd0\u884c Spiderpool\uff0c\u4f7f\u5f97 POD \u5bf9\u63a5 VPC \u7684\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u5728\u963f\u91cc\u4e91\u4e0a\u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1aVMware vsphere \u521b\u5efa\u96c6\u7fa4\uff1a\u5728 AWS \u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u5728 VMware vSphere \u5e73\u53f0\u4e0a\u8fd0\u884c ipvlan CNI\uff0c\u65e0\u9700\u6253\u5f00 vSwitch \u7684 \"\u6df7\u6742\"\u8f6c\u53d1\u6a21\u5f0f \uff0c\u4ece\u800c\u786e\u4fdd vSphere \u5e73\u53f0\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53c2\u8003 \u5b89\u88c5","title":"\u5728\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883\u4e0a\u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#overlay-cni-spiderpool-cni","text":"\u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u540c\u65f6\u63a5\u5165\u4e00\u4e2a Overlay CNI \u7f51\u5361\u548c\u591a\u4e2a spiderpool \u7684 Underlay CNI \u8f85\u52a9\u7f51\u5361\uff0c\u4ece\u800c\u8ba9 Pod \u540c\u65f6\u5177\u5907\u63a5\u5165 overlay \u548c underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 underlay CNI \u548c overlay CNI \u534f\u540c \u3002\u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e kind \u96c6\u7fa4\u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e Cilium \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc","title":"\u57fa\u4e8e Overlay CNI \u548c Spiderpool \u7684\u53cc CNI \u96c6\u7fa4"},{"location":"usage/readme-zh_CN/#ai-spiderpool","text":"AI \u96c6\u7fa4\u901a\u5e38\u4f7f\u7528\u591a\u8f68\u7684 RDMA \u7f51\u7edc\u4e3a GPU \u63d0\u4f9b\u901a\u4fe1\u3002Spiderpool \u53ef\u4e3a\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e SR-IOV \u4e3a\u5bb9\u5668\u63d0\u4f9b Infiniband \u548c RoCE RDMA \u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e Macvlan \u4e3a\u5bb9\u5668\u63d0\u4f9b RoCE RDMA \u7f51\u7edc","title":"\u4e3a AI \u96c6\u7fa4\u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#tls","text":"\u5b89\u88c5 spiderpool \u65f6\uff0c\u53ef\u6307\u5b9a TLS \u8bc1\u4e66\u7684\u751f\u6210\u65b9\u5f0f\uff0c\u53ef\u53c2\u8003 \u6587\u7ae0","title":"TLS \u8bc1\u4e66"},{"location":"usage/readme-zh_CN/#spiderpool_3","text":"\u53ef\u53c2\u8003 \u5378\u8f7d","title":"\u5378\u8f7d Spiderpool"},{"location":"usage/readme-zh_CN/#spiderpool_4","text":"\u53ef\u53c2\u8003 \u5347\u7ea7","title":"\u5347\u7ea7 Spiderpool"},{"location":"usage/readme-zh_CN/#spiderpool_5","text":"","title":"\u4f7f\u7528 Spiderpool"},{"location":"usage/readme-zh_CN/#ipam","text":"\u5e94\u7528\u53ef\u4ee5\u5171\u4eab\u4e00\u4e2a IP \u6c60\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u53ef\u4ee5\u72ec\u4eab\u4e00\u4e2a IP \u5730\u5740\u6c60\uff0c\u5e76\u56fa\u5b9a\u6240\u6709 Pod \u7684 IP \u4f7f\u7528\u8303\u56f4\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u6709\u72b6\u6001\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u6bcf\u4e00\u4e2a Pod \u6301\u4e45\u5316\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\uff0c\u540c\u65f6\u5728\u6269\u7f29\u65f6\u53ef\u63a7\u5236\u6240\u6709 Pod \u6240\u4f7f\u7528\u7684 IP \u8303\u56f4\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301\u4e3a kubevirt \u63d0\u4f9b underlay \u7f51\u7edc\uff0c\u56fa\u5b9a\u865a\u62df\u673a\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u5bf9\u4e8e\u4e00\u4e2a\u8de8\u5b50\u7f51\u90e8\u7f72\u7684\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u5176\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 Subnet \u529f\u80fd\uff0c\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7ba1\u7406\u5458\u7684\u804c\u8d23\u5206\u79bb\uff0c \u53e6\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u4e3a\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\u81ea\u52a8\u7ba1\u7406 IP \u6c60\uff0c\u5305\u62ec\u81ea\u52a8\u521b\u5efa\u3001\u6269\u7f29\u5bb9 IP\u3001\u5220\u9664 \u56fa\u5b9a IP \u6c60\uff0c \u8fd9\u80fd\u591f\u51cf\u5c11\u5927\u91cf\u7684\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8be5\u529f\u80fd\u9664\u4e86\u652f\u6301 K8S \u539f\u751f\u7684\u5e94\u7528\u63a7\u5236\u5668\uff0c\u540c\u65f6\u652f\u6301\u57fa\u4e8e operator \u5b9e\u73b0\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u63a7\u5236\u5668\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u8bbe\u7f6e\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\uff0c\u4e5f\u53ef\u79df\u6237\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\u3002\u540c\u65f6\uff0cIP \u6c60\u65e2\u53ef\u4ee5\u88ab\u6574\u4e2a\u96c6\u7fa4\u5171\u4eab\uff0c \u4e5f\u53ef\u88ab\u9650\u5b9a\u4e3a\u88ab\u4e00\u4e2a\u79df\u6237\u4f7f\u7528\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u57fa\u4e8e\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u6ee1\u8db3\u6bcf\u4e2a\u8282\u70b9\u7cbe\u7ec6\u5316\u7684\u5b50\u7f51\u89c4\u5212\u9700\u6c42\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u53ef\u4ee5\u901a\u8fc7 IP \u6c60\u548c Pod annotaiton \u7b49\u591a\u79cd\u65b9\u5f0f\u5b9a\u5236\u81ea\u5b9a\u4e49\u8def\u7531\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e94\u7528\u53ef\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\uff0c\u5b9e\u73b0 IP \u8d44\u6e90\u7684\u5907\u7528\u6548\u679c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8bbe\u7f6e\u5168\u5c40\u7684\u9884\u7559 IP\uff0c\u8ba9 IPAM \u4e0d\u5206\u914d\u51fa\u8fd9\u4e9b IP \u5730\u5740\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u5df2\u7528 IP \u51b2\u7a81\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u62a5\u544a \u3002 \u5408\u7406\u7684 IP \u56de\u6536\u673a\u5236\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6216\u5e94\u7528\u5728\u6545\u969c\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u53ca\u65f6\u5206\u914d\u5230 IP \u5730\u5740\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"IPAM \u529f\u80fd"},{"location":"usage/readme-zh_CN/#_2","text":"\u652f\u6301\u4e3a Pod \u591a\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff1b\u5e2e\u52a9\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u534f\u8c03\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u4e22\u5305\uff1b\u652f\u6301\u5b9a\u5236\u54ea\u5f20\u7f51\u5361\u7684\u7f51\u5173\u4f5c\u4e3a\u7f3a\u7701\u8def\u7531\u3002 \u5bf9\u4e8e Pod \u5177\u5907\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e Pod \u5177\u5907\u4e00\u4e2a overlay \u7f51\u5361\u548c\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u591a\u7f51\u5361\u529f\u80fd"},{"location":"usage/readme-zh_CN/#_3","text":"\u652f\u6301 RDMA \u7f51\u5361\u7684 shared \u548c exclusive \u6a21\u5f0f\uff0c\u80fd\u57fa\u4e8e maclan\u3001ipvlan \u548c SR-IOV CNI \u4e3a\u5e94\u7528\u63d0\u4f9b RDMA \u901a\u4fe1\u8bbe\u5907\u3002\u5177\u4f53\u53ef\u53c2\u8003 SR-IOV \u4f8b\u5b50 \u548c Macvlan \u4f8b\u5b50 . coordinator \u63d2\u4ef6\u80fd\u591f\u4f9d\u636e\u7f51\u5361\u7684 IP \u5730\u5740\u6765\u91cd\u65b0\u914d\u7f6e MAC \u5730\u5740\uff0c\u4f7f\u4e24\u8005\u4e00\u4e00\u5bf9\u5e94\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u907f\u514d\u7f51\u7edc\u4e2d\u7684\u4ea4\u6362\u8def\u7531\u8bbe\u5907\u66f4\u65b0 ARP \u8f6c\u53d1\u89c4\u5219\uff0c\u907f\u514d\u4e22\u5305\u3002\u53ef\u53c2\u8003 \u6587\u7ae0 \u3002 \u5bf9 Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\uff0c \u63d0\u4f9b\u4e86\u57fa\u4e8e kube-proxy \u548c eBPF kube-proxy replacement \u8bbf\u95ee ClusterIP \u8bbf\u95ee\uff0c\u5e76\u8054\u901a Pod \u548c\u5bbf\u4e3b\u673a\u901a\u4fe1\uff0c\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u67e5\u95ee\u9898\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u80fd\u591f\u5e2e\u52a9\u5b9e\u65bd IP \u5730\u5740\u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u8bc1 Pod \u901a\u4fe1\u6b63\u5e38\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u591a\u96c6\u7fa4\u7f51\u7edc\u53ef\u57fa\u4e8e\u76f8\u540c\u7684 underlay \u7f51\u7edc\u6216\u8005 Submariner \u5b9e\u73b0\u8054\u901a\u3002","title":"\u8fde\u901a\u6027\u529f\u80fd"},{"location":"usage/readme-zh_CN/#_4","text":"\u5728 Pod \u542f\u52a8\u65f6\uff0c\u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u52a8\u6001\u521b\u5efa BOND \u63a5\u53e3\u548c VLAN \u5b50\u63a5\u53e3\uff0c\u4ee5\u5e2e\u52a9 Macvlan CNI \u548c ipvlan CNI \u51c6\u5907 master \u63a5\u53e3\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u4ee5\u6700\u4f73\u5b9e\u8df5\u7684 CNI \u914d\u7f6e\u6765\u4fbf\u6377\u5730\u751f\u6210 Multus NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5e76\u4e14\u4fdd\u8bc1\u5176\u6b63\u786e\u7684 JSON \u683c\u5f0f\u6765\u63d0\u9ad8\u4f7f\u7528\u4f53\u9a8c\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u8fd0\u7ef4\u7ba1\u7406\u529f\u80fd"},{"location":"usage/readme-zh_CN/#_5","text":"\u6307\u6807 \u652f\u6301 AMD64 \u548c ARM64 \u6240\u6709\u7684\u529f\u80fd\u90fd\u80fd\u591f\u5728 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u4e0b\u5de5\u4f5c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u5176\u5b83\u529f\u80fd"},{"location":"usage/readme/","text":"Usage Index English \uff5c \u7b80\u4f53\u4e2d\u6587 Install Spiderpool Install Spiderpool on bare metal environment Pods can have one or multiple underlay CNI networks, and enable connect to the underlay network. Please refer to underlay network case for more details. Please refer to the following examples for installation: Create a cluster using macvlan Create a cluster using SR-IOV CNI Create a cluster using ovs Create a cluster: calico CNI for fixed IP addresses Create a cluster: weave CNI for fixed IP addresses Install Spiderpool on VMs and Public Cloud Environments On VMs and Public Cloud environments, it could use the Spiderpool to directly access underlay network of the VPC : Create a cluster on Alibaba Cloud with ipvlan-based networking Create a cluster on VMware vSphere Create a cluster on AWS with ipvlan-based networking For VMware vSphere platform, you can run ipvlan CNI without enabling \"promiscuous\" mode for vSwitch to ensure forwarding performance. Refer to the installation guide for details. Install Spiderpool with Overlay CNI for dual CNI case Pods can own one overlay CNI interfaces and multiple underlay CNI interfaces of the Spiderpool, and enable connect to both overlay and underlay networks. Please refer to dual CNIs case for more details. Please refer to the following examples for installation: Create a dual-network cluster using kind Create a dual-network cluster using calico and macvlan CNI Create a dual-network cluster using Cilium and macvlan CNI Install Spiderpool for AI cluster AI clusters typically use multi-path RDMA networks to provide communication for GPUs. Spiderpool can enable RDMA communication capabilities for containers. Create a cluster: provide Infiniband and RoCE RDMA network with SR-IOV Create a cluster: provide RoCE RDMA network with Macvlan TLS Certificate During the Spiderpool installation, you can choose a method for TLS certificate generation. For more information, refer to the article . Uninstall Spiderpool For instructions on how to uninstall Spiderpool, please refer to the uninstall guide . Upgrade Spiderpool For instructions on how to upgrade Spiderpool, please refer to the upgrade guide . Use Spiderpool IPAM Applications can share an IP pool. See the example for reference. Stateless applications can have a dedicated IP address pool with fixed IP usage range for all Pods. Refer to the example for more details. For stateful applications, each Pod can be allocated a persistent fixed IP address. It also provides control over the IP range used by all Pods during scaling operations. Refer to the example for details. Underlay networking support is available for kubevirt, allowing fixed IP addresses for virtual machines. Refer to the example for details. Applications deployed across subnets can be assigned different subnet IP addresses for each replica. Refer to the example for details. The Subnet feature separates responsibilities between infrastructure administrators and application ones. It automates IP pool management for applications with fixed IP requirements, enabling automatic creation, scaling, and deletion of fixed IP pools. This greatly reduces operational burden. See the example for practical use cases. In addition to supporting native Kubernetes application controllers, Spiderpool's Subnet feature complements third-party application controllers implemented using operators. Refer to the example for more details. Default IP pools can be set at either the cluster-level or tenant-level. IP pools can be shared throughout the entire cluster or restricted to specific tenants. Check out the example for details. IP pool based on node topology caters to fine-grained subnet planning requirements for each node. Refer to the example for details. Custom routing can be achieved through IP pools, Pod annotations, and other methods. Refer to the example for details. Multiple IP pools can be configured by applications to provide redundancy for IP resources. Refer to the example for details.. Global reserved IP addresses can be specified to prevent IPAM from allocating those addresses, thereby avoiding conflicts with externally used IPs. Refer to the example for details. Efficient performance in IP address allocation and release is ensured. Refer to the report for details.. Well-designed IP reclamation mechanisms promptly allocate IP addresses during cluster or application recovery processes. Refer to the example for details. Multiple Network Interfaces Features Spiderpool offers the ability to assign IP addresses from different subnets to multiple network interfaces of a Pod. This feature ensures coordinated policy routing among all interfaces, guaranteeing consistent data paths for outgoing and incoming requests and mitigating packet loss. Moreover, it allows for customization of the default route using a specific network interface's gateway. For Pods with multiple underlay CNI network interfaces, you can refer to the example . For Pods with one overlay network interface and multiple underlay interfaces, you can refer to the example . Connectivity Support for shared and exclusive modes of RDMA network cards enables applications to utilize RDMA communication devices via maclan, ipvlan, and SR-IOV CNI. For more details, see the SR-IOV example and Macvlan example . coordinator plugin facilitates MAC address reconfiguration based on the IP address of the network interface, ensuring a one-to-one correspondence between them. This approach prevents the need to update ARP forwarding rules in network switches and routers, thus eliminating packet loss. Read the article for further information. Spiderpool enables access to ClusterIP through kube-proxy and eBPF kube-proxy replacement for plugins such as Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI . This allows seamless communication between Pods and the host machine, thereby resolving Pod health check issues. Refer to the example for details. Spiderpool assists in IP address conflict detection and gateway reachability checks, ensuring uninterrupted Pod communication. Refer to the example for details. The network of Multi-cluster could be connected by a same underlay network, or Submariner . Operations and Management Spiderpool dynamically creates BOND interfaces and VLAN sub-interfaces on the host machine during Pod startup. This feature assists in setting up master interfaces for Macvlan CNI and ipvlan CNI . Check out the example for implementation details. Convenient generation of Multus NetworkAttachmentDefinition instances with optimized CNI configurations. Spiderpool ensures correct JSON formatting to enhance user experience. See the example for details. Other Features Metrics Support for AMD64 and ARM64 architectures All features are compatible with ipv4-only, ipv6-only, and dual-stack scenarios. Refer to the example for use cases.","title":"Index"},{"location":"usage/readme/#usage-index","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"Usage Index"},{"location":"usage/readme/#install-spiderpool","text":"","title":"Install Spiderpool"},{"location":"usage/readme/#install-spiderpool-on-bare-metal-environment","text":"Pods can have one or multiple underlay CNI networks, and enable connect to the underlay network. Please refer to underlay network case for more details. Please refer to the following examples for installation: Create a cluster using macvlan Create a cluster using SR-IOV CNI Create a cluster using ovs Create a cluster: calico CNI for fixed IP addresses Create a cluster: weave CNI for fixed IP addresses","title":"Install Spiderpool on bare metal environment"},{"location":"usage/readme/#install-spiderpool-on-vms-and-public-cloud-environments","text":"On VMs and Public Cloud environments, it could use the Spiderpool to directly access underlay network of the VPC : Create a cluster on Alibaba Cloud with ipvlan-based networking Create a cluster on VMware vSphere Create a cluster on AWS with ipvlan-based networking For VMware vSphere platform, you can run ipvlan CNI without enabling \"promiscuous\" mode for vSwitch to ensure forwarding performance. Refer to the installation guide for details.","title":"Install Spiderpool on VMs and Public Cloud Environments"},{"location":"usage/readme/#install-spiderpool-with-overlay-cni-for-dual-cni-case","text":"Pods can own one overlay CNI interfaces and multiple underlay CNI interfaces of the Spiderpool, and enable connect to both overlay and underlay networks. Please refer to dual CNIs case for more details. Please refer to the following examples for installation: Create a dual-network cluster using kind Create a dual-network cluster using calico and macvlan CNI Create a dual-network cluster using Cilium and macvlan CNI","title":"Install Spiderpool with Overlay CNI for dual CNI case"},{"location":"usage/readme/#install-spiderpool-for-ai-cluster","text":"AI clusters typically use multi-path RDMA networks to provide communication for GPUs. Spiderpool can enable RDMA communication capabilities for containers. Create a cluster: provide Infiniband and RoCE RDMA network with SR-IOV Create a cluster: provide RoCE RDMA network with Macvlan","title":"Install Spiderpool for AI cluster"},{"location":"usage/readme/#tls-certificate","text":"During the Spiderpool installation, you can choose a method for TLS certificate generation. For more information, refer to the article .","title":"TLS Certificate"},{"location":"usage/readme/#uninstall-spiderpool","text":"For instructions on how to uninstall Spiderpool, please refer to the uninstall guide .","title":"Uninstall Spiderpool"},{"location":"usage/readme/#upgrade-spiderpool","text":"For instructions on how to upgrade Spiderpool, please refer to the upgrade guide .","title":"Upgrade Spiderpool"},{"location":"usage/readme/#use-spiderpool","text":"","title":"Use Spiderpool"},{"location":"usage/readme/#ipam","text":"Applications can share an IP pool. See the example for reference. Stateless applications can have a dedicated IP address pool with fixed IP usage range for all Pods. Refer to the example for more details. For stateful applications, each Pod can be allocated a persistent fixed IP address. It also provides control over the IP range used by all Pods during scaling operations. Refer to the example for details. Underlay networking support is available for kubevirt, allowing fixed IP addresses for virtual machines. Refer to the example for details. Applications deployed across subnets can be assigned different subnet IP addresses for each replica. Refer to the example for details. The Subnet feature separates responsibilities between infrastructure administrators and application ones. It automates IP pool management for applications with fixed IP requirements, enabling automatic creation, scaling, and deletion of fixed IP pools. This greatly reduces operational burden. See the example for practical use cases. In addition to supporting native Kubernetes application controllers, Spiderpool's Subnet feature complements third-party application controllers implemented using operators. Refer to the example for more details. Default IP pools can be set at either the cluster-level or tenant-level. IP pools can be shared throughout the entire cluster or restricted to specific tenants. Check out the example for details. IP pool based on node topology caters to fine-grained subnet planning requirements for each node. Refer to the example for details. Custom routing can be achieved through IP pools, Pod annotations, and other methods. Refer to the example for details. Multiple IP pools can be configured by applications to provide redundancy for IP resources. Refer to the example for details.. Global reserved IP addresses can be specified to prevent IPAM from allocating those addresses, thereby avoiding conflicts with externally used IPs. Refer to the example for details. Efficient performance in IP address allocation and release is ensured. Refer to the report for details.. Well-designed IP reclamation mechanisms promptly allocate IP addresses during cluster or application recovery processes. Refer to the example for details.","title":"IPAM"},{"location":"usage/readme/#multiple-network-interfaces-features","text":"Spiderpool offers the ability to assign IP addresses from different subnets to multiple network interfaces of a Pod. This feature ensures coordinated policy routing among all interfaces, guaranteeing consistent data paths for outgoing and incoming requests and mitigating packet loss. Moreover, it allows for customization of the default route using a specific network interface's gateway. For Pods with multiple underlay CNI network interfaces, you can refer to the example . For Pods with one overlay network interface and multiple underlay interfaces, you can refer to the example .","title":"Multiple Network Interfaces Features"},{"location":"usage/readme/#connectivity","text":"Support for shared and exclusive modes of RDMA network cards enables applications to utilize RDMA communication devices via maclan, ipvlan, and SR-IOV CNI. For more details, see the SR-IOV example and Macvlan example . coordinator plugin facilitates MAC address reconfiguration based on the IP address of the network interface, ensuring a one-to-one correspondence between them. This approach prevents the need to update ARP forwarding rules in network switches and routers, thus eliminating packet loss. Read the article for further information. Spiderpool enables access to ClusterIP through kube-proxy and eBPF kube-proxy replacement for plugins such as Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI . This allows seamless communication between Pods and the host machine, thereby resolving Pod health check issues. Refer to the example for details. Spiderpool assists in IP address conflict detection and gateway reachability checks, ensuring uninterrupted Pod communication. Refer to the example for details. The network of Multi-cluster could be connected by a same underlay network, or Submariner .","title":"Connectivity"},{"location":"usage/readme/#operations-and-management","text":"Spiderpool dynamically creates BOND interfaces and VLAN sub-interfaces on the host machine during Pod startup. This feature assists in setting up master interfaces for Macvlan CNI and ipvlan CNI . Check out the example for implementation details. Convenient generation of Multus NetworkAttachmentDefinition instances with optimized CNI configurations. Spiderpool ensures correct JSON formatting to enhance user experience. See the example for details.","title":"Operations and Management"},{"location":"usage/readme/#other-features","text":"Metrics Support for AMD64 and ARM64 architectures All features are compatible with ipv4-only, ipv6-only, and dual-stack scenarios. Refer to the example for use cases.","title":"Other Features"},{"location":"usage/reserved-ip-zh_CN/","text":"Reserved IP \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u901a\u8fc7 ReservedIP CR \u4e3a\u6574\u4e2a Kubernetes \u96c6\u7fa4\u4fdd\u7559\u4e00\u4e9b IP \u5730\u5740\uff0c\u8fd9\u4e9b IP \u5730\u5740\u5c06\u4e0d\u4f1a\u88ab IPAM \u5206\u914d\u3002 Reserved IP \u529f\u80fd \u5f53\u660e\u786e\u67d0\u4e2a IP \u5730\u5740\u5df2\u7ecf\u88ab\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528\u65f6\uff0c\u4e3a\u4e86\u907f\u514d IP \u51b2\u7a81\uff0c\u4ece\u5b58\u91cf\u7684 IPPool \u5b9e\u4f8b\u627e\u5230\u8be5 IP \u5730\u5740\u5e76\u5254\u9664\uff0c\u4e5f\u8bb8\u662f\u4e00\u4ef6\u8017\u65f6\u8017\u529b\u7684\u5de5\u4f5c\u3002\u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u5e0c\u671b\u5b58\u91cf\u6216\u8005\u672a\u6765\u7684\u6240\u6709 IPPool \u8d44\u6e90\u4e2d\uff0c\u90fd\u4e0d\u4f1a\u5206\u914d\u51fa\u8be5 IP \u5730\u5740\u3002\u56e0\u6b64\uff0c\u53ef\u5728 ReservedIP CR \u4e2d\u8bbe\u7f6e\u5e0c\u671b\u4e0d\u88ab\u96c6\u7fa4\u6240\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u8bbe\u7f6e\u540e\uff0c\u5373\u4f7f\u662f\u5728 IPPool \u5bf9\u8c61\u4e2d\u5305\u542b\u4e86\u8be5 IP \u5730\u5740\uff0cIPAM \u63d2\u4ef6\u4e5f\u4e0d\u4f1a\u628a\u8fd9\u4e9b IP \u5730\u5740\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u3002 ReservedIP \u4e2d\u7684 IP \u5730\u5740\u53ef\u4ee5\u662f\uff1a \u660e\u786e\u8be5 IP \u5730\u5740\u88ab\u96c6\u7fa4\u5916\u90e8\u4e3b\u673a\u4f7f\u7528 \u660e\u786e\u8be5 IP \u5730\u5740\u4e0d\u80fd\u88ab\u4f7f\u7528\u4e8e\u7f51\u7edc\u901a\u4fe1\uff0c\u4f8b\u5982\u5b50\u7f51 IP\u3001\u5e7f\u64ad IP \u7b49 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m \u521b\u5efa ReservedIP \u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u6307\u5b9a spec.ips \u4e3a 10.6.168.131-10.6.168.132 \uff0c\u5e76\u521b\u5efa ReservedIP\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF \u521b\u5efa IPPool \u521b\u5efa\u4e00\u4e2a spec.ips \u4e3a 10.6.168.131-10.6.168.133 \uff0c\u5171\u8ba1 3 \u4e2a IP \u5730\u5740\u7684 IPPool \u3002\u901a\u8fc7\u4e0e\u4e0a\u8ff0\u7684 ReservedIP \u5bf9\u6bd4\u53ef\u77e5\uff0c\u8be5 IP \u6c60\u53ea\u6709 1 \u4e2a IP \u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF \u4f7f\u7528\u5982\u4e0b\u7684 Yaml \uff0c\u521b\u5efa\u4e00\u4e2a\u5177\u6709 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5e76\u4ece\u4e0a\u9762\u7684 IPPool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\u7684 IP \u6c60\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u56e0\u4e3a IP \u6c60\u4e2d\u7684\u4e24\u4e2a IP \u88ab ReservedIP CR \u6240\u4fdd\u7559\uff0cIP \u6c60\u4e2d\u53ea\u6709\u4e00\u4e2a IP \u53ef\u7528\u3002\u5e94\u7528\u53ea\u4f1a\u6709\u4e00\u4e2a Pod \u53ef\u4ee5\u6210\u529f\u8fd0\u884c\uff0c\u53e6\u4e00\u4e2a Pod \u7531\u4e8e \"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\" \u800c\u521b\u5efa\u5931\u8d25\u3002 ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> \u5982\u679c\u5e94\u7528\u7684 Pod \u5df2\u7ecf\u5206\u914d\u4e86\u8981\u4fdd\u7559\u7684 IP \u5730\u5740\uff0c\u5c06\u8be5 IP \u5730\u5740\u6dfb\u52a0\u5230 ReservedIP CR \u4e2d\uff0c\u5f53\u5e94\u7528\u526f\u672c\u91cd\u542f\u540e\uff0c\u526f\u672c\u5c06\u65e0\u6cd5\u8fd0\u884c\u3002\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06 Pod \u6240\u5206\u914d\u7684 IP \u5730\u5740\u52a0\u5165\u5230 ReservedIP CR \u4e2d\uff0c\u7136\u540e\u91cd\u542f Pod \uff0cPod \u56e0\"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\"\u800c\u542f\u52a8\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> \u4fdd\u7559 IP \u88ab\u79fb\u9664\u540e\uff0cPod \u53ef\u4ee5\u83b7\u5f97 IP \u5730\u5740\u5e76\u8fd0\u884c\u3002 ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none> \u603b\u7ed3 SpiderReservedIP \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u66f4\u52a0\u5bb9\u6613\u7684\u8fdb\u884c\u7f51\u7edc\u89c4\u5212\u3002","title":"Reserved IP"},{"location":"usage/reserved-ip-zh_CN/#reserved-ip","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Reserved IP"},{"location":"usage/reserved-ip-zh_CN/#_1","text":"Spiderpool \u901a\u8fc7 ReservedIP CR \u4e3a\u6574\u4e2a Kubernetes \u96c6\u7fa4\u4fdd\u7559\u4e00\u4e9b IP \u5730\u5740\uff0c\u8fd9\u4e9b IP \u5730\u5740\u5c06\u4e0d\u4f1a\u88ab IPAM \u5206\u914d\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/reserved-ip-zh_CN/#reserved-ip_1","text":"\u5f53\u660e\u786e\u67d0\u4e2a IP \u5730\u5740\u5df2\u7ecf\u88ab\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528\u65f6\uff0c\u4e3a\u4e86\u907f\u514d IP \u51b2\u7a81\uff0c\u4ece\u5b58\u91cf\u7684 IPPool \u5b9e\u4f8b\u627e\u5230\u8be5 IP \u5730\u5740\u5e76\u5254\u9664\uff0c\u4e5f\u8bb8\u662f\u4e00\u4ef6\u8017\u65f6\u8017\u529b\u7684\u5de5\u4f5c\u3002\u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u5e0c\u671b\u5b58\u91cf\u6216\u8005\u672a\u6765\u7684\u6240\u6709 IPPool \u8d44\u6e90\u4e2d\uff0c\u90fd\u4e0d\u4f1a\u5206\u914d\u51fa\u8be5 IP \u5730\u5740\u3002\u56e0\u6b64\uff0c\u53ef\u5728 ReservedIP CR \u4e2d\u8bbe\u7f6e\u5e0c\u671b\u4e0d\u88ab\u96c6\u7fa4\u6240\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u8bbe\u7f6e\u540e\uff0c\u5373\u4f7f\u662f\u5728 IPPool \u5bf9\u8c61\u4e2d\u5305\u542b\u4e86\u8be5 IP \u5730\u5740\uff0cIPAM \u63d2\u4ef6\u4e5f\u4e0d\u4f1a\u628a\u8fd9\u4e9b IP \u5730\u5740\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u3002 ReservedIP \u4e2d\u7684 IP \u5730\u5740\u53ef\u4ee5\u662f\uff1a \u660e\u786e\u8be5 IP \u5730\u5740\u88ab\u96c6\u7fa4\u5916\u90e8\u4e3b\u673a\u4f7f\u7528 \u660e\u786e\u8be5 IP \u5730\u5740\u4e0d\u80fd\u88ab\u4f7f\u7528\u4e8e\u7f51\u7edc\u901a\u4fe1\uff0c\u4f8b\u5982\u5b50\u7f51 IP\u3001\u5e7f\u64ad IP \u7b49","title":"Reserved IP \u529f\u80fd"},{"location":"usage/reserved-ip-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/reserved-ip-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/reserved-ip-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool.","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/reserved-ip-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/reserved-ip-zh_CN/#reservedip","text":"\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u6307\u5b9a spec.ips \u4e3a 10.6.168.131-10.6.168.132 \uff0c\u5e76\u521b\u5efa ReservedIP\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF","title":"\u521b\u5efa ReservedIP"},{"location":"usage/reserved-ip-zh_CN/#ippool","text":"\u521b\u5efa\u4e00\u4e2a spec.ips \u4e3a 10.6.168.131-10.6.168.133 \uff0c\u5171\u8ba1 3 \u4e2a IP \u5730\u5740\u7684 IPPool \u3002\u901a\u8fc7\u4e0e\u4e0a\u8ff0\u7684 ReservedIP \u5bf9\u6bd4\u53ef\u77e5\uff0c\u8be5 IP \u6c60\u53ea\u6709 1 \u4e2a IP \u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF \u4f7f\u7528\u5982\u4e0b\u7684 Yaml \uff0c\u521b\u5efa\u4e00\u4e2a\u5177\u6709 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5e76\u4ece\u4e0a\u9762\u7684 IPPool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\u7684 IP \u6c60\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u56e0\u4e3a IP \u6c60\u4e2d\u7684\u4e24\u4e2a IP \u88ab ReservedIP CR \u6240\u4fdd\u7559\uff0cIP \u6c60\u4e2d\u53ea\u6709\u4e00\u4e2a IP \u53ef\u7528\u3002\u5e94\u7528\u53ea\u4f1a\u6709\u4e00\u4e2a Pod \u53ef\u4ee5\u6210\u529f\u8fd0\u884c\uff0c\u53e6\u4e00\u4e2a Pod \u7531\u4e8e \"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\" \u800c\u521b\u5efa\u5931\u8d25\u3002 ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> \u5982\u679c\u5e94\u7528\u7684 Pod \u5df2\u7ecf\u5206\u914d\u4e86\u8981\u4fdd\u7559\u7684 IP \u5730\u5740\uff0c\u5c06\u8be5 IP \u5730\u5740\u6dfb\u52a0\u5230 ReservedIP CR \u4e2d\uff0c\u5f53\u5e94\u7528\u526f\u672c\u91cd\u542f\u540e\uff0c\u526f\u672c\u5c06\u65e0\u6cd5\u8fd0\u884c\u3002\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06 Pod \u6240\u5206\u914d\u7684 IP \u5730\u5740\u52a0\u5165\u5230 ReservedIP CR \u4e2d\uff0c\u7136\u540e\u91cd\u542f Pod \uff0cPod \u56e0\"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\"\u800c\u542f\u52a8\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> \u4fdd\u7559 IP \u88ab\u79fb\u9664\u540e\uff0cPod \u53ef\u4ee5\u83b7\u5f97 IP \u5730\u5740\u5e76\u8fd0\u884c\u3002 ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none>","title":"\u521b\u5efa IPPool"},{"location":"usage/reserved-ip-zh_CN/#_4","text":"SpiderReservedIP \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u66f4\u52a0\u5bb9\u6613\u7684\u8fdb\u884c\u7f51\u7edc\u89c4\u5212\u3002","title":"\u603b\u7ed3"},{"location":"usage/reserved-ip/","text":"Reserved IP English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool reserve some IP addresses for the whole Kubernetes cluster through the ReservedIP CR, ensuring that these addresses are not allocated by IPAM. Features of Reserved IP To avoid IP conflicts when it is known that an IP address is being used externally to the cluster, it can be a time-consuming and labor-intensive task to remove that IP address from existing IPPool instances. Furthermore, network administrators want to ensure that this IP address is not allocated from any current or future IPPool resources. To address these concerns, the ReservedIP CR allows for the specification of IP addresses that should not be utilized by the cluster. Even if an IPPool instance includes those IP addresses, the IPAM plugin will refrain from assigning them to Pods. The IP addresses specified in the ReservedIP CR serve two purposes: Clearly identify those IP addresses already in use by hosts outside the cluster. Explicitly prevent the utilization of those IP addresses for network communication, such as subnet IPs or broadcast IPs. Prerequisites A ready Kubernetes kubernetes. Helm has been already installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master\uff1athe interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create a Macvlan SpiderMultusConfig that will automatically generate the corresponding Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m Create reserved IPs To create reserved IPs, use the following YAML to specify spec.ips as 10.6.168.131-10.6.168.132 : cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF Create an IP pool Create an IP pool with spec.ips set to 10.6.168.131-10.6.168.133 , containing a total of 3 IP addresses. However, given the previously created reserved IPs, only 1 IP address is available in this IP pool. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF To allocate IP addresses from this IP pool, use the following YAML to create a Deployment with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for assigning IP addresses to the application cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF Because both IP addresses in the IP pool are reserved by the ReservedIP CR, only one IP address is available in the pool. This means that only one Pod of the application can run successfully, while the other Pod fails to create due to the \"all IPs have been exhausted\" error. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> If a Pod of the application already has been assigned a reserved IP, adding that IP address to the ReservedIP CR will result in the replica failing to run after restarting. Use the following command to add the Pod's allocated IP address to the ReservedIP CR, and then restart the Pod. As expected, the Pod will fail to start due to the \"all IPs have been exhausted\" error. ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> Once the reserved IP is removed, the Pod can obtain an IP address and run successfully. ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none> Conclusion SpiderReservedIP simplifies network planning for infrastructure administrators.","title":"IPAM of Reserved IP"},{"location":"usage/reserved-ip/#reserved-ip","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"Reserved IP"},{"location":"usage/reserved-ip/#introduction","text":"Spiderpool reserve some IP addresses for the whole Kubernetes cluster through the ReservedIP CR, ensuring that these addresses are not allocated by IPAM.","title":"Introduction"},{"location":"usage/reserved-ip/#features-of-reserved-ip","text":"To avoid IP conflicts when it is known that an IP address is being used externally to the cluster, it can be a time-consuming and labor-intensive task to remove that IP address from existing IPPool instances. Furthermore, network administrators want to ensure that this IP address is not allocated from any current or future IPPool resources. To address these concerns, the ReservedIP CR allows for the specification of IP addresses that should not be utilized by the cluster. Even if an IPPool instance includes those IP addresses, the IPAM plugin will refrain from assigning them to Pods. The IP addresses specified in the ReservedIP CR serve two purposes: Clearly identify those IP addresses already in use by hosts outside the cluster. Explicitly prevent the utilization of those IP addresses for network communication, such as subnet IPs or broadcast IPs.","title":"Features of Reserved IP"},{"location":"usage/reserved-ip/#prerequisites","text":"A ready Kubernetes kubernetes. Helm has been already installed.","title":"Prerequisites"},{"location":"usage/reserved-ip/#steps","text":"","title":"Steps"},{"location":"usage/reserved-ip/#install-spiderpool","text":"Refer to Installation to install Spiderpool.","title":"Install Spiderpool"},{"location":"usage/reserved-ip/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master\uff1athe interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create a Macvlan SpiderMultusConfig that will automatically generate the corresponding Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"Install CNI"},{"location":"usage/reserved-ip/#create-reserved-ips","text":"To create reserved IPs, use the following YAML to specify spec.ips as 10.6.168.131-10.6.168.132 : cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF","title":"Create reserved IPs"},{"location":"usage/reserved-ip/#create-an-ip-pool","text":"Create an IP pool with spec.ips set to 10.6.168.131-10.6.168.133 , containing a total of 3 IP addresses. However, given the previously created reserved IPs, only 1 IP address is available in this IP pool. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF To allocate IP addresses from this IP pool, use the following YAML to create a Deployment with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for assigning IP addresses to the application cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF Because both IP addresses in the IP pool are reserved by the ReservedIP CR, only one IP address is available in the pool. This means that only one Pod of the application can run successfully, while the other Pod fails to create due to the \"all IPs have been exhausted\" error. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> If a Pod of the application already has been assigned a reserved IP, adding that IP address to the ReservedIP CR will result in the replica failing to run after restarting. Use the following command to add the Pod's allocated IP address to the ReservedIP CR, and then restart the Pod. As expected, the Pod will fail to start due to the \"all IPs have been exhausted\" error. ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> Once the reserved IP is removed, the Pod can obtain an IP address and run successfully. ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none>","title":"Create an IP pool"},{"location":"usage/reserved-ip/#conclusion","text":"SpiderReservedIP simplifies network planning for infrastructure administrators.","title":"Conclusion"},{"location":"usage/route-zh_CN/","text":"\u8def\u7531\u652f\u6301 \u7b80\u4f53\u4e2d\u6587 \uff5c English \u4ecb\u7ecd Spiderpool \u63d0\u4f9b\u4e86\u4e3a Pod \u914d\u7f6e\u8def\u7531\u4fe1\u606f\u7684\u529f\u80fd\u3002 \u642d\u914d\u7f51\u5173\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531 \u4e3a SpiderIPPool \u8d44\u6e90\u8bbe\u7f6e**\u7f51\u5173\u5730\u5740**( spec.gateway )\u540e\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8be5\u7f51\u5173\u5730\u5740\u4e3a Pod \u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 \u7ee7\u627f IP \u6c60\u8def\u7531 \u6211\u4eec\u4e5f\u53ef\u4e3a SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u8def\u7531( spec.routes )\uff0c\u521b\u5efa Pod \u65f6\u4f1a\u7ee7\u627f\u8be5\u8def\u7531\uff1a \u5f53 SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u540e\uff0c\u8bf7\u52ff\u4e3a\u8def\u7531\u5b57\u6bb5\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1 \u81ea\u5b9a\u4e49\u8def\u7531 \u6211\u4eec\u4e5f\u652f\u6301\u4e3a\u5e94\u7528\u914d\u7f6e\u81ea\u5b9a\u4e49\u8def\u7531\u7684\u529f\u80fd\uff0c\u53ea\u9700\u4e3a Pod \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/routes : \u5f53 SpiderIPPool \u8d44\u6e90\u4e2d\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u3001\u6216\u914d\u7f6e\u4e86\u9ed8\u8ba4\u8def\u7531\u540e\uff0c\u8bf7\u52ff\u4e3a Pod \u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"\u8def\u7531\u652f\u6301"},{"location":"usage/route-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 \uff5c English","title":"\u8def\u7531\u652f\u6301"},{"location":"usage/route-zh_CN/#_2","text":"Spiderpool \u63d0\u4f9b\u4e86\u4e3a Pod \u914d\u7f6e\u8def\u7531\u4fe1\u606f\u7684\u529f\u80fd\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/route-zh_CN/#_3","text":"\u4e3a SpiderIPPool \u8d44\u6e90\u8bbe\u7f6e**\u7f51\u5173\u5730\u5740**( spec.gateway )\u540e\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8be5\u7f51\u5173\u5730\u5740\u4e3a Pod \u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0","title":"\u642d\u914d\u7f51\u5173\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531"},{"location":"usage/route-zh_CN/#ip","text":"\u6211\u4eec\u4e5f\u53ef\u4e3a SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u8def\u7531( spec.routes )\uff0c\u521b\u5efa Pod \u65f6\u4f1a\u7ee7\u627f\u8be5\u8def\u7531\uff1a \u5f53 SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u540e\uff0c\u8bf7\u52ff\u4e3a\u8def\u7531\u5b57\u6bb5\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1","title":"\u7ee7\u627f IP \u6c60\u8def\u7531"},{"location":"usage/route-zh_CN/#_4","text":"\u6211\u4eec\u4e5f\u652f\u6301\u4e3a\u5e94\u7528\u914d\u7f6e\u81ea\u5b9a\u4e49\u8def\u7531\u7684\u529f\u80fd\uff0c\u53ea\u9700\u4e3a Pod \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/routes : \u5f53 SpiderIPPool \u8d44\u6e90\u4e2d\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u3001\u6216\u914d\u7f6e\u4e86\u9ed8\u8ba4\u8def\u7531\u540e\uff0c\u8bf7\u52ff\u4e3a Pod \u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"\u81ea\u5b9a\u4e49\u8def\u7531"},{"location":"usage/route/","text":"Route support English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool supports the configuration of routing information for Pods. Configure Default Route with Gateway When setting the gateway address ( spec.gateway ) for a SpiderIPPool resource, a default route will be generated for Pods based on that gateway address: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 Inherit IP Pool Routes SpiderIPPool resources also support configuring routes ( spec.routes ), which will be inherited by Pods during their creation process: If a gateway address is configured for the SpiderIPPool resource, avoid setting default routes in the routes field. Both dst and gw fields are required. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1 Customize Routes You can customize routes for Pods by adding the annotation ipam.spidernet.io/routes : When a gateway address or default route is configured in the SpiderIPPool resource, avoid configuring default routes for Pods. Both dst and gw fields are required. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"Route Support"},{"location":"usage/route/#route-support","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"Route support"},{"location":"usage/route/#introduction","text":"Spiderpool supports the configuration of routing information for Pods.","title":"Introduction"},{"location":"usage/route/#configure-default-route-with-gateway","text":"When setting the gateway address ( spec.gateway ) for a SpiderIPPool resource, a default route will be generated for Pods based on that gateway address: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0","title":"Configure Default Route with Gateway"},{"location":"usage/route/#inherit-ip-pool-routes","text":"SpiderIPPool resources also support configuring routes ( spec.routes ), which will be inherited by Pods during their creation process: If a gateway address is configured for the SpiderIPPool resource, avoid setting default routes in the routes field. Both dst and gw fields are required. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1","title":"Inherit IP Pool Routes"},{"location":"usage/route/#customize-routes","text":"You can customize routes for Pods by adding the annotation ipam.spidernet.io/routes : When a gateway address or default route is configured in the SpiderIPPool resource, avoid configuring default routes for Pods. Both dst and gw fields are required. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"Customize Routes"},{"location":"usage/spider-affinity-zh_CN/","text":"SpiderIPPool Affinity \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderIPPool \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u56e0\u6b64\uff0c\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a\u5b50\u7f51\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002 \u5feb\u901f\u5165\u95e8 \u5728 SpiderIPPool CRD \u91cc\uff0c\u6211\u4eec\u6709\u5b9a\u4e49\u5f88\u591a\u7684\u5b57\u6bb5\u6765\u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528\uff0c\u5982: spec.podAffinity \u5b57\u6bb5\u53ef\u63a7\u5236\u8be5\u6c60\u662f\u5426\u53ef\u88ab Pod \u4f7f\u7528\u3002 spec.namespaceName \u548c spec.namespaceAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u7684Namespace\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( namespaceName \u4f18\u5148\u7ea7\u9ad8\u4e8e namespaceAffinity ) spec.nodeName \u548c spec.nodeAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u6240\u5728\u7684\u8282\u70b9\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( nodeName \u4f18\u5148\u7ea7\u9ad8\u4e8e nodeAffinity ) multusName \u5b57\u6bb5\u4f1a\u5224\u65ad\u5f53\u524d\u7f51\u5361\u662f\u5426\u4e0e multus \u7684 net-attach-def \u8d44\u6e90\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002 \u8fd9\u4e9b\u5b57\u6bb5\u4e0d\u4ec5\u8d77\u5230**\u8fc7\u6ee4**\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u4e5f\u4f1a\u8d77\u5230\u4e00\u4e2a**\u6392\u5e8f**\u7684\u6548\u679c\uff0c\u82e5\u5339\u914d\u7684\u5b57\u6bb5\u8d8a\u591a\uff0c\u8d8a\u4f18\u5148\u4f7f\u7528\u8be5 IP \u6c60\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-pod-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 podAffinity : matchLabels : app : test-app-3 nodeName : - master - worker1 namespaceName : - kube-system - default multusName : - kube-system/macvlan-vlan0 \u5e94\u7528\u4eb2\u548c\u6027 \u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u7531\u4e8e\u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff0c\u4e00\u7ec4 Deployment \u5b83\u7684\u6240\u6709 Pod \u671f\u671b\u80fd\u591f\u5728\u4e00\u4e2a\u56fa\u5b9a\u7684 IP \u5730\u5740\u8303\u56f4\u5185\u8f6e\u6eda\u5206\u914d IP \u5730\u5740\uff0c\u4ee5\u914d\u5408\u9632\u706b\u5899\u7684\u653e\u884c\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0 Underlay \u7f51\u7edc\u4e0b\u7684\u5357\u5317\u901a\u4fe1\u3002 \u5728\u793e\u533a\u73b0\u6709\u65b9\u6848\u4e2d\uff0c\u662f\u901a\u8fc7\u5728 Deployment \u4e0a\u5199\u5173\u4e8e IP \u5730\u5740\u7684\u6ce8\u89e3\u6765\u5b9e\u73b0\u3002\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\uff0c\u5982\uff1a \u968f\u7740\u5e94\u7528\u7684\u6269\u5bb9\uff0c\u9700\u8981\u4eba\u4e3a\u624b\u52a8\u7684\u4fee\u6539\u5e94\u7528\u7684 annotaiton \uff0c\u91cd\u65b0\u89c4\u5212 IP \u5730\u5740\u3002 annotaiton \u65b9\u5f0f\u7684 IP \u7ba1\u7406\uff0c\u8131\u94a9\u4e8e\u5b83\u4eec\u81ea\u8eab\u7684 IPPool CR \u673a\u5236\uff0c\u5f62\u6210\u7ba1\u7406\u4e0a\u7684\u7a7a\u767d\uff0c\u65e0\u6cd5\u83b7\u77e5\u54ea\u4e9b IP \u53ef\u7528\u3002 \u4e0d\u540c\u5e94\u7528\u95f4\u6781\u5176\u5bb9\u6613\u5206\u914d\u4e86\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u4ece\u800c\u5bfc\u81f4\u5e94\u7528\u521b\u5efa\u5931\u8d25\u3002 \u5bf9\u6b64\uff0cSpiderpool \u501f\u52a9\u4e8e IPPool \u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u7684\u7279\u70b9\uff0c\u5e76\u7ed3\u5408\u8bbe\u7f6e IPPool \u7684 podAffinity \uff0c\u53ef\u5b9e\u73b0\u540c\u4e00\u7ec4\u6216\u8005\u591a\u7ec4\u5e94\u7528\u7684\u4eb2\u548c\u7ed1\u5b9a\uff0c\u65e2\u4fdd\u8bc1\u4e86 IP \u7ba1\u7406\u65b9\u5f0f\u7684\u7edf\u4e00\uff0c\u53c8\u89e3\u8026 \"\u5e94\u7528\u6269\u5bb9\" \u548c \"IP \u5730\u5740\u6269\u5bb9\"\uff0c\u4e5f\u56fa\u5b9a\u4e86\u5e94\u7528\u7684 IP \u4f7f\u7528\u8303\u56f4\u3002 \u521b\u5efa\u5e94\u7528\u4eb2\u548c\u6027\u7684 IPPool SpiderIPPool \u63d0\u4f9b\u4e86 podAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u7684 selector.matchLabels \u7b26\u5408\u8be5 podAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 YAML\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u5e94\u7528\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a app: test-app-3 Pod \u7b26\u5408\u6761\u4ef6\u7684 selector.matchLabel \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF \u521b\u5efa\u6307\u5b9a matchLabels \u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c \u4f1a\u521b\u5efa\u4e00\u7ec4 Deployment \u5e94\u7528\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u5e94\u7528\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label\u3002 \u6700\u7ec8\uff0c\u521b\u5efa\u5e94\u7528\u540e\uff0cPod \u7684 matchLabels \u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u8bbe\u7f6e\uff0c\u6210\u529f\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> \u521b\u5efa\u53e6\u4e00\u4e2a\u5e94\u7528\uff0c\u5e76\u6307\u5b9a\u4e00\u4e2a\u4e0d\u7b26\u5408 IPPool \u5e94\u7528\u4eb2\u548c\u7684 matchLabels \uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label \u4e3a test-unmatch-labels \uff0c\u4e0d\u5339\u914d IPPool \u4eb2\u548c\u6027\u3002 \u5f53 Pod \u7684 matchLabels \u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u65f6\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none> \u5e94\u7528\u5171\u4eab\u7684 IPPool \u521b\u5efa\u5e94\u7528\u5171\u4eab\u7684 IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 \u521b\u5efa\u4e24\u4e2a deployment\uff0c\u5176 Pod \u8bbe\u7f6e\u6ce8\u91ca \u201cipam.spidernet.io/ippool\u201d \u4ee5\u663e\u5f0f\u6307\u5b9a\u6c60\u9009\u62e9\u89c4\u5219\u3002\u5b83\u5c06\u6210\u529f\u83b7\u5f97IP\u5730\u5740 kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u786e\u8ba4\u6700\u7ec8\u72b6\u6001 kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker \u8282\u70b9\u4eb2\u548c\u6027 \u4e0d\u540c\u7684 node \u4e0a\uff0c\u53ef\u7528\u7684 IP \u8303\u56f4\u4e5f\u8bb8\u5e76\u4e0d\u76f8\u540c\uff0c\u4f8b\u5982\uff1a \u540c\u4e00\u6570\u636e\u4e2d\u5fc3\u5185\uff0c\u96c6\u7fa4\u63a5\u5165\u7684 node \u5206\u5c5e\u4e0d\u540c subnet \u3002 \u5355\u4e2a\u96c6\u7fa4\u4e2d\uff0cnode \u8de8\u8d8a\u4e86\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\u3002 \u5728\u4ee5\u4e0a\u573a\u666f\u4e2d\uff0c\u5f53\u540c\u4e00\u4e2a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u4e86\u4e0d\u540c\u7684 node \u4e0a\uff0c\u9700\u8981\u5206\u914d\u4e0d\u540c subnet \u4e0b\u7684 underlay IP \u5730\u5740\u3002\u5728\u5f53\u524d\u793e\u533a\u73b0\u6709\u65b9\u6848\uff0c\u5b83\u4eec\u5e76\u4e0d\u80fd\u6ee1\u8db3\u8fd9\u6837\u7684\u9700\u6c42\u3002 \u5bf9\u6b64\uff0cSpiderpool \u63d0\u4f9b\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u7684\u65b9\u5f0f\uff0c\u80fd\u5f88\u597d\u7684\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002Spiderpool \u7684 SpiderIPPool CR \u4e2d\uff0c\u63d0\u4f9b\u4e86 nodeAffinity \u4e0e nodeName \u5b57\u6bb5\uff0c\u7528\u4e8e\u8bbe\u7f6e node label selector\uff0c\u4ece\u800c\u5b9e\u73b0 IPPool \u548c node \u4e4b\u95f4\u4eb2\u548c\u6027\uff0c\u5f53 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e2a node \u4e0a\u540e\uff0cIPAM \u63d2\u4ef6\u80fd\u591f\u4ece\u4eb2\u548c\u7684 IPPool \u4e2d\u8fdb\u884c IP \u5730\u5740\u5206\u914d\u3002 \u521b\u5efa\u8282\u70b9\u4eb2\u548c\u7684 IPPool SpiderIPPool \u63d0\u4f9b\u4e86 nodeAffinity \u5b57\u6bb5\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 YAML\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u8282\u70b9\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a\u5728\u8fd0\u884c\u8be5\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a nodeName \uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 \u521b\u5efa\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u8282\u70b9\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 IP \u6c60\u3002 \u521b\u5efa\u5e94\u7528\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u53ea\u6709\u5f53 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 IPPool \u7684\u8282\u70b9\u4eb2\u548c\u8bbe\u7f6e\uff0c\u624d\u80fd\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none> \u79df\u6237\u4eb2\u548c\u6027 \u7ba1\u7406\u5458\u5f80\u5f80\u4f1a\u5728\u96c6\u7fa4\u5212\u5206\u591a\u79df\u6237\uff0c\u80fd\u66f4\u597d\u5730\u9694\u79bb\u3001\u7ba1\u7406\u548c\u534f\u4f5c\uff0c\u540c\u65f6\u4e5f\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u7075\u6d3b\u6027\u7b49\u3002\u9700\u8981\u4e0d\u540c\u529f\u80fd\u7684\u5e94\u7528\u90e8\u7f72\u5728\u4e0d\u540c\u79df\u6237\u4e0b\uff0c\u5bf9\u6b64\uff0c\u671f\u671b\u5b9e\u73b0\u4e00\u4e2a IPPool \u80fd\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a namespace \u4e0b\u7684\u5e94\u7528\u5b9e\u73b0\u4eb2\u548c\uff0c\u800c\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u521b\u5efa\uff0c\u80fd\u5e2e\u52a9\u7ba1\u7406\u5458\u51cf\u5c11\u8fd0\u7ef4\u8d1f\u62c5\u3002 \u5f53\u524d\u793e\u533a\u4e2d\u5e76\u6ca1\u6709\u89e3\u51b3\u4e0a\u8ff0\u573a\u666f\u7684\u6709\u6548\u65b9\u6848\uff0cSpiderpool \u901a\u8fc7\u8bbe\u7f6e SpiderIPPool CR \u4e2d\u7684 namespaceAffinity \u6216 namespaceName \u5b57\u6bb5\uff0c\u5b9e\u73b0\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u79df\u6237\u7684\u4eb2\u548c\u6027\uff0c\u4ece\u800c\u4f7f\u5f97\u6ee1\u8db3\u6761\u4ef6\u7684\u5e94\u7528\u624d\u80fd\u591f\u4ece IPPool \u4e2d\u5206\u914d\u5230 IP \u5730\u5740\u3002 \u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool \u521b\u5efa\u79df\u6237 ~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created \u4f7f\u7528\u5982\u4e0b\u7684 YAML\uff0c\u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool\u3002 SpiderIPPool \u63d0\u4f9b\u4e86 namespaceAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u79df\u6237\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a namespaceName \uff0c\u5f53 namespaceName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u88ab\u521b\u5efa\u65f6\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u4e0d\u7b26\u5408 namespaceName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 namespaceName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 \u521b\u5efa\u6307\u5b9a\u79df\u6237\u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c\u4f1a\u521b\u5efa\u4e00\u7ec4\u5728\u79df\u6237 test-ns1 \u4e0b\u7684 Deployment \u5e94\u7528\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u79df\u6237\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 namespace \uff1a\u6307\u5b9a\u5e94\u7528\u6240\u5728\u79df\u6237\u3002 \u6700\u7ec8, \u521b\u5efa\u5e94\u7528\u540e\uff0c\u5728\u79df\u6237\u5185\u7684\u5e94\u7528 Pod \u6210\u529f\u4ece\u6240\u4eb2\u548c\u7684 IPPool \u4e2d\u5206\u914d\u5230\u4e86 IP \u5730\u5740\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> \u521b\u5efa\u4e00\u4e2a\u4e0d\u5728\u4e0a\u8ff0 test-ns1 \u79df\u6237\u5185\u7684\u5e94\u7528\uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\uff0c\u81ea\u52a8\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u4f7f\u7528\u8be5 IPPool\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF \u5f53 Pod \u6240\u5c5e\u79df\u6237\u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u79df\u6237\u4eb2\u548c\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none> \u7f51\u5361\u914d\u7f6e\u4eb2\u548c\u6027 \u5f53\u4e3a\u5e94\u7528\u521b\u5efa\u591a\u7f51\u5361\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a**\u96c6\u7fa4\u7ea7\u522b\u7f3a\u7701\u6c60**\u6307\u5b9a multus \u7684 net-attach-def \u5b9e\u4f8b\u4eb2\u548c\u6027\u3002\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u901a\u8fc7\u6ce8\u89e3 ipam.spidernet.io/ippools \u663e\u5f0f\u6307\u5b9a\u7f51\u5361\u4e0e IPPool \u8d44\u6e90\u7684\u7ed1\u5b9a\u5173\u7cfb\u66f4\u4e3a\u7b80\u5355\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true , \u4ee5\u6b64\u51cf\u5c11\u4e3a\u5e94\u7528\u6253\u4e0a ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6ce8\u89e3\uff0c\u8ba9\u4f53\u9a8c\u66f4\u4e3a\u7b80\u5355\u3002 spec.multusName \u5b57\u6bb5\u914d\u7f6e\u8be5 IPPool \u5bf9\u5e94\u7684 multus \u7f51\u5361\u914d\u7f6e\u3002(\u82e5\u60a8\u672a\u6307\u5b9a\u5bf9\u5e94 multus \u7684 net-attach-def \u5b9e\u4f8b\u7684 namespace\uff0c\u6211\u4eec\u4f1a\u9ed8\u8ba4\u5c06\u5176\u89c6\u4e3a\u5c5e\u4e8e spiderpool \u5b89\u88c5\u65f6\u7684\u547d\u540d\u7a7a\u95f4) \u521b\u5efa\u591a\u7f51\u5361\u7684\u5e94\u7528\u3002\u6211\u4eec\u53ea\u9700\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c \u4f1a\u521b\u5efa\u6709\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF v1.multus-cni.io/default-network \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\u3002(\u82e5\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u800c\u76f4\u63a5\u4f7f\u7528 multus \u96c6\u7fa4\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\uff0c\u8bf7\u5728 helm \u5b89\u88c5 spiderpool \u65f6\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ) k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u989d\u5916\u7f51\u5361\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u603b\u7ed3 SpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c \u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c \u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002","title":"SpiderIPPool Affinity"},{"location":"usage/spider-affinity-zh_CN/#spiderippool-affinity","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderIPPool Affinity"},{"location":"usage/spider-affinity-zh_CN/#_1","text":"SpiderIPPool \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u56e0\u6b64\uff0c\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a\u5b50\u7f51\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-affinity-zh_CN/#_2","text":"\u5728 SpiderIPPool CRD \u91cc\uff0c\u6211\u4eec\u6709\u5b9a\u4e49\u5f88\u591a\u7684\u5b57\u6bb5\u6765\u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528\uff0c\u5982: spec.podAffinity \u5b57\u6bb5\u53ef\u63a7\u5236\u8be5\u6c60\u662f\u5426\u53ef\u88ab Pod \u4f7f\u7528\u3002 spec.namespaceName \u548c spec.namespaceAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u7684Namespace\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( namespaceName \u4f18\u5148\u7ea7\u9ad8\u4e8e namespaceAffinity ) spec.nodeName \u548c spec.nodeAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u6240\u5728\u7684\u8282\u70b9\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( nodeName \u4f18\u5148\u7ea7\u9ad8\u4e8e nodeAffinity ) multusName \u5b57\u6bb5\u4f1a\u5224\u65ad\u5f53\u524d\u7f51\u5361\u662f\u5426\u4e0e multus \u7684 net-attach-def \u8d44\u6e90\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002 \u8fd9\u4e9b\u5b57\u6bb5\u4e0d\u4ec5\u8d77\u5230**\u8fc7\u6ee4**\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u4e5f\u4f1a\u8d77\u5230\u4e00\u4e2a**\u6392\u5e8f**\u7684\u6548\u679c\uff0c\u82e5\u5339\u914d\u7684\u5b57\u6bb5\u8d8a\u591a\uff0c\u8d8a\u4f18\u5148\u4f7f\u7528\u8be5 IP \u6c60\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-pod-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 podAffinity : matchLabels : app : test-app-3 nodeName : - master - worker1 namespaceName : - kube-system - default multusName : - kube-system/macvlan-vlan0","title":"\u5feb\u901f\u5165\u95e8"},{"location":"usage/spider-affinity-zh_CN/#_3","text":"\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u7531\u4e8e\u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff0c\u4e00\u7ec4 Deployment \u5b83\u7684\u6240\u6709 Pod \u671f\u671b\u80fd\u591f\u5728\u4e00\u4e2a\u56fa\u5b9a\u7684 IP \u5730\u5740\u8303\u56f4\u5185\u8f6e\u6eda\u5206\u914d IP \u5730\u5740\uff0c\u4ee5\u914d\u5408\u9632\u706b\u5899\u7684\u653e\u884c\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0 Underlay \u7f51\u7edc\u4e0b\u7684\u5357\u5317\u901a\u4fe1\u3002 \u5728\u793e\u533a\u73b0\u6709\u65b9\u6848\u4e2d\uff0c\u662f\u901a\u8fc7\u5728 Deployment \u4e0a\u5199\u5173\u4e8e IP \u5730\u5740\u7684\u6ce8\u89e3\u6765\u5b9e\u73b0\u3002\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\uff0c\u5982\uff1a \u968f\u7740\u5e94\u7528\u7684\u6269\u5bb9\uff0c\u9700\u8981\u4eba\u4e3a\u624b\u52a8\u7684\u4fee\u6539\u5e94\u7528\u7684 annotaiton \uff0c\u91cd\u65b0\u89c4\u5212 IP \u5730\u5740\u3002 annotaiton \u65b9\u5f0f\u7684 IP \u7ba1\u7406\uff0c\u8131\u94a9\u4e8e\u5b83\u4eec\u81ea\u8eab\u7684 IPPool CR \u673a\u5236\uff0c\u5f62\u6210\u7ba1\u7406\u4e0a\u7684\u7a7a\u767d\uff0c\u65e0\u6cd5\u83b7\u77e5\u54ea\u4e9b IP \u53ef\u7528\u3002 \u4e0d\u540c\u5e94\u7528\u95f4\u6781\u5176\u5bb9\u6613\u5206\u914d\u4e86\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u4ece\u800c\u5bfc\u81f4\u5e94\u7528\u521b\u5efa\u5931\u8d25\u3002 \u5bf9\u6b64\uff0cSpiderpool \u501f\u52a9\u4e8e IPPool \u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u7684\u7279\u70b9\uff0c\u5e76\u7ed3\u5408\u8bbe\u7f6e IPPool \u7684 podAffinity \uff0c\u53ef\u5b9e\u73b0\u540c\u4e00\u7ec4\u6216\u8005\u591a\u7ec4\u5e94\u7528\u7684\u4eb2\u548c\u7ed1\u5b9a\uff0c\u65e2\u4fdd\u8bc1\u4e86 IP \u7ba1\u7406\u65b9\u5f0f\u7684\u7edf\u4e00\uff0c\u53c8\u89e3\u8026 \"\u5e94\u7528\u6269\u5bb9\" \u548c \"IP \u5730\u5740\u6269\u5bb9\"\uff0c\u4e5f\u56fa\u5b9a\u4e86\u5e94\u7528\u7684 IP \u4f7f\u7528\u8303\u56f4\u3002","title":"\u5e94\u7528\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#ippool","text":"SpiderIPPool \u63d0\u4f9b\u4e86 podAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u7684 selector.matchLabels \u7b26\u5408\u8be5 podAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 YAML\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u5e94\u7528\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a app: test-app-3 Pod \u7b26\u5408\u6761\u4ef6\u7684 selector.matchLabel \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF \u521b\u5efa\u6307\u5b9a matchLabels \u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c \u4f1a\u521b\u5efa\u4e00\u7ec4 Deployment \u5e94\u7528\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u5e94\u7528\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label\u3002 \u6700\u7ec8\uff0c\u521b\u5efa\u5e94\u7528\u540e\uff0cPod \u7684 matchLabels \u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u8bbe\u7f6e\uff0c\u6210\u529f\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> \u521b\u5efa\u53e6\u4e00\u4e2a\u5e94\u7528\uff0c\u5e76\u6307\u5b9a\u4e00\u4e2a\u4e0d\u7b26\u5408 IPPool \u5e94\u7528\u4eb2\u548c\u7684 matchLabels \uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label \u4e3a test-unmatch-labels \uff0c\u4e0d\u5339\u914d IPPool \u4eb2\u548c\u6027\u3002 \u5f53 Pod \u7684 matchLabels \u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u65f6\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none>","title":"\u521b\u5efa\u5e94\u7528\u4eb2\u548c\u6027\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#ippool_1","text":"\u521b\u5efa\u5e94\u7528\u5171\u4eab\u7684 IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 \u521b\u5efa\u4e24\u4e2a deployment\uff0c\u5176 Pod \u8bbe\u7f6e\u6ce8\u91ca \u201cipam.spidernet.io/ippool\u201d \u4ee5\u663e\u5f0f\u6307\u5b9a\u6c60\u9009\u62e9\u89c4\u5219\u3002\u5b83\u5c06\u6210\u529f\u83b7\u5f97IP\u5730\u5740 kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u786e\u8ba4\u6700\u7ec8\u72b6\u6001 kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker","title":"\u5e94\u7528\u5171\u4eab\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#_4","text":"\u4e0d\u540c\u7684 node \u4e0a\uff0c\u53ef\u7528\u7684 IP \u8303\u56f4\u4e5f\u8bb8\u5e76\u4e0d\u76f8\u540c\uff0c\u4f8b\u5982\uff1a \u540c\u4e00\u6570\u636e\u4e2d\u5fc3\u5185\uff0c\u96c6\u7fa4\u63a5\u5165\u7684 node \u5206\u5c5e\u4e0d\u540c subnet \u3002 \u5355\u4e2a\u96c6\u7fa4\u4e2d\uff0cnode \u8de8\u8d8a\u4e86\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\u3002 \u5728\u4ee5\u4e0a\u573a\u666f\u4e2d\uff0c\u5f53\u540c\u4e00\u4e2a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u4e86\u4e0d\u540c\u7684 node \u4e0a\uff0c\u9700\u8981\u5206\u914d\u4e0d\u540c subnet \u4e0b\u7684 underlay IP \u5730\u5740\u3002\u5728\u5f53\u524d\u793e\u533a\u73b0\u6709\u65b9\u6848\uff0c\u5b83\u4eec\u5e76\u4e0d\u80fd\u6ee1\u8db3\u8fd9\u6837\u7684\u9700\u6c42\u3002 \u5bf9\u6b64\uff0cSpiderpool \u63d0\u4f9b\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u7684\u65b9\u5f0f\uff0c\u80fd\u5f88\u597d\u7684\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002Spiderpool \u7684 SpiderIPPool CR \u4e2d\uff0c\u63d0\u4f9b\u4e86 nodeAffinity \u4e0e nodeName \u5b57\u6bb5\uff0c\u7528\u4e8e\u8bbe\u7f6e node label selector\uff0c\u4ece\u800c\u5b9e\u73b0 IPPool \u548c node \u4e4b\u95f4\u4eb2\u548c\u6027\uff0c\u5f53 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e2a node \u4e0a\u540e\uff0cIPAM \u63d2\u4ef6\u80fd\u591f\u4ece\u4eb2\u548c\u7684 IPPool \u4e2d\u8fdb\u884c IP \u5730\u5740\u5206\u914d\u3002","title":"\u8282\u70b9\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#ippool_2","text":"SpiderIPPool \u63d0\u4f9b\u4e86 nodeAffinity \u5b57\u6bb5\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 YAML\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u8282\u70b9\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a\u5728\u8fd0\u884c\u8be5\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a nodeName \uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 \u521b\u5efa\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u8282\u70b9\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 IP \u6c60\u3002 \u521b\u5efa\u5e94\u7528\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u53ea\u6709\u5f53 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 IPPool \u7684\u8282\u70b9\u4eb2\u548c\u8bbe\u7f6e\uff0c\u624d\u80fd\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none>","title":"\u521b\u5efa\u8282\u70b9\u4eb2\u548c\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#_5","text":"\u7ba1\u7406\u5458\u5f80\u5f80\u4f1a\u5728\u96c6\u7fa4\u5212\u5206\u591a\u79df\u6237\uff0c\u80fd\u66f4\u597d\u5730\u9694\u79bb\u3001\u7ba1\u7406\u548c\u534f\u4f5c\uff0c\u540c\u65f6\u4e5f\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u7075\u6d3b\u6027\u7b49\u3002\u9700\u8981\u4e0d\u540c\u529f\u80fd\u7684\u5e94\u7528\u90e8\u7f72\u5728\u4e0d\u540c\u79df\u6237\u4e0b\uff0c\u5bf9\u6b64\uff0c\u671f\u671b\u5b9e\u73b0\u4e00\u4e2a IPPool \u80fd\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a namespace \u4e0b\u7684\u5e94\u7528\u5b9e\u73b0\u4eb2\u548c\uff0c\u800c\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u521b\u5efa\uff0c\u80fd\u5e2e\u52a9\u7ba1\u7406\u5458\u51cf\u5c11\u8fd0\u7ef4\u8d1f\u62c5\u3002 \u5f53\u524d\u793e\u533a\u4e2d\u5e76\u6ca1\u6709\u89e3\u51b3\u4e0a\u8ff0\u573a\u666f\u7684\u6709\u6548\u65b9\u6848\uff0cSpiderpool \u901a\u8fc7\u8bbe\u7f6e SpiderIPPool CR \u4e2d\u7684 namespaceAffinity \u6216 namespaceName \u5b57\u6bb5\uff0c\u5b9e\u73b0\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u79df\u6237\u7684\u4eb2\u548c\u6027\uff0c\u4ece\u800c\u4f7f\u5f97\u6ee1\u8db3\u6761\u4ef6\u7684\u5e94\u7528\u624d\u80fd\u591f\u4ece IPPool \u4e2d\u5206\u914d\u5230 IP \u5730\u5740\u3002","title":"\u79df\u6237\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#ippool_3","text":"\u521b\u5efa\u79df\u6237 ~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created \u4f7f\u7528\u5982\u4e0b\u7684 YAML\uff0c\u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool\u3002 SpiderIPPool \u63d0\u4f9b\u4e86 namespaceAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u79df\u6237\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a namespaceName \uff0c\u5f53 namespaceName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u88ab\u521b\u5efa\u65f6\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u4e0d\u7b26\u5408 namespaceName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 namespaceName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 \u521b\u5efa\u6307\u5b9a\u79df\u6237\u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c\u4f1a\u521b\u5efa\u4e00\u7ec4\u5728\u79df\u6237 test-ns1 \u4e0b\u7684 Deployment \u5e94\u7528\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u79df\u6237\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 namespace \uff1a\u6307\u5b9a\u5e94\u7528\u6240\u5728\u79df\u6237\u3002 \u6700\u7ec8, \u521b\u5efa\u5e94\u7528\u540e\uff0c\u5728\u79df\u6237\u5185\u7684\u5e94\u7528 Pod \u6210\u529f\u4ece\u6240\u4eb2\u548c\u7684 IPPool \u4e2d\u5206\u914d\u5230\u4e86 IP \u5730\u5740\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> \u521b\u5efa\u4e00\u4e2a\u4e0d\u5728\u4e0a\u8ff0 test-ns1 \u79df\u6237\u5185\u7684\u5e94\u7528\uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\uff0c\u81ea\u52a8\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u4f7f\u7528\u8be5 IPPool\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF \u5f53 Pod \u6240\u5c5e\u79df\u6237\u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u79df\u6237\u4eb2\u548c\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none>","title":"\u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#_6","text":"\u5f53\u4e3a\u5e94\u7528\u521b\u5efa\u591a\u7f51\u5361\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a**\u96c6\u7fa4\u7ea7\u522b\u7f3a\u7701\u6c60**\u6307\u5b9a multus \u7684 net-attach-def \u5b9e\u4f8b\u4eb2\u548c\u6027\u3002\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u901a\u8fc7\u6ce8\u89e3 ipam.spidernet.io/ippools \u663e\u5f0f\u6307\u5b9a\u7f51\u5361\u4e0e IPPool \u8d44\u6e90\u7684\u7ed1\u5b9a\u5173\u7cfb\u66f4\u4e3a\u7b80\u5355\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true , \u4ee5\u6b64\u51cf\u5c11\u4e3a\u5e94\u7528\u6253\u4e0a ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6ce8\u89e3\uff0c\u8ba9\u4f53\u9a8c\u66f4\u4e3a\u7b80\u5355\u3002 spec.multusName \u5b57\u6bb5\u914d\u7f6e\u8be5 IPPool \u5bf9\u5e94\u7684 multus \u7f51\u5361\u914d\u7f6e\u3002(\u82e5\u60a8\u672a\u6307\u5b9a\u5bf9\u5e94 multus \u7684 net-attach-def \u5b9e\u4f8b\u7684 namespace\uff0c\u6211\u4eec\u4f1a\u9ed8\u8ba4\u5c06\u5176\u89c6\u4e3a\u5c5e\u4e8e spiderpool \u5b89\u88c5\u65f6\u7684\u547d\u540d\u7a7a\u95f4) \u521b\u5efa\u591a\u7f51\u5361\u7684\u5e94\u7528\u3002\u6211\u4eec\u53ea\u9700\u4ee5\u4e0b\u7684\u793a\u4f8b YAML \u4e2d\uff0c \u4f1a\u521b\u5efa\u6709\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF v1.multus-cni.io/default-network \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\u3002(\u82e5\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u800c\u76f4\u63a5\u4f7f\u7528 multus \u96c6\u7fa4\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\uff0c\u8bf7\u5728 helm \u5b89\u88c5 spiderpool \u65f6\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ) k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u989d\u5916\u7f51\u5361\u7684\u914d\u7f6e\u4fe1\u606f\u3002","title":"\u7f51\u5361\u914d\u7f6e\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#_7","text":"SpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c \u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c \u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-affinity/","text":"SpiderIPPool Affinity English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction SpiderIPPool is a representation of a collection of IP addresses. It allows storing different IP addresses from the same subnet in separate IPPool instances, ensuring that there is no overlap between address sets. This design provides flexibility in managing IP resources within the underlay network, especially when faced with limited availability. SpiderIPPool offers the ability to assign different SpiderIPPool instances to various applications and tenants through affinity rules, allowing for both shared subnet usage and micro-isolation. Quick Start In SpiderIPPool CRD , we defined lots of properties to use with affinities: spec.podAffinity controls whether the pool can be used by the Pod. spec.namespaceName and spec.namespaceAffinity verify if they match the Namespace of the Pod. If there is no match, the pool cannot be used. ( namespaceName takes precedence over namespaceAffinity ). spec.nodeName and spec.nodeAffinity verify if they match the node where the Pod is located. If there is no match, the pool cannot be used. ( nodeName takes precedence over nodeAffinity ). multusName determines whether the current network card which using the pool matches the CNI configuration used by the multus net-attach-def resource. If there is no match, the pool cannot be used. These fields not only serve as filters but also have a sorting effect . The more matching fields there are, the higher priority the IP pool has for usage. Application Affinity Firewalls are commonly used in clusters to manage communication between internal and external networks (north-south communication). To enforce secure access control, firewalls inspect and filter communication traffic while restricting outbound communication. In order to align with firewall policies and enable north-south communication within the underlay network, certain Deployments require all Pods to be assigned IP addresses within a specific range. Existing community solutions rely on annotations to handle IP address allocation for such cases. However, this approach has limitations: Manual modification of annotations becomes necessary as the application scales, leading to potential errors. IP management through annotations is far apart from the IPPool CR mechanism, resulting in a lack of visibility into available IP addresses. Conflicting IP addresses can easily be assigned to different applications, causing deployment failures. Spiderpool addresses these challenges by leveraging the flexibility of IPPools, where IP address collections can be adjusted. By combining this with the podAffinity setting in the SpiderIPPool CR, Spiderpool enables the binding of specific applications or groups of applications to particular IPPools. This ensures a unified approach to IP management, decouples application scaling from IP address scaling, and provides a fixed IP usage range for each application. Create IPPool with Application Affinity SpiderIPPool provides the podAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the Pods' selector.matchLabels match the specified podAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. Based on the above, using the following Yaml, create the following SpiderIPPool with application affinity, which will provide the IP address for the app: test-app-3 Pod's eligible selector.matchLabel . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF Creating Applications with Specific matchLabels. In the example YAML provided, a set of Deployment applications is created. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool : specify the IP pool with application affinity. v1.multus-cni.io/default-network : create a default network interface for the application. matchLabels : set the label for the application. After creating the application, the Pods with matchLabels that match the IPPool's application affinity successfully obtain IP addresses from that SpiderIPPool. The assigned IP addresses remain within the IP pool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> However, when creating another application with different matchLabels that do not meet the IPPool's application affinity, Spiderpool will reject IP address allocation. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF matchLabels : set the label of the application to test-unmatch-labels , which does not match IPPool affinity. Getting an IP address assignment fails as expected when the Pod's matchLabels do not match the application affinity for that IPPool. kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none> Shared IPPool Create an IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 Create two Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker Node Affinity Nodes in a cluster might have access to different IP ranges. Some scenarios include: Nodes in the same data center belonging to different subnets. Nodes spanning multiple data centers within a single cluster. In such cases, replicas of an application are scheduled on different nodes require IP addresses from different subnets. Current community solutions are limited to satisfy this needs. To address this problem, Spiderpool support node affinity solution. By setting the nodeAffinity and nodeName fields in the SpiderIPPool CR, administrators can define a node label selector. This enables the IPAM plugin to allocate IP addresses from the specified IPPool when Pods are scheduled on nodes that match the affinity rules. IPPool with Node Affinity SpiderIPPool offers the nodeAffinity field. When a Pod is scheduled on a node and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the node satisfies the specified nodeAffinity condition. Otherwise, it will be unable to allocate an IP address from that SpiderIPPool. To create a SpiderIPPool with node affinity, use the following YAML configuration. This SpiderIPPool will provide IP addresses for Pods running on the designated node. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool provides an additional option for node affinity: nodeName . When nodeName is specified, a Pod is scheduled on a specific node and attempts to allocate an IP address from the SpiderIPPool. If the node matches the specified nodeName , the IP address can be successfully allocated from that SpiderIPPool. If not, it will be unable to allocate an IP address from that SpiderIPPool. When nodeName is left empty, Spiderpool does not impose any allocation restrictions on Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 Create a set of DaemonSet applications using the following example YAML: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool : specify the IP pool with node affinity. v1.multus-cni.io/default-network : Identifies the IP pool used by the application. After creating an application, it can be observed that IP addresses are only allocated from the corresponding IPPool if the Pod's node matches the IPPool's node affinity. The IP address of the application remains within the assigned IPPool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none> IPPool with Namespace Affinity Cluster administrators often partition their clusters into multiple namespaces to improve isolation, management, collaboration, security, and resource utilization. When deploying applications under different namespaces, it becomes necessary to assign specific IPPools to each namespace, preventing applications from unrelated namespaces from using them. Spiderpool addresses this requirement by introducing the namespaceAffinity or namespaceName fields in the SpiderIPPool CR. This allows administrators to define affinity rules between IPPools and one or more namespaces, ensuring that only applications meeting the specified conditions can be allocated IP addresses from the respective IPPools. Create IPPool with Namespace Affinity ~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created To create an IPPool with namaspace affinity, use the following YAML: SpiderIPPool provides the namespaceAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it will only succeed if the Pod's namespace matches the specified namespaceAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool also offers another option for namespace affinity: namespaceName . When namespaceName is not empty, a Pod is created and attempts to allocate an IP address from the SpiderIPPool. If the namespace of the Pod matches the specified namespaceName , it can successfully obtain an IP from that SpiderIPPool. However, if the namespace does not match the namespaceName , it will be unable to allocate an IP address from that SpiderIPPool. When namespaceName is empty, Spiderpool does not impose any restrictions on IP allocation for Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 Create Applications in a Specified Namespace. In the provided YAML example, a set of Deployment applications is created under the test-ns1 namespace. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aspecify the IP pool with tenant affinity. v1.multus-cni.io/default-network : create a default network interface for the application. namespace : the namespace where the application resides. After creating the application, the Pods within the designated namespace successfully allocate IP addresses from the associated IPPool with namespace affinity. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> However, if an application is created outside the test-ns1 namespace, Spiderpool will reject IP address allocation, preventing unrelated namespace from using that IPPool. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF Getting an IP address assignment fails as expected when the Pod belongs to a namesapce that does not match the affinity of that IPPool. ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none> Multus affinity When creating multiple network interfaces for an application, we can specify the affinity of multus net-attach-def instance for the cluster-level default pool . This way is simpler compared to explicitly specifying the binding relationship between network interfaces and IPPool resources through the ipam.spidernet.io/ippools annotation. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 Set the spec.default field to true to simplify the experience by reducing the need to annotate the application with ipam.spidernet.io/ippool or ipam.spidernet.io/ippools . Configure the spec.multusName field to specify the multus net-attach-def instance. (If you do not specify the namespace of the corresponding multus net-attach-def instance, we will default to the namespace where Spiderpool is installed.) Create an application with multiple network interfaces, you can use the following example YAML: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF v1.multus-cni.io/default-network : Choose the default network configuration for the created application. (If you don't specify this annotation and directly use the clusterNetwork configuration of the multus, please specify the default network configuration during the installation of Spiderpool via Helm using the parameter --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ). k8s.v1.cni.cncf.io/networks : Selects the additional network configuration for the created application. Summary The set of IPs in the SpiderIPPool can be large or small. It can effectively address the limited IP address resources in the underlay network, and this feature allows different applications and tenants to bind to different SpiderIPPools through various affinity rules. It also enables sharing the same SpiderIPPool, allowing all applications to share the same subnet while achieving \"micro-segmentation.\"","title":"IPAM of IPPool Affinity"},{"location":"usage/spider-affinity/#spiderippool-affinity","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderIPPool Affinity"},{"location":"usage/spider-affinity/#introduction","text":"SpiderIPPool is a representation of a collection of IP addresses. It allows storing different IP addresses from the same subnet in separate IPPool instances, ensuring that there is no overlap between address sets. This design provides flexibility in managing IP resources within the underlay network, especially when faced with limited availability. SpiderIPPool offers the ability to assign different SpiderIPPool instances to various applications and tenants through affinity rules, allowing for both shared subnet usage and micro-isolation.","title":"Introduction"},{"location":"usage/spider-affinity/#quick-start","text":"In SpiderIPPool CRD , we defined lots of properties to use with affinities: spec.podAffinity controls whether the pool can be used by the Pod. spec.namespaceName and spec.namespaceAffinity verify if they match the Namespace of the Pod. If there is no match, the pool cannot be used. ( namespaceName takes precedence over namespaceAffinity ). spec.nodeName and spec.nodeAffinity verify if they match the node where the Pod is located. If there is no match, the pool cannot be used. ( nodeName takes precedence over nodeAffinity ). multusName determines whether the current network card which using the pool matches the CNI configuration used by the multus net-attach-def resource. If there is no match, the pool cannot be used. These fields not only serve as filters but also have a sorting effect . The more matching fields there are, the higher priority the IP pool has for usage.","title":"Quick Start"},{"location":"usage/spider-affinity/#application-affinity","text":"Firewalls are commonly used in clusters to manage communication between internal and external networks (north-south communication). To enforce secure access control, firewalls inspect and filter communication traffic while restricting outbound communication. In order to align with firewall policies and enable north-south communication within the underlay network, certain Deployments require all Pods to be assigned IP addresses within a specific range. Existing community solutions rely on annotations to handle IP address allocation for such cases. However, this approach has limitations: Manual modification of annotations becomes necessary as the application scales, leading to potential errors. IP management through annotations is far apart from the IPPool CR mechanism, resulting in a lack of visibility into available IP addresses. Conflicting IP addresses can easily be assigned to different applications, causing deployment failures. Spiderpool addresses these challenges by leveraging the flexibility of IPPools, where IP address collections can be adjusted. By combining this with the podAffinity setting in the SpiderIPPool CR, Spiderpool enables the binding of specific applications or groups of applications to particular IPPools. This ensures a unified approach to IP management, decouples application scaling from IP address scaling, and provides a fixed IP usage range for each application.","title":"Application Affinity"},{"location":"usage/spider-affinity/#create-ippool-with-application-affinity","text":"SpiderIPPool provides the podAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the Pods' selector.matchLabels match the specified podAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. Based on the above, using the following Yaml, create the following SpiderIPPool with application affinity, which will provide the IP address for the app: test-app-3 Pod's eligible selector.matchLabel . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF Creating Applications with Specific matchLabels. In the example YAML provided, a set of Deployment applications is created. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool : specify the IP pool with application affinity. v1.multus-cni.io/default-network : create a default network interface for the application. matchLabels : set the label for the application. After creating the application, the Pods with matchLabels that match the IPPool's application affinity successfully obtain IP addresses from that SpiderIPPool. The assigned IP addresses remain within the IP pool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> However, when creating another application with different matchLabels that do not meet the IPPool's application affinity, Spiderpool will reject IP address allocation. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF matchLabels : set the label of the application to test-unmatch-labels , which does not match IPPool affinity. Getting an IP address assignment fails as expected when the Pod's matchLabels do not match the application affinity for that IPPool. kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none>","title":"Create IPPool with Application Affinity"},{"location":"usage/spider-affinity/#shared-ippool","text":"Create an IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 Create two Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker","title":"Shared IPPool"},{"location":"usage/spider-affinity/#node-affinity","text":"Nodes in a cluster might have access to different IP ranges. Some scenarios include: Nodes in the same data center belonging to different subnets. Nodes spanning multiple data centers within a single cluster. In such cases, replicas of an application are scheduled on different nodes require IP addresses from different subnets. Current community solutions are limited to satisfy this needs. To address this problem, Spiderpool support node affinity solution. By setting the nodeAffinity and nodeName fields in the SpiderIPPool CR, administrators can define a node label selector. This enables the IPAM plugin to allocate IP addresses from the specified IPPool when Pods are scheduled on nodes that match the affinity rules.","title":"Node Affinity"},{"location":"usage/spider-affinity/#ippool-with-node-affinity","text":"SpiderIPPool offers the nodeAffinity field. When a Pod is scheduled on a node and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the node satisfies the specified nodeAffinity condition. Otherwise, it will be unable to allocate an IP address from that SpiderIPPool. To create a SpiderIPPool with node affinity, use the following YAML configuration. This SpiderIPPool will provide IP addresses for Pods running on the designated node. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool provides an additional option for node affinity: nodeName . When nodeName is specified, a Pod is scheduled on a specific node and attempts to allocate an IP address from the SpiderIPPool. If the node matches the specified nodeName , the IP address can be successfully allocated from that SpiderIPPool. If not, it will be unable to allocate an IP address from that SpiderIPPool. When nodeName is left empty, Spiderpool does not impose any allocation restrictions on Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 Create a set of DaemonSet applications using the following example YAML: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool : specify the IP pool with node affinity. v1.multus-cni.io/default-network : Identifies the IP pool used by the application. After creating an application, it can be observed that IP addresses are only allocated from the corresponding IPPool if the Pod's node matches the IPPool's node affinity. The IP address of the application remains within the assigned IPPool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none>","title":"IPPool with Node Affinity"},{"location":"usage/spider-affinity/#ippool-with-namespace-affinity","text":"Cluster administrators often partition their clusters into multiple namespaces to improve isolation, management, collaboration, security, and resource utilization. When deploying applications under different namespaces, it becomes necessary to assign specific IPPools to each namespace, preventing applications from unrelated namespaces from using them. Spiderpool addresses this requirement by introducing the namespaceAffinity or namespaceName fields in the SpiderIPPool CR. This allows administrators to define affinity rules between IPPools and one or more namespaces, ensuring that only applications meeting the specified conditions can be allocated IP addresses from the respective IPPools.","title":"IPPool with Namespace Affinity"},{"location":"usage/spider-affinity/#create-ippool-with-namespace-affinity","text":"~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created To create an IPPool with namaspace affinity, use the following YAML: SpiderIPPool provides the namespaceAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it will only succeed if the Pod's namespace matches the specified namespaceAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool also offers another option for namespace affinity: namespaceName . When namespaceName is not empty, a Pod is created and attempts to allocate an IP address from the SpiderIPPool. If the namespace of the Pod matches the specified namespaceName , it can successfully obtain an IP from that SpiderIPPool. However, if the namespace does not match the namespaceName , it will be unable to allocate an IP address from that SpiderIPPool. When namespaceName is empty, Spiderpool does not impose any restrictions on IP allocation for Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 Create Applications in a Specified Namespace. In the provided YAML example, a set of Deployment applications is created under the test-ns1 namespace. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF ipam.spidernet.io/ippool \uff1aspecify the IP pool with tenant affinity. v1.multus-cni.io/default-network : create a default network interface for the application. namespace : the namespace where the application resides. After creating the application, the Pods within the designated namespace successfully allocate IP addresses from the associated IPPool with namespace affinity. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> However, if an application is created outside the test-ns1 namespace, Spiderpool will reject IP address allocation, preventing unrelated namespace from using that IPPool. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF Getting an IP address assignment fails as expected when the Pod belongs to a namesapce that does not match the affinity of that IPPool. ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none>","title":"Create IPPool with Namespace Affinity"},{"location":"usage/spider-affinity/#multus-affinity","text":"When creating multiple network interfaces for an application, we can specify the affinity of multus net-attach-def instance for the cluster-level default pool . This way is simpler compared to explicitly specifying the binding relationship between network interfaces and IPPool resources through the ipam.spidernet.io/ippools annotation. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 Set the spec.default field to true to simplify the experience by reducing the need to annotate the application with ipam.spidernet.io/ippool or ipam.spidernet.io/ippools . Configure the spec.multusName field to specify the multus net-attach-def instance. (If you do not specify the namespace of the corresponding multus net-attach-def instance, we will default to the namespace where Spiderpool is installed.) Create an application with multiple network interfaces, you can use the following example YAML: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF v1.multus-cni.io/default-network : Choose the default network configuration for the created application. (If you don't specify this annotation and directly use the clusterNetwork configuration of the multus, please specify the default network configuration during the installation of Spiderpool via Helm using the parameter --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ). k8s.v1.cni.cncf.io/networks : Selects the additional network configuration for the created application.","title":"Multus affinity"},{"location":"usage/spider-affinity/#summary","text":"The set of IPs in the SpiderIPPool can be large or small. It can effectively address the limited IP address resources in the underlay network, and this feature allows different applications and tenants to bind to different SpiderIPPools through various affinity rules. It also enables sharing the same SpiderIPPool, allowing all applications to share the same subnet while achieving \"micro-segmentation.\"","title":"Summary"},{"location":"usage/spider-ippool-zh_CN/","text":"SpiderIPPool \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderIPPool \u8d44\u6e90\u4ee3\u8868 Spiderpool \u4e3a Pod \u5206\u914d IP \u7684 IP \u5730\u5740\u8303\u56f4\u3002 \u8bf7\u53c2\u7167 SpiderIPPool CRD \u4e3a\u4f60\u7684\u96c6\u7fa4\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002 SpiderIPPool \u529f\u80fd \u5355\u53cc\u6808\u4ee5\u53ca IPv6 \u652f\u6301 IP \u5730\u5740\u8303\u56f4\u63a7\u5236 \u7f51\u5173\u8def\u7531\u63a7\u5236 \u4ec5\u7528\u4ee5\u53ca\u5168\u5c40\u7f3a\u7701\u6c60\u63a7\u5236 \u642d\u914d\u5404\u79cd\u8d44\u6e90\u4eb2\u548c\u6027\u4f7f\u7528\u63a7\u5236 \u4f7f\u7528\u4ecb\u7ecd \u5355\u53cc\u6808\u63a7\u5236 Spiderpool \u652f\u6301 IPv4-only, IPv6-only, \u53cc\u6808\u8fd9\u4e09\u79cd IP \u5730\u5740\u5206\u914d\u65b9\u5f0f\uff0c\u53ef\u901a\u8fc7 configmap \u914d\u7f6e\u6765\u63a7\u5236\u3002 \u901a\u8fc7 Helm \u5b89\u88c5\u65f6\u53ef\u914d\u7f6e\u53c2\u6570\u6765\u6307\u5b9a\uff1a --set ipam.enableIPv4=true --set ipam.enableIPv6=true \u3002 \u5f53\u6211\u4eec Spiderpool \u73af\u5883\u5f00\u542f\u53cc\u6808\u914d\u7f6e\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u6307\u5b9a\u4f7f\u7528\u54ea\u4e9b IPv4 \u548c IPv6 \u6c60\u6765\u5206\u914d IP \u5730\u5740\uff1a \u5728\u53cc\u6808\u73af\u5883\u4e0b\uff0c\u4f60\u4e5f\u53ef\u4e3a Pod \u53ea\u5206\u914d IPv4/IPv6 \u7684IP\uff0c\u5982: ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u6307\u5b9a IPPool \u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740 \u8be5\u529f\u80fd\u6709\uff1a \u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60 \u3001 \u4f7f\u7528 Namespace \u6ce8\u89e3\u6307\u5b9a\u6c60 \u3001 \u4f7f\u7528 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60 \u548c \u4e3a SpiderIPPool \u8bbe\u7f6e\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b \u4e00\u51714\u79cd\u4f7f\u7528\u65b9\u6848\u3002 \u5bf9\u4e8e\u8fd94\u79cd\u6307\u5b9a\u4f7f\u7528 SpiderIPPool \u89c4\u5219\u7684\u4f18\u5148\u7ea7\uff0c\u8bf7\u53c2\u8003 IP \u5019\u9009\u6c60\u89c4\u5219 \u989d\u5916\uff0c\u6307\u5b9a IP \u6c60\u7684\u65b9\u5f0f(Pod Annotation, Namespace \u6ce8\u89e3, CNI \u914d\u7f6e\u6587\u4ef6)\uff0c\u8fd8\u53ef\u4f7f\u7528\u901a\u914d\u7b26 * , ? \u548c [] \u6765\u5339\u914d\u671f\u671b\u7684 IP \u6c60\u3002\u5982: ipam.spidernet.io/ippool: '{\"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4*\"]}'\u3002 '*': \u5339\u914d\u96f6\u4e2a\u6216\u591a\u4e2a\u5b57\u7b26\u3002\u4f8b\u5982\uff0c\"ab\" \u53ef\u4ee5\u5339\u914d \"ab\"\u3001\"abc\"\u3001\"abcd\"\u7b49\u7b49\u3002 '?': \u5339\u914d\u4e00\u4e2a\u5355\u72ec\u7684\u5b57\u7b26\u3002\u4f8b\u5982\uff0c\"a?c\" \u53ef\u4ee5\u5339\u914d \"abc\"\u3001\"adc\"\u3001\"axc\"\u7b49\u7b49\u3002 '[]': \u5339\u914d\u6307\u5b9a\u8303\u56f4\u5185\u7684\u4e00\u4e2a\u5b57\u7b26\u3002\u60a8\u53ef\u4ee5\u5728\u65b9\u62ec\u53f7\u5185\u6307\u5b9a\u5b57\u7b26\u7684\u9009\u62e9\uff0c\u6216\u8005\u4f7f\u7528\u8fde\u5b57\u7b26\u6307\u5b9a\u5b57\u7b26\u8303\u56f4\u3002\u4f8b\u5982\uff0c\"[abc]\" \u53ef\u4ee5\u5339\u914d \"a\"\u3001\"b\"\u3001\"c\"\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u5b57\u7b26\u3002 \u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60 \u53ef\u501f\u52a9\u6ce8\u89e3 ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6807\u8bb0\u5728 Pod \u7684 Annotation\u4e0a\u6765\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b IP \u6c60, \u6ce8\u89e3 ipam.spidernet.io/ippools \u591a\u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u3002\u6b64\u5916\u8fd8\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a IP \u6c60\u4ee5\u4f9b\u5907\u9009\uff0c\u5f53\u67d0\u4e2a\u6c60\u7684 IP \u88ab\u7528\u5b8c\u540e\uff0c\u53ef\u7ee7\u7eed\u4ece\u4f60\u6307\u5b9a\u7684\u5176\u4ed6\u6c60\u4e2d\u5206\u914d\u5730\u5740\u3002 ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\", \"wildcard-v4?\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\", \"wildcard-v6*\"] } \u5728\u4f7f\u7528\u6ce8\u89e3 ipam.spidernet.io/ippools \u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u65f6\uff0c\u4f60\u53ef\u663e\u5f0f\u7684\u901a\u8fc7\u6307\u5b9a interface \u5b57\u6bb5\u6807\u660e\u7f51\u5361\u540d\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7**\u6570\u7ec4\u987a\u5e8f\u6392\u5217**\u8ba9\u7b2c\u51e0\u5f20\u5361\u7528\u54ea\u4e9b IP \u6c60\u3002\u53e6\u5916\uff0c\u5b57\u6bb5 cleangateway \u6807\u660e\u662f\u5426\u9700\u8981\u6839\u636e IP \u6c60\u4e2d\u7684 gateway \u5b57\u6bb5\u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff0c\u5f53 cleangateway \u4e3a true \u6807\u660e\u65e0\u9700\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002(\u9ed8\u8ba4\u4e3afalse) \u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0c\u4e00\u822c\u65e0\u6cd5\u4e3a\u8def\u7531\u8868 main \u8868\u751f\u6210\u4e24\u6761\u53ca\u4ee5\u4e0a\u7684\u9ed8\u8ba4\u8def\u7531\u3002\u642d\u914d Coordinator \u63d2\u4ef6\u53ef\u4e3a\u4f60\u5904\u7406\u597d\u8be5\u95ee\u9898\uff0c\u56e0\u6b64\u4f60\u53ef\u4ee5\u5ffd\u7565 cleangateway \u5b57\u6bb5\u3002\u6216\u8005\u5728\u5355\u72ec\u4f7f\u7528 Spiderpool IPAM \u63d2\u4ef6\u65f6\uff0c\u53ef\u501f\u52a9 cleangateway: true \u6765\u6807\u660e\u4e0d\u6839\u636e IP \u6c60 gateway \u5b57\u6bb5\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002 ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"] },{ \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": false }] \u4f7f\u7528 Namespace \u6ce8\u89e3\u6307\u5b9a\u6c60 \u6211\u4eec\u53ef\u4ee5\u4e3a Namespace \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u548c ipam.spidernet.io/default-ipv6-ippool , \u5f53\u5e94\u7528\u90e8\u7f72\u65f6\uff0c\u53ef\u4ece\u5e94\u7528\u6240\u5728 Namespace \u7684\u6ce8\u89e3\u4e2d\u9009\u62e9 IP \u6c60\u4f7f\u7528\uff1a \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\u65f6\uff0c\u4f18\u5148\u4f7f\u7528\u6b64\u5904 Namespace \u6ce8\u89e3\u89c4\u5219\u3002 apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\", \"ns-v4-ippool2\", \"wildcard-v4*\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\", \"ns-v6-ippool2\", \"wildcard-v6?\"]' name : kube-system ... \u4f7f\u7528 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60 \u6211\u4eec\u53ef\u4ee5\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6307\u5b9a\u7f3a\u7701\u7684 IPv4 \u548c IPv6 \u6c60\u4ee5\u4f9b\u5e94\u7528\u9009\u62e9\u8be5 CNI \u914d\u7f6e\u65f6\u4f7f\u7528\uff0c\u5177\u4f53\u53ef\u53c2\u7167 CNI\u914d\u7f6e \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\uff0c\u4e14\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u5c06\u4f18\u5148\u4f7f\u7528\u6b64\u5904 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60\u89c4\u5219\u3002 { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" , \"wildcard-v4-ippool[123]\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" , \"wildcard-v6-ippool[456]\" ] } } \u4e3a SpiderIPPool \u8bbe\u7f6e\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b \u5728 SpiderIPPool CRD \u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230 spec.default \u5b57\u6bb5\u662f\u4e00\u4e2a bool \u7c7b\u578b\uff0c\u5f53\u6211\u4eec\u6ca1\u6709\u901a\u8fc7 Annotation \u6216 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a IPPool \u65f6\uff0c\u7cfb\u7edf\u4f1a\u6839\u636e\u8be5\u5b57\u6bb5\u6311\u9009\u51fa\u96c6\u7fa4\u9ed8\u8ba4\u6c60\u4f7f\u7528: \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528IP\u6c60\uff0c\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u4e14\u672a\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a IP \u6c60\u65f6\uff0c\u6b64\u5904\u4f1a\u751f\u6548\u3002 \u53ef\u4e3a\u591a\u4e2a IPPool \u8d44\u6e90\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ... SpiderIPPool \u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528 \u5177\u4f53\u8bf7\u53c2\u8003 IP \u6c60\u4eb2\u548c\u6027\u642d\u914d SpiderIPPool \u7f51\u5173\u4e0e\u8def\u7531\u914d\u7f6e \u5177\u4f53\u8bf7\u53c2\u8003 \u8def\u7531\u529f\u80fd \u56e0\u6b64 Pod \u4f1a\u62ff\u5230\u57fa\u4e8e\u7f51\u5173\u7684\u9ed8\u8ba4\u8def\u7531\uff0c\u4ee5\u53ca\u6b64 IP \u6c60\u4e0a\u7684\u81ea\u5b9a\u4e49\u8def\u7531\u3002(\u82e5 IP \u6c60\u4e0d\u8bbe\u7f6e\u7f51\u5173\uff0c\u5219\u4e0d\u4f1a\u751f\u6548\u9ed8\u8ba4\u8def\u7531) \u547d\u4ee4\u884c\u5de5\u4f5c (kubectl) \u67e5\u770b\u6269\u5c55\u5b57\u6bb5 \u4e3a\u4e86\u66f4\u7b80\u5355\u65b9\u4fbf\u7684\u67e5\u770b SpiderIPPool \u8d44\u6e90\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u6211\u4eec\u8865\u5145\u4e86\u4e00\u4e9b\u6269\u5c55\u5b57\u6bb5\u53ef\u8ba9\u7528\u6237\u901a\u8fc7 kubectl get sp -o wide \u67e5\u770b: ALLOCATED-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u5df2\u5206\u914d\u7684 IP \u6570\u91cf TOTAL-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u7684\u603b IP \u6570\u91cf DEFAULT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b DISABLE \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u88ab\u7981\u7528 NODENAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684\u8282\u70b9 MULTUSNAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684 multus \u5b9e\u4f8b APP-NAMESPACE \u5b57\u6bb5\u5c5e\u4e8e SpiderSubnet \u529f\u80fd\u72ec\u6709\uff0c\u8868\u660e\u8be5\u6c60\u662f\u4e00\u4e2a\u7cfb\u7edf\u81ea\u52a8\u521b\u5efa\u7684\u6c60\uff0c\u540c\u65f6\u8be5\u5b57\u6bb5\u8868\u660e\u5176\u5bf9\u5e94\u5e94\u7528\u7684\u547d\u540d\u7a7a\u95f4\u3002 ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ] \u6307\u6807(metric) \u6211\u4eec\u4e5f\u4e3a SpiderIPPool \u8d44\u6e90\u8865\u5145\u4e86\u76f8\u5173\u7684\u6307\u6807\u4fe1\u606f\uff0c\u8be6\u60c5\u8bf7\u770b metric","title":"SpiderIPPool"},{"location":"usage/spider-ippool-zh_CN/#spiderippool","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderIPPool"},{"location":"usage/spider-ippool-zh_CN/#_1","text":"SpiderIPPool \u8d44\u6e90\u4ee3\u8868 Spiderpool \u4e3a Pod \u5206\u914d IP \u7684 IP \u5730\u5740\u8303\u56f4\u3002 \u8bf7\u53c2\u7167 SpiderIPPool CRD \u4e3a\u4f60\u7684\u96c6\u7fa4\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_1","text":"\u5355\u53cc\u6808\u4ee5\u53ca IPv6 \u652f\u6301 IP \u5730\u5740\u8303\u56f4\u63a7\u5236 \u7f51\u5173\u8def\u7531\u63a7\u5236 \u4ec5\u7528\u4ee5\u53ca\u5168\u5c40\u7f3a\u7701\u6c60\u63a7\u5236 \u642d\u914d\u5404\u79cd\u8d44\u6e90\u4eb2\u548c\u6027\u4f7f\u7528\u63a7\u5236","title":"SpiderIPPool \u529f\u80fd"},{"location":"usage/spider-ippool-zh_CN/#_2","text":"","title":"\u4f7f\u7528\u4ecb\u7ecd"},{"location":"usage/spider-ippool-zh_CN/#_3","text":"Spiderpool \u652f\u6301 IPv4-only, IPv6-only, \u53cc\u6808\u8fd9\u4e09\u79cd IP \u5730\u5740\u5206\u914d\u65b9\u5f0f\uff0c\u53ef\u901a\u8fc7 configmap \u914d\u7f6e\u6765\u63a7\u5236\u3002 \u901a\u8fc7 Helm \u5b89\u88c5\u65f6\u53ef\u914d\u7f6e\u53c2\u6570\u6765\u6307\u5b9a\uff1a --set ipam.enableIPv4=true --set ipam.enableIPv6=true \u3002 \u5f53\u6211\u4eec Spiderpool \u73af\u5883\u5f00\u542f\u53cc\u6808\u914d\u7f6e\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u6307\u5b9a\u4f7f\u7528\u54ea\u4e9b IPv4 \u548c IPv6 \u6c60\u6765\u5206\u914d IP \u5730\u5740\uff1a \u5728\u53cc\u6808\u73af\u5883\u4e0b\uff0c\u4f60\u4e5f\u53ef\u4e3a Pod \u53ea\u5206\u914d IPv4/IPv6 \u7684IP\uff0c\u5982: ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ]","title":"\u5355\u53cc\u6808\u63a7\u5236"},{"location":"usage/spider-ippool-zh_CN/#ippool-ip","text":"\u8be5\u529f\u80fd\u6709\uff1a \u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60 \u3001 \u4f7f\u7528 Namespace \u6ce8\u89e3\u6307\u5b9a\u6c60 \u3001 \u4f7f\u7528 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60 \u548c \u4e3a SpiderIPPool \u8bbe\u7f6e\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b \u4e00\u51714\u79cd\u4f7f\u7528\u65b9\u6848\u3002 \u5bf9\u4e8e\u8fd94\u79cd\u6307\u5b9a\u4f7f\u7528 SpiderIPPool \u89c4\u5219\u7684\u4f18\u5148\u7ea7\uff0c\u8bf7\u53c2\u8003 IP \u5019\u9009\u6c60\u89c4\u5219 \u989d\u5916\uff0c\u6307\u5b9a IP \u6c60\u7684\u65b9\u5f0f(Pod Annotation, Namespace \u6ce8\u89e3, CNI \u914d\u7f6e\u6587\u4ef6)\uff0c\u8fd8\u53ef\u4f7f\u7528\u901a\u914d\u7b26 * , ? \u548c [] \u6765\u5339\u914d\u671f\u671b\u7684 IP \u6c60\u3002\u5982: ipam.spidernet.io/ippool: '{\"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4*\"]}'\u3002 '*': \u5339\u914d\u96f6\u4e2a\u6216\u591a\u4e2a\u5b57\u7b26\u3002\u4f8b\u5982\uff0c\"ab\" \u53ef\u4ee5\u5339\u914d \"ab\"\u3001\"abc\"\u3001\"abcd\"\u7b49\u7b49\u3002 '?': \u5339\u914d\u4e00\u4e2a\u5355\u72ec\u7684\u5b57\u7b26\u3002\u4f8b\u5982\uff0c\"a?c\" \u53ef\u4ee5\u5339\u914d \"abc\"\u3001\"adc\"\u3001\"axc\"\u7b49\u7b49\u3002 '[]': \u5339\u914d\u6307\u5b9a\u8303\u56f4\u5185\u7684\u4e00\u4e2a\u5b57\u7b26\u3002\u60a8\u53ef\u4ee5\u5728\u65b9\u62ec\u53f7\u5185\u6307\u5b9a\u5b57\u7b26\u7684\u9009\u62e9\uff0c\u6216\u8005\u4f7f\u7528\u8fde\u5b57\u7b26\u6307\u5b9a\u5b57\u7b26\u8303\u56f4\u3002\u4f8b\u5982\uff0c\"[abc]\" \u53ef\u4ee5\u5339\u914d \"a\"\u3001\"b\"\u3001\"c\"\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u5b57\u7b26\u3002","title":"\u6307\u5b9a IPPool \u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740"},{"location":"usage/spider-ippool-zh_CN/#pod-annotation-ip","text":"\u53ef\u501f\u52a9\u6ce8\u89e3 ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6807\u8bb0\u5728 Pod \u7684 Annotation\u4e0a\u6765\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b IP \u6c60, \u6ce8\u89e3 ipam.spidernet.io/ippools \u591a\u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u3002\u6b64\u5916\u8fd8\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a IP \u6c60\u4ee5\u4f9b\u5907\u9009\uff0c\u5f53\u67d0\u4e2a\u6c60\u7684 IP \u88ab\u7528\u5b8c\u540e\uff0c\u53ef\u7ee7\u7eed\u4ece\u4f60\u6307\u5b9a\u7684\u5176\u4ed6\u6c60\u4e2d\u5206\u914d\u5730\u5740\u3002 ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\", \"wildcard-v4?\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\", \"wildcard-v6*\"] } \u5728\u4f7f\u7528\u6ce8\u89e3 ipam.spidernet.io/ippools \u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u65f6\uff0c\u4f60\u53ef\u663e\u5f0f\u7684\u901a\u8fc7\u6307\u5b9a interface \u5b57\u6bb5\u6807\u660e\u7f51\u5361\u540d\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7**\u6570\u7ec4\u987a\u5e8f\u6392\u5217**\u8ba9\u7b2c\u51e0\u5f20\u5361\u7528\u54ea\u4e9b IP \u6c60\u3002\u53e6\u5916\uff0c\u5b57\u6bb5 cleangateway \u6807\u660e\u662f\u5426\u9700\u8981\u6839\u636e IP \u6c60\u4e2d\u7684 gateway \u5b57\u6bb5\u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff0c\u5f53 cleangateway \u4e3a true \u6807\u660e\u65e0\u9700\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002(\u9ed8\u8ba4\u4e3afalse) \u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0c\u4e00\u822c\u65e0\u6cd5\u4e3a\u8def\u7531\u8868 main \u8868\u751f\u6210\u4e24\u6761\u53ca\u4ee5\u4e0a\u7684\u9ed8\u8ba4\u8def\u7531\u3002\u642d\u914d Coordinator \u63d2\u4ef6\u53ef\u4e3a\u4f60\u5904\u7406\u597d\u8be5\u95ee\u9898\uff0c\u56e0\u6b64\u4f60\u53ef\u4ee5\u5ffd\u7565 cleangateway \u5b57\u6bb5\u3002\u6216\u8005\u5728\u5355\u72ec\u4f7f\u7528 Spiderpool IPAM \u63d2\u4ef6\u65f6\uff0c\u53ef\u501f\u52a9 cleangateway: true \u6765\u6807\u660e\u4e0d\u6839\u636e IP \u6c60 gateway \u5b57\u6bb5\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002 ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"] },{ \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": false }]","title":"\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60"},{"location":"usage/spider-ippool-zh_CN/#namespace","text":"\u6211\u4eec\u53ef\u4ee5\u4e3a Namespace \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u548c ipam.spidernet.io/default-ipv6-ippool , \u5f53\u5e94\u7528\u90e8\u7f72\u65f6\uff0c\u53ef\u4ece\u5e94\u7528\u6240\u5728 Namespace \u7684\u6ce8\u89e3\u4e2d\u9009\u62e9 IP \u6c60\u4f7f\u7528\uff1a \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\u65f6\uff0c\u4f18\u5148\u4f7f\u7528\u6b64\u5904 Namespace \u6ce8\u89e3\u89c4\u5219\u3002 apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\", \"ns-v4-ippool2\", \"wildcard-v4*\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\", \"ns-v6-ippool2\", \"wildcard-v6?\"]' name : kube-system ...","title":"\u4f7f\u7528 Namespace \u6ce8\u89e3\u6307\u5b9a\u6c60"},{"location":"usage/spider-ippool-zh_CN/#cni","text":"\u6211\u4eec\u53ef\u4ee5\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6307\u5b9a\u7f3a\u7701\u7684 IPv4 \u548c IPv6 \u6c60\u4ee5\u4f9b\u5e94\u7528\u9009\u62e9\u8be5 CNI \u914d\u7f6e\u65f6\u4f7f\u7528\uff0c\u5177\u4f53\u53ef\u53c2\u7167 CNI\u914d\u7f6e \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\uff0c\u4e14\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u5c06\u4f18\u5148\u4f7f\u7528\u6b64\u5904 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60\u89c4\u5219\u3002 { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" , \"wildcard-v4-ippool[123]\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" , \"wildcard-v6-ippool[456]\" ] } }","title":"\u4f7f\u7528 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_2","text":"\u5728 SpiderIPPool CRD \u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230 spec.default \u5b57\u6bb5\u662f\u4e00\u4e2a bool \u7c7b\u578b\uff0c\u5f53\u6211\u4eec\u6ca1\u6709\u901a\u8fc7 Annotation \u6216 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a IPPool \u65f6\uff0c\u7cfb\u7edf\u4f1a\u6839\u636e\u8be5\u5b57\u6bb5\u6311\u9009\u51fa\u96c6\u7fa4\u9ed8\u8ba4\u6c60\u4f7f\u7528: \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528IP\u6c60\uff0c\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u4e14\u672a\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a IP \u6c60\u65f6\uff0c\u6b64\u5904\u4f1a\u751f\u6548\u3002 \u53ef\u4e3a\u591a\u4e2a IPPool \u8d44\u6e90\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ...","title":"\u4e3a SpiderIPPool \u8bbe\u7f6e\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_3","text":"\u5177\u4f53\u8bf7\u53c2\u8003 IP \u6c60\u4eb2\u548c\u6027\u642d\u914d","title":"SpiderIPPool \u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_4","text":"\u5177\u4f53\u8bf7\u53c2\u8003 \u8def\u7531\u529f\u80fd \u56e0\u6b64 Pod \u4f1a\u62ff\u5230\u57fa\u4e8e\u7f51\u5173\u7684\u9ed8\u8ba4\u8def\u7531\uff0c\u4ee5\u53ca\u6b64 IP \u6c60\u4e0a\u7684\u81ea\u5b9a\u4e49\u8def\u7531\u3002(\u82e5 IP \u6c60\u4e0d\u8bbe\u7f6e\u7f51\u5173\uff0c\u5219\u4e0d\u4f1a\u751f\u6548\u9ed8\u8ba4\u8def\u7531)","title":"SpiderIPPool \u7f51\u5173\u4e0e\u8def\u7531\u914d\u7f6e"},{"location":"usage/spider-ippool-zh_CN/#kubectl","text":"\u4e3a\u4e86\u66f4\u7b80\u5355\u65b9\u4fbf\u7684\u67e5\u770b SpiderIPPool \u8d44\u6e90\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u6211\u4eec\u8865\u5145\u4e86\u4e00\u4e9b\u6269\u5c55\u5b57\u6bb5\u53ef\u8ba9\u7528\u6237\u901a\u8fc7 kubectl get sp -o wide \u67e5\u770b: ALLOCATED-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u5df2\u5206\u914d\u7684 IP \u6570\u91cf TOTAL-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u7684\u603b IP \u6570\u91cf DEFAULT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b DISABLE \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u88ab\u7981\u7528 NODENAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684\u8282\u70b9 MULTUSNAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684 multus \u5b9e\u4f8b APP-NAMESPACE \u5b57\u6bb5\u5c5e\u4e8e SpiderSubnet \u529f\u80fd\u72ec\u6709\uff0c\u8868\u660e\u8be5\u6c60\u662f\u4e00\u4e2a\u7cfb\u7edf\u81ea\u52a8\u521b\u5efa\u7684\u6c60\uff0c\u540c\u65f6\u8be5\u5b57\u6bb5\u8868\u660e\u5176\u5bf9\u5e94\u5e94\u7528\u7684\u547d\u540d\u7a7a\u95f4\u3002 ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ]","title":"\u547d\u4ee4\u884c\u5de5\u4f5c (kubectl) \u67e5\u770b\u6269\u5c55\u5b57\u6bb5"},{"location":"usage/spider-ippool-zh_CN/#metric","text":"\u6211\u4eec\u4e5f\u4e3a SpiderIPPool \u8d44\u6e90\u8865\u5145\u4e86\u76f8\u5173\u7684\u6307\u6807\u4fe1\u606f\uff0c\u8be6\u60c5\u8bf7\u770b metric","title":"\u6307\u6807(metric)"},{"location":"usage/spider-ippool/","text":"SpiderIPPool English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction SpiderIPPool resources represent the IP address ranges allocated by Spiderpool for Pods. To create SpiderIPPool resources in your cluster, refer to the SpiderIPPool CRD . SpiderIPPool Features Single-stack, dual-stack, and IPv6 Support IP address range control Gateway route control Exclusive or global default pool control Compatible with various resource affinity settings Usage Single-stack and Dual-stack Control Spiderpool supports three modes of IP address allocation: IPv4-only, IPv6-only, and dual-stack. Refer to configmap for details. When installing Spiderpool via Helm, you can use configuration parameters to specify: --set ipam.enableIPv4=true --set ipam.enableIPv6=true . If dual-stack mode is enabled, you can manually specify which IPv4 and IPv6 pools should be used for IP address allocation: In a dual-stack environment, you can also configure Pods to only receive IPv4 or IPv6 addresses using the annotation ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' . apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Specify IPPool to Allocate IP Addresses to Applications This feature owns 4 usage options including: Use Pod Annotation to Specify IP Pool , Use Namespace Annotation to Specify IP Pool , Use CNI Configuration File to Specify IP Pool and Set Cluster Default Level for SpiderIPPool . For the priority rules when specifying the SpiderIPPool, refer to the Candidate Pool Acquisition . Additionally, with the following ways of specifying IPPools(Pod Annotation, Namespace Annotation, CNI configuration file) you can also use wildcards ' ', '?' and '[]' to match the desired IPPools. For example: ipam.spidernet.io/ippool: '{\"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4 \"]}' '*': Matches zero or more characters. For example, \"ab\" can match \"ab\", \"abc\", \"abcd\", and so on. '?': Matches a single character. For example, \"a?c\" can match \"abc\", \"adc\", \"axc\", and so on. '[]': Matches a specified range of characters. You can specify the choices of characters inside the brackets, or use a hyphen to specify a character range. For example, \"[abc]\" can match any one of the characters \"a\", \"b\", or \"c\". Use Pod Annotation to Specify IP Pool You can use annotations like ipam.spidernet.io/ippool or ipam.spidernet.io/ippools on a Pod's annotation to indicate which IP pools should be used. The ipam.spidernet.io/ippools annotation is primarily used for specifying multiple network interfaces. Additionally, you can specify multiple pools as fallback options. If one pool's IP addresses are exhausted, addresses can be allocated from the other specified pools. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\", \"wildcard-v4?\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\", \"wildcard-v6*\"] } When using the annotation ipam.spidernet.io/ippools for specifying multiple network interfaces, you can explicitly indicate the interface name by specifying the interface field. Alternatively, you can use array ordering to determine which IP pools are assigned to which network interfaces. Additionally, the cleangateway field indicates whether a default route should be generated based on the gateway field of the IPPool. When cleangateway is set to true, it means that no default route needs to be generated (default is false). In scenarios with multiple network interfaces, it is generally not possible to generate two or more default routes in the main routing table. The plugin Coordinator already solved this problem and you can ignore clengateway field. If you want to use Spiderpool IPAM plugin alone, you can use cleangateway: true to indicate that a default route should not be generated based on the IPPool gateway field. ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"] },{ \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": false }] Use Namespace Annotation to Specify IP Pool You can annotate the Namespace with ipam.spidernet.io/default-ipv4-ippool and ipam.spidernet.io/default-ipv6-ippool . When deploying applications, IP pools can be selected based on these annotations of the application's namespace: If IP pool is not explicitly specified, rules defined in the Namespace annotation take precedence. apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\", \"ns-v4-ippool2\", \"wildcard-v4*\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\", \"ns-v6-ippool2\", \"wildcard-v6?\"]' name : kube-system ... Use CNI Configuration File to Specify IP Pool You can specify the default IPv4 and IPv6 pools for an application in the CNI configuration file. For more details, refer to CNI Configuration If IP pool is not explicitly specified using Pod Annotation and no IP pool is specified through Namespace annotation, the rules defined in the CNI configuration file take precedence. { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" , \"wildcard-v4-ippool[123]\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" , \"wildcard-v6-ippool[456]\" ] } } Set Cluster Default Level for SpiderIPPool In the SpiderIPPool CRD , the spec.default field is a boolean type. It determines the cluster default pool when no specific IPPool is specified through annotations or the CNI configuration file: If no IP pool is specified using Pod annotations, and no IP pool is specified through Namespace annotations, and no IP pool is specified in the CNI configuration file, the system will use the pool defined by this field as the cluster default. Multiple IPPool resources can be set as the cluster default level. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ... Use SpiderIPPool with Affinity Refer to SpiderIPPool Affinity for details. SpiderIPPool Gateway and Route Configuration Refer to Route Support for details. As a result, Pods will receive the default route based on the gateway, as well as custom routes defined in the IP pool. If the IP pool does not have a gateway configured, the default route will not take effect. View Extended Fields with Command Line (kubectl) To simplify the viewing of properties related to SpiderIPPool resources, we have added some additional fields that can be displayed using the kubectl get sp -o wide command: ALLOCATED-IP-COUNT represents the number of allocated IP addresses in the pool. TOTAL-IP-COUNT represents the total number of IP addresses in the pool. DEFAULT indicates whether the pool is set as the cluster default level. DISABLE indicates whether the pool is disabled. NODENAME indicates the nodes have an affinity with the pool. MULTUSNAME indicates the Multus instances have an affinity with the pool. APP-NAMESPACE is specific to the SpiderSubnet feature. It signifies that the pool is automatically created by the system and corresponds to the namespace of the associated application. ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ] Metrics We have also supplemented SpiderIPPool resources with relevant metric information. For more details, refer to metric","title":"IPAM of SpiderIPPool"},{"location":"usage/spider-ippool/#spiderippool","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderIPPool"},{"location":"usage/spider-ippool/#introduction","text":"SpiderIPPool resources represent the IP address ranges allocated by Spiderpool for Pods. To create SpiderIPPool resources in your cluster, refer to the SpiderIPPool CRD .","title":"Introduction"},{"location":"usage/spider-ippool/#spiderippool-features","text":"Single-stack, dual-stack, and IPv6 Support IP address range control Gateway route control Exclusive or global default pool control Compatible with various resource affinity settings","title":"SpiderIPPool Features"},{"location":"usage/spider-ippool/#usage","text":"","title":"Usage"},{"location":"usage/spider-ippool/#single-stack-and-dual-stack-control","text":"Spiderpool supports three modes of IP address allocation: IPv4-only, IPv6-only, and dual-stack. Refer to configmap for details. When installing Spiderpool via Helm, you can use configuration parameters to specify: --set ipam.enableIPv4=true --set ipam.enableIPv6=true . If dual-stack mode is enabled, you can manually specify which IPv4 and IPv6 pools should be used for IP address allocation: In a dual-stack environment, you can also configure Pods to only receive IPv4 or IPv6 addresses using the annotation ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' . apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ]","title":"Single-stack and Dual-stack Control"},{"location":"usage/spider-ippool/#specify-ippool-to-allocate-ip-addresses-to-applications","text":"This feature owns 4 usage options including: Use Pod Annotation to Specify IP Pool , Use Namespace Annotation to Specify IP Pool , Use CNI Configuration File to Specify IP Pool and Set Cluster Default Level for SpiderIPPool . For the priority rules when specifying the SpiderIPPool, refer to the Candidate Pool Acquisition . Additionally, with the following ways of specifying IPPools(Pod Annotation, Namespace Annotation, CNI configuration file) you can also use wildcards ' ', '?' and '[]' to match the desired IPPools. For example: ipam.spidernet.io/ippool: '{\"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4 \"]}' '*': Matches zero or more characters. For example, \"ab\" can match \"ab\", \"abc\", \"abcd\", and so on. '?': Matches a single character. For example, \"a?c\" can match \"abc\", \"adc\", \"axc\", and so on. '[]': Matches a specified range of characters. You can specify the choices of characters inside the brackets, or use a hyphen to specify a character range. For example, \"[abc]\" can match any one of the characters \"a\", \"b\", or \"c\".","title":"Specify IPPool to Allocate IP Addresses to Applications"},{"location":"usage/spider-ippool/#use-pod-annotation-to-specify-ip-pool","text":"You can use annotations like ipam.spidernet.io/ippool or ipam.spidernet.io/ippools on a Pod's annotation to indicate which IP pools should be used. The ipam.spidernet.io/ippools annotation is primarily used for specifying multiple network interfaces. Additionally, you can specify multiple pools as fallback options. If one pool's IP addresses are exhausted, addresses can be allocated from the other specified pools. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\", \"wildcard-v4?\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\", \"wildcard-v6*\"] } When using the annotation ipam.spidernet.io/ippools for specifying multiple network interfaces, you can explicitly indicate the interface name by specifying the interface field. Alternatively, you can use array ordering to determine which IP pools are assigned to which network interfaces. Additionally, the cleangateway field indicates whether a default route should be generated based on the gateway field of the IPPool. When cleangateway is set to true, it means that no default route needs to be generated (default is false). In scenarios with multiple network interfaces, it is generally not possible to generate two or more default routes in the main routing table. The plugin Coordinator already solved this problem and you can ignore clengateway field. If you want to use Spiderpool IPAM plugin alone, you can use cleangateway: true to indicate that a default route should not be generated based on the IPPool gateway field. ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"] },{ \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\", \"wildcard-v4-ippool[123]\"], \"ipv6\": [\"demo-v6-ippool1\", \"wildcard-v6-ippool[123]\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\", \"wildcard-v4-ippool[456]\"], \"ipv6\": [\"demo-v6-ippool2\", \"wildcard-v6-ippool[456]\"], \"cleangateway\": false }]","title":"Use Pod Annotation to Specify IP Pool"},{"location":"usage/spider-ippool/#use-namespace-annotation-to-specify-ip-pool","text":"You can annotate the Namespace with ipam.spidernet.io/default-ipv4-ippool and ipam.spidernet.io/default-ipv6-ippool . When deploying applications, IP pools can be selected based on these annotations of the application's namespace: If IP pool is not explicitly specified, rules defined in the Namespace annotation take precedence. apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\", \"ns-v4-ippool2\", \"wildcard-v4*\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\", \"ns-v6-ippool2\", \"wildcard-v6?\"]' name : kube-system ...","title":"Use Namespace Annotation to Specify IP Pool"},{"location":"usage/spider-ippool/#use-cni-configuration-file-to-specify-ip-pool","text":"You can specify the default IPv4 and IPv6 pools for an application in the CNI configuration file. For more details, refer to CNI Configuration If IP pool is not explicitly specified using Pod Annotation and no IP pool is specified through Namespace annotation, the rules defined in the CNI configuration file take precedence. { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" , \"wildcard-v4-ippool[123]\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" , \"wildcard-v6-ippool[456]\" ] } }","title":"Use CNI Configuration File to Specify IP Pool"},{"location":"usage/spider-ippool/#set-cluster-default-level-for-spiderippool","text":"In the SpiderIPPool CRD , the spec.default field is a boolean type. It determines the cluster default pool when no specific IPPool is specified through annotations or the CNI configuration file: If no IP pool is specified using Pod annotations, and no IP pool is specified through Namespace annotations, and no IP pool is specified in the CNI configuration file, the system will use the pool defined by this field as the cluster default. Multiple IPPool resources can be set as the cluster default level. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ...","title":"Set Cluster Default Level for SpiderIPPool"},{"location":"usage/spider-ippool/#use-spiderippool-with-affinity","text":"Refer to SpiderIPPool Affinity for details.","title":"Use SpiderIPPool with Affinity"},{"location":"usage/spider-ippool/#spiderippool-gateway-and-route-configuration","text":"Refer to Route Support for details. As a result, Pods will receive the default route based on the gateway, as well as custom routes defined in the IP pool. If the IP pool does not have a gateway configured, the default route will not take effect.","title":"SpiderIPPool Gateway and Route Configuration"},{"location":"usage/spider-ippool/#view-extended-fields-with-command-line-kubectl","text":"To simplify the viewing of properties related to SpiderIPPool resources, we have added some additional fields that can be displayed using the kubectl get sp -o wide command: ALLOCATED-IP-COUNT represents the number of allocated IP addresses in the pool. TOTAL-IP-COUNT represents the total number of IP addresses in the pool. DEFAULT indicates whether the pool is set as the cluster default level. DISABLE indicates whether the pool is disabled. NODENAME indicates the nodes have an affinity with the pool. MULTUSNAME indicates the Multus instances have an affinity with the pool. APP-NAMESPACE is specific to the SpiderSubnet feature. It signifies that the pool is automatically created by the system and corresponds to the namespace of the associated application. ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ]","title":"View Extended Fields with Command Line (kubectl)"},{"location":"usage/spider-ippool/#metrics","text":"We have also supplemented SpiderIPPool resources with relevant metric information. For more details, refer to metric","title":"Metrics"},{"location":"usage/spider-multus-config-zh_CN/","text":"SpiderMultusConfig \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u63d0\u4f9b\u4e86 Spidermultusconfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f00\u6e90\u9879\u76ee Multus CNI \u914d\u7f6e\u7ba1\u7406\u7684\u6269\u5c55\u3002 SpiderMultusConfig \u529f\u80fd Multus \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u5b83\u901a\u8fc7\u8c03\u5ea6\u7b2c\u4e09\u65b9 CNI \u9879\u76ee\uff0c\u80fd\u591f\u5b9e\u73b0\u4e3a Pod \u63a5\u5165\u591a\u5f20\u7f51\u5361\u3002\u5e76\u4e14\uff0cMultus \u53ef\u4ee5\u901a\u8fc7 CRD \u65b9\u5f0f\u7ba1\u7406 CNI \u914d\u7f6e\uff0c\u907f\u514d\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u624b\u52a8\u7f16\u8f91 CNI \u914d\u7f6e\u6587\u4ef6\u3002\u4f46\u521b\u5efa Multus CR \u65f6\uff0c\u9700\u8981\u624b\u52a8\u4e66\u5199 JSON \u683c\u5f0f\u7684 CNI \u914d\u7f6e\u5b57\u7b26\u4e32\u3002\u5c06\u4f1a\u5bfc\u81f4\u5982\u4e0b\u95ee\u9898\u3002 \u4eba\u4e3a\u4e66\u5199\u5bb9\u6613\u51fa\u73b0 JSON \u683c\u5f0f\u9519\u8bef\uff0c\u589e\u52a0 Pod \u542f\u52a8\u5931\u8d25\u7684\u6392\u969c\u6210\u672c\u3002 CNI \u79cd\u7c7b\u591a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5404\u4e2a\u914d\u7f6e\u9879\u4e5f\u5f88\u591a\uff0c\u4e0d\u5bb9\u6613\u8bb0\u5fc6\uff0c\u7ecf\u5e38\u9700\u8981\u8fdb\u884c\u8d44\u6599\u67e5\u9605\uff0c\u7528\u6237\u4f53\u9a8c\u4e0d\u53cb\u597d\u3002 Spidermultusconfig CR \u57fa\u4e8e spec \u4e2d\u7684\u5b9a\u4e49\u81ea\u52a8\u751f\u6210 Multus CR\uff0c\u6539\u8fdb\u4e86\u5982\u4e0a\u95ee\u9898\uff0c\u5e76\u4e14\u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u70b9\uff1a \u8bef\u64cd\u4f5c\u5220\u9664 Multus CR\uff0cSpidermultusconfig \u5c06\u4f1a\u81ea\u52a8\u91cd\u5efa\uff1b\u63d0\u5347\u8fd0\u7ef4\u5bb9\u9519\u80fd\u529b\u3002 \u652f\u6301\u4f17\u591a CNI\uff0c\u5982 Macvlan\u3001IPvlan\u3001Ovs\u3001SR-IOV\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cr-name \u81ea\u5b9a\u4e49 Multus CR \u7684\u540d\u5b57\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cni-version \u81ea\u5b9a\u4e49\u8bbe\u7f6e CNI \u7684\u7248\u672c\u3002 \u5b8c\u5584\u7684 Webhook \u673a\u5236\uff0c\u63d0\u524d\u89c4\u907f\u4e00\u4e9b\u4eba\u4e3a\u9519\u8bef\uff0c\u964d\u4f4e\u540e\u7eed\u6392\u969c\u6210\u672c\u3002 \u652f\u6301 Spiderpool \u7684 CNI plugin\uff1a ifacer \u3001 coordinator \uff0c\u63d0\u9ad8\u4e86 Spiderpool \u7684 CNI plugin \u7684\u914d\u7f6e\u4f53\u9a8c\u3002 \u5728\u5df2\u5b58\u5728 Multus CR \u5b9e\u4f8b\u65f6\uff0c\u521b\u5efa\u4e0e\u5176\u540c\u540d Spidermultusconfig CR \uff0cMultus CR \u5b9e\u4f8b\u5c06\u4f1a\u88ab\u7eb3\u7ba1\uff0c\u5176\u914d\u7f6e\u5185\u5bb9\u5c06\u4f1a\u88ab\u8986\u76d6\u3002\u5982\u679c\u4e0d\u60f3\u53d1\u751f\u88ab\u8986\u76d6\u7684\u60c5\u51b5\uff0c\u8bf7\u907f\u514d\u521b\u5efa\u4e0e\u5b58\u91cf Multus CR \u5b9e\u4f8b\u540c\u540d\u7684 Spidermultusconfig CR \u5b9e\u4f8b\u6216\u8005\u5728 Spidermultusconfig CR \u4e2d\u6307\u5b9a multus.spidernet.io/cr-name \u4ee5\u66f4\u6539\u81ea\u52a8\u751f\u6210\u7684 Multus CR \u7684\u540d\u5b57\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u521b\u5efa CNI \u914d\u7f6e SpiderMultusConfig CR \u652f\u6301\u7684 CNI \u7c7b\u578b\u4f17\u591a\uff0c\u8ddf\u968f\u4e0b\u9762\u7ae0\u8282\u4e86\u89e3\uff0c\u8fdb\u884c\u521b\u5efa\u3002 \u8282\u70b9\u7f51\u5361\u540d\u79f0\u4e00\u81f4\u6027 Multus \u7684 NetworkAttachmentDefinition CR \u901a\u8fc7\u5b57\u6bb5 master \u6307\u5b9a\u8282\u70b9\u4e0a\u7684\u7f51\u5361\uff0c\u5f53\u5e94\u7528\u4f7f\u7528\u4e86\u8be5 CR \u4f46\u5e94\u7528\u7684\u591a\u4e2a Pod \u526f\u672c\u88ab\u8c03\u5ea6\u5230\u4e86\u4e0d\u540c\u8282\u70b9\u4e0a\uff0c\u800c\u6709\u4e9b\u8282\u70b9\u4e0a\u5374\u4e0d\u5b58\u5728 master \u6240\u6307\u5b9a\u7684\u7f51\u5361\u540d\uff0c\u8fd9\u5c06\u5bfc\u81f4\u4e00\u4e9b Pod \u526f\u672c\u65e0\u6cd5\u6b63\u5e38\u8fd0\u884c\u3002\u5bf9\u6b64\uff0c\u53ef\u4ee5\u53c2\u8003\u672c\u7ae0\u8282\uff0c\u5c06\u8282\u70b9\u4e0a\u7684\u7f51\u5361\u540d\u79f0\u4e00\u81f4\u5316\u3002 \u5728\u672c\u7ae0\u8282\u4e2d\u5c06\u4f7f\u7528 udev \u6765\u66f4\u6539\u8282\u70b9\u7684\u7f51\u5361\u540d\u3002udev \u662f Linux \u7cfb\u7edf\u4e2d\u7528\u4e8e\u8bbe\u5907\u7ba1\u7406\u7684\u5b50\u7cfb\u7edf\uff0c\u53ef\u4ee5\u901a\u8fc7\u89c4\u5219\u6587\u4ef6\u6765\u5b9a\u4e49\u8bbe\u5907\u7684\u5c5e\u6027\u548c\u884c\u4e3a\u3002\u4e0b\u5217\u662f\u901a\u8fc7 udev \u66f4\u6539\u8282\u70b9\u7684\u7f51\u5361\u540d\u7684\u6b65\u9aa4\uff0c\u60a8\u9700\u8981\u5728\u6bcf\u4e2a\u8981\u66f4\u6539\u7f51\u5361\u540d\u79f0\u7684\u8282\u70b9\u4e0a\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\uff1a \u786e\u5b9a\u9700\u8981\u53d8\u66f4\u7f51\u5361\u540d\u79f0\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 ip link show \u67e5\u770b\uff0c\u5e76\u5c06\u7f51\u5361\u72b6\u6001\u8bbe\u7f6e\u4e3a down \uff0c\u4f8b\u5982\uff0c\u672c\u6587\u4e2d\u7684 ens256 \u3002 # \u901a\u8fc7 `ip link set` \u547d\u4ee4\u5c06\u7f51\u5361\u72b6\u6001\u8bbe\u7f6e\u4e3a down\uff0c\u907f\u514d\u5728\u53d8\u66f4\u7f51\u5361\u540d\u65f6\u56e0 \"Device or resource busy\" \u800c\u5931\u8d25\u3002 ~# ip link set ens256 down ~# ip link show ens256 4 : ens256: <BROADCAST,MULTICAST> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff \u521b\u5efa udev \u89c4\u5219\u6587\u4ef6\uff1a\u5728 /etc/udev/rules.d/ \u76ee\u5f55\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u89c4\u5219\u6587\u4ef6\uff0c\u4f8b\u5982\uff1a 10-network.rules \uff0c\u5e76\u7f16\u5199 udev \u89c4\u5219\uff0c\u5982\u4e0b\u3002 ~# vim 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"<MAC\u5730\u5740>\" , NAME = \"<\u65b0\u7f51\u5361\u540d>\" # \u5728\u4e0a\u8ff0\u89c4\u5219\u4e2d\uff0c\u60a8\u9700\u8981\u5c06 <MAC\u5730\u5740> \u66ff\u6362\u4e3a\u5f53\u524d\u8981\u4fee\u6539\u7f51\u5361\u7684 MAC \u5730\u5740\uff0c\u5c06 <\u65b0\u7f51\u5361\u540d> \u66ff\u6362\u4e3a\u60a8\u5e0c\u671b\u8bbe\u7f6e\u7684\u65b0\u7f51\u5361\u540d\u3002 \u4f8b\u5982\uff1a ~# cat 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"00:50:56:b4:99:16\" , NAME = \"eth1\" \u4f7f udev \u5b88\u62a4\u8fdb\u7a0b\u91cd\u65b0\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 ~# udevadm control --reload-rules \u89e6\u53d1\u6240\u6709\u8bbe\u5907\u7684 add \u4e8b\u4ef6\uff0c\u4f7f\u914d\u7f6e\u751f\u6548 ~# udevadm trigger -c add \u68c0\u67e5\u7f51\u5361\u540d\u79f0\u53d8\u66f4\u6210\u529f\u3002 ~# ip link set eth1 up ~# ip link show eth1 4 : eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff \u6ce8\u610f\uff1a\u5728\u66f4\u6539\u7f51\u5361\u540d\u4e4b\u524d\uff0c\u786e\u4fdd\u4e86\u89e3\u7cfb\u7edf\u548c\u7f51\u7edc\u7684\u914d\u7f6e\uff0c\u7406\u89e3\u66f4\u6539\u53ef\u80fd\u5bf9\u5176\u4ed6\u76f8\u5173\u7ec4\u4ef6\u6216\u914d\u7f6e\u4ea7\u751f\u7684\u5f71\u54cd\uff0c\u5e76\u5efa\u8bae\u5907\u4efd\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u548c\u6570\u636e\u3002\u53e6\u5916\uff0c\u5177\u4f53\u7684\u66f4\u6539\u6b65\u9aa4\u53ef\u80fd\u56e0 Linux \u53d1\u884c\u7248\uff08\u6587\u4e2d\u4f7f\u7528 Centos 7\uff09\u800c\u6709\u6240\u5dee\u5f02\u3002 \u521b\u5efa Macvlan \u914d\u7f6e \u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u521b\u5efa IPvlan \u914d\u7f6e \u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5355\u4e2a\u4e3b\u63a5\u53e3\u4e0d\u80fd\u540c\u65f6\u88ab Macvlan \u548c IPvlan \u6240\u5974\u5f79\u3002 IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 IPvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u521b\u5efa Sriov \u914d\u7f6e \u5982\u4e0b\u662f\u521b\u5efa Sriov SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u4e00\u4e2a\u57fa\u7840\u7684\u4f8b\u5b50 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u66f4\u591a\u4fe1\u606f\u53ef\u53c2\u8003 Sriov-cni \u4f7f\u7528 \u914d\u7f6e\u542f\u7528 RDMA \u529f\u80fd cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' \u914d\u7f6e Sriov \u7f51\u7edc\u5e26\u5bbd \u6211\u4eec\u53ef\u901a\u8fc7 SpiderMultusConfig \u914d\u7f6e Sriov \u7684\u7f51\u7edc\u5e26\u5bbd: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps \u548c MaxTxRateMbps \u914d\u7f6e\u6b64 CNI \u914d\u7f6e\u6587\u4ef6\u7684\u7f51\u7edc\u4f20\u8f93\u5e26\u5bbd\u8303\u56f4\u4e3a: [100,1000] \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' Ifacer \u4f7f\u7528\u914d\u7f6e Ifacer \u63d2\u4ef6\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5728\u521b\u5efa Pod \u65f6\uff0c\u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361 \u6216\u8005 Vlan \u7f51\u5361\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u5e95\u5c42\u7f51\u7edc\u3002\u66f4\u591a\u4fe1\u606f\u53c2\u8003 Ifacer \u3002 \u81ea\u52a8\u521b\u5efa Vlan \u63a5\u53e3 \u5982\u679c\u6211\u4eec\u9700\u8981 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5\u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u6ce8\u5165 vlanID \u7684\u914d\u7f6e\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Vlan \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0cIPVLAN_MASTER_INTERFACE \u4e3a ens192\uff0cvlanID \u4e3a 100 \u4e3a\u914d\u7f6e\u4f8b\u5b50: ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e ens192 \u521b\u5efa\u4e00\u4e2a VLAN tag \u4e3a 100 \u7684\u5b50\u63a5\u53e3, \u540d\u4e3a ens192.100 main CNI: IPVlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: ens192.100 , \u4e5f\u5c31\u662f\u901a\u8fc7 ifacer \u521b\u5efa\u7684 VLAN \u5b50\u63a5\u53e3: ens192.100 \u6ce8\u610f: \u901a\u8fc7 ifacer \u521b\u5efa\u7684\u7f51\u5361\u4e0d\u662f\u6301\u4e45\u5316\u7684\uff0c\u91cd\u542f\u8282\u70b9\u6216\u8005\u4eba\u4e3a\u5220\u9664\u5c06\u4f1a\u88ab\u4e22\u5931\u3002\u91cd\u542f Pod \u4f1a\u81ea\u52a8\u6dfb\u52a0\u56de\u6765\u3002 \u6709\u65f6\u5019\u7f51\u7edc\u7ba1\u7406\u5458\u5df2\u7ecf\u521b\u5efa\u597d VLAN \u5b50\u63a5\u53e3\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u4f7f\u7528 ifacer \u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u3002\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u914d\u7f6e master \u5b57\u6bb5\u4e3a: ens192.100 \uff0c\u5e76\u4e14\u4e0d\u914d\u7f6e VLAN ID , \u5982\u4e0b: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100 \u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361 \u5982\u679c\u6211\u4eec\u9700\u8981 Bond \u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5 Bond \u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u914d\u7f6e\u591a\u4e2a master \u63a5\u53e3\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Bond \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0c\u4e3b\u673a\u63a5\u53e3 ens192, ens224 \u4e3a slave \u521b\u5efa Bond \u63a5\u53e3\u4e3a\u4f8b\u5b50: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u914d\u7f6e\u8bf4\u660e: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e [\"ens192\",\"ens224\"] \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 bond \u63a5\u53e3\uff0cmode \u4e3a 1(active-backup)\u3002 IPVlan \u4f5c\u4e3a main CNI\uff0c\u5176 master \u5b57\u6bb5\u7684\u503c\u4e3a: bond0 \uff0c bond0 \u627f\u63a5 Pod \u7684\u7f51\u7edc\u6d41\u91cf\u3002 \u521b\u5efa Bond \u5982\u679c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options \u5b9e\u73b0\u3002 \u8f93\u5165\u683c\u5f0f\u4e3a: \"primary=ens160;arp_interval=1\",\u591a\u4e2a\u53c2\u6570\u7528\";\"\u8fde\u63a5 \u5982\u679c\u6211\u4eec\u9700\u8981\u57fa\u4e8e\u5df2\u521b\u5efa\u7684 Bond \u7f51\u5361 bond0 \u521b\u5efa Vlan \u5b50\u63a5\u53e3\uff0c\u4ee5\u6b64 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u914d\u7f6e: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u4f7f\u7528\u4ee5\u4e0a\u914d\u7f6e\u521b\u5efa Pod\uff0c ifacer \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa\u4e00\u5f20 bond \u7f51\u5361 bond0 \u4ee5\u53ca\u4e00\u5f20 Vlan \u7f51\u5361 bond0.100 \u3002 \u5176\u4ed6 CNI \u914d\u7f6e \u521b\u5efa\u5176\u4ed6 CNI \u914d\u7f6e\uff0c\u5982 Ovs: \u53c2\u8003 \u521b\u5efa Ovs ChainCNI \u914d\u7f6e \u5982\u679c\u60a8\u9700\u8981\u4e3a CNI \u914d\u7f6e\u9644\u52a0 ChainCNI \u7684\u914d\u7f6e\uff0c\u6bd4\u5982\u9700\u8981\u4f7f\u7528 tuning \u63d2\u4ef6\u914d\u7f6e Pod \u7684\u7cfb\u7edf\u5185\u6838\u53c2\u6570\uff08\u5982 net.core.somaxconn \u7b49\u53c2\u6570\uff09\u3002\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u914d\u7f6e\u5b9e\u73b0(\u4ee5 MacVlan CNI \u4e3a\u4f8b)\uff1a \u521b\u5efa SpiderMultusConfig: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens192 chainCNIJsonData: - | { \"type\": \"tuning\", \"sysctl\": { \"net.core.somaxconn\": \"4096\" } } EOF \u6ce8\u610f chainCNIJsonData \u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u5fc5\u987b\u662f\u5408\u6cd5\u7684 json \u5b57\u7b26\u4e32\u3002\u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: generation: 1 name: macvlan-conf namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"},{\"type\":\"tuning\", \"sysctl\": {\"net.core.somaxconn\": \"4096\"}}]}' \u603b\u7ed3 SpiderMultusConfig CR \u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\uff0c\u63d0\u5347\u4e86\u521b\u5efa\u4f53\u9a8c\uff0c\u964d\u4f4e\u4e86\u8fd0\u7ef4\u6210\u672c\u3002","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config-zh_CN/#spidermultusconfig","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config-zh_CN/#_1","text":"Spiderpool \u63d0\u4f9b\u4e86 Spidermultusconfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f00\u6e90\u9879\u76ee Multus CNI \u914d\u7f6e\u7ba1\u7406\u7684\u6269\u5c55\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-multus-config-zh_CN/#spidermultusconfig_1","text":"Multus \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u5b83\u901a\u8fc7\u8c03\u5ea6\u7b2c\u4e09\u65b9 CNI \u9879\u76ee\uff0c\u80fd\u591f\u5b9e\u73b0\u4e3a Pod \u63a5\u5165\u591a\u5f20\u7f51\u5361\u3002\u5e76\u4e14\uff0cMultus \u53ef\u4ee5\u901a\u8fc7 CRD \u65b9\u5f0f\u7ba1\u7406 CNI \u914d\u7f6e\uff0c\u907f\u514d\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u624b\u52a8\u7f16\u8f91 CNI \u914d\u7f6e\u6587\u4ef6\u3002\u4f46\u521b\u5efa Multus CR \u65f6\uff0c\u9700\u8981\u624b\u52a8\u4e66\u5199 JSON \u683c\u5f0f\u7684 CNI \u914d\u7f6e\u5b57\u7b26\u4e32\u3002\u5c06\u4f1a\u5bfc\u81f4\u5982\u4e0b\u95ee\u9898\u3002 \u4eba\u4e3a\u4e66\u5199\u5bb9\u6613\u51fa\u73b0 JSON \u683c\u5f0f\u9519\u8bef\uff0c\u589e\u52a0 Pod \u542f\u52a8\u5931\u8d25\u7684\u6392\u969c\u6210\u672c\u3002 CNI \u79cd\u7c7b\u591a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5404\u4e2a\u914d\u7f6e\u9879\u4e5f\u5f88\u591a\uff0c\u4e0d\u5bb9\u6613\u8bb0\u5fc6\uff0c\u7ecf\u5e38\u9700\u8981\u8fdb\u884c\u8d44\u6599\u67e5\u9605\uff0c\u7528\u6237\u4f53\u9a8c\u4e0d\u53cb\u597d\u3002 Spidermultusconfig CR \u57fa\u4e8e spec \u4e2d\u7684\u5b9a\u4e49\u81ea\u52a8\u751f\u6210 Multus CR\uff0c\u6539\u8fdb\u4e86\u5982\u4e0a\u95ee\u9898\uff0c\u5e76\u4e14\u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u70b9\uff1a \u8bef\u64cd\u4f5c\u5220\u9664 Multus CR\uff0cSpidermultusconfig \u5c06\u4f1a\u81ea\u52a8\u91cd\u5efa\uff1b\u63d0\u5347\u8fd0\u7ef4\u5bb9\u9519\u80fd\u529b\u3002 \u652f\u6301\u4f17\u591a CNI\uff0c\u5982 Macvlan\u3001IPvlan\u3001Ovs\u3001SR-IOV\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cr-name \u81ea\u5b9a\u4e49 Multus CR \u7684\u540d\u5b57\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cni-version \u81ea\u5b9a\u4e49\u8bbe\u7f6e CNI \u7684\u7248\u672c\u3002 \u5b8c\u5584\u7684 Webhook \u673a\u5236\uff0c\u63d0\u524d\u89c4\u907f\u4e00\u4e9b\u4eba\u4e3a\u9519\u8bef\uff0c\u964d\u4f4e\u540e\u7eed\u6392\u969c\u6210\u672c\u3002 \u652f\u6301 Spiderpool \u7684 CNI plugin\uff1a ifacer \u3001 coordinator \uff0c\u63d0\u9ad8\u4e86 Spiderpool \u7684 CNI plugin \u7684\u914d\u7f6e\u4f53\u9a8c\u3002 \u5728\u5df2\u5b58\u5728 Multus CR \u5b9e\u4f8b\u65f6\uff0c\u521b\u5efa\u4e0e\u5176\u540c\u540d Spidermultusconfig CR \uff0cMultus CR \u5b9e\u4f8b\u5c06\u4f1a\u88ab\u7eb3\u7ba1\uff0c\u5176\u914d\u7f6e\u5185\u5bb9\u5c06\u4f1a\u88ab\u8986\u76d6\u3002\u5982\u679c\u4e0d\u60f3\u53d1\u751f\u88ab\u8986\u76d6\u7684\u60c5\u51b5\uff0c\u8bf7\u907f\u514d\u521b\u5efa\u4e0e\u5b58\u91cf Multus CR \u5b9e\u4f8b\u540c\u540d\u7684 Spidermultusconfig CR \u5b9e\u4f8b\u6216\u8005\u5728 Spidermultusconfig CR \u4e2d\u6307\u5b9a multus.spidernet.io/cr-name \u4ee5\u66f4\u6539\u81ea\u52a8\u751f\u6210\u7684 Multus CR \u7684\u540d\u5b57\u3002","title":"SpiderMultusConfig \u529f\u80fd"},{"location":"usage/spider-multus-config-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/spider-multus-config-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/spider-multus-config-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool.","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/spider-multus-config-zh_CN/#cni","text":"SpiderMultusConfig CR \u652f\u6301\u7684 CNI \u7c7b\u578b\u4f17\u591a\uff0c\u8ddf\u968f\u4e0b\u9762\u7ae0\u8282\u4e86\u89e3\uff0c\u8fdb\u884c\u521b\u5efa\u3002","title":"\u521b\u5efa CNI \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#_4","text":"Multus \u7684 NetworkAttachmentDefinition CR \u901a\u8fc7\u5b57\u6bb5 master \u6307\u5b9a\u8282\u70b9\u4e0a\u7684\u7f51\u5361\uff0c\u5f53\u5e94\u7528\u4f7f\u7528\u4e86\u8be5 CR \u4f46\u5e94\u7528\u7684\u591a\u4e2a Pod \u526f\u672c\u88ab\u8c03\u5ea6\u5230\u4e86\u4e0d\u540c\u8282\u70b9\u4e0a\uff0c\u800c\u6709\u4e9b\u8282\u70b9\u4e0a\u5374\u4e0d\u5b58\u5728 master \u6240\u6307\u5b9a\u7684\u7f51\u5361\u540d\uff0c\u8fd9\u5c06\u5bfc\u81f4\u4e00\u4e9b Pod \u526f\u672c\u65e0\u6cd5\u6b63\u5e38\u8fd0\u884c\u3002\u5bf9\u6b64\uff0c\u53ef\u4ee5\u53c2\u8003\u672c\u7ae0\u8282\uff0c\u5c06\u8282\u70b9\u4e0a\u7684\u7f51\u5361\u540d\u79f0\u4e00\u81f4\u5316\u3002 \u5728\u672c\u7ae0\u8282\u4e2d\u5c06\u4f7f\u7528 udev \u6765\u66f4\u6539\u8282\u70b9\u7684\u7f51\u5361\u540d\u3002udev \u662f Linux \u7cfb\u7edf\u4e2d\u7528\u4e8e\u8bbe\u5907\u7ba1\u7406\u7684\u5b50\u7cfb\u7edf\uff0c\u53ef\u4ee5\u901a\u8fc7\u89c4\u5219\u6587\u4ef6\u6765\u5b9a\u4e49\u8bbe\u5907\u7684\u5c5e\u6027\u548c\u884c\u4e3a\u3002\u4e0b\u5217\u662f\u901a\u8fc7 udev \u66f4\u6539\u8282\u70b9\u7684\u7f51\u5361\u540d\u7684\u6b65\u9aa4\uff0c\u60a8\u9700\u8981\u5728\u6bcf\u4e2a\u8981\u66f4\u6539\u7f51\u5361\u540d\u79f0\u7684\u8282\u70b9\u4e0a\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\uff1a \u786e\u5b9a\u9700\u8981\u53d8\u66f4\u7f51\u5361\u540d\u79f0\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 ip link show \u67e5\u770b\uff0c\u5e76\u5c06\u7f51\u5361\u72b6\u6001\u8bbe\u7f6e\u4e3a down \uff0c\u4f8b\u5982\uff0c\u672c\u6587\u4e2d\u7684 ens256 \u3002 # \u901a\u8fc7 `ip link set` \u547d\u4ee4\u5c06\u7f51\u5361\u72b6\u6001\u8bbe\u7f6e\u4e3a down\uff0c\u907f\u514d\u5728\u53d8\u66f4\u7f51\u5361\u540d\u65f6\u56e0 \"Device or resource busy\" \u800c\u5931\u8d25\u3002 ~# ip link set ens256 down ~# ip link show ens256 4 : ens256: <BROADCAST,MULTICAST> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff \u521b\u5efa udev \u89c4\u5219\u6587\u4ef6\uff1a\u5728 /etc/udev/rules.d/ \u76ee\u5f55\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u89c4\u5219\u6587\u4ef6\uff0c\u4f8b\u5982\uff1a 10-network.rules \uff0c\u5e76\u7f16\u5199 udev \u89c4\u5219\uff0c\u5982\u4e0b\u3002 ~# vim 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"<MAC\u5730\u5740>\" , NAME = \"<\u65b0\u7f51\u5361\u540d>\" # \u5728\u4e0a\u8ff0\u89c4\u5219\u4e2d\uff0c\u60a8\u9700\u8981\u5c06 <MAC\u5730\u5740> \u66ff\u6362\u4e3a\u5f53\u524d\u8981\u4fee\u6539\u7f51\u5361\u7684 MAC \u5730\u5740\uff0c\u5c06 <\u65b0\u7f51\u5361\u540d> \u66ff\u6362\u4e3a\u60a8\u5e0c\u671b\u8bbe\u7f6e\u7684\u65b0\u7f51\u5361\u540d\u3002 \u4f8b\u5982\uff1a ~# cat 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"00:50:56:b4:99:16\" , NAME = \"eth1\" \u4f7f udev \u5b88\u62a4\u8fdb\u7a0b\u91cd\u65b0\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 ~# udevadm control --reload-rules \u89e6\u53d1\u6240\u6709\u8bbe\u5907\u7684 add \u4e8b\u4ef6\uff0c\u4f7f\u914d\u7f6e\u751f\u6548 ~# udevadm trigger -c add \u68c0\u67e5\u7f51\u5361\u540d\u79f0\u53d8\u66f4\u6210\u529f\u3002 ~# ip link set eth1 up ~# ip link show eth1 4 : eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff \u6ce8\u610f\uff1a\u5728\u66f4\u6539\u7f51\u5361\u540d\u4e4b\u524d\uff0c\u786e\u4fdd\u4e86\u89e3\u7cfb\u7edf\u548c\u7f51\u7edc\u7684\u914d\u7f6e\uff0c\u7406\u89e3\u66f4\u6539\u53ef\u80fd\u5bf9\u5176\u4ed6\u76f8\u5173\u7ec4\u4ef6\u6216\u914d\u7f6e\u4ea7\u751f\u7684\u5f71\u54cd\uff0c\u5e76\u5efa\u8bae\u5907\u4efd\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u548c\u6570\u636e\u3002\u53e6\u5916\uff0c\u5177\u4f53\u7684\u66f4\u6539\u6b65\u9aa4\u53ef\u80fd\u56e0 Linux \u53d1\u884c\u7248\uff08\u6587\u4e2d\u4f7f\u7528 Centos 7\uff09\u800c\u6709\u6240\u5dee\u5f02\u3002","title":"\u8282\u70b9\u7f51\u5361\u540d\u79f0\u4e00\u81f4\u6027"},{"location":"usage/spider-multus-config-zh_CN/#macvlan","text":"\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"\u521b\u5efa Macvlan \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#ipvlan","text":"\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5355\u4e2a\u4e3b\u63a5\u53e3\u4e0d\u80fd\u540c\u65f6\u88ab Macvlan \u548c IPvlan \u6240\u5974\u5f79\u3002 IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 IPvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"\u521b\u5efa IPvlan \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#sriov","text":"\u5982\u4e0b\u662f\u521b\u5efa Sriov SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u4e00\u4e2a\u57fa\u7840\u7684\u4f8b\u5b50 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u66f4\u591a\u4fe1\u606f\u53ef\u53c2\u8003 Sriov-cni \u4f7f\u7528 \u914d\u7f6e\u542f\u7528 RDMA \u529f\u80fd cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' \u914d\u7f6e Sriov \u7f51\u7edc\u5e26\u5bbd \u6211\u4eec\u53ef\u901a\u8fc7 SpiderMultusConfig \u914d\u7f6e Sriov \u7684\u7f51\u7edc\u5e26\u5bbd: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps \u548c MaxTxRateMbps \u914d\u7f6e\u6b64 CNI \u914d\u7f6e\u6587\u4ef6\u7684\u7f51\u7edc\u4f20\u8f93\u5e26\u5bbd\u8303\u56f4\u4e3a: [100,1000] \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}'","title":"\u521b\u5efa Sriov \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#ifacer","text":"Ifacer \u63d2\u4ef6\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5728\u521b\u5efa Pod \u65f6\uff0c\u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361 \u6216\u8005 Vlan \u7f51\u5361\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u5e95\u5c42\u7f51\u7edc\u3002\u66f4\u591a\u4fe1\u606f\u53c2\u8003 Ifacer \u3002","title":"Ifacer \u4f7f\u7528\u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#vlan","text":"\u5982\u679c\u6211\u4eec\u9700\u8981 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5\u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u6ce8\u5165 vlanID \u7684\u914d\u7f6e\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Vlan \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0cIPVLAN_MASTER_INTERFACE \u4e3a ens192\uff0cvlanID \u4e3a 100 \u4e3a\u914d\u7f6e\u4f8b\u5b50: ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e ens192 \u521b\u5efa\u4e00\u4e2a VLAN tag \u4e3a 100 \u7684\u5b50\u63a5\u53e3, \u540d\u4e3a ens192.100 main CNI: IPVlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: ens192.100 , \u4e5f\u5c31\u662f\u901a\u8fc7 ifacer \u521b\u5efa\u7684 VLAN \u5b50\u63a5\u53e3: ens192.100 \u6ce8\u610f: \u901a\u8fc7 ifacer \u521b\u5efa\u7684\u7f51\u5361\u4e0d\u662f\u6301\u4e45\u5316\u7684\uff0c\u91cd\u542f\u8282\u70b9\u6216\u8005\u4eba\u4e3a\u5220\u9664\u5c06\u4f1a\u88ab\u4e22\u5931\u3002\u91cd\u542f Pod \u4f1a\u81ea\u52a8\u6dfb\u52a0\u56de\u6765\u3002 \u6709\u65f6\u5019\u7f51\u7edc\u7ba1\u7406\u5458\u5df2\u7ecf\u521b\u5efa\u597d VLAN \u5b50\u63a5\u53e3\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u4f7f\u7528 ifacer \u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u3002\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u914d\u7f6e master \u5b57\u6bb5\u4e3a: ens192.100 \uff0c\u5e76\u4e14\u4e0d\u914d\u7f6e VLAN ID , \u5982\u4e0b: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100","title":"\u81ea\u52a8\u521b\u5efa Vlan \u63a5\u53e3"},{"location":"usage/spider-multus-config-zh_CN/#bond","text":"\u5982\u679c\u6211\u4eec\u9700\u8981 Bond \u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5 Bond \u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u914d\u7f6e\u591a\u4e2a master \u63a5\u53e3\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Bond \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0c\u4e3b\u673a\u63a5\u53e3 ens192, ens224 \u4e3a slave \u521b\u5efa Bond \u63a5\u53e3\u4e3a\u4f8b\u5b50: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u914d\u7f6e\u8bf4\u660e: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e [\"ens192\",\"ens224\"] \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 bond \u63a5\u53e3\uff0cmode \u4e3a 1(active-backup)\u3002 IPVlan \u4f5c\u4e3a main CNI\uff0c\u5176 master \u5b57\u6bb5\u7684\u503c\u4e3a: bond0 \uff0c bond0 \u627f\u63a5 Pod \u7684\u7f51\u7edc\u6d41\u91cf\u3002 \u521b\u5efa Bond \u5982\u679c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options \u5b9e\u73b0\u3002 \u8f93\u5165\u683c\u5f0f\u4e3a: \"primary=ens160;arp_interval=1\",\u591a\u4e2a\u53c2\u6570\u7528\";\"\u8fde\u63a5 \u5982\u679c\u6211\u4eec\u9700\u8981\u57fa\u4e8e\u5df2\u521b\u5efa\u7684 Bond \u7f51\u5361 bond0 \u521b\u5efa Vlan \u5b50\u63a5\u53e3\uff0c\u4ee5\u6b64 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u914d\u7f6e: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u4f7f\u7528\u4ee5\u4e0a\u914d\u7f6e\u521b\u5efa Pod\uff0c ifacer \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa\u4e00\u5f20 bond \u7f51\u5361 bond0 \u4ee5\u53ca\u4e00\u5f20 Vlan \u7f51\u5361 bond0.100 \u3002","title":"\u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361"},{"location":"usage/spider-multus-config-zh_CN/#cni_1","text":"\u521b\u5efa\u5176\u4ed6 CNI \u914d\u7f6e\uff0c\u5982 Ovs: \u53c2\u8003 \u521b\u5efa Ovs","title":"\u5176\u4ed6 CNI \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#chaincni","text":"\u5982\u679c\u60a8\u9700\u8981\u4e3a CNI \u914d\u7f6e\u9644\u52a0 ChainCNI \u7684\u914d\u7f6e\uff0c\u6bd4\u5982\u9700\u8981\u4f7f\u7528 tuning \u63d2\u4ef6\u914d\u7f6e Pod \u7684\u7cfb\u7edf\u5185\u6838\u53c2\u6570\uff08\u5982 net.core.somaxconn \u7b49\u53c2\u6570\uff09\u3002\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u914d\u7f6e\u5b9e\u73b0(\u4ee5 MacVlan CNI \u4e3a\u4f8b)\uff1a \u521b\u5efa SpiderMultusConfig: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens192 chainCNIJsonData: - | { \"type\": \"tuning\", \"sysctl\": { \"net.core.somaxconn\": \"4096\" } } EOF \u6ce8\u610f chainCNIJsonData \u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u5fc5\u987b\u662f\u5408\u6cd5\u7684 json \u5b57\u7b26\u4e32\u3002\u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: generation: 1 name: macvlan-conf namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"},{\"type\":\"tuning\", \"sysctl\": {\"net.core.somaxconn\": \"4096\"}}]}'","title":"ChainCNI \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#_5","text":"SpiderMultusConfig CR \u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\uff0c\u63d0\u5347\u4e86\u521b\u5efa\u4f53\u9a8c\uff0c\u964d\u4f4e\u4e86\u8fd0\u7ef4\u6210\u672c\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-multus-config/","text":"SpiderMultusConfig English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool introduces the SpiderMultusConfig CR to automate the management of Multus NetworkAttachmentDefinition CR and extend the capabilities of Multus CNI configurations. SpiderMultusConfig Features Multus is a CNI plugin project that enables Pods to access multiple network interfaces by leveraging third-party CNI plugins. While Multus allows management of CNI configurations through CRDs, manually writing JSON-formatted CNI configuration strings can lead to error: Human errors in writing JSON format may cause troubleshooting difficulties and Pod startup failures. There are numerous CNIs with various configuration options, making it difficult to remember them all, and users often need to refer to documentation, resulting in a poor user experience. To address these issues, SpiderMultusConfig automatically generates the Multus CR based on its spec . It offers several features: In case of accidental deletion of a Multus CR, SpiderMultusConfig will automatically recreate it, improving operational fault tolerance. Support for various CNIs, such as Macvlan, IPvlan, Ovs, and SR-IOV. The annotation multus.spidernet.io/cr-name allows users to define a custom name for Multus CR. The annotation multus.spidernet.io/cni-version enables specifying a specific CNI version. A robust webhook mechanism is involved to proactively detect and prevent human errors, reducing troubleshooting efforts. Spiderpool's CNI plugins, including ifacer and coordinator are integrated, enhancing the overall configuration experience. It is important to note that when creating a SpiderMultusConfig CR with the same name as an existing Multus CR, the Multus CR instance will be managed by SpiderMultusConfig, and its configuration will be overwritten. To avoid overwriting existing Multus CR instances, it is recommended to either refrain from creating SpiderMultusConfig CR instances with the same name or specify a different name for the generated Multus CR using the multus.spidernet.io/cr-name annotation in the SpiderMultusConfig CR. Prerequisites A ready Kubernetes cluster. Helm has been installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. Create CNI Configurations SpiderMultusConfig CR supports various types of CNIs. The following sections explain how to create these configurations. Node NIC name consistency Multus's NetworkAttachmentDefinition CR specifies the NIC on the node through the field master . When an application uses this CR but multiple Pod copies of the application are scheduled to different nodes, and the NIC name specified by master does not exist on some nodes, This will cause some Pod replicas to not function properly. In this regard, you can refer to this chapter to make the NIC names on the nodes consistent. In this chapter, udev will be used to change the NIC name of the node. udev is a subsystem used for device management in Linux systems. It can define device attributes and behaviors through rule files. The following are the steps to change the node NIC name through udev. You need to do the following on each node where you want to change the NIC name: Confirm that the NIC name needs to be changed. You can use the ip link show to view it and set the NIC status to down , for example, ens256 in this article. # Use the `ip link set` command to set the NIC status to down to avoid failure due to \"Device or resource busy\" when changing the NIC name. ~# ip link set ens256 down ~# ip link show ens256 4 : ens256: <BROADCAST,MULTICAST> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff Create a udev rule file: Create a new rule file in the /etc/udev/rules.d/ directory, for example: 10-network.rules , and write the udev rule as follows. ~# vim 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"<MAC address>\" , NAME = \"<New NIC name>\" # In the above rules, you need to replace <MAC address> with the MAC address of the current NIC you want to modify, and replace <new NIC name> with the new NIC name you want to set. For example: ~# cat 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"00:50:56:b4:99:16\" , NAME = \"eth1\" Cause the udev daemon to reload the configuration file. ~# udevadm control --reload-rules Trigger the add event of all devices to make the configuration take effect. ~# udevadm trigger -c add Check that the NIC name has been changed successfully. ~# ip link set eth1 up ~# ip link show eth1 4 : eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff Note: Before changing the NIC name, make sure to understand the configuration of the system and network, understand the impact that the change may have on other related components or configurations, and it is recommended to back up related configuration files and data. The exact steps may vary depending on the Linux distribution (Centos 7 is used in this article). Create Macvlan Configurations Here is an example of creating Macvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF Create the Macvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' Create IPvlan Configurations Here is an example of creating IPvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. When using IPVlan as the cluster's CNI, the kernel version must be higher than 4.2. A single main interface cannot be used by both Macvlan and IPvlan simultaneously. IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF Create the IPvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' Create Sriov Configuration Here is an example of creating Sriov SpiderMultusConfig configuration: Basic example cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' For more information, refer to sriov-cni usage Enable RDMA feature cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' Configure Sriov-CNI Network Bandwidth We can configure the network bandwidth of Sriov through SpiderMultusConfig: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps and MaxTxRateMbps configure the transmission bandwidth range for pods created with this configuration: [100,1000]. After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' Ifacer Configurations The Ifacer plug-in can help us automatically create a Bond NIC or VLAN NIC when creating a pod to undertake the pod's underlying network. For more information, refer to Ifacer . Ifacer create vlan interface If we need a VLAN sub-interface to take over the underlying network of the pod, and the interface has not yet been created on the node. We can inject the configuration of the vlanID in Spidermultusconfig so that when the corresponding Multus NetworkAttachmentDefinition CR is generated, it will be injected The ifacer plug-in will dynamically create a VLAN interface on the host when the pod is created, which is used to undertake the pod's underlay network. The following is an example of CNI as IPVlan, IPVLAN_MASTER_INTERFACE as ens192, and vlanID as 100. ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a sub-interface with a VLAN tag of 100 named ens192.100 based on ens192 . main CNI: The value of the master field of IPVlan is: ens192.100 , which is the VLAN sub-interface created by 'ifacer': ens192.100 . Note: The NIC created by ifacer is not persistent, and will be lost if the node is restarted or manually deleted. Restarting the pod is automatically added back. Sometimes the network administrator has already created the VLAN sub-interface, and we don't need to use ifacer to create the VLAN sub-interface. We can directly configure the master field as: ens192.100 and not configure the VLAN ID, as follows: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100 Ifacer create bond interface If we need a bond interface to take over the underlying network of the pod, and the bond interface has not yet been created on the node. We can configure multiple master interfaces in Spidermultusconfig so that the corresponding Multus NetworkAttachmentDefinition CR is generated and injected The ifacer' plug-in will dynamically create a bond interface on the host when the pod is created, which is used to undertake the underlying network of the pod. The following is an example of CNI as IPVlan, host interface ens192, and ens224 as slave to create a bond interface: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } Configuration description: ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a bond interface named 'bond0' based on [\"ens192\",\"ens224\"] with mode 1 (active-backup). main CNI: The value of the master field of IPvlan is: bond0 , bond0 takes over the network traffic of the pod. Create a Bond If you need a more advanced configuration, you can do so by configuring SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options. The input format is: \"primary=ens160; arp_interval=1\", use \";\" for multiple parameters. If we need to create a VLAN sub-interface based on the created BOND NIC: bond0, so that the VLAN sub-interface undertakes the underlying network of the pod, we can refer to the following configuration: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF When creating a pod with the above configuration, ifacer will create a bond NIC bond0 and a VLAN NIC bond0.100 on the host. Other CNI Configurationsi To create other types of CNI configurations, such OVS, refer to Ovs . ChainCNI \u914d\u7f6e If you need to add ChainCNI to the CNI configuration, for example, you need to use the tuning plug-in to configure the system kernel parameters of the pod (such as net.core.somaxconn, etc.). This can be achieved with the following configuration (MacVlan CNI as an example): Create a SpiderMultusConfig CR: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens192 chainCNIJsonData: - | { \"type\": \"tuning\", \"sysctl\": { \"net.core.somaxconn\": \"4096\" } } EOF Note that every element of Shanknick Senda must be a legitimate Ethan string, you can refer to as follow manifest. When created, view the corresponding Maltus Netwalk-Atahement-De Finity object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: generation: 1 name: macvlan-conf namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"},{\"type\":\"tuning\", \"sysctl\": {\"net.core.somaxconn\": \"4096\"}}]}' Conclusion SpiderMultusConfig CR automates the management of Multus NetworkAttachmentDefinition CRs, improving the experience of creating configurations and reducing operational costs.","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config/#spidermultusconfig","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config/#introduction","text":"Spiderpool introduces the SpiderMultusConfig CR to automate the management of Multus NetworkAttachmentDefinition CR and extend the capabilities of Multus CNI configurations.","title":"Introduction"},{"location":"usage/spider-multus-config/#spidermultusconfig-features","text":"Multus is a CNI plugin project that enables Pods to access multiple network interfaces by leveraging third-party CNI plugins. While Multus allows management of CNI configurations through CRDs, manually writing JSON-formatted CNI configuration strings can lead to error: Human errors in writing JSON format may cause troubleshooting difficulties and Pod startup failures. There are numerous CNIs with various configuration options, making it difficult to remember them all, and users often need to refer to documentation, resulting in a poor user experience. To address these issues, SpiderMultusConfig automatically generates the Multus CR based on its spec . It offers several features: In case of accidental deletion of a Multus CR, SpiderMultusConfig will automatically recreate it, improving operational fault tolerance. Support for various CNIs, such as Macvlan, IPvlan, Ovs, and SR-IOV. The annotation multus.spidernet.io/cr-name allows users to define a custom name for Multus CR. The annotation multus.spidernet.io/cni-version enables specifying a specific CNI version. A robust webhook mechanism is involved to proactively detect and prevent human errors, reducing troubleshooting efforts. Spiderpool's CNI plugins, including ifacer and coordinator are integrated, enhancing the overall configuration experience. It is important to note that when creating a SpiderMultusConfig CR with the same name as an existing Multus CR, the Multus CR instance will be managed by SpiderMultusConfig, and its configuration will be overwritten. To avoid overwriting existing Multus CR instances, it is recommended to either refrain from creating SpiderMultusConfig CR instances with the same name or specify a different name for the generated Multus CR using the multus.spidernet.io/cr-name annotation in the SpiderMultusConfig CR.","title":"SpiderMultusConfig Features"},{"location":"usage/spider-multus-config/#prerequisites","text":"A ready Kubernetes cluster. Helm has been installed.","title":"Prerequisites"},{"location":"usage/spider-multus-config/#steps","text":"","title":"Steps"},{"location":"usage/spider-multus-config/#install-spiderpool","text":"Refer to Installation to install Spiderpool.","title":"Install Spiderpool"},{"location":"usage/spider-multus-config/#create-cni-configurations","text":"SpiderMultusConfig CR supports various types of CNIs. The following sections explain how to create these configurations.","title":"Create CNI Configurations"},{"location":"usage/spider-multus-config/#node-nic-name-consistency","text":"Multus's NetworkAttachmentDefinition CR specifies the NIC on the node through the field master . When an application uses this CR but multiple Pod copies of the application are scheduled to different nodes, and the NIC name specified by master does not exist on some nodes, This will cause some Pod replicas to not function properly. In this regard, you can refer to this chapter to make the NIC names on the nodes consistent. In this chapter, udev will be used to change the NIC name of the node. udev is a subsystem used for device management in Linux systems. It can define device attributes and behaviors through rule files. The following are the steps to change the node NIC name through udev. You need to do the following on each node where you want to change the NIC name: Confirm that the NIC name needs to be changed. You can use the ip link show to view it and set the NIC status to down , for example, ens256 in this article. # Use the `ip link set` command to set the NIC status to down to avoid failure due to \"Device or resource busy\" when changing the NIC name. ~# ip link set ens256 down ~# ip link show ens256 4 : ens256: <BROADCAST,MULTICAST> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff Create a udev rule file: Create a new rule file in the /etc/udev/rules.d/ directory, for example: 10-network.rules , and write the udev rule as follows. ~# vim 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"<MAC address>\" , NAME = \"<New NIC name>\" # In the above rules, you need to replace <MAC address> with the MAC address of the current NIC you want to modify, and replace <new NIC name> with the new NIC name you want to set. For example: ~# cat 10 -network.rules SUBSYSTEM == \"net\" , ACTION == \"add\" , ATTR { address }== \"00:50:56:b4:99:16\" , NAME = \"eth1\" Cause the udev daemon to reload the configuration file. ~# udevadm control --reload-rules Trigger the add event of all devices to make the configuration take effect. ~# udevadm trigger -c add Check that the NIC name has been changed successfully. ~# ip link set eth1 up ~# ip link show eth1 4 : eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00 :50:56:b4:99:16 brd ff:ff:ff:ff:ff:ff Note: Before changing the NIC name, make sure to understand the configuration of the system and network, understand the impact that the change may have on other related components or configurations, and it is recommended to back up related configuration files and data. The exact steps may vary depending on the Linux distribution (Centos 7 is used in this article).","title":"Node NIC name consistency"},{"location":"usage/spider-multus-config/#create-macvlan-configurations","text":"Here is an example of creating Macvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF Create the Macvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"Create Macvlan Configurations"},{"location":"usage/spider-multus-config/#create-ipvlan-configurations","text":"Here is an example of creating IPvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. When using IPVlan as the cluster's CNI, the kernel version must be higher than 4.2. A single main interface cannot be used by both Macvlan and IPvlan simultaneously. IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF Create the IPvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"Create IPvlan Configurations"},{"location":"usage/spider-multus-config/#create-sriov-configuration","text":"Here is an example of creating Sriov SpiderMultusConfig configuration: Basic example cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' For more information, refer to sriov-cni usage Enable RDMA feature cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' Configure Sriov-CNI Network Bandwidth We can configure the network bandwidth of Sriov through SpiderMultusConfig: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps and MaxTxRateMbps configure the transmission bandwidth range for pods created with this configuration: [100,1000]. After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}'","title":"Create Sriov Configuration"},{"location":"usage/spider-multus-config/#ifacer-configurations","text":"The Ifacer plug-in can help us automatically create a Bond NIC or VLAN NIC when creating a pod to undertake the pod's underlying network. For more information, refer to Ifacer .","title":"Ifacer Configurations"},{"location":"usage/spider-multus-config/#ifacer-create-vlan-interface","text":"If we need a VLAN sub-interface to take over the underlying network of the pod, and the interface has not yet been created on the node. We can inject the configuration of the vlanID in Spidermultusconfig so that when the corresponding Multus NetworkAttachmentDefinition CR is generated, it will be injected The ifacer plug-in will dynamically create a VLAN interface on the host when the pod is created, which is used to undertake the pod's underlay network. The following is an example of CNI as IPVlan, IPVLAN_MASTER_INTERFACE as ens192, and vlanID as 100. ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a sub-interface with a VLAN tag of 100 named ens192.100 based on ens192 . main CNI: The value of the master field of IPVlan is: ens192.100 , which is the VLAN sub-interface created by 'ifacer': ens192.100 . Note: The NIC created by ifacer is not persistent, and will be lost if the node is restarted or manually deleted. Restarting the pod is automatically added back. Sometimes the network administrator has already created the VLAN sub-interface, and we don't need to use ifacer to create the VLAN sub-interface. We can directly configure the master field as: ens192.100 and not configure the VLAN ID, as follows: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100","title":"Ifacer create vlan interface"},{"location":"usage/spider-multus-config/#ifacer-create-bond-interface","text":"If we need a bond interface to take over the underlying network of the pod, and the bond interface has not yet been created on the node. We can configure multiple master interfaces in Spidermultusconfig so that the corresponding Multus NetworkAttachmentDefinition CR is generated and injected The ifacer' plug-in will dynamically create a bond interface on the host when the pod is created, which is used to undertake the underlying network of the pod. The following is an example of CNI as IPVlan, host interface ens192, and ens224 as slave to create a bond interface: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } Configuration description: ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a bond interface named 'bond0' based on [\"ens192\",\"ens224\"] with mode 1 (active-backup). main CNI: The value of the master field of IPvlan is: bond0 , bond0 takes over the network traffic of the pod. Create a Bond If you need a more advanced configuration, you can do so by configuring SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options. The input format is: \"primary=ens160; arp_interval=1\", use \";\" for multiple parameters. If we need to create a VLAN sub-interface based on the created BOND NIC: bond0, so that the VLAN sub-interface undertakes the underlying network of the pod, we can refer to the following configuration: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF When creating a pod with the above configuration, ifacer will create a bond NIC bond0 and a VLAN NIC bond0.100 on the host.","title":"Ifacer create bond interface"},{"location":"usage/spider-multus-config/#other-cni-configurationsi","text":"To create other types of CNI configurations, such OVS, refer to Ovs .","title":"Other CNI Configurationsi"},{"location":"usage/spider-multus-config/#chaincni","text":"If you need to add ChainCNI to the CNI configuration, for example, you need to use the tuning plug-in to configure the system kernel parameters of the pod (such as net.core.somaxconn, etc.). This can be achieved with the following configuration (MacVlan CNI as an example): Create a SpiderMultusConfig CR: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens192 chainCNIJsonData: - | { \"type\": \"tuning\", \"sysctl\": { \"net.core.somaxconn\": \"4096\" } } EOF Note that every element of Shanknick Senda must be a legitimate Ethan string, you can refer to as follow manifest. When created, view the corresponding Maltus Netwalk-Atahement-De Finity object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: generation: 1 name: macvlan-conf namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"},{\"type\":\"tuning\", \"sysctl\": {\"net.core.somaxconn\": \"4096\"}}]}'","title":"ChainCNI \u914d\u7f6e"},{"location":"usage/spider-multus-config/#conclusion","text":"SpiderMultusConfig CR automates the management of Multus NetworkAttachmentDefinition CRs, improving the experience of creating configurations and reducing operational costs.","title":"Conclusion"},{"location":"usage/spider-subnet-zh_CN/","text":"SpiderSubnet \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 SpiderIPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002 SpiderSubnet \u529f\u80fd \u542f\u7528 Subnet \u529f\u80fd\u65f6\uff0c\u6bcf\u4e00\u4e2a IPPool \u5b9e\u4f8b\u90fd\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684 Subnet \u5b9e\u4f8b\uff0cIPPool \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u5fc5\u987b\u662f Subnet \u5b9e\u4f8b\u4e2d IP \u5730\u5740\u7684\u5b50\u96c6\uff0cIPPool \u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u51fa\u73b0\u91cd\u53e0 IP \u5730\u5740\uff0c\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\u7684\u5404\u79cd\u8def\u7531\u5c5e\u6027\uff0c\u9ed8\u8ba4\u7ee7\u627f Subnet \u5b9e\u4f8b\u4e2d\u7684\u8bbe\u7f6e\u3002 \u5728\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e26\u6765\u4e86\u5982\u4e0b\u4e24\u79cd\u5b9e\u8df5\u624b\u6bb5\uff0c\u4ece\u800c\u5b8c\u6210\u5e94\u7528\u7ba1\u7406\u5458\u548c\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u804c\u8d23\u89e3\u8026\uff1a \u624b\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\uff0c\u53ef\u57fa\u4e8e\u5bf9\u5e94\u7684 Subnet \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u7ea6\u675f\uff0c\u6765\u83b7\u77e5\u53ef\u4f7f\u7528\u54ea\u4e9b IP \u5730\u5740\u3002 \u81ea\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u53ef\u5728 Pod annotation \u4e2d\u6ce8\u660e\u4f7f\u7528\u7684 Subnet \u5b9e\u4f8b\u540d\uff0c\u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u81ea\u52a8\u6839\u636e Subnet \u5b9e\u4f8b\u4e2d\u7684\u53ef\u7528 IP \u5730\u5740\u6765\u521b\u5efa\u56fa\u5b9a IP \u7684 IPPool \u5b9e\u4f8b\uff0c\u4ece\u4e2d\u5206\u914d IP \u5730\u5740\u7ed9 Pod\u3002\u5e76\u4e14 Spiderpool \u80fd\u591f\u81ea\u52a8\u76d1\u63a7\u5e94\u7528\u7684\u6269\u7f29\u5bb9\u548c\u5220\u9664\u4e8b\u4ef6\uff0c\u81ea\u52a8\u5b8c\u6210 IPPool \u4e2d\u7684 IP \u5730\u5740\u6269\u7f29\u5bb9\u548c\u5220\u9664\u3002 SpiderSubnet \u529f\u80fd\u8fd8\u652f\u6301\u4f17\u591a\u7684\u63a7\u5236\u5668\uff0c\u5982\uff1aReplicaSet\u3001Deployment\u3001Statefulset\u3001Daemonset\u3001Job\u3001Cronjob\uff0c\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\u7b49\u3002\u5bf9\u4e8e\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 \u793a\u4f8b \u3002 \u8be5\u529f\u80fd\u5e76\u4e0d\u652f\u6301\u81ea\u4e3b\u5f0f Pod\u3002 \u6ce8\u610f\uff1a\u5728 v0.7.0 \u7248\u672c\u4e4b\u524d\uff0c\u5728\u542f\u52a8 SpiderSubnet \u529f\u80fd\u4e0b\u4f60\u5fc5\u987b\u5f97\u5148\u521b\u5efa\u4e00\u4e2a SpiderSubnet \u8d44\u6e90\u624d\u53ef\u4ee5\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002\u5728v0.7.0\u7248\u672c\u5f00\u59cb\uff0c\u652f\u6301\u521b\u5efa\u4e00\u4e2a\u72ec\u7acb\u7684 SpiderIPPool \u8d44\u6e90\u800c\u4e0d\u4f9d\u8d56\u4e8e SpiderSubnet \u8d44\u6e90\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5\u6559\u7a0b \u6765\u5b89\u88c5 Spiderpool. \u8bf7\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 --ipam.spiderSubnet.enable=true --ipam.spiderSubnet.autoPool.enable=true . \u5176\u4e2d\uff0c ipam.spiderSubnet.autoPool.enable \u63d0\u4f9b \u81ea\u52a8\u521b\u5efa IPPool \u7684\u80fd\u529b\u3002 \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u548c ens224 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 ens192 \u4e0e ens224 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m \u521b\u5efa Subnet ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF \u4f7f\u7528\u5982\u4e0a\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderSubnet\uff0c\u5e76\u5206\u522b\u4e3a\u5176\u914d\u7f6e\u7f51\u5173\u4e0e\u8def\u7531\u4fe1\u606f\u3002 ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } \u81ea\u52a8\u56fa\u5b9a\u5355\u7f51\u5361 IP \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnet \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 1 \u4e2a\u5bf9\u5e94\u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002(\u6ce8\u610f\uff1a\u4e0d\u652f\u6301\u901a\u914d\u7b26\u7684\u5f62\u5f0f\u3002) ipam.spidernet.io/ippool-ip-number \uff1a\u7528\u4e8e\u6307\u5b9a\u521b\u5efa IP \u6c60 \u4e2d \u7684 IP \u6570\u91cf\u3002\u8be5 annotation \u7684\u5199\u6cd5\u652f\u6301\u4e24\u79cd\u65b9\u5f0f\uff1a\u4e00\u79cd\u662f\u6570\u5b57\u7684\u65b9\u5f0f\u6307\u5b9a IP \u6c60\u7684\u56fa\u5b9a\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a1 \uff1b\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u52a0\u53f7\u548c\u6570\u5b57\u6307\u5b9a IP \u6c60\u7684\u76f8\u5bf9\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a+1 \uff0c\u5373\u8868\u793a IP \u6c60\u4e2d\u7684\u6570\u91cf\u4f1a\u81ea\u52a8\u5b9e\u65f6\u4fdd\u6301\u5728\u5e94\u7528\u7684\u526f\u672c\u6570\u7684\u57fa\u7840\u4e0a\u591a 1 \u4e2a IP\uff0c\u4ee5\u89e3\u51b3\u5e94\u7528\u5728\u5f39\u6027\u6269\u7f29\u5bb9\u7684\u65f6\u6709\u4e34\u65f6\u7684 IP \u53ef\u7528\u3002 ipam.spidernet.io/ippool-reclaim \uff1a \u5176\u8868\u793a\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u662f\u5426\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\u800c\u88ab\u56de\u6536\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e Pod \u7684\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u81ea\u52a8\u6c60\u4f1a\u81ea\u52a8\u7ee7\u627f\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u4e3a\u5b9e\u73b0\u56fa\u5b9a IP \u6c60\u6548\u679c\uff0cSpiderpool \u4f1a\u7ed9\u81ea\u52a8\u6c60\u8865\u5145\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u5185\u5efa Label \u548c PodAffinity\uff0c\u5b83\u4eec\u7528\u4e8e\u6307\u5411\u5e94\u7528\uff0c\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\uff0c\u660e\u786e\u8be5\u6c60\u53ea\u670d\u52a1\u4e8e\u6307\u5b9a\u7684\u5e94\u7528\u3002\u5f53 ipam.spidernet.io/ippool-reclaim: false \u65f6\uff0c\u5220\u9664\u5e94\u7528\u540e\uff0cIP\u4f1a\u88ab\u56de\u6536\uff0c\u4f46\u81ea\u52a8\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002\u5982\u679c\u671f\u671b\u8be5\u6c60\u80fd\u88ab\u5176\u4ed6\u5e94\u7528\u6240\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u6458\u9664\u8fd9\u4e9b\u5185\u5efa Label \u548c PodAffinity\u3002 Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name \u7ecf\u8fc7\u591a\u6b21\u6d4b\u8bd5\uff0c\u4e0d\u65ad\u91cd\u542f Pod\uff0c\u5176 Pod \u7684 IP \u90fd\u88ab\u56fa\u5b9a\u5728 IP \u6c60\u8303\u56f4\u5185: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none> \u56fa\u5b9a IP \u6c60\u7684\u540d\u5b57 \u56fa\u5b9a IP \u6c60\u662f\u6839\u636e\u5e94\u7528\u81ea\u52a8\u521b\u5efa\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u552f\u4e00\u4e14\u53ef\u67e5\u8be2\u7684\u540d\u5b57\u3002\u76ee\u524d\u547d\u540d\u89c4\u5219\u9075\u5faa\u4ee5\u4e0b\u683c\u5f0f\uff1a auto{ipVersion}-{appName}-{NicName}-{Max5RandomCharacter} ipVersion\uff1a\u8868\u793a IPv4 \u6216\u8005 IPv6 \u7684\u6c60\uff0c\u503c\u4e3a 4 \u6216\u8005 6 appName\uff1a\u8868\u793a\u5e94\u7528\u7684\u540d\u5b57 NicName\uff1a\u8868\u793a\u5206\u914d\u7ed9 POD \u7684\u7f51\u5361\u540d\u5b57 Max5RandomCharacter\uff1a\u8868\u793a\u4ece\u5e94\u7528 UUID \u4e2d\u751f\u6210\u7684 5 \u4f4d\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u7528\u4e8e\u533a\u5206\u4e0d\u540c\u5e94\u7528\u7684\u56fa\u5b9a IP \u6c60 \u4f8b\u5982, \u679c\u4f60\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a nginx \u7684 deployment\uff0c\u5176\u7f51\u5361\u540d\u4e3a eth0\uff0c\u90a3\u4e48\u5b83\u7684\u56fa\u5b9a IP \u6c60\u540d\u5b57\u4e3a auto4-nginx-eth0-9a2b3 \u52a8\u6001\u6269\u7f29\u56fa\u5b9a IP \u6c60 \u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-ip-number : '+1'\uff0c\u5176\u8868\u793a\u5e94\u7528\u5206\u914d\u5230\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u7684\u526f\u672c\u6570\u591a 1 \u4e2a\uff0c\u5728\u5e94\u7528\u6eda\u52a8\u66f4\u65b0\u65f6\uff0c\u80fd\u591f\u907f\u514d\u65e7 Pod \u672a\u5220\u9664\uff0c\u65b0 Pod \u6ca1\u6709\u53ef\u7528 IP \u7684\u95ee\u9898\u3002 \u4ee5\u4e0b\u6f14\u793a\u4e86\u6269\u5bb9\u573a\u666f\uff0c\u5c06\u5e94\u7528\u7684\u526f\u672c\u6570\u4ece 2 \u6269\u5bb9\u5230 3\uff0c\u5e94\u7528\u5bf9\u5e94\u7684\u4e24\u4e2a\u56fa\u5b9a IP \u6c60\u4f1a\u81ea\u52a8\u4ece 3 \u4e2a IP \u6269\u5bb9\u5230 4 \u4e2a IP\uff0c\u4e00\u76f4\u4fdd\u6301\u4e00\u4e2a\u5197\u4f59 IP\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false \u901a\u8fc7\u4e0a\u8ff0\uff0cSpiderpool \u5bf9\u4e8e\u5e94\u7528\u6269\u7f29\u5bb9\u7684\u573a\u666f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u5373\u53ef\u3002 \u81ea\u52a8\u56de\u6536 IP \u6c60 \u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-reclaim \uff0c\u8be5\u6ce8\u89e3\u9ed8\u8ba4\u503c\u4e3a true \uff0c\u4e3a true \u65f6\uff0c\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\uff0c\u5c06\u81ea\u52a8\u5220\u9664\u5bf9\u5e94\u7684\u81ea\u52a8\u6c60\u3002\u5728\u672c\u6587\u4e2d\u8bbe\u7f6e\u4e3a false \uff0c\u5176\u8868\u793a\u5220\u9664\u5e94\u7528\u65f6\uff0c\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u4f1a\u56de\u6536\u5176\u4e2d\u88ab\u5206\u914d\u7684 IP \uff0c\u4f46\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002 \u5bf9\u4e8e\u9700\u8981\u4fdd\u7559\u6c60\u7684\u5e94\u7528\u573a\u666f\uff0c\u53ef\u4ee5\u662f\u5f53\u5e94\u7528\u518d\u6b21\u4ee5\u540c\u540d\u7684 deployment \u6216\u8005 statefulset \u6765\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u80fd\u591f\u7ee7\u7eed\u4f7f\u7528\u539f\u6709\u521b\u5efa\u7684 IP \u6c60\uff0c\u4ee5\u4fdd\u6301 IP \u4e0d\u53d8\u3002 ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false \u4f7f\u7528\u4e0a\u8ff0\u6240\u793a\u7684\u5e94\u7528 Yaml\uff0c\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u4e0d\u4f1a\u518d\u6b21\u521b\u5efa\u65b0\u7684 IP \u6c60\uff0c\u5c06\u81ea\u52a8\u590d\u7528\u65e7 IP \u6c60\uff0c\u5e76\u4e14\u5176\u526f\u672c\u6570\u548c IP \u6c60\u7684 IP \u5206\u914d\u60c5\u51b5\u4e0e\u5b9e\u9645\u76f8\u540c\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false \u81ea\u52a8\u56fa\u5b9a\u591a\u7f51\u5361 IP \u5982\u679c\u60a8\u5e0c\u671b\u4e3a Pod \u5b9e\u73b0\u591a\u7f51\u5361 IP \u7684\u56fa\u5b9a\uff0c\u53c2\u8003\u672c\u7ae0\u8282\u3002\u5728\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u6bcf\u4e2a\u526f\u672c\u62e5\u6709\u591a\u5f20\u7f51\u5361\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnets \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 2 \u4e2a\u5c5e\u4e8e\u4e0d\u540c Underlay \u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u53e6\u4e00\u5f20\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a 2 \u4e2a Underlay \u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u5bf9\u5e94\u7684\u56fa\u5b9a IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528 Pod \u7684\u4e24\u5f20\u7f51\u5361\u5206\u522b\u5f62\u6210\u7ed1\u5b9a\u3002\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u7684\u56fa\u5b9a\u6c60\u90fd\u5c06\u4f1a\u81ea\u52a8\u7ee7\u627f\u5176\u6240\u5f52\u5c5e\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet \u4e5f\u652f\u6301\u591a\u7f51\u5361\u7684\u52a8\u6001 IP \u6269\u7f29\u5bb9\u3001\u81ea\u52a8\u56de\u6536 IP \u6c60\u7b49\u529f\u80fd\u3002 \u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u7ee7\u627f\u5b50\u7f51\u5c5e\u6027 \u5982\u4e0b\u662f\u4e00\u4e2a\u5f52\u5c5e\u4e8e\u5b50\u7f51 subnet-6 \uff0c\u5b50\u7f51\u53f7\u4e3a\uff1a 10.6.0.0/16 \u7684 IPPool \u5b9e\u4f8b\u793a\u4f8b\u3002\u8be5 IPPool \u5b9e\u4f8b\u7684\u53ef\u7528 IP \u8303\u56f4\u5fc5\u987b\u662f\u5b50\u7f51 subnet-6.spec.ips \u7684\u5b50\u96c6\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF \u4f7f\u7528\u4e0a\u8ff0 Yaml\uff0c\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u5230\u5b83\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684\u5b50\u7f51\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u5bf9\u5e94\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u603b\u7ed3 SpiderSubnet \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u5c06\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u5458\u7684\u8d23\u4efb\u5206\u5f00\uff0c\u652f\u6301\u81ea\u52a8\u521b\u5efa\u548c\u52a8\u6001\u6269\u5c55\u56fa\u5b9a IPPool \u5230\u6bcf\u4e2a\u9700\u8981\u9759\u6001 IP \u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"SpiderSubnet"},{"location":"usage/spider-subnet-zh_CN/#spidersubnet","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderSubnet"},{"location":"usage/spider-subnet-zh_CN/#_1","text":"SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 SpiderIPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-subnet-zh_CN/#spidersubnet_1","text":"\u542f\u7528 Subnet \u529f\u80fd\u65f6\uff0c\u6bcf\u4e00\u4e2a IPPool \u5b9e\u4f8b\u90fd\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684 Subnet \u5b9e\u4f8b\uff0cIPPool \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u5fc5\u987b\u662f Subnet \u5b9e\u4f8b\u4e2d IP \u5730\u5740\u7684\u5b50\u96c6\uff0cIPPool \u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u51fa\u73b0\u91cd\u53e0 IP \u5730\u5740\uff0c\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\u7684\u5404\u79cd\u8def\u7531\u5c5e\u6027\uff0c\u9ed8\u8ba4\u7ee7\u627f Subnet \u5b9e\u4f8b\u4e2d\u7684\u8bbe\u7f6e\u3002 \u5728\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e26\u6765\u4e86\u5982\u4e0b\u4e24\u79cd\u5b9e\u8df5\u624b\u6bb5\uff0c\u4ece\u800c\u5b8c\u6210\u5e94\u7528\u7ba1\u7406\u5458\u548c\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u804c\u8d23\u89e3\u8026\uff1a \u624b\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\uff0c\u53ef\u57fa\u4e8e\u5bf9\u5e94\u7684 Subnet \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u7ea6\u675f\uff0c\u6765\u83b7\u77e5\u53ef\u4f7f\u7528\u54ea\u4e9b IP \u5730\u5740\u3002 \u81ea\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u53ef\u5728 Pod annotation \u4e2d\u6ce8\u660e\u4f7f\u7528\u7684 Subnet \u5b9e\u4f8b\u540d\uff0c\u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u81ea\u52a8\u6839\u636e Subnet \u5b9e\u4f8b\u4e2d\u7684\u53ef\u7528 IP \u5730\u5740\u6765\u521b\u5efa\u56fa\u5b9a IP \u7684 IPPool \u5b9e\u4f8b\uff0c\u4ece\u4e2d\u5206\u914d IP \u5730\u5740\u7ed9 Pod\u3002\u5e76\u4e14 Spiderpool \u80fd\u591f\u81ea\u52a8\u76d1\u63a7\u5e94\u7528\u7684\u6269\u7f29\u5bb9\u548c\u5220\u9664\u4e8b\u4ef6\uff0c\u81ea\u52a8\u5b8c\u6210 IPPool \u4e2d\u7684 IP \u5730\u5740\u6269\u7f29\u5bb9\u548c\u5220\u9664\u3002 SpiderSubnet \u529f\u80fd\u8fd8\u652f\u6301\u4f17\u591a\u7684\u63a7\u5236\u5668\uff0c\u5982\uff1aReplicaSet\u3001Deployment\u3001Statefulset\u3001Daemonset\u3001Job\u3001Cronjob\uff0c\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\u7b49\u3002\u5bf9\u4e8e\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 \u793a\u4f8b \u3002 \u8be5\u529f\u80fd\u5e76\u4e0d\u652f\u6301\u81ea\u4e3b\u5f0f Pod\u3002 \u6ce8\u610f\uff1a\u5728 v0.7.0 \u7248\u672c\u4e4b\u524d\uff0c\u5728\u542f\u52a8 SpiderSubnet \u529f\u80fd\u4e0b\u4f60\u5fc5\u987b\u5f97\u5148\u521b\u5efa\u4e00\u4e2a SpiderSubnet \u8d44\u6e90\u624d\u53ef\u4ee5\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002\u5728v0.7.0\u7248\u672c\u5f00\u59cb\uff0c\u652f\u6301\u521b\u5efa\u4e00\u4e2a\u72ec\u7acb\u7684 SpiderIPPool \u8d44\u6e90\u800c\u4e0d\u4f9d\u8d56\u4e8e SpiderSubnet \u8d44\u6e90\u3002","title":"SpiderSubnet \u529f\u80fd"},{"location":"usage/spider-subnet-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/spider-subnet-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/spider-subnet-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5\u6559\u7a0b \u6765\u5b89\u88c5 Spiderpool. \u8bf7\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 --ipam.spiderSubnet.enable=true --ipam.spiderSubnet.autoPool.enable=true . \u5176\u4e2d\uff0c ipam.spiderSubnet.autoPool.enable \u63d0\u4f9b \u81ea\u52a8\u521b\u5efa IPPool \u7684\u80fd\u529b\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/spider-subnet-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u548c ens224 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 ens192 \u4e0e ens224 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/spider-subnet-zh_CN/#subnet","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF \u4f7f\u7528\u5982\u4e0a\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderSubnet\uff0c\u5e76\u5206\u522b\u4e3a\u5176\u914d\u7f6e\u7f51\u5173\u4e0e\u8def\u7531\u4fe1\u606f\u3002 ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 }","title":"\u521b\u5efa Subnet"},{"location":"usage/spider-subnet-zh_CN/#ip","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnet \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 1 \u4e2a\u5bf9\u5e94\u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002(\u6ce8\u610f\uff1a\u4e0d\u652f\u6301\u901a\u914d\u7b26\u7684\u5f62\u5f0f\u3002) ipam.spidernet.io/ippool-ip-number \uff1a\u7528\u4e8e\u6307\u5b9a\u521b\u5efa IP \u6c60 \u4e2d \u7684 IP \u6570\u91cf\u3002\u8be5 annotation \u7684\u5199\u6cd5\u652f\u6301\u4e24\u79cd\u65b9\u5f0f\uff1a\u4e00\u79cd\u662f\u6570\u5b57\u7684\u65b9\u5f0f\u6307\u5b9a IP \u6c60\u7684\u56fa\u5b9a\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a1 \uff1b\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u52a0\u53f7\u548c\u6570\u5b57\u6307\u5b9a IP \u6c60\u7684\u76f8\u5bf9\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a+1 \uff0c\u5373\u8868\u793a IP \u6c60\u4e2d\u7684\u6570\u91cf\u4f1a\u81ea\u52a8\u5b9e\u65f6\u4fdd\u6301\u5728\u5e94\u7528\u7684\u526f\u672c\u6570\u7684\u57fa\u7840\u4e0a\u591a 1 \u4e2a IP\uff0c\u4ee5\u89e3\u51b3\u5e94\u7528\u5728\u5f39\u6027\u6269\u7f29\u5bb9\u7684\u65f6\u6709\u4e34\u65f6\u7684 IP \u53ef\u7528\u3002 ipam.spidernet.io/ippool-reclaim \uff1a \u5176\u8868\u793a\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u662f\u5426\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\u800c\u88ab\u56de\u6536\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e Pod \u7684\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u81ea\u52a8\u6c60\u4f1a\u81ea\u52a8\u7ee7\u627f\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u4e3a\u5b9e\u73b0\u56fa\u5b9a IP \u6c60\u6548\u679c\uff0cSpiderpool \u4f1a\u7ed9\u81ea\u52a8\u6c60\u8865\u5145\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u5185\u5efa Label \u548c PodAffinity\uff0c\u5b83\u4eec\u7528\u4e8e\u6307\u5411\u5e94\u7528\uff0c\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\uff0c\u660e\u786e\u8be5\u6c60\u53ea\u670d\u52a1\u4e8e\u6307\u5b9a\u7684\u5e94\u7528\u3002\u5f53 ipam.spidernet.io/ippool-reclaim: false \u65f6\uff0c\u5220\u9664\u5e94\u7528\u540e\uff0cIP\u4f1a\u88ab\u56de\u6536\uff0c\u4f46\u81ea\u52a8\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002\u5982\u679c\u671f\u671b\u8be5\u6c60\u80fd\u88ab\u5176\u4ed6\u5e94\u7528\u6240\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u6458\u9664\u8fd9\u4e9b\u5185\u5efa Label \u548c PodAffinity\u3002 Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name \u7ecf\u8fc7\u591a\u6b21\u6d4b\u8bd5\uff0c\u4e0d\u65ad\u91cd\u542f Pod\uff0c\u5176 Pod \u7684 IP \u90fd\u88ab\u56fa\u5b9a\u5728 IP \u6c60\u8303\u56f4\u5185: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none>","title":"\u81ea\u52a8\u56fa\u5b9a\u5355\u7f51\u5361 IP"},{"location":"usage/spider-subnet-zh_CN/#ip_1","text":"\u56fa\u5b9a IP \u6c60\u662f\u6839\u636e\u5e94\u7528\u81ea\u52a8\u521b\u5efa\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u552f\u4e00\u4e14\u53ef\u67e5\u8be2\u7684\u540d\u5b57\u3002\u76ee\u524d\u547d\u540d\u89c4\u5219\u9075\u5faa\u4ee5\u4e0b\u683c\u5f0f\uff1a auto{ipVersion}-{appName}-{NicName}-{Max5RandomCharacter} ipVersion\uff1a\u8868\u793a IPv4 \u6216\u8005 IPv6 \u7684\u6c60\uff0c\u503c\u4e3a 4 \u6216\u8005 6 appName\uff1a\u8868\u793a\u5e94\u7528\u7684\u540d\u5b57 NicName\uff1a\u8868\u793a\u5206\u914d\u7ed9 POD \u7684\u7f51\u5361\u540d\u5b57 Max5RandomCharacter\uff1a\u8868\u793a\u4ece\u5e94\u7528 UUID \u4e2d\u751f\u6210\u7684 5 \u4f4d\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u7528\u4e8e\u533a\u5206\u4e0d\u540c\u5e94\u7528\u7684\u56fa\u5b9a IP \u6c60 \u4f8b\u5982, \u679c\u4f60\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a nginx \u7684 deployment\uff0c\u5176\u7f51\u5361\u540d\u4e3a eth0\uff0c\u90a3\u4e48\u5b83\u7684\u56fa\u5b9a IP \u6c60\u540d\u5b57\u4e3a auto4-nginx-eth0-9a2b3","title":"\u56fa\u5b9a IP \u6c60\u7684\u540d\u5b57"},{"location":"usage/spider-subnet-zh_CN/#ip_2","text":"\u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-ip-number : '+1'\uff0c\u5176\u8868\u793a\u5e94\u7528\u5206\u914d\u5230\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u7684\u526f\u672c\u6570\u591a 1 \u4e2a\uff0c\u5728\u5e94\u7528\u6eda\u52a8\u66f4\u65b0\u65f6\uff0c\u80fd\u591f\u907f\u514d\u65e7 Pod \u672a\u5220\u9664\uff0c\u65b0 Pod \u6ca1\u6709\u53ef\u7528 IP \u7684\u95ee\u9898\u3002 \u4ee5\u4e0b\u6f14\u793a\u4e86\u6269\u5bb9\u573a\u666f\uff0c\u5c06\u5e94\u7528\u7684\u526f\u672c\u6570\u4ece 2 \u6269\u5bb9\u5230 3\uff0c\u5e94\u7528\u5bf9\u5e94\u7684\u4e24\u4e2a\u56fa\u5b9a IP \u6c60\u4f1a\u81ea\u52a8\u4ece 3 \u4e2a IP \u6269\u5bb9\u5230 4 \u4e2a IP\uff0c\u4e00\u76f4\u4fdd\u6301\u4e00\u4e2a\u5197\u4f59 IP\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false \u901a\u8fc7\u4e0a\u8ff0\uff0cSpiderpool \u5bf9\u4e8e\u5e94\u7528\u6269\u7f29\u5bb9\u7684\u573a\u666f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u5373\u53ef\u3002","title":"\u52a8\u6001\u6269\u7f29\u56fa\u5b9a IP \u6c60"},{"location":"usage/spider-subnet-zh_CN/#ip_3","text":"\u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-reclaim \uff0c\u8be5\u6ce8\u89e3\u9ed8\u8ba4\u503c\u4e3a true \uff0c\u4e3a true \u65f6\uff0c\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\uff0c\u5c06\u81ea\u52a8\u5220\u9664\u5bf9\u5e94\u7684\u81ea\u52a8\u6c60\u3002\u5728\u672c\u6587\u4e2d\u8bbe\u7f6e\u4e3a false \uff0c\u5176\u8868\u793a\u5220\u9664\u5e94\u7528\u65f6\uff0c\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u4f1a\u56de\u6536\u5176\u4e2d\u88ab\u5206\u914d\u7684 IP \uff0c\u4f46\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002 \u5bf9\u4e8e\u9700\u8981\u4fdd\u7559\u6c60\u7684\u5e94\u7528\u573a\u666f\uff0c\u53ef\u4ee5\u662f\u5f53\u5e94\u7528\u518d\u6b21\u4ee5\u540c\u540d\u7684 deployment \u6216\u8005 statefulset \u6765\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u80fd\u591f\u7ee7\u7eed\u4f7f\u7528\u539f\u6709\u521b\u5efa\u7684 IP \u6c60\uff0c\u4ee5\u4fdd\u6301 IP \u4e0d\u53d8\u3002 ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false \u4f7f\u7528\u4e0a\u8ff0\u6240\u793a\u7684\u5e94\u7528 Yaml\uff0c\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u4e0d\u4f1a\u518d\u6b21\u521b\u5efa\u65b0\u7684 IP \u6c60\uff0c\u5c06\u81ea\u52a8\u590d\u7528\u65e7 IP \u6c60\uff0c\u5e76\u4e14\u5176\u526f\u672c\u6570\u548c IP \u6c60\u7684 IP \u5206\u914d\u60c5\u51b5\u4e0e\u5b9e\u9645\u76f8\u540c\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false","title":"\u81ea\u52a8\u56de\u6536 IP \u6c60"},{"location":"usage/spider-subnet-zh_CN/#ip_4","text":"\u5982\u679c\u60a8\u5e0c\u671b\u4e3a Pod \u5b9e\u73b0\u591a\u7f51\u5361 IP \u7684\u56fa\u5b9a\uff0c\u53c2\u8003\u672c\u7ae0\u8282\u3002\u5728\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u6bcf\u4e2a\u526f\u672c\u62e5\u6709\u591a\u5f20\u7f51\u5361\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnets \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 2 \u4e2a\u5c5e\u4e8e\u4e0d\u540c Underlay \u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u53e6\u4e00\u5f20\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a 2 \u4e2a Underlay \u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u5bf9\u5e94\u7684\u56fa\u5b9a IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528 Pod \u7684\u4e24\u5f20\u7f51\u5361\u5206\u522b\u5f62\u6210\u7ed1\u5b9a\u3002\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u7684\u56fa\u5b9a\u6c60\u90fd\u5c06\u4f1a\u81ea\u52a8\u7ee7\u627f\u5176\u6240\u5f52\u5c5e\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet \u4e5f\u652f\u6301\u591a\u7f51\u5361\u7684\u52a8\u6001 IP \u6269\u7f29\u5bb9\u3001\u81ea\u52a8\u56de\u6536 IP \u6c60\u7b49\u529f\u80fd\u3002","title":"\u81ea\u52a8\u56fa\u5b9a\u591a\u7f51\u5361 IP"},{"location":"usage/spider-subnet-zh_CN/#ippool","text":"\u5982\u4e0b\u662f\u4e00\u4e2a\u5f52\u5c5e\u4e8e\u5b50\u7f51 subnet-6 \uff0c\u5b50\u7f51\u53f7\u4e3a\uff1a 10.6.0.0/16 \u7684 IPPool \u5b9e\u4f8b\u793a\u4f8b\u3002\u8be5 IPPool \u5b9e\u4f8b\u7684\u53ef\u7528 IP \u8303\u56f4\u5fc5\u987b\u662f\u5b50\u7f51 subnet-6.spec.ips \u7684\u5b50\u96c6\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF \u4f7f\u7528\u4e0a\u8ff0 Yaml\uff0c\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u5230\u5b83\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684\u5b50\u7f51\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u5bf9\u5e94\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 }","title":"\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u7ee7\u627f\u5b50\u7f51\u5c5e\u6027"},{"location":"usage/spider-subnet-zh_CN/#_4","text":"SpiderSubnet \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u5c06\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u5458\u7684\u8d23\u4efb\u5206\u5f00\uff0c\u652f\u6301\u81ea\u52a8\u521b\u5efa\u548c\u52a8\u6001\u6269\u5c55\u56fa\u5b9a IPPool \u5230\u6bcf\u4e2a\u9700\u8981\u9759\u6001 IP \u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-subnet/","text":"SpiderSubnet English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction The SpiderSubnet resource represents a set of IP addresses. When application administrators need to allocate fixed IP addresses for their applications, they usually have to rely on platform administrators to provide the available IPs and routing information. However, this collaboration between different operational teams can lead to complex workflows for creating each application. With the Spiderpool's SpiderSubnet, this process is greatly simplified. SpiderSubnet can automatically allocate IP addresses from the subnet to SpiderIPPool, while also allowing applications to have fixed IP addresses. This automation significantly reduces operational costs and streamlines the workflow. SpiderSubnet features When the Subnet feature is enabled, each instance of IPPool belongs to the same subnet as the Subnet instance. The IP addresses in the IPPool instance must be a subset of those in the Subnet instance, and there should be no overlapping IP addresses among different IPPool instances. By default, the routing configuration of IPPool instances inherits the settings from the corresponding Subnet instance. To allocate fixed IP addresses for applications and decouple the roles of application administrators and their network counterparts, the following two practices can be adopted: Manually create IPPool: application administrators manually create IPPool instances, ensuring that the range of available IP addresses are defined in the corresponding Subnet instance. This allows them to have control over which specific IP addresses are used. Automatically create IPPool: application administrators can specify the name of the Subnet instance in the Pod annotation. Spiderpool automatically creates an IPPool instance with fixed IP addresses coming from the Subnet instance. The IP addresses in the instance are then allocated to Pods. Spiderpool also monitors application scaling and deletion events, automatically adjusting the IP pool size or removing IPs as needed. SpiderSubnet also supports several controllers, including ReplicaSet, Deployment, StatefulSet, DaemonSet, Job, CronJob, and k8s extended operator. If you need to use a third-party controller, you can refer to the doc Spiderpool supports operator . This feature does not support the bare Pod. Notice: Before v0.7.0 version, you have to create a SpiderSubnet resource before you create a SpiderIPPool resource with SpiderSubnet feature enabled. Since v0.7.0 version, you can create an orphan SpiderIPPool without a SpiderSubnet resource. Prerequisites A ready Kubernetes cluster. Helm has already been installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. And make sure that the helm installs the option --ipam.spiderSubnet.enable=true --ipam.spiderSubnet.autoPool.enable=true . The ipam.spiderSubnet.autoPool.enable provide the Automatically create IPPool ability. Install CNI To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF With the provided configuration, we create the following two Macvlan SpiderMultusConfigs that will automatically generate a Multus NetworkAttachmentDefinition CR corresponding to the host's ens192 and ens224 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m Create Subnets ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF Apply the above YAML configuration to create two SpiderSubnet instances and configure gateway and routing information for each of them. ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } Automatically fix IPs for single NIC The following YAML example creates two replicas of a Deployment application: ipam.spidernet.io/subnet : specifies the Spiderpool subnet. Spiderpool automatically selects IP addresses from this subnet to create a fixed IP pool associated with the application, ensuring fixed IP assignment. (Notice: this feature don't support wildcard.) ipam.spidernet.io/ippool-ip-number : specifies the number of IP addresses in the IP pool. This annotation can be written in two ways: specifying a fixed quantity using a numeric value, such as ipam.spidernet.io/ippool-ip-number\uff1a1 , or specifying a relative quantity using a plus and a number, such as ipam.spidernet.io/ippool-ip-number\uff1a+1 . The latter means that the IP pool will dynamically maintain an additional IP address based on the number of replicas, ensuring temporary IPs are available during elastic scaling. ipam.spidernet.io/ippool-reclaim : indicate whether the automatically created fixed IP pool should be reclaimed upon application deletion. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF When creating the application, Spiderpool selects random IP addresses from the specified subnet to create a fixed IP pool that is bound to the Pod's network interface. The automatic pool automatically inherits the gateway and routing of the subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } To achieve the desired fixed IP pool, Spiderpool adds built-in labels and PodAffinity to bind the pool to the specific application. With the annotation of ipam.spidernet.io/ippool-reclaim: false , IP addresses are reclaimed upon application deletion, but the automatic pool itself remains intact. If you want the pool to be available for other applications, you need to manually remove these built-in labels and PodAffinity. Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name After multiple tests and Pod restarts, the Pod's IP remains fixed within the IP pool range: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none> Fixed IP Pool Name Fixed IP pools are automatically created based on applications, so they need unique and easily queryable names. Currently, the naming convention follows this pattern: auto{ipVersion}-{appName}-{NicName}-{Max5RandomCharacter} ipVersion: Indicates whether it's an IPv4 or IPv6 pool, value is either 4 or 6 appName: Represents the application name NicName: Represents the network interface name assigned to the POD Max5RandomCharacter: Represents a 5-character random string generated from the application's UUID to distinguish fixed IP pools of different applications For example: If you create a deployment named nginx with network interface eth0, its fixed IP pool name would be auto4-nginx-eth0-9a2b3 Dynamically scale fixed IP pools When creating the application, the annotation ipam.spidernet.io/ippool-ip-number : '+1' is specified to allocate one extra fixed IP compared to the number of replicas. This configuration prevents any issues during rolling updates, ensuring that new Pods have available IPs while the old Pods are not deleted yet. Let's consider a scaling scenario where the replica count increases from 2 to 3. In this case, the two fixed IP pools associated with the application will automatically scale from 3 IPs to 4 IPs, maintaining one redundant IP address as expected: ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false With the information mentioned, scaling the application in Spiderpool is as simple as adjusting the replica count for the application. Automatically reclaim IP pools During application creation, the annotation ipam.spidernet.io/ippool-reclaim is specified. Its default value of true indicates that when the application is deleted, the corresponding automatic pool is also removed. However, false in this case means that upon application deletion, the assigned IPs within the automatically created fixed IP pool will be reclaimed, while retaining the pool itself. For scenarios requiring pool retention, when the application is recreated with the same name as a deployment or statefulset, it can continue to use the previously created IP pool, maintaining IP consistency. ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false With the provided application YAML, creating an application with the same name again will automatically reuse the existing IP pools. Instead of creating new IP pools, the previously created ones will be utilized. This ensures consistency in the replica count and the IP allocation within the pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false Automatically fix IPs for multiple NICs To assign fixed IPs to the multiple NICs of Pods, follow the instructions in this section. In the example YAML below, a Deployment with two replicas is created, each having multiple network interfaces. The annotations therein include: ipam.spidernet.io/subnets : specify the subnet for Spiderpool. Spiderpool will randomly select IPs from this subnet to create fixed IP pools associated with the application, ensuring persistent IP assignment. In this example, this annotation creates two fixed IP pools belonging to two different underlay subnets for the Pods. v1.multus-cni.io/default-network : create a default network interface for the application. k8s.v1.cni.cncf.io/networks : create an additional network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF When creating the application, Spiderpool randomly selects IPs from the specified two Underlay subnets to create fixed IP pools. These pools are then associated with the two network interfaces of the application's Pods. Each network interface's fixed pool automatically inherits the gateway, routing, and other properties of its respective subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet also supports dynamic IP scaling for multiple network interfaces and automatic reclamation of IP pools. Manually create IPPool instances inheriting the subnet's properties Below is an example of creating an IPPool instance that inherits the properties of subnet-6 with a subnet ID of 10.6.0.0/16 . The available IP range of this IPPool instance must be a subset of subnet-6.spec.ips . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF Using the provided YAML, you can manually create an IPPool instance that will inherit the attributes of the subnet having the specified subnet ID, such as gateway, routing, and other properties. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } Conclusion SpiderSubnet helps to separate the roles of infrastructure administrators and their application counterparts by enabling automatic creation and dynamic scaling of fixed IP pools for applications that require static IPs.","title":"IPAM of SpiderSubnet"},{"location":"usage/spider-subnet/#spidersubnet","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderSubnet"},{"location":"usage/spider-subnet/#introduction","text":"The SpiderSubnet resource represents a set of IP addresses. When application administrators need to allocate fixed IP addresses for their applications, they usually have to rely on platform administrators to provide the available IPs and routing information. However, this collaboration between different operational teams can lead to complex workflows for creating each application. With the Spiderpool's SpiderSubnet, this process is greatly simplified. SpiderSubnet can automatically allocate IP addresses from the subnet to SpiderIPPool, while also allowing applications to have fixed IP addresses. This automation significantly reduces operational costs and streamlines the workflow.","title":"Introduction"},{"location":"usage/spider-subnet/#spidersubnet-features","text":"When the Subnet feature is enabled, each instance of IPPool belongs to the same subnet as the Subnet instance. The IP addresses in the IPPool instance must be a subset of those in the Subnet instance, and there should be no overlapping IP addresses among different IPPool instances. By default, the routing configuration of IPPool instances inherits the settings from the corresponding Subnet instance. To allocate fixed IP addresses for applications and decouple the roles of application administrators and their network counterparts, the following two practices can be adopted: Manually create IPPool: application administrators manually create IPPool instances, ensuring that the range of available IP addresses are defined in the corresponding Subnet instance. This allows them to have control over which specific IP addresses are used. Automatically create IPPool: application administrators can specify the name of the Subnet instance in the Pod annotation. Spiderpool automatically creates an IPPool instance with fixed IP addresses coming from the Subnet instance. The IP addresses in the instance are then allocated to Pods. Spiderpool also monitors application scaling and deletion events, automatically adjusting the IP pool size or removing IPs as needed. SpiderSubnet also supports several controllers, including ReplicaSet, Deployment, StatefulSet, DaemonSet, Job, CronJob, and k8s extended operator. If you need to use a third-party controller, you can refer to the doc Spiderpool supports operator . This feature does not support the bare Pod. Notice: Before v0.7.0 version, you have to create a SpiderSubnet resource before you create a SpiderIPPool resource with SpiderSubnet feature enabled. Since v0.7.0 version, you can create an orphan SpiderIPPool without a SpiderSubnet resource.","title":"SpiderSubnet features"},{"location":"usage/spider-subnet/#prerequisites","text":"A ready Kubernetes cluster. Helm has already been installed.","title":"Prerequisites"},{"location":"usage/spider-subnet/#steps","text":"","title":"Steps"},{"location":"usage/spider-subnet/#install-spiderpool","text":"Refer to Installation to install Spiderpool. And make sure that the helm installs the option --ipam.spiderSubnet.enable=true --ipam.spiderSubnet.autoPool.enable=true . The ipam.spiderSubnet.autoPool.enable provide the Automatically create IPPool ability.","title":"Install Spiderpool"},{"location":"usage/spider-subnet/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF With the provided configuration, we create the following two Macvlan SpiderMultusConfigs that will automatically generate a Multus NetworkAttachmentDefinition CR corresponding to the host's ens192 and ens224 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m","title":"Install CNI"},{"location":"usage/spider-subnet/#create-subnets","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF Apply the above YAML configuration to create two SpiderSubnet instances and configure gateway and routing information for each of them. ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 }","title":"Create Subnets"},{"location":"usage/spider-subnet/#automatically-fix-ips-for-single-nic","text":"The following YAML example creates two replicas of a Deployment application: ipam.spidernet.io/subnet : specifies the Spiderpool subnet. Spiderpool automatically selects IP addresses from this subnet to create a fixed IP pool associated with the application, ensuring fixed IP assignment. (Notice: this feature don't support wildcard.) ipam.spidernet.io/ippool-ip-number : specifies the number of IP addresses in the IP pool. This annotation can be written in two ways: specifying a fixed quantity using a numeric value, such as ipam.spidernet.io/ippool-ip-number\uff1a1 , or specifying a relative quantity using a plus and a number, such as ipam.spidernet.io/ippool-ip-number\uff1a+1 . The latter means that the IP pool will dynamically maintain an additional IP address based on the number of replicas, ensuring temporary IPs are available during elastic scaling. ipam.spidernet.io/ippool-reclaim : indicate whether the automatically created fixed IP pool should be reclaimed upon application deletion. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF When creating the application, Spiderpool selects random IP addresses from the specified subnet to create a fixed IP pool that is bound to the Pod's network interface. The automatic pool automatically inherits the gateway and routing of the subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } To achieve the desired fixed IP pool, Spiderpool adds built-in labels and PodAffinity to bind the pool to the specific application. With the annotation of ipam.spidernet.io/ippool-reclaim: false , IP addresses are reclaimed upon application deletion, but the automatic pool itself remains intact. If you want the pool to be available for other applications, you need to manually remove these built-in labels and PodAffinity. Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name After multiple tests and Pod restarts, the Pod's IP remains fixed within the IP pool range: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none>","title":"Automatically fix IPs for single NIC"},{"location":"usage/spider-subnet/#fixed-ip-pool-name","text":"Fixed IP pools are automatically created based on applications, so they need unique and easily queryable names. Currently, the naming convention follows this pattern: auto{ipVersion}-{appName}-{NicName}-{Max5RandomCharacter} ipVersion: Indicates whether it's an IPv4 or IPv6 pool, value is either 4 or 6 appName: Represents the application name NicName: Represents the network interface name assigned to the POD Max5RandomCharacter: Represents a 5-character random string generated from the application's UUID to distinguish fixed IP pools of different applications For example: If you create a deployment named nginx with network interface eth0, its fixed IP pool name would be auto4-nginx-eth0-9a2b3","title":"Fixed IP Pool Name"},{"location":"usage/spider-subnet/#dynamically-scale-fixed-ip-pools","text":"When creating the application, the annotation ipam.spidernet.io/ippool-ip-number : '+1' is specified to allocate one extra fixed IP compared to the number of replicas. This configuration prevents any issues during rolling updates, ensuring that new Pods have available IPs while the old Pods are not deleted yet. Let's consider a scaling scenario where the replica count increases from 2 to 3. In this case, the two fixed IP pools associated with the application will automatically scale from 3 IPs to 4 IPs, maintaining one redundant IP address as expected: ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false With the information mentioned, scaling the application in Spiderpool is as simple as adjusting the replica count for the application.","title":"Dynamically scale fixed IP pools"},{"location":"usage/spider-subnet/#automatically-reclaim-ip-pools","text":"During application creation, the annotation ipam.spidernet.io/ippool-reclaim is specified. Its default value of true indicates that when the application is deleted, the corresponding automatic pool is also removed. However, false in this case means that upon application deletion, the assigned IPs within the automatically created fixed IP pool will be reclaimed, while retaining the pool itself. For scenarios requiring pool retention, when the application is recreated with the same name as a deployment or statefulset, it can continue to use the previously created IP pool, maintaining IP consistency. ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false With the provided application YAML, creating an application with the same name again will automatically reuse the existing IP pools. Instead of creating new IP pools, the previously created ones will be utilized. This ensures consistency in the replica count and the IP allocation within the pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false","title":"Automatically reclaim IP pools"},{"location":"usage/spider-subnet/#automatically-fix-ips-for-multiple-nics","text":"To assign fixed IPs to the multiple NICs of Pods, follow the instructions in this section. In the example YAML below, a Deployment with two replicas is created, each having multiple network interfaces. The annotations therein include: ipam.spidernet.io/subnets : specify the subnet for Spiderpool. Spiderpool will randomly select IPs from this subnet to create fixed IP pools associated with the application, ensuring persistent IP assignment. In this example, this annotation creates two fixed IP pools belonging to two different underlay subnets for the Pods. v1.multus-cni.io/default-network : create a default network interface for the application. k8s.v1.cni.cncf.io/networks : create an additional network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF When creating the application, Spiderpool randomly selects IPs from the specified two Underlay subnets to create fixed IP pools. These pools are then associated with the two network interfaces of the application's Pods. Each network interface's fixed pool automatically inherits the gateway, routing, and other properties of its respective subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet also supports dynamic IP scaling for multiple network interfaces and automatic reclamation of IP pools.","title":"Automatically fix IPs for multiple NICs"},{"location":"usage/spider-subnet/#manually-create-ippool-instances-inheriting-the-subnets-properties","text":"Below is an example of creating an IPPool instance that inherits the properties of subnet-6 with a subnet ID of 10.6.0.0/16 . The available IP range of this IPPool instance must be a subset of subnet-6.spec.ips . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF Using the provided YAML, you can manually create an IPPool instance that will inherit the attributes of the subnet having the specified subnet ID, such as gateway, routing, and other properties. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 }","title":"Manually create IPPool instances inheriting the subnet's properties"},{"location":"usage/spider-subnet/#conclusion","text":"SpiderSubnet helps to separate the roles of infrastructure administrators and their application counterparts by enabling automatic creation and dynamic scaling of fixed IP pools for applications that require static IPs.","title":"Conclusion"},{"location":"usage/statefulset-zh_CN/","text":"StatefulSet \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u7531\u4e8e StatefulSet \u591a\u7528\u5728\u6709\u72b6\u6001\u7684\u670d\u52a1\u4e2d\uff0c\u56e0\u6b64\u5bf9\u7f51\u7edc\u7a33\u5b9a\u7684\u6807\u8bc6\u4fe1\u606f\u6709\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002Spiderpool \u80fd\u4fdd\u8bc1 StatefulSet \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 StatefulSet \u56fa\u5b9a\u5730\u5740 StatefulSet \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a StatefulSet \u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u7684\u6545\u969c\u91cd\u5efa\u7684\u60c5\u51b5 StatefulSet \u5728\u526f\u672c\u6570\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5220\u9664 Pod \u4f7f\u5176\u91cd\u542f\u7684\u60c5\u51b5 \u6b64\u5916\uff0cStatefulSet \u548c Deployment \u63a7\u5236\u5668\uff0c\u5bf9\u4e8e IP \u5730\u5740\u56fa\u5b9a\u7684\u9700\u6c42\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u662f Pod UUID \u53d1\u751f\u4e86\u53d8\u5316\uff0c\u5176\u662f\u6709\u72b6\u6001\u7684\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u5e0c\u671b\u8be5 Pod \u91cd\u542f\u524d\u540e\uff0c\u4ecd\u80fd\u5206\u914d\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u5bf9\u4e8e Deployment\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u5b57\u548c Pod UUID \u90fd\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u6240\u4ee5\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5e76\u4e0d\u8981\u65b0\u8001\u4ea4\u66ff\u7684 Pod \u4f7f\u7528\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u6211\u4eec\u53ef\u80fd\u53ea\u5e0c\u671b Deployment \u4e2d\u6240\u6709\u526f\u672c\u6240\u4f7f\u7528\u7684 IP \u662f\u56fa\u5b9a\u5728\u67d0\u4e2a IP \u8303\u56f4\u5185\u5373\u53ef\u3002 \u5f00\u6e90\u793e\u533a\u7684\u4f17\u591a CNI \u65b9\u6848\u5e76\u4e0d\u80fd\u5f88\u597d\u7684\u652f\u6301 StatefulSet \u7684\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002\u800c Spiderpool \u63d0\u4f9b\u7684 StatefulSet \u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u8bc1 StatefulSet Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cStatefulSet \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba StatefulSet \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u914d\u5230\u76f8\u540c IP\u3002\u82e5\u5173\u95ed\uff0cStatefulSet \u5e94\u7528\u5c06\u88ab\u5f53\u505a\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7 --set ipam.enableStatefulSet=false \u5173\u95ed\u3002 \u5728 StatefulSet \u526f\u672c\u7ecf\u7531 \u7f29\u5bb9 \u5230 \u6269\u5bb9 \u7684\u53d8\u5316\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u5e76\u4e0d\u4fdd\u8bc1\u65b0\u6269\u5bb9 Pod \u80fd\u591f\u83b7\u53d6\u5230\u4e4b\u524d\u7f29\u5bb9 Pod \u7684 IP \u5730\u5740\u3002 \u5728 v0.9.4 \u53ca\u4e4b\u524d\u7684\u7684\u7248\u672c\uff0c\u5f53 StatefulSet \u51c6\u5907\u5c31\u7eea\u5e76\u4e14\u5176 Pod \u6b63\u5728\u8fd0\u884c\u65f6\uff0c\u5373\u4f7f\u4fee\u6539 StatefulSet \u6ce8\u89e3\u6307\u5b9a\u4e86\u53e6\u4e00\u4e2a IP \u6c60\uff0c\u5e76\u91cd\u542f Pod\uff0cPod IP \u5730\u5740\u4e5f\u4e0d\u4f1a\u751f\u6548\u5230\u65b0\u7684 IP \u6c60\u8303\u56f4\u5185\uff0c\u800c\u662f\u7ee7\u7eed\u4f7f\u7528\u65e7\u7684\u56fa\u5b9a IP\u3002\u5f53\u5927\u4e8e 0.9.4 \u7248\u672c\u4e4b\u540e\u66f4\u6362 IP \u6c60\u91cd\u542f Pod \u4f1a\u5b8c\u6210 IP \u5730\u5740\u5207\u6362\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableStatefulSet=true \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m \u521b\u5efa IPPool ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF \u521b\u5efa StatefulSet \u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 StatefulSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9\u4e00\u4e9b IP \u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\uff0c\u5b9e\u73b0 StatefulSet \u5e94\u7528\u7684 IP \u56fa\u5b9a\u6548\u679c\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8\uff0c\u5728 StatefulSet \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> \u91cd\u542f StatefulSet Pod\uff0c\u89c2\u5bdf\u5230\u6bcf\u4e2a Pod \u7684 IP \u5747\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> \u6269\u5bb9\u3001\u7f29\u5bb9 StatefulSet Pod \uff0c\u89c2\u5bdf\u6bcf\u4e2a Pod \u7684 IP \u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none> \u603b\u7ed3 Spiderpool \u80fd\u4fdd\u8bc1 Statefulset Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 Statefulset \u7c7b\u578b\u63a7\u5236\u5668\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002","title":"StatefulSet"},{"location":"usage/statefulset-zh_CN/#statefulset","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"StatefulSet"},{"location":"usage/statefulset-zh_CN/#_1","text":"\u7531\u4e8e StatefulSet \u591a\u7528\u5728\u6709\u72b6\u6001\u7684\u670d\u52a1\u4e2d\uff0c\u56e0\u6b64\u5bf9\u7f51\u7edc\u7a33\u5b9a\u7684\u6807\u8bc6\u4fe1\u606f\u6709\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002Spiderpool \u80fd\u4fdd\u8bc1 StatefulSet \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/statefulset-zh_CN/#statefulset_1","text":"StatefulSet \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a StatefulSet \u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u7684\u6545\u969c\u91cd\u5efa\u7684\u60c5\u51b5 StatefulSet \u5728\u526f\u672c\u6570\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5220\u9664 Pod \u4f7f\u5176\u91cd\u542f\u7684\u60c5\u51b5 \u6b64\u5916\uff0cStatefulSet \u548c Deployment \u63a7\u5236\u5668\uff0c\u5bf9\u4e8e IP \u5730\u5740\u56fa\u5b9a\u7684\u9700\u6c42\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u662f Pod UUID \u53d1\u751f\u4e86\u53d8\u5316\uff0c\u5176\u662f\u6709\u72b6\u6001\u7684\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u5e0c\u671b\u8be5 Pod \u91cd\u542f\u524d\u540e\uff0c\u4ecd\u80fd\u5206\u914d\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u5bf9\u4e8e Deployment\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u5b57\u548c Pod UUID \u90fd\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u6240\u4ee5\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5e76\u4e0d\u8981\u65b0\u8001\u4ea4\u66ff\u7684 Pod \u4f7f\u7528\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u6211\u4eec\u53ef\u80fd\u53ea\u5e0c\u671b Deployment \u4e2d\u6240\u6709\u526f\u672c\u6240\u4f7f\u7528\u7684 IP \u662f\u56fa\u5b9a\u5728\u67d0\u4e2a IP \u8303\u56f4\u5185\u5373\u53ef\u3002 \u5f00\u6e90\u793e\u533a\u7684\u4f17\u591a CNI \u65b9\u6848\u5e76\u4e0d\u80fd\u5f88\u597d\u7684\u652f\u6301 StatefulSet \u7684\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002\u800c Spiderpool \u63d0\u4f9b\u7684 StatefulSet \u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u8bc1 StatefulSet Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cStatefulSet \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba StatefulSet \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u914d\u5230\u76f8\u540c IP\u3002\u82e5\u5173\u95ed\uff0cStatefulSet \u5e94\u7528\u5c06\u88ab\u5f53\u505a\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7 --set ipam.enableStatefulSet=false \u5173\u95ed\u3002 \u5728 StatefulSet \u526f\u672c\u7ecf\u7531 \u7f29\u5bb9 \u5230 \u6269\u5bb9 \u7684\u53d8\u5316\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u5e76\u4e0d\u4fdd\u8bc1\u65b0\u6269\u5bb9 Pod \u80fd\u591f\u83b7\u53d6\u5230\u4e4b\u524d\u7f29\u5bb9 Pod \u7684 IP \u5730\u5740\u3002 \u5728 v0.9.4 \u53ca\u4e4b\u524d\u7684\u7684\u7248\u672c\uff0c\u5f53 StatefulSet \u51c6\u5907\u5c31\u7eea\u5e76\u4e14\u5176 Pod \u6b63\u5728\u8fd0\u884c\u65f6\uff0c\u5373\u4f7f\u4fee\u6539 StatefulSet \u6ce8\u89e3\u6307\u5b9a\u4e86\u53e6\u4e00\u4e2a IP \u6c60\uff0c\u5e76\u91cd\u542f Pod\uff0cPod IP \u5730\u5740\u4e5f\u4e0d\u4f1a\u751f\u6548\u5230\u65b0\u7684 IP \u6c60\u8303\u56f4\u5185\uff0c\u800c\u662f\u7ee7\u7eed\u4f7f\u7528\u65e7\u7684\u56fa\u5b9a IP\u3002\u5f53\u5927\u4e8e 0.9.4 \u7248\u672c\u4e4b\u540e\u66f4\u6362 IP \u6c60\u91cd\u542f Pod \u4f1a\u5b8c\u6210 IP \u5730\u5740\u5207\u6362\u3002","title":"StatefulSet \u56fa\u5b9a\u5730\u5740"},{"location":"usage/statefulset-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/statefulset-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/statefulset-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableStatefulSet=true","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/statefulset-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/statefulset-zh_CN/#ippool","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF","title":"\u521b\u5efa IPPool"},{"location":"usage/statefulset-zh_CN/#statefulset_2","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 StatefulSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9\u4e00\u4e9b IP \u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\uff0c\u5b9e\u73b0 StatefulSet \u5e94\u7528\u7684 IP \u56fa\u5b9a\u6548\u679c\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8\uff0c\u5728 StatefulSet \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> \u91cd\u542f StatefulSet Pod\uff0c\u89c2\u5bdf\u5230\u6bcf\u4e2a Pod \u7684 IP \u5747\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> \u6269\u5bb9\u3001\u7f29\u5bb9 StatefulSet Pod \uff0c\u89c2\u5bdf\u6bcf\u4e2a Pod \u7684 IP \u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none>","title":"\u521b\u5efa StatefulSet \u5e94\u7528"},{"location":"usage/statefulset-zh_CN/#_4","text":"Spiderpool \u80fd\u4fdd\u8bc1 Statefulset Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 Statefulset \u7c7b\u578b\u63a7\u5236\u5668\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002","title":"\u603b\u7ed3"},{"location":"usage/statefulset/","text":"StatefulSet English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Due to StatefulSet being commonly used for stateful services, there is a higher demand for stable network identifiers. Spiderpool ensures that StatefulSet Pods consistently retain the same IP address, even in scenarios such as restarts or rebuilds. StatefulSet features StatefulSet utilizes fixed addresses in the following scenarios: When a StatefulSet Pod fails and needs to be reconstructed. Once a Pod is deleted and needs to be restarted and the replicas of the StatefulSet remains unchanged. The requirements for fixed IP address differ between StatefulSet and Deployment: For StatefulSet, the Pod's name remains the same throughout Pod restarts despite its changed UUID. As the Pod is stateful, application administrators hope that each Pod continues to be assigned the same IP address after restarts. For Deployment, both the Pod name and its UUID change after restarts. Deployment Pods are stateless, so there is no need to maintain the same IP address between Pod restarts. Instead, administrators often prefer IP addresses to be allocated within a specified range for all replicas in Deployment. Many open-source CNI solutions provide limited support for fixing IP addresses for StatefulSet. However, Spiderpool's StatefulSet solution guarantees consistent allocation of the same IP address to the Pods during restarts and rebuilds. This feature is enabled by default. When it is enabled, StatefulSet Pods can be assigned fixed IP addresses from a specified IP pool range. Whether or not using a fixed IP pool, StatefulSet Pods will consistently receive the same IP address. StatefulSet applications will be treated as stateless if the feature is disabled. You can disable it during the installation of Spiderpool using Helm via the flag --set ipam.enableStatefulSet=false . During the transition from scaling down to scaling up StatefulSet replicas, Spiderpool does not guarantee that new Pods will inherit the IP addresses previously used by the scaled-down Pods. In version 0.9.4 and prior versions, when a StatefulSet is ready and its Pod is running, even if you modify the StatefulSet annotation to specify a different IP pool and restart the Pod, the Pod's IP address will not switch to the new IP pool range but will continue to use the old fixed IP. Starting from version 0.9.4 and above, changing the IP pool and restarting the Pod will complete the IP address switch. Prerequisites A ready Kubernetes cluster. Helm has already been installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. And make sure that the helm installs the option ipam.enableStatefulSet=true . Install CNI To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create the following Macvlan SpiderMultusConfig that will automatically generate a Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m Create an IP pool ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF Create StatefulSet applications The following YAML example creates a StatefulSet application with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for Spiderpool. Spiderpool automatically selects some IP addresses from this pool and bind them to the application, ensuring that the IP addresses remain fixed for the StatefulSet application. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF When the StatefulSet application is created, Spiderpool will select a random set of IP addresses from the specified IP pool and bind them to the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> Upon restarting StatefulSet Pods, it is observed that each Pod retains its assigned IP address. ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> Upon scaling up or down the StatefulSet Pods, the IP addresses of each Pod change as expected. ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none> Conclusion Spiderpool ensures that StatefulSet Pods maintain a consistent IP address even during scenarios like restarts or rebuilds, satisfying the requirement for fixed IP addresses in StatefulSet.","title":"IPAM for StatefulSet"},{"location":"usage/statefulset/#statefulset","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"StatefulSet"},{"location":"usage/statefulset/#introduction","text":"Due to StatefulSet being commonly used for stateful services, there is a higher demand for stable network identifiers. Spiderpool ensures that StatefulSet Pods consistently retain the same IP address, even in scenarios such as restarts or rebuilds.","title":"Introduction"},{"location":"usage/statefulset/#statefulset-features","text":"StatefulSet utilizes fixed addresses in the following scenarios: When a StatefulSet Pod fails and needs to be reconstructed. Once a Pod is deleted and needs to be restarted and the replicas of the StatefulSet remains unchanged. The requirements for fixed IP address differ between StatefulSet and Deployment: For StatefulSet, the Pod's name remains the same throughout Pod restarts despite its changed UUID. As the Pod is stateful, application administrators hope that each Pod continues to be assigned the same IP address after restarts. For Deployment, both the Pod name and its UUID change after restarts. Deployment Pods are stateless, so there is no need to maintain the same IP address between Pod restarts. Instead, administrators often prefer IP addresses to be allocated within a specified range for all replicas in Deployment. Many open-source CNI solutions provide limited support for fixing IP addresses for StatefulSet. However, Spiderpool's StatefulSet solution guarantees consistent allocation of the same IP address to the Pods during restarts and rebuilds. This feature is enabled by default. When it is enabled, StatefulSet Pods can be assigned fixed IP addresses from a specified IP pool range. Whether or not using a fixed IP pool, StatefulSet Pods will consistently receive the same IP address. StatefulSet applications will be treated as stateless if the feature is disabled. You can disable it during the installation of Spiderpool using Helm via the flag --set ipam.enableStatefulSet=false . During the transition from scaling down to scaling up StatefulSet replicas, Spiderpool does not guarantee that new Pods will inherit the IP addresses previously used by the scaled-down Pods. In version 0.9.4 and prior versions, when a StatefulSet is ready and its Pod is running, even if you modify the StatefulSet annotation to specify a different IP pool and restart the Pod, the Pod's IP address will not switch to the new IP pool range but will continue to use the old fixed IP. Starting from version 0.9.4 and above, changing the IP pool and restarting the Pod will complete the IP address switch.","title":"StatefulSet features"},{"location":"usage/statefulset/#prerequisites","text":"A ready Kubernetes cluster. Helm has already been installed.","title":"Prerequisites"},{"location":"usage/statefulset/#steps","text":"","title":"Steps"},{"location":"usage/statefulset/#install-spiderpool","text":"Refer to Installation to install Spiderpool. And make sure that the helm installs the option ipam.enableStatefulSet=true .","title":"Install Spiderpool"},{"location":"usage/statefulset/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create the following Macvlan SpiderMultusConfig that will automatically generate a Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"Install CNI"},{"location":"usage/statefulset/#create-an-ip-pool","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF","title":"Create an IP pool"},{"location":"usage/statefulset/#create-statefulset-applications","text":"The following YAML example creates a StatefulSet application with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for Spiderpool. Spiderpool automatically selects some IP addresses from this pool and bind them to the application, ensuring that the IP addresses remain fixed for the StatefulSet application. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF When the StatefulSet application is created, Spiderpool will select a random set of IP addresses from the specified IP pool and bind them to the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> Upon restarting StatefulSet Pods, it is observed that each Pod retains its assigned IP address. ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> Upon scaling up or down the StatefulSet Pods, the IP addresses of each Pod change as expected. ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none>","title":"Create StatefulSet applications"},{"location":"usage/statefulset/#conclusion","text":"Spiderpool ensures that StatefulSet Pods maintain a consistent IP address even during scenarios like restarts or rebuilds, satisfying the requirement for fixed IP addresses in StatefulSet.","title":"Conclusion"},{"location":"usage/submariner-zh_CN/","text":"\u8054\u901a\u591a\u96c6\u7fa4\u7f51\u7edc English | \u7b80\u4f53\u4e2d\u6587 \u80cc\u666f Spiderpool \u4e3a\u4ec0\u4e48\u9700\u8981\u591a\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u65b9\u6848\uff1f\u5982\u679c\u6211\u4eec\u7684\u4e0d\u540c\u96c6\u7fa4\u5206\u5e03\u5728\u540c\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc3\uff0c\u90a3\u5b83\u4eec\u4e4b\u95f4\u7684\u7f51\u7edc\u662f\u5929\u7136\u8054\u901a\u7684\u3002\u4f46\u5982\u679c\u5b83\u4eec\u5206\u5e03\u5728\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u96c6\u7fa4\u5b50\u7f51\u662f\u76f8\u4e92\u9694\u79bb\uff0c\u4e0d\u80fd\u76f4\u63a5\u8de8\u6570\u636e\u4e2d\u5fc3\u8054\u901a\u3002\u6240\u4ee5 Spiderpool \u9700\u8981\u4e00\u4e2a\u591a\u96c6\u7fa4\u7684\u7f51\u7edc\u8054\u901a\u65b9\u6848\uff0c\u6765\u89e3\u51b3\u8de8\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8bbf\u95ee\u95ee\u9898\u3002 Submariner \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u65b9\u6848\uff0c\u5b83\u501f\u52a9\u96a7\u9053\u6280\u672f\u6253\u901a\u4e0d\u540c Kubernetes \u96c6\u7fa4(\u8fd0\u884c\u5728\u672c\u5730\u6216\u8005\u516c\u6709\u4e91)\u4e4b\u95f4 Pod \u4e0e Service \u7684\u76f4\u63a5\u901a\u4fe1\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u8003 Submariner Document . \u6211\u4eec\u53ef\u4ee5\u501f\u52a9 Submariner \u6765\u5e2e\u52a9 Spiderpool \u89e3\u51b3\u8de8\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8bbf\u95ee\u95ee\u9898\u3002 \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e2a\u529f\u80fd\u3002 \u524d\u7f6e\u6761\u4ef6 \u81f3\u5c11\u4e24\u5957\u672a\u5b89\u88c5 CNI \u7684 Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Helm \u3001 Subctl \u5de5\u5177 \u573a\u666f\u4ecb\u7ecd \u4e0b\u9762\u662f\u672c\u6b21\u5b9e\u9a8c\u7684\u7f51\u7edc\u62d3\u6251\u56fe: \u8fd9\u4e2a\u7f51\u7edc\u62d3\u6251\u56fe\u4ecb\u7ecd\u4e86\u4ee5\u4e0b\u4fe1\u606f: \u96c6\u7fa4 ClusterA \u548c ClusterB \u5206\u5e03\u5728\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u5b83\u4eec\u7684\u96c6\u7fa4 Underlay \u5b50\u7f51(172.110.0.0/16 \u548c 172.111.0.0/16) \u56e0\u4e3a\u5728\u4e0d\u540c\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u9694\u79bb\uff0c\u4e0d\u80fd\u76f4\u63a5\u901a\u4fe1\u3002\u7f51\u5173\u8282\u70b9\u53ef\u4ee5\u901a\u8fc7 ens192 \u7f51\u5361(10.6.0.0/16) \u4e92\u76f8\u8054\u901a\u3002 \u4e24\u5957\u96c6\u7fa4\u901a\u8fc7 Submariner \u5efa\u7acb\u7684 IPSec \u96a7\u9053\u8fde\u63a5\u8d77\u6765\uff0c\u96a7\u9053\u57fa\u4e8e \u7f51\u5173\u8282\u70b9\u7684 ens192 \u7f51\u5361\u5efa\u7acb\uff0c\u5e76\u4e14\u4e5f\u901a\u8fc7 ens192 \u7f51\u5361\u8bbf\u95ee Submariner \u7684 Broker \u7ec4\u4ef6\u3002 \u5feb\u901f\u5f00\u59cb \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u914d\u7f6e IP \u6c60 \u56e0\u4e3a Submariner \u6682\u4e0d\u652f\u6301\u591a\u4e2a\u5b50\u7f51\uff0c\u53ef\u4ee5\u5c06\u96c6\u7fa4\u7684 PodCIDR \u62c6\u5206\u4e3a\u591a\u4e2a\u5c0f\u7684\u5b50\u7f51\uff0c\u6307\u5b9a MacVlan Pod \u5206\u522b\u4ece\u5404\u81ea\u5c0f\u5b50\u7f51\u4e2d\u83b7\u53d6 IP \u8fdb\u884c Underrlay \u901a\u8baf\u3002\u6ce8\u610f: \u9700\u8981\u4fdd\u8bc1\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u96c6\u7fa4 cluster-a \u7684 PodCIDR \u4e3a: 172.110.0.0/16, \u4ece\u8fd9\u4e2a\u5927\u5b50\u7f51\u4e2d\u521b\u5efa\u591a\u4e2a\u5c0f\u5b50\u7f51(172.110.1.0/24)\u4f9b Pod \u4f7f\u7528: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-a spec: default: true ips: - \"172.110.1.1-172.110.1.200\" subnet: 172.110.1.0/24 gateway: 172.110.0.1 EOF \u96c6\u7fa4 cluster-b \u7684 PodCIDR \u4e3a: 172.111.0.0/16, \u4ece\u8fd9\u4e2a\u5927\u5b50\u7f51\u4e2d\u521b\u5efa\u5c0f\u5b50\u7f51(172.111.1.0/24)\u4f9b Pod \u4f7f\u7528: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-b spec: default: true ips: - \"172.111.1.1-172.111.1.200\" subnet: 172.111.0.0/24 gateway: 172.111.0.1 EOF \u914d\u7f6e SpiderMultusConfig \u5b9e\u4f8b\u3002 \u5728\u96c6\u7fa4 cluster-a \u4e0a\u914d\u7f6e: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-a coordinator: hijackCIDR: - 10.243.0.0/18 - 172.111.0.0/16 EOF \u5728\u96c6\u7fa4 cluster-b \u4e0a\u914d\u7f6e: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-b coordinator: hijackCIDR: - 10.233.0.0/18 - 172.110.0.0/16 EOF \u9700\u8981\u914d\u7f6e\u4e3b\u673a\u63a5\u53e3 ens224 \u4f5c\u4e3a Macvlan \u7236\u63a5\u53e3\uff0cMacvlan \u5c06\u4ee5\u8be5\u7f51\u5361\u521b\u5efa\u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528\u3002 \u9700\u8981\u914d\u7f6e coordinator.hijackCIDR , \u914d\u7f6e\u5bf9\u7aef\u96c6\u7fa4\u7684 Service \u548c Pod \u7684\u5b50\u7f51\u4fe1\u606f\u3002\u542f\u52a8 Pod \u65f6\uff0ccoordinator \u5c06\u4f1a\u5728 Pod \u4e2d\u63d2\u5165\u8fd9\u4e9b\u5b50\u7f51\u7684\u8def\u7531\uff0c\u4f7f\u8bbf\u95ee\u8fd9\u4e9b\u76ee\u6807\u65f6\u4ece\u8282\u70b9\u8f6c\u53d1\u3002\u4ece\u800c\u66f4\u597d\u7684\u4e0e Subamriner \u534f\u540c\u5de5\u4f5c\u3002 \u5b89\u88c5 Submariner \u901a\u8fc7 Subctl \u5de5\u5177\u5b89\u88c5 Submariner, \u53ef\u53c2\u8003 Submariner\u5b98\u65b9\u6587\u6863 \u3002 \u4f46\u6ce8\u610f\u6267\u884c subctl join \u7684\u65f6\u5019\uff0c\u9700\u8981\u624b\u52a8\u6307\u5b9a\u4e0a\u4e2a\u6b65\u9aa4\u4e2d\u63d0\u5230\u7684 MacVlan Underlay Pod \u7684\u5b50\u7f51\u3002 # clusterA subctl join --kubeconfig cluster-a.config broker-info.subm --clusterid = cluster-a --clustercidr = 172 .110.0.0/16 # clusterB subctl join --kubeconfig cluster-b.config broker-info.subm --clusterid = cluster-b --clustercidr = 172 .111.0.0/16 \u76ee\u524d Submariner \u53ea\u652f\u6301\u6307\u5b9a\u5355\u4e2a Pod \u5b50\u7f51\uff0c\u4e0d\u652f\u6301\u591a\u4e2a\u5b50\u7f51 \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u68c0\u67e5 submariner \u7ec4\u4ef6\u8fd0\u884c\u72b6\u6001: [ root@controller-node-1 ~ ] # subctl show all Cluster \"cluster.local\" \u2713 Detecting broker ( s ) NAMESPACE NAME COMPONENTS GLOBALNET GLOBALNET CIDR DEFAULT GLOBALNET SIZE DEFAULT DOMAINS submariner-k8s-broker submariner-broker service-discovery, connectivity no 242 .0.0.0/8 65536 \u2713 Showing Connections GATEWAY CLUSTER REMOTE IP NAT CABLE DRIVER SUBNETS STATUS RTT avg. controller-node-1 cluster-b 10 .6.168.74 no libreswan 10 .243.0.0/18, 172 .111.0.0/16 connected 661 .938\u00b5s \u2713 Showing Endpoints CLUSTER ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster01 10 .6.168.73 140 .207.201.152 libreswan local cluster02 10 .6.168.74 140 .207.201.152 libreswan remote \u2713 Showing Gateways NODE HA STATUS SUMMARY controller-node-1 active All connections ( 1 ) are established \u2713 Showing Network details Discovered network details via Submariner: Network plugin: \"\" Service CIDRs: [ 10 .233.0.0/18 ] Cluster CIDRs: [ 172 .110.0.0/16 ] \u2713 Showing versions COMPONENT REPOSITORY CONFIGURED RUNNING submariner-gateway quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-routeagent quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-metrics-proxy quay.io/submariner 0 .16.0 release-0.16-d48224e08e06 submariner-operator quay.io/submariner 0 .16.0 release-0.16-0807883713b0 submariner-lighthouse-agent quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 submariner-lighthouse-coredns quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 \u5982\u4e0a\u5c55\u793a: submariner \u7ec4\u4ef6\u6b63\u5e38\uff0c\u5e76\u4e14\u96a7\u9053\u6210\u529f\u5efa\u7acb\u3002 \u5982\u679c\u9047\u5230\u96a7\u9053\u4e0d\u80fd\u6210\u529f\u5efa\u7acb\uff0c\u5e76\u4e14 submariner-gateway pod \u4e00\u76f4\u5904\u4e8e CrashLoopBackOff \u72b6\u6001\u3002\u53ef\u80fd\u7684\u539f\u56e0\u6709\u4ee5\u4e0b: \u8bf7\u9009\u62e9\u5408\u9002\u7684\u8282\u70b9\u4f5c\u4e3a\u7f51\u5173\u8282\u70b9\uff0c\u786e\u4fdd\u5b83\u4eec\u80fd\u591f\u4e92\u76f8\u8054\u901a\uff0c\u5426\u5219\u96a7\u9053\u5c06\u65e0\u6cd5\u5efa\u7acb \u5982\u679c Pod \u65e5\u5fd7\u8f93\u51fa: \"Error creating local endpoint object error=\"error getting CNI Interface IP address: unable to find CNI Interface on the host which has IP from [\\\"172.100.0.0/16\\\"].Please disable the health check if your CNI does not expose a pod IP on the nodes\". \u8bf7\u68c0\u67e5\u7f51\u5173\u8282\u70b9\u662f\u5426\u914d\u7f6e \"172.100.0.0/16\" \u7f51\u6bb5\u7684\u5730\u5740\uff0c\u5982\u679c\u6ca1\u6709\u8bf7\u914d\u7f6e\u3002\u6216\u8005\u6267\u884c subctl join \u7684\u65f6\u5019\u5173\u95ed\u7f51\u5173\u7684\u5065\u5eb7\u68c0\u6d4b\u529f\u80fd: subctl join --health-check=false ... \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4ee5\u4e0b\u7684\u547d\u4ee4\u5206\u522b\u5728\u96c6\u7fa4 cluster-a \u548c cluster-b \u521b\u5efa\u6d4b\u8bd5\u7684 Pod \u548c Service: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-confg labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u7684\u8fd0\u884c\u72b6\u6001: \u5728\u96c6\u7fa4 Cluster-a \u4e0a\u67e5\u770b: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-696bf7cf7d-bkstk 1 /1 Running 0 20m 172 .110.1.131 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .233.62.51 <none> 80 /TCP 20m \u5728\u96c6\u7fa4 Cluster-b \u4e0a\u67e5\u770b: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-8f5cdd468-5zr8n 1 /1 Running 0 21m 172 .111.1.136 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .243.2.135 <none> 80 /TCP 21m \u6d4b\u8bd5 Pod \u8de8\u96c6\u7fa4\u901a\u4fe1\u60c5\u51b5: \u5148\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u67e5\u770b\u8def\u7531\u4fe1\u606f\uff0c\u786e\u4fdd\u8bbf\u95ee\u5bf9\u7aef Pod \u548c Service \u65f6\uff0c\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u534f\u8bae\u6808\u8f6c\u53d1: # \u5728\u96c6\u7fa4 Cluster-a \u6267\u884c: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ip route 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.64.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.74.89 dev veth0 src 172 .110.168.131 10 .243.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 172 .110.1.0/24 dev eth0 src 172 .110.168.131 172 .110.168.73 dev veth0 src 172 .110.168.131 172 .111.0.0/16 via 10 .7.168.73 dev veth0 src 172 .110.168.131 \u4ece\u8def\u7531\u4fe1\u606f\u786e\u8ba4 10.243.0.0/18 \u548c 172.111.0.0/16 \u4ece veth0 \u8f6c\u53d1\u3002 \u6d4b\u8bd5\u96c6\u7fa4 Cluster-a \u7684 Pod \u8bbf\u95ee\u5bf9\u7aef\u96c6\u7fa4 Cluster-b \u7684 Pod : [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ping -c2 172.111.168.136 PING 172 .111.168.136 ( 172 .111.168.136 ) : 56 data bytes 64 bytes from 172 .111.168.136: seq = 0 ttl = 62 time = 0 .900 ms 64 bytes from 172 .111.168.136: seq = 1 ttl = 62 time = 0 .796 ms --- 172 .111.168.136 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .796/0.848/0.900 ms \u6d4b\u8bd5\u96c6\u7fa4 Cluster-a \u7684 Pod \u8bbf\u95ee\u5bf9\u7aef\u96c6\u7fa4 Cluster-b \u7684 Service: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- curl -I 10.243.2.135 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Fri, 08 Dec 2023 03 :32:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 08 Dec 2023 03 :32:04 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes \u603b\u7ed3 Spiderpool \u53ef\u5728 Submariner \u7684\u5e2e\u52a9\u4e0b\uff0c\u89e3\u51b3\u4e0d\u540c\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u95ee\u9898\u3002","title":"\u8054\u901a\u591a\u96c6\u7fa4\u7f51\u7edc"},{"location":"usage/submariner-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"\u8054\u901a\u591a\u96c6\u7fa4\u7f51\u7edc"},{"location":"usage/submariner-zh_CN/#_2","text":"Spiderpool \u4e3a\u4ec0\u4e48\u9700\u8981\u591a\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u65b9\u6848\uff1f\u5982\u679c\u6211\u4eec\u7684\u4e0d\u540c\u96c6\u7fa4\u5206\u5e03\u5728\u540c\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc3\uff0c\u90a3\u5b83\u4eec\u4e4b\u95f4\u7684\u7f51\u7edc\u662f\u5929\u7136\u8054\u901a\u7684\u3002\u4f46\u5982\u679c\u5b83\u4eec\u5206\u5e03\u5728\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u96c6\u7fa4\u5b50\u7f51\u662f\u76f8\u4e92\u9694\u79bb\uff0c\u4e0d\u80fd\u76f4\u63a5\u8de8\u6570\u636e\u4e2d\u5fc3\u8054\u901a\u3002\u6240\u4ee5 Spiderpool \u9700\u8981\u4e00\u4e2a\u591a\u96c6\u7fa4\u7684\u7f51\u7edc\u8054\u901a\u65b9\u6848\uff0c\u6765\u89e3\u51b3\u8de8\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8bbf\u95ee\u95ee\u9898\u3002 Submariner \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u65b9\u6848\uff0c\u5b83\u501f\u52a9\u96a7\u9053\u6280\u672f\u6253\u901a\u4e0d\u540c Kubernetes \u96c6\u7fa4(\u8fd0\u884c\u5728\u672c\u5730\u6216\u8005\u516c\u6709\u4e91)\u4e4b\u95f4 Pod \u4e0e Service \u7684\u76f4\u63a5\u901a\u4fe1\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u8003 Submariner Document . \u6211\u4eec\u53ef\u4ee5\u501f\u52a9 Submariner \u6765\u5e2e\u52a9 Spiderpool \u89e3\u51b3\u8de8\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8bbf\u95ee\u95ee\u9898\u3002 \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e2a\u529f\u80fd\u3002","title":"\u80cc\u666f"},{"location":"usage/submariner-zh_CN/#_3","text":"\u81f3\u5c11\u4e24\u5957\u672a\u5b89\u88c5 CNI \u7684 Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Helm \u3001 Subctl \u5de5\u5177","title":"\u524d\u7f6e\u6761\u4ef6"},{"location":"usage/submariner-zh_CN/#_4","text":"\u4e0b\u9762\u662f\u672c\u6b21\u5b9e\u9a8c\u7684\u7f51\u7edc\u62d3\u6251\u56fe: \u8fd9\u4e2a\u7f51\u7edc\u62d3\u6251\u56fe\u4ecb\u7ecd\u4e86\u4ee5\u4e0b\u4fe1\u606f: \u96c6\u7fa4 ClusterA \u548c ClusterB \u5206\u5e03\u5728\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u5b83\u4eec\u7684\u96c6\u7fa4 Underlay \u5b50\u7f51(172.110.0.0/16 \u548c 172.111.0.0/16) \u56e0\u4e3a\u5728\u4e0d\u540c\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u9694\u79bb\uff0c\u4e0d\u80fd\u76f4\u63a5\u901a\u4fe1\u3002\u7f51\u5173\u8282\u70b9\u53ef\u4ee5\u901a\u8fc7 ens192 \u7f51\u5361(10.6.0.0/16) \u4e92\u76f8\u8054\u901a\u3002 \u4e24\u5957\u96c6\u7fa4\u901a\u8fc7 Submariner \u5efa\u7acb\u7684 IPSec \u96a7\u9053\u8fde\u63a5\u8d77\u6765\uff0c\u96a7\u9053\u57fa\u4e8e \u7f51\u5173\u8282\u70b9\u7684 ens192 \u7f51\u5361\u5efa\u7acb\uff0c\u5e76\u4e14\u4e5f\u901a\u8fc7 ens192 \u7f51\u5361\u8bbf\u95ee Submariner \u7684 Broker \u7ec4\u4ef6\u3002","title":"\u573a\u666f\u4ecb\u7ecd"},{"location":"usage/submariner-zh_CN/#_5","text":"","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"usage/submariner-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u914d\u7f6e IP \u6c60 \u56e0\u4e3a Submariner \u6682\u4e0d\u652f\u6301\u591a\u4e2a\u5b50\u7f51\uff0c\u53ef\u4ee5\u5c06\u96c6\u7fa4\u7684 PodCIDR \u62c6\u5206\u4e3a\u591a\u4e2a\u5c0f\u7684\u5b50\u7f51\uff0c\u6307\u5b9a MacVlan Pod \u5206\u522b\u4ece\u5404\u81ea\u5c0f\u5b50\u7f51\u4e2d\u83b7\u53d6 IP \u8fdb\u884c Underrlay \u901a\u8baf\u3002\u6ce8\u610f: \u9700\u8981\u4fdd\u8bc1\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u96c6\u7fa4 cluster-a \u7684 PodCIDR \u4e3a: 172.110.0.0/16, \u4ece\u8fd9\u4e2a\u5927\u5b50\u7f51\u4e2d\u521b\u5efa\u591a\u4e2a\u5c0f\u5b50\u7f51(172.110.1.0/24)\u4f9b Pod \u4f7f\u7528: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-a spec: default: true ips: - \"172.110.1.1-172.110.1.200\" subnet: 172.110.1.0/24 gateway: 172.110.0.1 EOF \u96c6\u7fa4 cluster-b \u7684 PodCIDR \u4e3a: 172.111.0.0/16, \u4ece\u8fd9\u4e2a\u5927\u5b50\u7f51\u4e2d\u521b\u5efa\u5c0f\u5b50\u7f51(172.111.1.0/24)\u4f9b Pod \u4f7f\u7528: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-b spec: default: true ips: - \"172.111.1.1-172.111.1.200\" subnet: 172.111.0.0/24 gateway: 172.111.0.1 EOF \u914d\u7f6e SpiderMultusConfig \u5b9e\u4f8b\u3002 \u5728\u96c6\u7fa4 cluster-a \u4e0a\u914d\u7f6e: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-a coordinator: hijackCIDR: - 10.243.0.0/18 - 172.111.0.0/16 EOF \u5728\u96c6\u7fa4 cluster-b \u4e0a\u914d\u7f6e: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-b coordinator: hijackCIDR: - 10.233.0.0/18 - 172.110.0.0/16 EOF \u9700\u8981\u914d\u7f6e\u4e3b\u673a\u63a5\u53e3 ens224 \u4f5c\u4e3a Macvlan \u7236\u63a5\u53e3\uff0cMacvlan \u5c06\u4ee5\u8be5\u7f51\u5361\u521b\u5efa\u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528\u3002 \u9700\u8981\u914d\u7f6e coordinator.hijackCIDR , \u914d\u7f6e\u5bf9\u7aef\u96c6\u7fa4\u7684 Service \u548c Pod \u7684\u5b50\u7f51\u4fe1\u606f\u3002\u542f\u52a8 Pod \u65f6\uff0ccoordinator \u5c06\u4f1a\u5728 Pod \u4e2d\u63d2\u5165\u8fd9\u4e9b\u5b50\u7f51\u7684\u8def\u7531\uff0c\u4f7f\u8bbf\u95ee\u8fd9\u4e9b\u76ee\u6807\u65f6\u4ece\u8282\u70b9\u8f6c\u53d1\u3002\u4ece\u800c\u66f4\u597d\u7684\u4e0e Subamriner \u534f\u540c\u5de5\u4f5c\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/submariner-zh_CN/#submariner","text":"\u901a\u8fc7 Subctl \u5de5\u5177\u5b89\u88c5 Submariner, \u53ef\u53c2\u8003 Submariner\u5b98\u65b9\u6587\u6863 \u3002 \u4f46\u6ce8\u610f\u6267\u884c subctl join \u7684\u65f6\u5019\uff0c\u9700\u8981\u624b\u52a8\u6307\u5b9a\u4e0a\u4e2a\u6b65\u9aa4\u4e2d\u63d0\u5230\u7684 MacVlan Underlay Pod \u7684\u5b50\u7f51\u3002 # clusterA subctl join --kubeconfig cluster-a.config broker-info.subm --clusterid = cluster-a --clustercidr = 172 .110.0.0/16 # clusterB subctl join --kubeconfig cluster-b.config broker-info.subm --clusterid = cluster-b --clustercidr = 172 .111.0.0/16 \u76ee\u524d Submariner \u53ea\u652f\u6301\u6307\u5b9a\u5355\u4e2a Pod \u5b50\u7f51\uff0c\u4e0d\u652f\u6301\u591a\u4e2a\u5b50\u7f51 \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u68c0\u67e5 submariner \u7ec4\u4ef6\u8fd0\u884c\u72b6\u6001: [ root@controller-node-1 ~ ] # subctl show all Cluster \"cluster.local\" \u2713 Detecting broker ( s ) NAMESPACE NAME COMPONENTS GLOBALNET GLOBALNET CIDR DEFAULT GLOBALNET SIZE DEFAULT DOMAINS submariner-k8s-broker submariner-broker service-discovery, connectivity no 242 .0.0.0/8 65536 \u2713 Showing Connections GATEWAY CLUSTER REMOTE IP NAT CABLE DRIVER SUBNETS STATUS RTT avg. controller-node-1 cluster-b 10 .6.168.74 no libreswan 10 .243.0.0/18, 172 .111.0.0/16 connected 661 .938\u00b5s \u2713 Showing Endpoints CLUSTER ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster01 10 .6.168.73 140 .207.201.152 libreswan local cluster02 10 .6.168.74 140 .207.201.152 libreswan remote \u2713 Showing Gateways NODE HA STATUS SUMMARY controller-node-1 active All connections ( 1 ) are established \u2713 Showing Network details Discovered network details via Submariner: Network plugin: \"\" Service CIDRs: [ 10 .233.0.0/18 ] Cluster CIDRs: [ 172 .110.0.0/16 ] \u2713 Showing versions COMPONENT REPOSITORY CONFIGURED RUNNING submariner-gateway quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-routeagent quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-metrics-proxy quay.io/submariner 0 .16.0 release-0.16-d48224e08e06 submariner-operator quay.io/submariner 0 .16.0 release-0.16-0807883713b0 submariner-lighthouse-agent quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 submariner-lighthouse-coredns quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 \u5982\u4e0a\u5c55\u793a: submariner \u7ec4\u4ef6\u6b63\u5e38\uff0c\u5e76\u4e14\u96a7\u9053\u6210\u529f\u5efa\u7acb\u3002 \u5982\u679c\u9047\u5230\u96a7\u9053\u4e0d\u80fd\u6210\u529f\u5efa\u7acb\uff0c\u5e76\u4e14 submariner-gateway pod \u4e00\u76f4\u5904\u4e8e CrashLoopBackOff \u72b6\u6001\u3002\u53ef\u80fd\u7684\u539f\u56e0\u6709\u4ee5\u4e0b: \u8bf7\u9009\u62e9\u5408\u9002\u7684\u8282\u70b9\u4f5c\u4e3a\u7f51\u5173\u8282\u70b9\uff0c\u786e\u4fdd\u5b83\u4eec\u80fd\u591f\u4e92\u76f8\u8054\u901a\uff0c\u5426\u5219\u96a7\u9053\u5c06\u65e0\u6cd5\u5efa\u7acb \u5982\u679c Pod \u65e5\u5fd7\u8f93\u51fa: \"Error creating local endpoint object error=\"error getting CNI Interface IP address: unable to find CNI Interface on the host which has IP from [\\\"172.100.0.0/16\\\"].Please disable the health check if your CNI does not expose a pod IP on the nodes\". \u8bf7\u68c0\u67e5\u7f51\u5173\u8282\u70b9\u662f\u5426\u914d\u7f6e \"172.100.0.0/16\" \u7f51\u6bb5\u7684\u5730\u5740\uff0c\u5982\u679c\u6ca1\u6709\u8bf7\u914d\u7f6e\u3002\u6216\u8005\u6267\u884c subctl join \u7684\u65f6\u5019\u5173\u95ed\u7f51\u5173\u7684\u5065\u5eb7\u68c0\u6d4b\u529f\u80fd: subctl join --health-check=false ...","title":"\u5b89\u88c5 Submariner"},{"location":"usage/submariner-zh_CN/#_6","text":"\u4f7f\u7528\u4ee5\u4e0b\u7684\u547d\u4ee4\u5206\u522b\u5728\u96c6\u7fa4 cluster-a \u548c cluster-b \u521b\u5efa\u6d4b\u8bd5\u7684 Pod \u548c Service: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-confg labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u7684\u8fd0\u884c\u72b6\u6001: \u5728\u96c6\u7fa4 Cluster-a \u4e0a\u67e5\u770b: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-696bf7cf7d-bkstk 1 /1 Running 0 20m 172 .110.1.131 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .233.62.51 <none> 80 /TCP 20m \u5728\u96c6\u7fa4 Cluster-b \u4e0a\u67e5\u770b: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-8f5cdd468-5zr8n 1 /1 Running 0 21m 172 .111.1.136 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .243.2.135 <none> 80 /TCP 21m \u6d4b\u8bd5 Pod \u8de8\u96c6\u7fa4\u901a\u4fe1\u60c5\u51b5: \u5148\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u67e5\u770b\u8def\u7531\u4fe1\u606f\uff0c\u786e\u4fdd\u8bbf\u95ee\u5bf9\u7aef Pod \u548c Service \u65f6\uff0c\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u534f\u8bae\u6808\u8f6c\u53d1: # \u5728\u96c6\u7fa4 Cluster-a \u6267\u884c: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ip route 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.64.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.74.89 dev veth0 src 172 .110.168.131 10 .243.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 172 .110.1.0/24 dev eth0 src 172 .110.168.131 172 .110.168.73 dev veth0 src 172 .110.168.131 172 .111.0.0/16 via 10 .7.168.73 dev veth0 src 172 .110.168.131 \u4ece\u8def\u7531\u4fe1\u606f\u786e\u8ba4 10.243.0.0/18 \u548c 172.111.0.0/16 \u4ece veth0 \u8f6c\u53d1\u3002 \u6d4b\u8bd5\u96c6\u7fa4 Cluster-a \u7684 Pod \u8bbf\u95ee\u5bf9\u7aef\u96c6\u7fa4 Cluster-b \u7684 Pod : [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ping -c2 172.111.168.136 PING 172 .111.168.136 ( 172 .111.168.136 ) : 56 data bytes 64 bytes from 172 .111.168.136: seq = 0 ttl = 62 time = 0 .900 ms 64 bytes from 172 .111.168.136: seq = 1 ttl = 62 time = 0 .796 ms --- 172 .111.168.136 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .796/0.848/0.900 ms \u6d4b\u8bd5\u96c6\u7fa4 Cluster-a \u7684 Pod \u8bbf\u95ee\u5bf9\u7aef\u96c6\u7fa4 Cluster-b \u7684 Service: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- curl -I 10.243.2.135 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Fri, 08 Dec 2023 03 :32:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 08 Dec 2023 03 :32:04 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/submariner-zh_CN/#_7","text":"Spiderpool \u53ef\u5728 Submariner \u7684\u5e2e\u52a9\u4e0b\uff0c\u89e3\u51b3\u4e0d\u540c\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u96c6\u7fa4\u7f51\u7edc\u8054\u901a\u95ee\u9898\u3002","title":"\u603b\u7ed3"},{"location":"usage/submariner/","text":"Multi-Cluster Networking English | \u7b80\u4f53\u4e2d\u6587 Backaround Why Spiderpool need a multi-cluster network connectivity solution? Spiderpool requires a multi-cluster network connectivity solution for the following reasons: if our different clusters are distributed in the same data center, their networks are naturally interconnected. However, if they are spread across different data centers, the cluster subnets are isolated from each other and cannot communicate directly across data centers. Therefore, Spiderpool needs a multi-cluster network connectivity solution to address the issue of cross-data center multi-cluster network access. Submariner is an open-source multi-cluster network connectivity solution that uses tunneling technology to establish direct communication between Pods and Services in different Kubernetes clusters (running locally or in the public cloud). For more information, please refer to the Submariner Document . We can leverage Submariner to assist Spiderpool in addressing cross-data center multi-cluster network access issues. we will provide a detailed explanation of this feature. Prerequisites At least two Kubernetes clusters without CNI installed. Helm and Subctl tools are already installed. Network Topology Diagram This network topology diagram provides the following information: Clusters ClusterA and ClusterB are distributed across different data centers, and their respective cluster underlay subnets (172.110.0.0/16 and 172.111.0.0/16) cannot communicate directly due to network isolation in different data center networks. The gateway nodes can communicate with each other through the ens192 interface (10.6.0.0/16). The two clusters are connected through an IPSec tunnel established by Submariner. The tunnel is based on the ens192 interface of the gateway nodes, and it also accesses the Submariner Broker component through the ens192 interface. Quick Start Install Spiderpool You can refer to the installation guide to install Spiderpool. Configure SpiderIPPool Since Submariner currently does not support multiple subnets, you can split the PodCIDR of each cluster into smaller subnets. Specify MacVlan Pods to get IP addresses from their respective smaller subnets for Underlay communication. Note: Ensure that these smaller subnets correspond to the connected Underlay subnets. For example, the PodCIDR of cluster-a is 172.110.0.0/16, you can create multiple smaller subnets (e.g., 172.110.1.0/24) within this larger subnet for Pod usage: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-a spec: default: true ips: - \"172.110.1.1-172.110.1.200\" subnet: 172.110.1.0/24 gateway: 172.110.0.1 EOF the PodCIDR of cluster-b is 172.111.0.0/16, you can create multiple smaller subnets (e.g., 172.111.1.0/24) within this larger subnet for Pod usage: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-b spec: default: true ips: - \"172.111.1.1-172.111.1.200\" subnet: 172.111.1.0/24 gateway: 172.111.0.1 EOF Configure SpiderMultusConfig Configure an SpiderMultusConfig in Cluster-a: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-a coordinator: hijackCIDR: - 10.243.0.0/18 - 172.111.0.0/16 EOF Configure an SpiderMultusConfig in Cluster-b: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-b coordinator: hijackCIDR: - 10.233.0.0/18 - 172.110.0.0/16 EOF Configuration of the host interface ens224 as the parent interface for Macvlan. Macvlan will create sub-interfaces on this network card for Pod use. Configuration of coordinator.hijackCIDR to specify the subnet information for the Service and Pods in the remote cluster. When a Pod is started, the coordinator will insert routes for these subnets into the Pod, enabling traffic to these destinations to be forwarded from the node. This facilitates better collaboration with Submariner. Install Subamriner To install Submariner using the Subctl tool, you can refer to the official Submariner documentation . However, when executing subctl join , make sure to manually specify the subnet for the MacVlan Underlay Pods mentioned in the previous steps. # clusterA subctl join --kubeconfig cluster-a.config broker-info.subm --clusterid = cluster-a --clustercidr = 172 .110.0.0/16 # clusterB subctl join --kubeconfig cluster-b.config broker-info.subm --clusterid = cluster-b --clustercidr = 172 .111.0.0/16 Currently, Submariner only supports specifying a single Pod subnet and does not support multiple subnets. After the installation is complete, check the status of the Submariner components: [ root@controller-node-1 ~ ] # subctl show all Cluster \"cluster.local\" \u2713 Detecting broker ( s ) NAMESPACE NAME COMPONENTS GLOBALNET GLOBALNET CIDR DEFAULT GLOBALNET SIZE DEFAULT DOMAINS submariner-k8s-broker submariner-broker service-discovery, connectivity no 242 .0.0.0/8 65536 \u2713 Showing Connections GATEWAY CLUSTER REMOTE IP NAT CABLE DRIVER SUBNETS STATUS RTT avg. controller-node-1 cluster-b 10 .6.168.74 no libreswan 10 .243.0.0/18, 172 .111.0.0/16 connected 661 .938\u00b5s \u2713 Showing Endpoints CLUSTER ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster01 10 .6.168.73 140 .207.201.152 libreswan local cluster02 10 .6.168.74 140 .207.201.152 libreswan remote \u2713 Showing Gateways NODE HA STATUS SUMMARY controller-node-1 active All connections ( 1 ) are established \u2713 Showing Network details Discovered network details via Submariner: Network plugin: \"\" Service CIDRs: [ 10 .233.0.0/18 ] Cluster CIDRs: [ 172 .110.0.0/16 ] \u2713 Showing versions COMPONENT REPOSITORY CONFIGURED RUNNING submariner-gateway quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-routeagent quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-metrics-proxy quay.io/submariner 0 .16.0 release-0.16-d48224e08e06 submariner-operator quay.io/submariner 0 .16.0 release-0.16-0807883713b0 submariner-lighthouse-agent quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 submariner-lighthouse-coredns quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 As shown above, the Submariner components are running normally, and the tunnels have been successfully established. If you encounter issues with the tunnel not being established and the submariner-gateway pod remains in a CrashLoopBackOff state, potential reasons could include: Select suitable nodes as gateway nodes, ensuring they can communicate with each other. Otherwise, the tunnel will not be established. If the pod logs show: \"Error creating local endpoint object error=\"error getting CNI Interface IP address: unable to find CNI Interface on the host which has IP from [\\\"172.100.0.0/16\\\"].Please disable the health check if your CNI does not expose a pod IP on the nodes\", Please check if the gateway nodes are configured with addresses in the \"172.100.0.0/16\" subnet. If not, configure them. Alternatively, when executing subctl join, you can disable the health-check feature for the gateways: subctl join --health-check=false ... Create Deployment Use the following commands to create test Pods and Services in clusters cluster-a and cluster-b: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-confg labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the running status of the Pods: View in Cluster-a: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-696bf7cf7d-bkstk 1 /1 Running 0 20m 172 .110.1.131 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .233.62.51 <none> 80 /TCP 20m View in Cluster-b: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-8f5cdd468-5zr8n 1 /1 Running 0 21m 172 .111.1.136 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .243.2.135 <none> 80 /TCP 21m Test communication between Pods across clusters: First, enter the Pod and check the routing information to ensure that when accessing the remote Pod and Service, traffic goes through the host's network protocol stack for forwarding: # In Cluster-a : [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ip route 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.64.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.74.89 dev veth0 src 172 .110.168.131 10 .243.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 172 .110.1.0/24 dev eth0 src 172 .110.168.131 172 .110.168.73 dev veth0 src 172 .110.168.131 172 .111.0.0/16 via 10 .7.168.73 dev veth0 src 172 .110.168.131 Confirm from the routing information that 10.243.0.0/18 and 172.111.0.0/16 are forwarded through veth0. Test access from a Pod in Cluster-A to a Pod in the remote Cluster-b: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ping -c2 172.111.168.136 PING 172 .111.168.136 ( 172 .111.168.136 ) : 56 data bytes 64 bytes from 172 .111.168.136: seq = 0 ttl = 62 time = 0 .900 ms 64 bytes from 172 .111.168.136: seq = 1 ttl = 62 time = 0 .796 ms --- 172 .111.168.136 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .796/0.848/0.900 ms Test access from a Pod in Cluster-a to a Service in the remote Cluster-b: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- curl -I 10.243.2.135 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Fri, 08 Dec 2023 03 :32:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 08 Dec 2023 03 :32:04 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes Summary Spiderpool can address the challenge of cross-datacenter multi-cluster network connectivity with the assistance of Submariner.","title":"Multi-Cluster Networking"},{"location":"usage/submariner/#multi-cluster-networking","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Multi-Cluster Networking"},{"location":"usage/submariner/#backaround","text":"Why Spiderpool need a multi-cluster network connectivity solution? Spiderpool requires a multi-cluster network connectivity solution for the following reasons: if our different clusters are distributed in the same data center, their networks are naturally interconnected. However, if they are spread across different data centers, the cluster subnets are isolated from each other and cannot communicate directly across data centers. Therefore, Spiderpool needs a multi-cluster network connectivity solution to address the issue of cross-data center multi-cluster network access. Submariner is an open-source multi-cluster network connectivity solution that uses tunneling technology to establish direct communication between Pods and Services in different Kubernetes clusters (running locally or in the public cloud). For more information, please refer to the Submariner Document . We can leverage Submariner to assist Spiderpool in addressing cross-data center multi-cluster network access issues. we will provide a detailed explanation of this feature.","title":"Backaround"},{"location":"usage/submariner/#prerequisites","text":"At least two Kubernetes clusters without CNI installed. Helm and Subctl tools are already installed.","title":"Prerequisites"},{"location":"usage/submariner/#network-topology-diagram","text":"This network topology diagram provides the following information: Clusters ClusterA and ClusterB are distributed across different data centers, and their respective cluster underlay subnets (172.110.0.0/16 and 172.111.0.0/16) cannot communicate directly due to network isolation in different data center networks. The gateway nodes can communicate with each other through the ens192 interface (10.6.0.0/16). The two clusters are connected through an IPSec tunnel established by Submariner. The tunnel is based on the ens192 interface of the gateway nodes, and it also accesses the Submariner Broker component through the ens192 interface.","title":"Network Topology Diagram"},{"location":"usage/submariner/#quick-start","text":"","title":"Quick Start"},{"location":"usage/submariner/#install-spiderpool","text":"You can refer to the installation guide to install Spiderpool. Configure SpiderIPPool Since Submariner currently does not support multiple subnets, you can split the PodCIDR of each cluster into smaller subnets. Specify MacVlan Pods to get IP addresses from their respective smaller subnets for Underlay communication. Note: Ensure that these smaller subnets correspond to the connected Underlay subnets. For example, the PodCIDR of cluster-a is 172.110.0.0/16, you can create multiple smaller subnets (e.g., 172.110.1.0/24) within this larger subnet for Pod usage: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-a spec: default: true ips: - \"172.110.1.1-172.110.1.200\" subnet: 172.110.1.0/24 gateway: 172.110.0.1 EOF the PodCIDR of cluster-b is 172.111.0.0/16, you can create multiple smaller subnets (e.g., 172.111.1.0/24) within this larger subnet for Pod usage: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: cluster-b spec: default: true ips: - \"172.111.1.1-172.111.1.200\" subnet: 172.111.1.0/24 gateway: 172.111.0.1 EOF Configure SpiderMultusConfig Configure an SpiderMultusConfig in Cluster-a: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-a coordinator: hijackCIDR: - 10.243.0.0/18 - 172.111.0.0/16 EOF Configure an SpiderMultusConfig in Cluster-b: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ens224 ippools: ipv4: - cluster-b coordinator: hijackCIDR: - 10.233.0.0/18 - 172.110.0.0/16 EOF Configuration of the host interface ens224 as the parent interface for Macvlan. Macvlan will create sub-interfaces on this network card for Pod use. Configuration of coordinator.hijackCIDR to specify the subnet information for the Service and Pods in the remote cluster. When a Pod is started, the coordinator will insert routes for these subnets into the Pod, enabling traffic to these destinations to be forwarded from the node. This facilitates better collaboration with Submariner.","title":"Install Spiderpool"},{"location":"usage/submariner/#install-subamriner","text":"To install Submariner using the Subctl tool, you can refer to the official Submariner documentation . However, when executing subctl join , make sure to manually specify the subnet for the MacVlan Underlay Pods mentioned in the previous steps. # clusterA subctl join --kubeconfig cluster-a.config broker-info.subm --clusterid = cluster-a --clustercidr = 172 .110.0.0/16 # clusterB subctl join --kubeconfig cluster-b.config broker-info.subm --clusterid = cluster-b --clustercidr = 172 .111.0.0/16 Currently, Submariner only supports specifying a single Pod subnet and does not support multiple subnets. After the installation is complete, check the status of the Submariner components: [ root@controller-node-1 ~ ] # subctl show all Cluster \"cluster.local\" \u2713 Detecting broker ( s ) NAMESPACE NAME COMPONENTS GLOBALNET GLOBALNET CIDR DEFAULT GLOBALNET SIZE DEFAULT DOMAINS submariner-k8s-broker submariner-broker service-discovery, connectivity no 242 .0.0.0/8 65536 \u2713 Showing Connections GATEWAY CLUSTER REMOTE IP NAT CABLE DRIVER SUBNETS STATUS RTT avg. controller-node-1 cluster-b 10 .6.168.74 no libreswan 10 .243.0.0/18, 172 .111.0.0/16 connected 661 .938\u00b5s \u2713 Showing Endpoints CLUSTER ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster01 10 .6.168.73 140 .207.201.152 libreswan local cluster02 10 .6.168.74 140 .207.201.152 libreswan remote \u2713 Showing Gateways NODE HA STATUS SUMMARY controller-node-1 active All connections ( 1 ) are established \u2713 Showing Network details Discovered network details via Submariner: Network plugin: \"\" Service CIDRs: [ 10 .233.0.0/18 ] Cluster CIDRs: [ 172 .110.0.0/16 ] \u2713 Showing versions COMPONENT REPOSITORY CONFIGURED RUNNING submariner-gateway quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-routeagent quay.io/submariner 0 .16.0 release-0.16-d1b6c9e194f8 submariner-metrics-proxy quay.io/submariner 0 .16.0 release-0.16-d48224e08e06 submariner-operator quay.io/submariner 0 .16.0 release-0.16-0807883713b0 submariner-lighthouse-agent quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 submariner-lighthouse-coredns quay.io/submariner 0 .16.0 release-0.16-6f1d3f22e806 As shown above, the Submariner components are running normally, and the tunnels have been successfully established. If you encounter issues with the tunnel not being established and the submariner-gateway pod remains in a CrashLoopBackOff state, potential reasons could include: Select suitable nodes as gateway nodes, ensuring they can communicate with each other. Otherwise, the tunnel will not be established. If the pod logs show: \"Error creating local endpoint object error=\"error getting CNI Interface IP address: unable to find CNI Interface on the host which has IP from [\\\"172.100.0.0/16\\\"].Please disable the health check if your CNI does not expose a pod IP on the nodes\", Please check if the gateway nodes are configured with addresses in the \"172.100.0.0/16\" subnet. If not, configure them. Alternatively, when executing subctl join, you can disable the health-check feature for the gateways: subctl join --health-check=false ...","title":"Install Subamriner"},{"location":"usage/submariner/#create-deployment","text":"Use the following commands to create test Pods and Services in clusters cluster-a and cluster-b: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-confg labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the running status of the Pods: View in Cluster-a: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-696bf7cf7d-bkstk 1 /1 Running 0 20m 172 .110.1.131 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .233.62.51 <none> 80 /TCP 20m View in Cluster-b: [ root@controller-node-1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-8f5cdd468-5zr8n 1 /1 Running 0 21m 172 .111.1.136 controller-node-1 <none> <none> [ root@controller-node-1 ~ ] # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-app-svc ClusterIP 10 .243.2.135 <none> 80 /TCP 21m Test communication between Pods across clusters: First, enter the Pod and check the routing information to ensure that when accessing the remote Pod and Service, traffic goes through the host's network protocol stack for forwarding: # In Cluster-a : [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ip route 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.64.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 10 .233.74.89 dev veth0 src 172 .110.168.131 10 .243.0.0/18 via 10 .7.168.73 dev veth0 src 172 .110.168.131 172 .110.1.0/24 dev eth0 src 172 .110.168.131 172 .110.168.73 dev veth0 src 172 .110.168.131 172 .111.0.0/16 via 10 .7.168.73 dev veth0 src 172 .110.168.131 Confirm from the routing information that 10.243.0.0/18 and 172.111.0.0/16 are forwarded through veth0. Test access from a Pod in Cluster-A to a Pod in the remote Cluster-b: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- ping -c2 172.111.168.136 PING 172 .111.168.136 ( 172 .111.168.136 ) : 56 data bytes 64 bytes from 172 .111.168.136: seq = 0 ttl = 62 time = 0 .900 ms 64 bytes from 172 .111.168.136: seq = 1 ttl = 62 time = 0 .796 ms --- 172 .111.168.136 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .796/0.848/0.900 ms Test access from a Pod in Cluster-a to a Service in the remote Cluster-b: [ root@controller-node-1 ~ ] # kubectl exec -it test-app-696bf7cf7d-bkstk -- curl -I 10.243.2.135 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Fri, 08 Dec 2023 03 :32:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 08 Dec 2023 03 :32:04 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Create Deployment"},{"location":"usage/submariner/#summary","text":"Spiderpool can address the challenge of cross-datacenter multi-cluster network connectivity with the assistance of Submariner.","title":"Summary"},{"location":"usage/underlay_cni_service-zh_CN/","text":"Underlay CNI \u8bbf\u95ee Service \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI(\u5982 Macvlan\u3001IPVlan\u3001Sriov-CNI \u7b49)\u4e00\u822c\u5bf9\u63a5\u5e95\u5c42\u7f51\u7edc\uff0c\u5f80\u5f80\u5e76\u4e0d\u539f\u751f\u652f\u6301\u8bbf\u95ee\u96c6\u7fa4\u7684 Service \u3002\u8fd9\u5927\u591a\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee Service \u9700\u8981\u7ecf\u8fc7\u4ea4\u6362\u673a\u7684\u7f51\u5173\u8f6c\u53d1\uff0c \u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 Service \u7684\u8def\u7531\uff0c\u9020\u6210\u65e0\u6cd5\u6b63\u786e\u8def\u7531\u8bbf\u95ee Service \u7684\u62a5\u6587\uff0c\u4ece\u800c\u4e22\u5305\u3002Spiderpool \u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u7684\u65b9\u6848\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898: \u901a\u8fc7 kube-proxy \u8bbf\u95ee Service \u901a\u8fc7 cgroup eBPF \u5b9e\u73b0 service \u8bbf\u95ee Service \u8fd9\u4e24\u79cd\u65b9\u6848\u90fd\u89e3\u51b3\u4e86 Underlay CNI \u65e0\u6cd5\u8bbf\u95ee Service \u7684\u95ee\u9898\uff0c\u4f46\u5b9e\u73b0\u539f\u7406\u6709\u4e9b\u4e0d\u540c\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u5f0f: \u57fa\u4e8e kube-proxy \u5b9e\u73b0 service \u8bbf\u95ee Spiderpool \u5185\u7f6e coordinator \u63d2\u4ef6\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u65e0\u7f1d\u5bf9\u63a5 kube-proxy \u4ee5\u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service\u3002 \u6839\u636e\u4e0d\u540c\u7684\u573a\u666f\uff0c coordinator \u53ef\u4ee5\u8fd0\u884c\u5728 underlay \u6216 overlay \u6a21\u5f0f\uff0c\u867d\u7136\u5b9e\u73b0\u65b9\u5f0f\u7a0d\u663e\u4e0d\u540c\uff0c\u4f46 \u6838\u5fc3\u539f\u7406\u90fd\u662f\u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u7684\u4e3b\u673a\u7f51\u7edc\u534f\u8bae\u6808\u4e0a\uff0c\u518d\u7ecf\u8fc7 Kube-proxy \u521b\u5efa\u7684 IPtables \u89c4\u5219\u505a\u8f6c\u53d1\u3002 \u4e0b\u9762\u662f\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b\u56fe\u4ecb\u7ecd: \u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Underlay \u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u63d2\u4ef6\u5c06\u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u5c06\u4e00\u7aef\u653e\u7f6e\u4e8e\u4e3b\u673a\uff0c\u53e6\u4e00\u7aef\u653e\u7f6e\u4e0e Pod \u7684 network namespace \u4e2d\uff0c\u7136\u540e\u5728 Pod \u91cc\u9762\u8bbe\u7f6e\u4e00\u4e9b\u8def\u7531\u89c4\u5219\uff0c \u4f7f Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth \u8bbe\u5907\u8f6c\u53d1\u3002 coordinator \u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c \u5b83\u5c06\u81ea\u52a8\u5224\u65ad\u5e94\u8be5\u8fd0\u884c underlay \u6216 overlay \u6a21\u5f0f\u3002\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> \u5373\u53ef\u3002 \u5f53\u4ee5 Underlay \u6a21\u5f0f\u521b\u5efa Pod \u540e\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 \u662f\u8282\u70b9\u7684 IP,\u786e\u4fdd Pod \u8bbf\u95ee\u672c\u8282\u70b9\u65f6\u4ece veth0 \u8f6c\u53d1\u3002 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 \u662f\u96c6\u7fa4 Service \u7684 CIDR, \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth0 \u8f6c\u53d1\u3002 \u8fd9\u4e2a\u65b9\u6848\u5f3a\u70c8\u4f9d\u8d56\u4e0e kube-proxy \u7684 MASQUERADE , \u5426\u5219\u56de\u590d\u62a5\u6587\u5c06\u76f4\u63a5\u8f6c\u53d1\u7ed9\u6e90 Pod, \u5982\u679c\u7ecf\u8fc7\u4e00\u4e9b\u5b89\u5168\u8bbe\u5907\uff0c\u5c06\u4f1a\u4e22\u5f03\u6570\u636e\u5305\u3002\u6240\u4ee5\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e kube-proxy \u7684 masqueradeAll \u4e3a true\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u4e0d\u540c\uff0c \u65e0\u9700\u5f00\u542f masqueradeAll , \u5b83\u4eec\u4e4b\u95f4\u7684\u8bbf\u95ee\u5c06\u4f1a\u88ab SNAT\u3002 \u5982\u679c Pod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u76f8\u540c\uff0c\u90a3\u6211\u4eec\u5fc5\u987b\u8981\u8bbe\u7f6e masqueradeAll \u4e3a true\u3002 \u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Overlay \u6a21\u5f0f \u914d\u7f6e coordinator \u4e3a Overlay \u6a21\u5f0f\u540c\u6837\u4e5f\u80fd\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u4f20\u7edf\u7684 Overlay \u7c7b\u578b(\u5982 Calico \u548c Cilium \u7b49)\u7684 CNI \u5df2\u7ecf\u5b8c\u7f8e\u89e3\u51b3\u4e86\u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u5b83\uff0c\u5e2e\u52a9 Underlay Pod \u8bbf\u95ee Service\u3002 \u6211\u4eec\u53ef\u4ee5\u4e3a Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\uff0c eth0 \u4e3a Overlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002 net1 \u4e3a Underlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1 Pod \u5357\u5317\u5411\u6d41\u91cf\u3002 \u901a\u8fc7 coordinator \u8bbe\u7f6e\u7684\u7b56\u7565\u8def\u7531\u8868\u9879 \u786e\u4fdd Pod \u8bbf\u95ee Service \u65f6\u4ece eth0 \u8f6c\u53d1, \u56de\u590d\u62a5\u6587\u4e5f\u8f6c\u53d1\u7ed9 eth0\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u8c03\u7528\u7f51\u5361\u662f\u5426\u4e0d\u662f eth0 \u3002\u5982\u679c\u4e0d\u662f\uff0c\u786e\u8ba4 Pod \u4e2d\u4e0d\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a overlay \u6a21\u5f0f\u3002 \u5728 overlay \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u540c\u6b65\u96c6\u7fa4\u7f3a\u7701 CNI \u7684 Pod \u5b50\u7f51\uff0c\u8fd9\u4e9b\u5b50\u7f51\u7528\u4e8e\u5728\u591a\u7f51\u5361 Pod \u4e2d\u8bbe\u7f6e\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u5b83\u8bbf\u95ee\u7531\u7f3a\u7701 CNI \u521b\u5efa\u7684 Pod \u4e4b\u95f4\u7684\u6b63\u5e38\u901a\u4fe1\u65f6\uff0c\u4ece eth0 \u8f6c\u53d1\u3002\u8fd9\u4e2a\u914d\u7f6e\u5bf9\u5e94 spidercoordinator.spec.podCIDRType \uff0c\u9ed8\u8ba4\u4e3a auto , \u53ef\u9009\u503c: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] \u8fd9\u4e9b\u8def\u7531\u662f\u5728 Pod \u542f\u52a8\u65f6\u6ce8\u5165\u7684\uff0c\u5982\u679c\u76f8\u5173\u7684 CIDR \u53d1\u751f\u4e86\u53d8\u52a8\uff0c\u65e0\u6cd5\u81ea\u52a8\u751f\u6548\u5230\u5df2\u7ecf Running \u7684 Pod \u4e2d\uff0c\u8fd9\u9700\u8981\u91cd\u542f Pod \u624d\u80fd\u751f\u6548\u3002 \u66f4\u591a\u8be6\u60c5\u53c2\u8003 CRD-Spidercoordinator \u5f53\u4ee5 Overlay \u6a21\u5f0f\u521b\u5efa Pod \u5e76\u8fdb\u5165 Pod \u7f51\u7edc\u547d\u4ee4\u7a7a\u95f4\uff0c\u67e5\u770b\u8def\u7531\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip rule 0 : from all lookup local 32759 : from 10 .233.105.154 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32759: from 10.233.105.154 lookup 100 : \u786e\u4fdd\u4ece eth0 (calico \u7f51\u5361)\u53d1\u51fa\u7684\u6570\u636e\u5305\u8d70 table 100\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b: \u9664\u4e86\u9ed8\u8ba4\u8def\u7531\uff0c\u6240\u6709\u8def\u7531\u90fd\u4fdd\u7559\u5728 Main \u8868\uff0c\u4f46\u4f1a\u628a net1 \u7684\u9ed8\u8ba4\u8def\u7531\u79fb\u52a8\u5230 table 100\u3002 \u8fd9\u4e9b\u7b56\u7565\u8def\u7531\u786e\u4fdd\u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0cUnderlay Pod \u4e5f\u80fd\u591f\u6b63\u5e38\u8bbf\u95ee Service\u3002 \u57fa\u4e8e cgroup eBPF \u5b9e\u73b0 service \u8bbf\u95ee \u4e0a\u9762\u6211\u4eec\u4ecb\u7ecd\u4e86\u5728 Spiderpool \u4e2d, \u6211\u4eec\u901a\u8fc7 coordinator \u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u5230\u4e3b\u673a\u8f6c\u53d1\uff0c \u518d\u7ecf\u8fc7\u4e3b\u673a\u4e0a Kube-proxy \u8bbe\u7f6e\u7684 iptables \u89c4\u5219 DNAT (\u5c06\u76ee\u6807\u5730\u5740\u6539\u4e3a\u76ee\u6807 Pod) \u4e4b\u540e\uff0c\u518d\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u8fd9\u53ef\u4ee5\u867d\u7136\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5ef6\u957f\u4e86\u6570\u636e\u8bbf\u95ee\u8def\u5f84\uff0c\u9020\u6210\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u5931\u3002 \u793e\u533a\u5f00\u6e90\u7684 CNI \u9879\u76ee: Cilium \u652f\u6301\u57fa\u4e8e eBPF \u6280\u672f\u5b8c\u5168\u66ff\u4ee3 kube-proxy \u7cfb\u7edf\u7ec4\u4ef6\u3002\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u89e3\u6790 Service\u3002\u5f53\u8bbf\u95ee Service \u65f6\uff0cService \u5730\u5740\u4f1a\u88ab Cilium \u6302\u8f7d\u7684 eBPF \u7a0b\u5e8f\u76f4\u63a5\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u7684 IP\uff0c\u8fd9\u6837\u6e90 Pod \u76f4\u63a5\u5bf9\u76ee\u6807 Pod \u53d1\u8d77\u8bbf\u95ee\uff0c\u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff0c\u6781\u5927\u7684\u7f29\u77ed\u4e86\u8bbf\u95ee\u8def\u5f84\uff0c\u4ece\u800c\u5b9e\u73b0\u52a0\u901f\u8bbf\u95ee Service\u3002\u501f\u52a9\u4e8e\u5f3a\u5927\u7684 Cilium\uff0c \u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b83\u5b9e\u73b0\u52a0\u901f Underlay CNI\u7684 Service \u8bbf\u95ee\u3002 \u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u76f8\u6bd4 kube proxy \u89e3\u6790\u65b9\u5f0f\uff0ccgroup eBPF \u65b9\u5f0f\u7684 \u7f51\u7edc\u5ef6\u65f6\u6709\u6700\u5927 25% \u7684\u6539\u5584\uff0c\u7f51\u7edc\u541e\u5410\u6709 50% \u7684\u63d0\u9ad8 \u3002 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e Macvlan CNI + Cilium \u52a0\u901f\u8bbf\u95ee Service\uff1a \u6ce8\u610f: \u9700\u8981\u786e\u4fdd\u96c6\u7fa4\u8282\u70b9\u7684\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u63d0\u524d\u51c6\u5907\u597d\u4e00\u4e2a\u672a\u5b89\u88c5 kube-proxy \u7ec4\u4ef6\u7684\u96c6\u7fa4\uff0c\u5982\u679c\u5df2\u7ecf\u5b89\u88c5 kube-proxy \u53ef\u53c2\u8003\u4e00\u4e0b\u547d\u4ee4\u5220\u9664 kube-proxy \u7ec4\u4ef6 ~# kubectl delete ds -n kube-system kube-proxy ~# # \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c ~# iptables-save | grep -v KUBE | iptables-restore \u4e5f\u53ef\u4ee5\u4f7f\u7528 kubeadm \u5b89\u88c5\u4e00\u4e2a\u65b0\u96c6\u7fa4, \u6ce8\u610f\u4e0d\u8981\u5b89\u88c5 kube-proxy: ~# kubeadm init --skip-phases = addon/kube-proxy \u5b89\u88c5 Cilium \u7ec4\u4ef6\uff0c\u6ce8\u610f\u5f00\u542f kube-proxy replacement \u529f\u80fd ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } \u5b89\u88c5\u5b8c\u6210\uff0c\u68c0\u67e5\u5b89\u88c5\u72b6\u6001\uff1a ~# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m \u5b89\u88c5 Spiderpool, \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none \u8bbe\u7f6e coordinator.podCIDRType=none, spiderpool \u5c06\u4e0d\u4f1a\u83b7\u53d6\u96c6\u7fa4\u7684 ServiceCIDR\u3002\u5728\u521b\u5efa Pod \u65f6\u4e5f\u5c31\u4e0d\u4f1a\u6ce8\u5165 Service \u76f8\u5173\u8def\u7531 \u8fd9\u6837\u8bbf\u95ee Service \u5b8c\u5168\u4f9d\u8d56 Cilium kube-proxy Replacement\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u521b\u5efa macvlan \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF \u9700\u8981\u786e\u4fdd ens192 \u5b58\u5728\u4e8e\u96c6\u7fa4\u8282\u70b9 \u5efa\u8bae\u8bbe\u7f6e enableCoordinator \u4e3a true, \u8fd9\u53ef\u4ee5\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u6d4b\u7684\u95ee\u9898 \u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\u7528\u4e8e\u6d4b\u8bd5\uff1a ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network : kube-system/macvlan-ens192\" NAME=ipvlan cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : DaemonSet metadata : name : ${NAME} labels : app : $NAME spec : selector : matchLabels : app : $NAME template : metadata : name : $NAME labels : app : $NAME annotations : ${ANNOTATION_MULTUS} spec : containers : - name : test-app image : nginx imagePullPolicy : IfNotPresent ports : - name : http containerPort : 80 protocol : TCP --- apiVersion : v1 kind : Service metadata : name : ${NAME} spec : - ports : name : http port : 80 protocol : TCP targetPort : 80 selector : app : ${NAME} type : ClusterIP EOF \u9a8c\u8bc1\u8bbf\u95ee Service \u7684\u8054\u901a\u6027\uff0c\u5e76\u67e5\u770b\u6027\u80fd\u662f\u5426\u63d0\u5347 ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes \u53e6\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\uff0c\u901a\u8fc7 tcpdump \u5de5\u5177\u67e5\u770b\u5230\u8bbf\u95ee Service \u7684\u6570\u636e\u5305\u4ece Pod \u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u53d1\u51fa\u65f6\uff0c\u76ee\u6807\u5730\u5740\u5df2\u7ecf\u88ab\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u5730\u5740: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 \u662f\u6e90 Pod \u7684 IP, 10.6.185.210 \u662f\u76ee\u6807 Pod \u7684 IP\uff0c\u53ef\u4ee5\u786e\u8ba4 Cilium \u89e3\u6790\u4e86 Service \u7684 IP\u3002 \u4f7f\u7528 sockperf \u5de5\u5177\u6d4b\u8bd5\u4f7f\u7528 Cilium \u52a0\u901f\u524d\u540e\uff0c \u6d4b\u5f97 Pod \u8de8\u8282\u70b9\u8bbf\u95ee ClusterIP \u7684\u6570\u636e\u5bf9\u6bd4: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 \u6839\u636e\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7 Cilium kube-proxy replacement \u4e4b\u540e\uff0c\u8bbf\u95ee Service \u5927\u7ea6\u52a0\u901f 30%\u3002\u66f4\u591a\u6d4b\u8bd5\u6570\u636e\u53c2\u8003 \u7f51\u7edc IO \u6027\u80fd \u7ed3\u8bba Underlay CNI \u8bbf\u95ee Service \u6709\u4ee5\u4e0a\u4e24\u79cd\u65b9\u6848\u89e3\u51b3\u3002kube-proxy \u7684\u65b9\u5f0f\u66f4\u52a0\u5e38\u7528\u7a33\u5b9a\uff0c\u5927\u90e8\u5206\u73af\u5883\u90fd\u53ef\u4ee5\u7a33\u5b9a\u4f7f\u7528\u3002 cgroup eBPF \u4e3a Underlay CNI \u8bbf\u95ee Service \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u53ef\u9009\u65b9\u6848\uff0c\u5e76\u4e14\u52a0\u901f\u4e86 Service \u8bbf\u95ee\uff0c\u5c3d\u7ba1\u8fd9\u6709\u4e00\u5b9a\u4f7f\u7528\u9650\u5236\u53ca\u95e8\u69db\uff0c\u4f46\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u7684\u9700\u6c42\u3002","title":"Underlay CNI \u8bbf\u95ee Service"},{"location":"usage/underlay_cni_service-zh_CN/#underlay-cni-service","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Underlay CNI \u8bbf\u95ee Service"},{"location":"usage/underlay_cni_service-zh_CN/#_1","text":"\u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI(\u5982 Macvlan\u3001IPVlan\u3001Sriov-CNI \u7b49)\u4e00\u822c\u5bf9\u63a5\u5e95\u5c42\u7f51\u7edc\uff0c\u5f80\u5f80\u5e76\u4e0d\u539f\u751f\u652f\u6301\u8bbf\u95ee\u96c6\u7fa4\u7684 Service \u3002\u8fd9\u5927\u591a\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee Service \u9700\u8981\u7ecf\u8fc7\u4ea4\u6362\u673a\u7684\u7f51\u5173\u8f6c\u53d1\uff0c \u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 Service \u7684\u8def\u7531\uff0c\u9020\u6210\u65e0\u6cd5\u6b63\u786e\u8def\u7531\u8bbf\u95ee Service \u7684\u62a5\u6587\uff0c\u4ece\u800c\u4e22\u5305\u3002Spiderpool \u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u7684\u65b9\u6848\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898: \u901a\u8fc7 kube-proxy \u8bbf\u95ee Service \u901a\u8fc7 cgroup eBPF \u5b9e\u73b0 service \u8bbf\u95ee Service \u8fd9\u4e24\u79cd\u65b9\u6848\u90fd\u89e3\u51b3\u4e86 Underlay CNI \u65e0\u6cd5\u8bbf\u95ee Service \u7684\u95ee\u9898\uff0c\u4f46\u5b9e\u73b0\u539f\u7406\u6709\u4e9b\u4e0d\u540c\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u5f0f:","title":"\u4ecb\u7ecd"},{"location":"usage/underlay_cni_service-zh_CN/#kube-proxy-service","text":"Spiderpool \u5185\u7f6e coordinator \u63d2\u4ef6\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u65e0\u7f1d\u5bf9\u63a5 kube-proxy \u4ee5\u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service\u3002 \u6839\u636e\u4e0d\u540c\u7684\u573a\u666f\uff0c coordinator \u53ef\u4ee5\u8fd0\u884c\u5728 underlay \u6216 overlay \u6a21\u5f0f\uff0c\u867d\u7136\u5b9e\u73b0\u65b9\u5f0f\u7a0d\u663e\u4e0d\u540c\uff0c\u4f46 \u6838\u5fc3\u539f\u7406\u90fd\u662f\u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u7684\u4e3b\u673a\u7f51\u7edc\u534f\u8bae\u6808\u4e0a\uff0c\u518d\u7ecf\u8fc7 Kube-proxy \u521b\u5efa\u7684 IPtables \u89c4\u5219\u505a\u8f6c\u53d1\u3002 \u4e0b\u9762\u662f\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b\u56fe\u4ecb\u7ecd: \u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Underlay \u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u63d2\u4ef6\u5c06\u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u5c06\u4e00\u7aef\u653e\u7f6e\u4e8e\u4e3b\u673a\uff0c\u53e6\u4e00\u7aef\u653e\u7f6e\u4e0e Pod \u7684 network namespace \u4e2d\uff0c\u7136\u540e\u5728 Pod \u91cc\u9762\u8bbe\u7f6e\u4e00\u4e9b\u8def\u7531\u89c4\u5219\uff0c \u4f7f Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth \u8bbe\u5907\u8f6c\u53d1\u3002 coordinator \u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c \u5b83\u5c06\u81ea\u52a8\u5224\u65ad\u5e94\u8be5\u8fd0\u884c underlay \u6216 overlay \u6a21\u5f0f\u3002\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> \u5373\u53ef\u3002 \u5f53\u4ee5 Underlay \u6a21\u5f0f\u521b\u5efa Pod \u540e\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 \u662f\u8282\u70b9\u7684 IP,\u786e\u4fdd Pod \u8bbf\u95ee\u672c\u8282\u70b9\u65f6\u4ece veth0 \u8f6c\u53d1\u3002 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 \u662f\u96c6\u7fa4 Service \u7684 CIDR, \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth0 \u8f6c\u53d1\u3002 \u8fd9\u4e2a\u65b9\u6848\u5f3a\u70c8\u4f9d\u8d56\u4e0e kube-proxy \u7684 MASQUERADE , \u5426\u5219\u56de\u590d\u62a5\u6587\u5c06\u76f4\u63a5\u8f6c\u53d1\u7ed9\u6e90 Pod, \u5982\u679c\u7ecf\u8fc7\u4e00\u4e9b\u5b89\u5168\u8bbe\u5907\uff0c\u5c06\u4f1a\u4e22\u5f03\u6570\u636e\u5305\u3002\u6240\u4ee5\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e kube-proxy \u7684 masqueradeAll \u4e3a true\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u4e0d\u540c\uff0c \u65e0\u9700\u5f00\u542f masqueradeAll , \u5b83\u4eec\u4e4b\u95f4\u7684\u8bbf\u95ee\u5c06\u4f1a\u88ab SNAT\u3002 \u5982\u679c Pod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u76f8\u540c\uff0c\u90a3\u6211\u4eec\u5fc5\u987b\u8981\u8bbe\u7f6e masqueradeAll \u4e3a true\u3002","title":"\u57fa\u4e8e kube-proxy \u5b9e\u73b0 service \u8bbf\u95ee"},{"location":"usage/underlay_cni_service-zh_CN/#coordinator-overlay","text":"\u914d\u7f6e coordinator \u4e3a Overlay \u6a21\u5f0f\u540c\u6837\u4e5f\u80fd\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u4f20\u7edf\u7684 Overlay \u7c7b\u578b(\u5982 Calico \u548c Cilium \u7b49)\u7684 CNI \u5df2\u7ecf\u5b8c\u7f8e\u89e3\u51b3\u4e86\u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u5b83\uff0c\u5e2e\u52a9 Underlay Pod \u8bbf\u95ee Service\u3002 \u6211\u4eec\u53ef\u4ee5\u4e3a Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\uff0c eth0 \u4e3a Overlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002 net1 \u4e3a Underlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1 Pod \u5357\u5317\u5411\u6d41\u91cf\u3002 \u901a\u8fc7 coordinator \u8bbe\u7f6e\u7684\u7b56\u7565\u8def\u7531\u8868\u9879 \u786e\u4fdd Pod \u8bbf\u95ee Service \u65f6\u4ece eth0 \u8f6c\u53d1, \u56de\u590d\u62a5\u6587\u4e5f\u8f6c\u53d1\u7ed9 eth0\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u8c03\u7528\u7f51\u5361\u662f\u5426\u4e0d\u662f eth0 \u3002\u5982\u679c\u4e0d\u662f\uff0c\u786e\u8ba4 Pod \u4e2d\u4e0d\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a overlay \u6a21\u5f0f\u3002 \u5728 overlay \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u540c\u6b65\u96c6\u7fa4\u7f3a\u7701 CNI \u7684 Pod \u5b50\u7f51\uff0c\u8fd9\u4e9b\u5b50\u7f51\u7528\u4e8e\u5728\u591a\u7f51\u5361 Pod \u4e2d\u8bbe\u7f6e\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u5b83\u8bbf\u95ee\u7531\u7f3a\u7701 CNI \u521b\u5efa\u7684 Pod \u4e4b\u95f4\u7684\u6b63\u5e38\u901a\u4fe1\u65f6\uff0c\u4ece eth0 \u8f6c\u53d1\u3002\u8fd9\u4e2a\u914d\u7f6e\u5bf9\u5e94 spidercoordinator.spec.podCIDRType \uff0c\u9ed8\u8ba4\u4e3a auto , \u53ef\u9009\u503c: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] \u8fd9\u4e9b\u8def\u7531\u662f\u5728 Pod \u542f\u52a8\u65f6\u6ce8\u5165\u7684\uff0c\u5982\u679c\u76f8\u5173\u7684 CIDR \u53d1\u751f\u4e86\u53d8\u52a8\uff0c\u65e0\u6cd5\u81ea\u52a8\u751f\u6548\u5230\u5df2\u7ecf Running \u7684 Pod \u4e2d\uff0c\u8fd9\u9700\u8981\u91cd\u542f Pod \u624d\u80fd\u751f\u6548\u3002 \u66f4\u591a\u8be6\u60c5\u53c2\u8003 CRD-Spidercoordinator \u5f53\u4ee5 Overlay \u6a21\u5f0f\u521b\u5efa Pod \u5e76\u8fdb\u5165 Pod \u7f51\u7edc\u547d\u4ee4\u7a7a\u95f4\uff0c\u67e5\u770b\u8def\u7531\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip rule 0 : from all lookup local 32759 : from 10 .233.105.154 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32759: from 10.233.105.154 lookup 100 : \u786e\u4fdd\u4ece eth0 (calico \u7f51\u5361)\u53d1\u51fa\u7684\u6570\u636e\u5305\u8d70 table 100\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b: \u9664\u4e86\u9ed8\u8ba4\u8def\u7531\uff0c\u6240\u6709\u8def\u7531\u90fd\u4fdd\u7559\u5728 Main \u8868\uff0c\u4f46\u4f1a\u628a net1 \u7684\u9ed8\u8ba4\u8def\u7531\u79fb\u52a8\u5230 table 100\u3002 \u8fd9\u4e9b\u7b56\u7565\u8def\u7531\u786e\u4fdd\u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0cUnderlay Pod \u4e5f\u80fd\u591f\u6b63\u5e38\u8bbf\u95ee Service\u3002","title":"\u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Overlay \u6a21\u5f0f"},{"location":"usage/underlay_cni_service-zh_CN/#cgroup-ebpf-service","text":"\u4e0a\u9762\u6211\u4eec\u4ecb\u7ecd\u4e86\u5728 Spiderpool \u4e2d, \u6211\u4eec\u901a\u8fc7 coordinator \u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u5230\u4e3b\u673a\u8f6c\u53d1\uff0c \u518d\u7ecf\u8fc7\u4e3b\u673a\u4e0a Kube-proxy \u8bbe\u7f6e\u7684 iptables \u89c4\u5219 DNAT (\u5c06\u76ee\u6807\u5730\u5740\u6539\u4e3a\u76ee\u6807 Pod) \u4e4b\u540e\uff0c\u518d\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u8fd9\u53ef\u4ee5\u867d\u7136\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5ef6\u957f\u4e86\u6570\u636e\u8bbf\u95ee\u8def\u5f84\uff0c\u9020\u6210\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u5931\u3002 \u793e\u533a\u5f00\u6e90\u7684 CNI \u9879\u76ee: Cilium \u652f\u6301\u57fa\u4e8e eBPF \u6280\u672f\u5b8c\u5168\u66ff\u4ee3 kube-proxy \u7cfb\u7edf\u7ec4\u4ef6\u3002\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u89e3\u6790 Service\u3002\u5f53\u8bbf\u95ee Service \u65f6\uff0cService \u5730\u5740\u4f1a\u88ab Cilium \u6302\u8f7d\u7684 eBPF \u7a0b\u5e8f\u76f4\u63a5\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u7684 IP\uff0c\u8fd9\u6837\u6e90 Pod \u76f4\u63a5\u5bf9\u76ee\u6807 Pod \u53d1\u8d77\u8bbf\u95ee\uff0c\u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff0c\u6781\u5927\u7684\u7f29\u77ed\u4e86\u8bbf\u95ee\u8def\u5f84\uff0c\u4ece\u800c\u5b9e\u73b0\u52a0\u901f\u8bbf\u95ee Service\u3002\u501f\u52a9\u4e8e\u5f3a\u5927\u7684 Cilium\uff0c \u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b83\u5b9e\u73b0\u52a0\u901f Underlay CNI\u7684 Service \u8bbf\u95ee\u3002 \u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u76f8\u6bd4 kube proxy \u89e3\u6790\u65b9\u5f0f\uff0ccgroup eBPF \u65b9\u5f0f\u7684 \u7f51\u7edc\u5ef6\u65f6\u6709\u6700\u5927 25% \u7684\u6539\u5584\uff0c\u7f51\u7edc\u541e\u5410\u6709 50% \u7684\u63d0\u9ad8 \u3002 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e Macvlan CNI + Cilium \u52a0\u901f\u8bbf\u95ee Service\uff1a \u6ce8\u610f: \u9700\u8981\u786e\u4fdd\u96c6\u7fa4\u8282\u70b9\u7684\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u63d0\u524d\u51c6\u5907\u597d\u4e00\u4e2a\u672a\u5b89\u88c5 kube-proxy \u7ec4\u4ef6\u7684\u96c6\u7fa4\uff0c\u5982\u679c\u5df2\u7ecf\u5b89\u88c5 kube-proxy \u53ef\u53c2\u8003\u4e00\u4e0b\u547d\u4ee4\u5220\u9664 kube-proxy \u7ec4\u4ef6 ~# kubectl delete ds -n kube-system kube-proxy ~# # \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c ~# iptables-save | grep -v KUBE | iptables-restore \u4e5f\u53ef\u4ee5\u4f7f\u7528 kubeadm \u5b89\u88c5\u4e00\u4e2a\u65b0\u96c6\u7fa4, \u6ce8\u610f\u4e0d\u8981\u5b89\u88c5 kube-proxy: ~# kubeadm init --skip-phases = addon/kube-proxy \u5b89\u88c5 Cilium \u7ec4\u4ef6\uff0c\u6ce8\u610f\u5f00\u542f kube-proxy replacement \u529f\u80fd ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } \u5b89\u88c5\u5b8c\u6210\uff0c\u68c0\u67e5\u5b89\u88c5\u72b6\u6001\uff1a ~# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m \u5b89\u88c5 Spiderpool, \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none \u8bbe\u7f6e coordinator.podCIDRType=none, spiderpool \u5c06\u4e0d\u4f1a\u83b7\u53d6\u96c6\u7fa4\u7684 ServiceCIDR\u3002\u5728\u521b\u5efa Pod \u65f6\u4e5f\u5c31\u4e0d\u4f1a\u6ce8\u5165 Service \u76f8\u5173\u8def\u7531 \u8fd9\u6837\u8bbf\u95ee Service \u5b8c\u5168\u4f9d\u8d56 Cilium kube-proxy Replacement\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u521b\u5efa macvlan \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF \u9700\u8981\u786e\u4fdd ens192 \u5b58\u5728\u4e8e\u96c6\u7fa4\u8282\u70b9 \u5efa\u8bae\u8bbe\u7f6e enableCoordinator \u4e3a true, \u8fd9\u53ef\u4ee5\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u6d4b\u7684\u95ee\u9898 \u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\u7528\u4e8e\u6d4b\u8bd5\uff1a ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network : kube-system/macvlan-ens192\" NAME=ipvlan cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : DaemonSet metadata : name : ${NAME} labels : app : $NAME spec : selector : matchLabels : app : $NAME template : metadata : name : $NAME labels : app : $NAME annotations : ${ANNOTATION_MULTUS} spec : containers : - name : test-app image : nginx imagePullPolicy : IfNotPresent ports : - name : http containerPort : 80 protocol : TCP --- apiVersion : v1 kind : Service metadata : name : ${NAME} spec : - ports : name : http port : 80 protocol : TCP targetPort : 80 selector : app : ${NAME} type : ClusterIP EOF \u9a8c\u8bc1\u8bbf\u95ee Service \u7684\u8054\u901a\u6027\uff0c\u5e76\u67e5\u770b\u6027\u80fd\u662f\u5426\u63d0\u5347 ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes \u53e6\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\uff0c\u901a\u8fc7 tcpdump \u5de5\u5177\u67e5\u770b\u5230\u8bbf\u95ee Service \u7684\u6570\u636e\u5305\u4ece Pod \u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u53d1\u51fa\u65f6\uff0c\u76ee\u6807\u5730\u5740\u5df2\u7ecf\u88ab\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u5730\u5740: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 \u662f\u6e90 Pod \u7684 IP, 10.6.185.210 \u662f\u76ee\u6807 Pod \u7684 IP\uff0c\u53ef\u4ee5\u786e\u8ba4 Cilium \u89e3\u6790\u4e86 Service \u7684 IP\u3002 \u4f7f\u7528 sockperf \u5de5\u5177\u6d4b\u8bd5\u4f7f\u7528 Cilium \u52a0\u901f\u524d\u540e\uff0c \u6d4b\u5f97 Pod \u8de8\u8282\u70b9\u8bbf\u95ee ClusterIP \u7684\u6570\u636e\u5bf9\u6bd4: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 \u6839\u636e\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7 Cilium kube-proxy replacement \u4e4b\u540e\uff0c\u8bbf\u95ee Service \u5927\u7ea6\u52a0\u901f 30%\u3002\u66f4\u591a\u6d4b\u8bd5\u6570\u636e\u53c2\u8003 \u7f51\u7edc IO \u6027\u80fd","title":"\u57fa\u4e8e cgroup eBPF \u5b9e\u73b0 service \u8bbf\u95ee"},{"location":"usage/underlay_cni_service-zh_CN/#_2","text":"Underlay CNI \u8bbf\u95ee Service \u6709\u4ee5\u4e0a\u4e24\u79cd\u65b9\u6848\u89e3\u51b3\u3002kube-proxy \u7684\u65b9\u5f0f\u66f4\u52a0\u5e38\u7528\u7a33\u5b9a\uff0c\u5927\u90e8\u5206\u73af\u5883\u90fd\u53ef\u4ee5\u7a33\u5b9a\u4f7f\u7528\u3002 cgroup eBPF \u4e3a Underlay CNI \u8bbf\u95ee Service \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u53ef\u9009\u65b9\u6848\uff0c\u5e76\u4e14\u52a0\u901f\u4e86 Service \u8bbf\u95ee\uff0c\u5c3d\u7ba1\u8fd9\u6709\u4e00\u5b9a\u4f7f\u7528\u9650\u5236\u53ca\u95e8\u69db\uff0c\u4f46\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u7684\u9700\u6c42\u3002","title":"\u7ed3\u8bba"},{"location":"usage/underlay_cni_service/","text":"Access to service for underlay CNI English | \u7b80\u4f53\u4e2d\u6587 Introduction At present, most Underlay-type CNIs (such as Macvlan, IPVlan, Sriov-CNI, etc.) In the community are generally connected to the underlying network, and often do not natively support accessing the Service of the cluster. This is mostly because underlay Pod access to the Service needs to be forwarded through the gateway of the switch. However, there is no route to the Service on the gateway, so the packets accessing the Service cannot be routed correctly, resulting in packet loss. Spiderpool provides the following two solutions to solve the problem of Underlay CNI accessing Service: Use kube-proxy to access Service Use cgroup eBPF to access Service Both of these ways solve the problem that Underlay CNI cannot access Service, but the implementation principle is somewhat different. Below we will introduce these two ways: Access service by kube-proxy Spiderpool has a built-in plugin called coordinator , which helps us seamlessly integrate with kube-proxy to achieve Underlay CNI access to Service. Depending on different scenarios, the coordinator can run in either underlay or overlay mode. Although the implementation methods are slightly different, the core principle is to hijack the traffic of Pods accessing Services onto the host network protocol stack and then forward it through the IPtables rules created by Kube-proxy. The following is a brief introduction to the data forwarding process flowchart: coordinator run in underlay Under this mode, the coordinator plugin will create a pair of Veth devices, with one end placed in the host and the other end placed in the network namespace of the Pod. Then set some routing rules inside the Pod to forward access to ClusterIP from the veth device. The coordinator defaults to auto mode, which will automatically determine whether to run in underlay or overlay mode. You only need to inject an annotation into the Pod: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> . After creating a Pod in Underlay mode, we enter the Pod and check the routing information: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 is the node's IP, which ensure where the Pod access the same node via veth0 . 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 is cluster service subnet, which ensure the Pod access ClusterIP via veth0 . This solution heavily relies on the MASQUERADE of kube-proxy, otherwise the reply packets will be directly forwarded to the source Pod, and if they pass through some security devices, the packets will be dropped. Therefore, in some special scenarios, we need to set masqueradeAll of kube-proxy to true. By default, the underlay subnet of a Pod is different from the clusterCIDR of the cluster, so there is no need to enable masqueradeAll , and access between them will be SNATed. If the underlay subnet of a Pod is the same as the clusterCIDR of the cluster, then we must set masqueradeAll to true. coordinator run in overlay Configuring coordinator as Overlay mode can also solve the problem of Underlay CNI accessing Service. The traditional Overlay type (such as Calico and Cilium etc.) CNI has perfectly solved the access to Service problem. We can use it to help Underlay Pods access Service. We can attach multiple network cards to the Pod, eth0 for creating by Overlay CNI, net1 for creating by Underlay CNI, and set up policy routing table items through coordinator to ensure that when a Pod accesses Service, it forwards from eth0 , and replies are also forwarded to eth0 . By default, the value of mode is auto(spidercoordinator CR spec.mode is auto), coordinator will automatically determine whether the current CNI call is not eth0 . If it's not, confirm that there is no veth0 network card in the Pod, then automatically determine it as overlay mode. In overlay mode, Spiderpool will automatically synchronize the cluster default CNI Pod subnets, which are used to set routes in multi-network card Pods to enable it to communicate normally with Pods created by the default CNI when it accesses Service from eth0 . This configuration corresponds to spidercoordinator.spec.podCIDRType , the default is auto , optional values: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] These routes are injected at the start of the Pod, and if the related CIDR changes, it cannot automatically take effect on already running Pods, this requires restarting the Pod to take effect. For more details, please refer to CRD-Spidercoordinator When creating a Pod in Overlay mode and entering the Pod network command space, view the routing information: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # ip rule 0 : from all lookup local 32765 : from 10 .6.212.227 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32762: from all to 10.233.64.0/18 lookup 100 : Ensure that when Pods access ClusterIP, they go through table 100 and are forwarded out from eth0 . In the default configuration: Except for the default route, all routes are retained in the Main table, but the default route for 'net1' is moved to table 100. These policy routes ensure that Underlay Pods can also normally access Service in multi-network card scenarios. Access service by cgroup eBPF In Spiderpool, we hijack the traffic of Pods accessing Services through a coordinator that forwards it to the host and then through the iptables rules set up by the host's Kube-proxy. This can solve the problem but may extend the data access path and cause some performance loss. The open-source CNI project, Cilium, supports replacing the kube-proxy system component entirely with eBPF technology. It can help us resolve Service addresses. When pod accessing a Service, the Service address will be directly resolved by the eBPF program mounted by Cilium on the target Pod, so that the source Pod can directly initiate access to the target Pod without going through the host's network protocol stack. This greatly shortens the access path and achieves acceleration in accessing Service. With the power of Cilium, we can also implement acceleration in accessing Service under the Underlay CNI through it. After testing, compared with kube-proxy manner, cgroup eBPF solution has an improvement of the performance Up to 25% on network delay, up to 50% on network throughput . The following steps demonstrate how to accelerate access to a Service on a cluster with 2 nodes based on Macvlan CNI + Cilium: NOTE: Please ensure that the kernel version of the cluster nodes is at least greater than 4.19 Prepare a cluster without the kube-proxy component installed in advance. If kube-proxy is already installed, you can refer to the following commands to remove the kube-proxy component: ~# kubectl delete ds -n kube-system kube-proxy ~# # run the command on every node ~# iptables-save | grep -v KUBE | iptables-restore To install the Cilium component, make sure to enable the kube-proxy replacement feature: ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } The installation is complete, check the pod's state: \uff5e# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m To install Spiderpool, see Install to install Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none set coordinator.podCIDRType=none, the spiderpool will not get the cluster's ServiceCIDR. Service-related routes are also not injected when pods are created. Access to the Service in this way is entirely dependent on Cilium kube-proxy Replacement. show the installation of Spiderpool: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Create a MacVLAN-related Multus configuration and create a companion IPPools resource: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF needs to ensure that ens192 exists on the cluster nodes recommends setting enableCoordinator to true, which can resolve issues with pod health detection Create a set of cross-node DaemonSet apps for testing: ANNOTATION_MULTUS = \"v1.multus-cni.io/default-network: kube-system/macvlan-ens192\" NAME = ipvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: ${NAME} spec: - ports: name: http port: 80 protocol: TCP targetPort: 80 selector: app: ${NAME} type: ClusterIP EOF Verify the connectivity of the access service and see if the performance is improved: ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes Open another terminal, enter the network space of the pod, and use the tcpdump tool to see that when the packet accessing the service is sent from the pod network namespace, the destination address has been resolved to the target pod address: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 is the IP of the source pod and 10.6.185.210 is the IP of the destination pod. Before and after using the sockperf tool to test the Cilium acceleration, the data comparison of pods accessing ClusterIP across nodes is obtained: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 According to the results, after Cilium kube-proxy replacement, access to the service is accelerated by about 30%. For more test data, please refer to Network I/O Performance Conclusion There are two solutions to the Underlay CNI Access Service. The kube-proxy method is more commonly used and stable, and can be used stably in most environments. cgroup eBPF is an alternative option for Underlay CNI to access the Service and accelerates Service access. Although there are certain restrictions and thresholds for use, it can meet the needs of users in specific scenarios.","title":"Access Service for Underlay CNI"},{"location":"usage/underlay_cni_service/#access-to-service-for-underlay-cni","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Access to service for underlay CNI"},{"location":"usage/underlay_cni_service/#introduction","text":"At present, most Underlay-type CNIs (such as Macvlan, IPVlan, Sriov-CNI, etc.) In the community are generally connected to the underlying network, and often do not natively support accessing the Service of the cluster. This is mostly because underlay Pod access to the Service needs to be forwarded through the gateway of the switch. However, there is no route to the Service on the gateway, so the packets accessing the Service cannot be routed correctly, resulting in packet loss. Spiderpool provides the following two solutions to solve the problem of Underlay CNI accessing Service: Use kube-proxy to access Service Use cgroup eBPF to access Service Both of these ways solve the problem that Underlay CNI cannot access Service, but the implementation principle is somewhat different. Below we will introduce these two ways:","title":"Introduction"},{"location":"usage/underlay_cni_service/#access-service-by-kube-proxy","text":"Spiderpool has a built-in plugin called coordinator , which helps us seamlessly integrate with kube-proxy to achieve Underlay CNI access to Service. Depending on different scenarios, the coordinator can run in either underlay or overlay mode. Although the implementation methods are slightly different, the core principle is to hijack the traffic of Pods accessing Services onto the host network protocol stack and then forward it through the IPtables rules created by Kube-proxy. The following is a brief introduction to the data forwarding process flowchart:","title":"Access service by kube-proxy"},{"location":"usage/underlay_cni_service/#coordinator-run-in-underlay","text":"Under this mode, the coordinator plugin will create a pair of Veth devices, with one end placed in the host and the other end placed in the network namespace of the Pod. Then set some routing rules inside the Pod to forward access to ClusterIP from the veth device. The coordinator defaults to auto mode, which will automatically determine whether to run in underlay or overlay mode. You only need to inject an annotation into the Pod: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> . After creating a Pod in Underlay mode, we enter the Pod and check the routing information: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 is the node's IP, which ensure where the Pod access the same node via veth0 . 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 is cluster service subnet, which ensure the Pod access ClusterIP via veth0 . This solution heavily relies on the MASQUERADE of kube-proxy, otherwise the reply packets will be directly forwarded to the source Pod, and if they pass through some security devices, the packets will be dropped. Therefore, in some special scenarios, we need to set masqueradeAll of kube-proxy to true. By default, the underlay subnet of a Pod is different from the clusterCIDR of the cluster, so there is no need to enable masqueradeAll , and access between them will be SNATed. If the underlay subnet of a Pod is the same as the clusterCIDR of the cluster, then we must set masqueradeAll to true.","title":"coordinator run in underlay"},{"location":"usage/underlay_cni_service/#coordinator-run-in-overlay","text":"Configuring coordinator as Overlay mode can also solve the problem of Underlay CNI accessing Service. The traditional Overlay type (such as Calico and Cilium etc.) CNI has perfectly solved the access to Service problem. We can use it to help Underlay Pods access Service. We can attach multiple network cards to the Pod, eth0 for creating by Overlay CNI, net1 for creating by Underlay CNI, and set up policy routing table items through coordinator to ensure that when a Pod accesses Service, it forwards from eth0 , and replies are also forwarded to eth0 . By default, the value of mode is auto(spidercoordinator CR spec.mode is auto), coordinator will automatically determine whether the current CNI call is not eth0 . If it's not, confirm that there is no veth0 network card in the Pod, then automatically determine it as overlay mode. In overlay mode, Spiderpool will automatically synchronize the cluster default CNI Pod subnets, which are used to set routes in multi-network card Pods to enable it to communicate normally with Pods created by the default CNI when it accesses Service from eth0 . This configuration corresponds to spidercoordinator.spec.podCIDRType , the default is auto , optional values: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] These routes are injected at the start of the Pod, and if the related CIDR changes, it cannot automatically take effect on already running Pods, this requires restarting the Pod to take effect. For more details, please refer to CRD-Spidercoordinator When creating a Pod in Overlay mode and entering the Pod network command space, view the routing information: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # ip rule 0 : from all lookup local 32765 : from 10 .6.212.227 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32762: from all to 10.233.64.0/18 lookup 100 : Ensure that when Pods access ClusterIP, they go through table 100 and are forwarded out from eth0 . In the default configuration: Except for the default route, all routes are retained in the Main table, but the default route for 'net1' is moved to table 100. These policy routes ensure that Underlay Pods can also normally access Service in multi-network card scenarios.","title":"coordinator run in overlay"},{"location":"usage/underlay_cni_service/#access-service-by-cgroup-ebpf","text":"In Spiderpool, we hijack the traffic of Pods accessing Services through a coordinator that forwards it to the host and then through the iptables rules set up by the host's Kube-proxy. This can solve the problem but may extend the data access path and cause some performance loss. The open-source CNI project, Cilium, supports replacing the kube-proxy system component entirely with eBPF technology. It can help us resolve Service addresses. When pod accessing a Service, the Service address will be directly resolved by the eBPF program mounted by Cilium on the target Pod, so that the source Pod can directly initiate access to the target Pod without going through the host's network protocol stack. This greatly shortens the access path and achieves acceleration in accessing Service. With the power of Cilium, we can also implement acceleration in accessing Service under the Underlay CNI through it. After testing, compared with kube-proxy manner, cgroup eBPF solution has an improvement of the performance Up to 25% on network delay, up to 50% on network throughput . The following steps demonstrate how to accelerate access to a Service on a cluster with 2 nodes based on Macvlan CNI + Cilium: NOTE: Please ensure that the kernel version of the cluster nodes is at least greater than 4.19 Prepare a cluster without the kube-proxy component installed in advance. If kube-proxy is already installed, you can refer to the following commands to remove the kube-proxy component: ~# kubectl delete ds -n kube-system kube-proxy ~# # run the command on every node ~# iptables-save | grep -v KUBE | iptables-restore To install the Cilium component, make sure to enable the kube-proxy replacement feature: ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } The installation is complete, check the pod's state: \uff5e# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m To install Spiderpool, see Install to install Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none set coordinator.podCIDRType=none, the spiderpool will not get the cluster's ServiceCIDR. Service-related routes are also not injected when pods are created. Access to the Service in this way is entirely dependent on Cilium kube-proxy Replacement. show the installation of Spiderpool: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Create a MacVLAN-related Multus configuration and create a companion IPPools resource: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF needs to ensure that ens192 exists on the cluster nodes recommends setting enableCoordinator to true, which can resolve issues with pod health detection Create a set of cross-node DaemonSet apps for testing: ANNOTATION_MULTUS = \"v1.multus-cni.io/default-network: kube-system/macvlan-ens192\" NAME = ipvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: ${NAME} spec: - ports: name: http port: 80 protocol: TCP targetPort: 80 selector: app: ${NAME} type: ClusterIP EOF Verify the connectivity of the access service and see if the performance is improved: ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes Open another terminal, enter the network space of the pod, and use the tcpdump tool to see that when the packet accessing the service is sent from the pod network namespace, the destination address has been resolved to the target pod address: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 is the IP of the source pod and 10.6.185.210 is the IP of the destination pod. Before and after using the sockperf tool to test the Cilium acceleration, the data comparison of pods accessing ClusterIP across nodes is obtained: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 According to the results, after Cilium kube-proxy replacement, access to the service is accelerated by about 30%. For more test data, please refer to Network I/O Performance","title":"Access service by cgroup eBPF"},{"location":"usage/underlay_cni_service/#conclusion","text":"There are two solutions to the Underlay CNI Access Service. The kube-proxy method is more commonly used and stable, and can be used stably in most environments. cgroup eBPF is an alternative option for Underlay CNI to access the Service and accelerates Service access. Although there are certain restrictions and thresholds for use, it can meet the needs of users in specific scenarios.","title":"Conclusion"},{"location":"usage/install/certificate/","text":"Certificates Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways. Auto certificates Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = auto Note that the default value of parameter spiderpoolController.tls.method is auto . Provided certificates If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA = ` cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT = ` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY = ` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = provided \\ --set spiderpoolController.tls.provided.tlsCa = ${ CA } \\ --set spiderpoolController.tls.provided.tlsCert = ${ SERVER_CERT } \\ --set spiderpoolController.tls.provided.tlsKey = ${ SERVER_KEY } Cert-manager certificates It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = certmanager \\ --set spiderpoolController.tls.certmanager.issuerName = ${ CERT_MANAGER_ISSUER_NAME }","title":"Certificates"},{"location":"usage/install/certificate/#certificates","text":"Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways.","title":"Certificates"},{"location":"usage/install/certificate/#auto-certificates","text":"Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = auto Note that the default value of parameter spiderpoolController.tls.method is auto .","title":"Auto certificates"},{"location":"usage/install/certificate/#provided-certificates","text":"If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA = ` cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT = ` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY = ` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = provided \\ --set spiderpoolController.tls.provided.tlsCa = ${ CA } \\ --set spiderpoolController.tls.provided.tlsCert = ${ SERVER_CERT } \\ --set spiderpoolController.tls.provided.tlsKey = ${ SERVER_KEY }","title":"Provided certificates"},{"location":"usage/install/certificate/#cert-manager-certificates","text":"It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = certmanager \\ --set spiderpoolController.tls.certmanager.issuerName = ${ CERT_MANAGER_ISSUER_NAME }","title":"Cert-manager certificates"},{"location":"usage/install/get-started-kind-zh_CN/","text":"Quick Start English | \u7b80\u4f53\u4e2d\u6587 Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u90e8\u7f72\u7b26\u5408\u60a8\u9700\u6c42\u7684\u96c6\u7fa4\uff0c\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002 \u5b89\u88c5\u8981\u6c42 \u83b7\u53d6 Spiderpool \u7a33\u5b9a\u7248\u672c\u7684\u4ee3\u7801\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002 ~# LATEST_RELEASE_VERISON = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | grep '\"tag_name\":' | grep -v rc | grep -Eo \"([0-9]+\\.[0-9]+\\.[0-9])\" | sort -r | head -n 1 ) ~# curl -Lo /tmp/ $LATEST_RELEASE_VERISON .tar.gz https://github.com/spidernet-io/spiderpool/archive/refs/tags/v $LATEST_RELEASE_VERISON .tar.gz ~# tar -xvf /tmp/ $LATEST_RELEASE_VERISON .tar.gz -C /tmp/ ~# cd /tmp/spiderpool- $LATEST_RELEASE_VERISON \u6267\u884c make dev-doctor \uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\u3002 \u6784\u5efa Spiderpool \u73af\u5883\u9700\u8981\u5177\u5907 Kubectl\u3001Kind\u3001Docker\u3001Helm\u3001yq \u5de5\u5177\u3002\u5982\u679c\u4f60\u7684\u672c\u673a\u4e0a\u7f3a\u5c11\uff0c\u8bf7\u8fd0\u884c test/scripts/install-tools.sh \u6765\u5b89\u88c5\u5b83\u4eec\u3002 \u5feb\u901f\u542f\u52a8 \u5982\u679c\u60a8\u5728\u4e2d\u56fd\u5927\u9646\uff0c\u5b89\u88c5\u65f6\u53ef\u4ee5\u989d\u5916\u6307\u5b9a\u53c2\u6570 -e E2E_CHINA_IMAGE_REGISTRY=true \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u521b\u5efa Spiderpool \u5355 CNI \u96c6\u7fa4 \u521b\u5efa Spiderpool \u548c Calico \u7684\u53cc CNI \u96c6\u7fa4 \u521b\u5efa Spiderpool \u548c Cilium \u7684\u53cc CNI \u96c6\u7fa4 \u5728\u8be5\u573a\u666f\u4e0b\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5373\u53ef\u8ba9\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740\uff0c\u540c\u65f6 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1\u3002\u5177\u4f53\u53ef\u53c2\u8003 \u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u5982\u4e0b\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a Macvlan \u7684\u5355 CNI \u96c6\u7fa4\uff0c\u5176\u4e2d\uff0c\u901a\u8fc7 kube-proxy \u5b9e\u65bd service \u89e3\u6790 ~# make setup_singleCni_macvlan \u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u4f60\u53ef\u4ee5\u4f53\u9a8c Pod \u5177\u5907\u53cc CNI \u7f51\u5361\u7684\u6548\u679c\u3002\u5177\u4f53\u53ef\u53c2\u8003 underlay CNI \u548c overlay CNI \u534f\u540c \u5982\u4e0b\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a Calico \u4e3a main CNI \uff0c\u5e76\u642d\u914d Spiderpool \u4e3a POD \u63a5\u5165\u7b2c\u4e8c\u5f20 underlay \u7f51\u5361\uff0c\u5176\u4e2d Calico \u57fa\u4e8e iptables datapath \u5de5\u4f5c\uff0c\u57fa\u4e8e kube-proxy \u5b9e\u73b0 service \u89e3\u6790\u3002 ~# make setup_dualCni_calico \u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u4f60\u53ef\u4ee5\u4f53\u9a8c Pod \u5177\u5907\u53cc CNI \u7f51\u5361\u7684\u6548\u679c\u3002\u5177\u4f53\u53ef\u53c2\u8003 underlay CNI \u548c overlay CNI \u534f\u540c \u5982\u4e0b\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a Cilium \u4e3a main CNI \uff0c\u5e76\u642d\u914d Spiderpool \u4e3a POD \u63a5\u5165\u7b2c\u4e8c\u5f20 underlay \u7f51\u5361\uff0c\u5176\u4e2d\u5f00\u542f\u4e86 Cilium \u7684 eBPF \u52a0\u901f\uff0c\u5e76\u5173\u95ed\u4e86 kube-proxy \u7ec4\u4ef6\uff0c\u57fa\u4e8e eBPF \u5b9e\u73b0 service \u89e3\u6790\u3002 \u786e\u8ba4\u64cd\u4f5c\u7cfb\u7edf Kernel \u7248\u672c\u53f7\u662f\u662f\u5426 >= 4.9.17\uff0c\u5185\u6838\u8fc7\u4f4e\u65f6\u5c06\u4f1a\u5bfc\u81f4\u5b89\u88c5\u5931\u8d25\uff0c\u63a8\u8350 Kernel 5.10+ \u3002 ~# make setup_dualCni_cilium \u9a8c\u8bc1\u5b89\u88c5 \u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002 ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config \u60a8\u53ef\u4ee5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectl get po -n kube-system | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> \u90e8\u7f72\u5e94\u7528 \u901a\u8fc7\u4e0a\u8ff0\u68c0\u67e5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\u3002\u5728\u672c\u7ae0\u8282\uff0c\u5c06\u4ecb\u7ecd\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u53bb\u4f7f\u7528 Spiderpool \u3002 Spiderpool \u63d0\u4f9b\u4e86 Spidermultusconfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f00\u6e90\u9879\u76ee Multus CNI \u914d\u7f6e\u7ba1\u7406\u7684\u6269\u5c55\u3002 \u57fa\u4e8e Spiderpool \u5355 CNI \u73af\u5883 \u57fa\u4e8e Spiderpool \u548c Calico \u7684\u53cc CNI \u73af\u5883 \u57fa\u4e8e Spiderpool \u548c Cilium \u7684\u53cc CNI \u73af\u5883 \u83b7\u53d6\u96c6\u7fa4\u7684 Spidermultusconfig CR \u4e0e IPPool CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system macvlan-vlan0 1h kube-system macvlan-vlan100 1h kube-system macvlan-vlan200 1h ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true ... \u521b\u5efa\u5e94\u7528\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u901a\u8fc7\u5b83\u6307\u5b9a Spidermultusconfig CR: kube-system/macvlan-vlan0 \uff0c\u5e76\u901a\u8fc7\u8be5\u914d\u7f6e\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u7531 Macvlan \u914d\u7f6e\u7f51\u7edc\u7684\u9ed8\u8ba4\u7f51\u5361 (eth0) \u3002 ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9 IP \u4e0e\u5e94\u7528\u7684\u9ed8\u8ba4\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF \u9a8c\u8bc1\u5e94\u7528\u521b\u5efa\u6210\u529f\u3002 ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-7fdbb59666-4k5m7 1 /1 Running 0 9s 172 .18.40.223 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 3 : eth0@if339: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 0a:96:54:6f:76:b4 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.223/16 brd 172 .18.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 4a:8b:09:d9:4c:0a brd ff:ff:ff:ff:ff:ff \u83b7\u53d6\u96c6\u7fa4\u7684 Spidermultusconfig CR \u4e0e IPPool CR ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system calico 3m11s kube-system macvlan-vlan0 2m20s kube-system macvlan-vlan100 2m19s kube-system macvlan-vlan200 2m19s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... \u521b\u5efa\u5e94\u7528\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa\u4e00\u4e2a\u5177\u5907\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a \u9ed8\u8ba4\u7f51\u5361\uff08eth0\uff09\u7531\u96c6\u7fa4\u7f3a\u7701 CNI Calico \u914d\u7f6e\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u901a\u8fc7\u8be5\u6ce8\u89e3\u989d\u5916\u4e3a\u5e94\u7528\u989d\u5916\u518d\u521b\u5efa\u4e00\u5f20\u7531 Macvlan \u914d\u7f6e\u7f51\u7edc\u7684\u7f51\u5361 (net1) \u3002 ipam.spidernet.io/ippools \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9 IP \u4e0e\u5e94\u7528\u7684 net1 \u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF \u9a8c\u8bc1\u5e94\u7528\u521b\u5efa\u6210\u529f ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-bv6rm 1 /1 Running 0 12s 10 .243.104.211 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 1a:1e:e1:f3:f9:4b brd ff:ff:ff:ff:ff:ff inet 10 .243.104.211/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 56 :b4:3d:a6:d2:d1 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.154/16 brd 172 .18.255.255 scope global net1 \u83b7\u53d6\u96c6\u7fa4\u7684 Spidermultusconfig CR \u4e0e IPPool CR ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system cilium 5m32s kube-system macvlan-vlan0 5m12s kube-system macvlan-vlan100 5m17s kube-system macvlan-vlan200 5m18s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... \u521b\u5efa\u5e94\u7528\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa\u4e00\u4e2a\u5177\u5907\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a \u9ed8\u8ba4\u7f51\u5361\uff08eth0\uff09\u7531\u96c6\u7fa4\u7f3a\u7701 CNI Cilium \u914d\u7f6e\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u901a\u8fc7\u8be5\u6ce8\u89e3\u989d\u5916\u4e3a\u5e94\u7528\u989d\u5916\u518d\u521b\u5efa\u4e00\u5f20\u7531 Macvlan \u914d\u7f6e\u7f51\u7edc\u7684\u7f51\u5361 (net1) \u3002 ipam.spidernet.io/ippools \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9 IP \u4e0e\u5e94\u7528\u7684 net1 \u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF \u9a8c\u8bc1\u5e94\u7528\u521b\u5efa\u6210\u529f ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-ml8d9 1 /1 Running 0 58s 10 .244.102.212 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 26 :f1:88:f9:7d:d7 brd ff:ff:ff:ff:ff:ff inet 10 .244.102.212/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether ca:71:99:ec:ec:28 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.228/16 brd 172 .18.255.255 scope global net1 \u73b0\u5728\u60a8\u53ef\u4ee5\u57fa\u4e8e Kind \u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684 \u66f4\u591a\u529f\u80fd \u3002 \u5378\u8f7d \u5378\u8f7d Kind \u96c6\u7fa4 \u6267\u884c make clean \u5378\u8f7d Kind \u96c6\u7fa4\u3002 \u5220\u9664\u6d4b\u8bd5\u955c\u50cf ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Quick Start"},{"location":"usage/install/get-started-kind-zh_CN/#quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u90e8\u7f72\u7b26\u5408\u60a8\u9700\u6c42\u7684\u96c6\u7fa4\uff0c\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002","title":"Quick Start"},{"location":"usage/install/get-started-kind-zh_CN/#_1","text":"\u83b7\u53d6 Spiderpool \u7a33\u5b9a\u7248\u672c\u7684\u4ee3\u7801\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002 ~# LATEST_RELEASE_VERISON = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | grep '\"tag_name\":' | grep -v rc | grep -Eo \"([0-9]+\\.[0-9]+\\.[0-9])\" | sort -r | head -n 1 ) ~# curl -Lo /tmp/ $LATEST_RELEASE_VERISON .tar.gz https://github.com/spidernet-io/spiderpool/archive/refs/tags/v $LATEST_RELEASE_VERISON .tar.gz ~# tar -xvf /tmp/ $LATEST_RELEASE_VERISON .tar.gz -C /tmp/ ~# cd /tmp/spiderpool- $LATEST_RELEASE_VERISON \u6267\u884c make dev-doctor \uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\u3002 \u6784\u5efa Spiderpool \u73af\u5883\u9700\u8981\u5177\u5907 Kubectl\u3001Kind\u3001Docker\u3001Helm\u3001yq \u5de5\u5177\u3002\u5982\u679c\u4f60\u7684\u672c\u673a\u4e0a\u7f3a\u5c11\uff0c\u8bf7\u8fd0\u884c test/scripts/install-tools.sh \u6765\u5b89\u88c5\u5b83\u4eec\u3002","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/get-started-kind-zh_CN/#_2","text":"\u5982\u679c\u60a8\u5728\u4e2d\u56fd\u5927\u9646\uff0c\u5b89\u88c5\u65f6\u53ef\u4ee5\u989d\u5916\u6307\u5b9a\u53c2\u6570 -e E2E_CHINA_IMAGE_REGISTRY=true \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u521b\u5efa Spiderpool \u5355 CNI \u96c6\u7fa4 \u521b\u5efa Spiderpool \u548c Calico \u7684\u53cc CNI \u96c6\u7fa4 \u521b\u5efa Spiderpool \u548c Cilium \u7684\u53cc CNI \u96c6\u7fa4 \u5728\u8be5\u573a\u666f\u4e0b\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5373\u53ef\u8ba9\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740\uff0c\u540c\u65f6 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1\u3002\u5177\u4f53\u53ef\u53c2\u8003 \u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u5982\u4e0b\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a Macvlan \u7684\u5355 CNI \u96c6\u7fa4\uff0c\u5176\u4e2d\uff0c\u901a\u8fc7 kube-proxy \u5b9e\u65bd service \u89e3\u6790 ~# make setup_singleCni_macvlan \u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u4f60\u53ef\u4ee5\u4f53\u9a8c Pod \u5177\u5907\u53cc CNI \u7f51\u5361\u7684\u6548\u679c\u3002\u5177\u4f53\u53ef\u53c2\u8003 underlay CNI \u548c overlay CNI \u534f\u540c \u5982\u4e0b\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a Calico \u4e3a main CNI \uff0c\u5e76\u642d\u914d Spiderpool \u4e3a POD \u63a5\u5165\u7b2c\u4e8c\u5f20 underlay \u7f51\u5361\uff0c\u5176\u4e2d Calico \u57fa\u4e8e iptables datapath \u5de5\u4f5c\uff0c\u57fa\u4e8e kube-proxy \u5b9e\u73b0 service \u89e3\u6790\u3002 ~# make setup_dualCni_calico \u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u4f60\u53ef\u4ee5\u4f53\u9a8c Pod \u5177\u5907\u53cc CNI \u7f51\u5361\u7684\u6548\u679c\u3002\u5177\u4f53\u53ef\u53c2\u8003 underlay CNI \u548c overlay CNI \u534f\u540c \u5982\u4e0b\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a Cilium \u4e3a main CNI \uff0c\u5e76\u642d\u914d Spiderpool \u4e3a POD \u63a5\u5165\u7b2c\u4e8c\u5f20 underlay \u7f51\u5361\uff0c\u5176\u4e2d\u5f00\u542f\u4e86 Cilium \u7684 eBPF \u52a0\u901f\uff0c\u5e76\u5173\u95ed\u4e86 kube-proxy \u7ec4\u4ef6\uff0c\u57fa\u4e8e eBPF \u5b9e\u73b0 service \u89e3\u6790\u3002 \u786e\u8ba4\u64cd\u4f5c\u7cfb\u7edf Kernel \u7248\u672c\u53f7\u662f\u662f\u5426 >= 4.9.17\uff0c\u5185\u6838\u8fc7\u4f4e\u65f6\u5c06\u4f1a\u5bfc\u81f4\u5b89\u88c5\u5931\u8d25\uff0c\u63a8\u8350 Kernel 5.10+ \u3002 ~# make setup_dualCni_cilium","title":"\u5feb\u901f\u542f\u52a8"},{"location":"usage/install/get-started-kind-zh_CN/#_3","text":"\u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002 ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config \u60a8\u53ef\u4ee5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectl get po -n kube-system | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none>","title":"\u9a8c\u8bc1\u5b89\u88c5"},{"location":"usage/install/get-started-kind-zh_CN/#_4","text":"\u901a\u8fc7\u4e0a\u8ff0\u68c0\u67e5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\u3002\u5728\u672c\u7ae0\u8282\uff0c\u5c06\u4ecb\u7ecd\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u53bb\u4f7f\u7528 Spiderpool \u3002 Spiderpool \u63d0\u4f9b\u4e86 Spidermultusconfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f00\u6e90\u9879\u76ee Multus CNI \u914d\u7f6e\u7ba1\u7406\u7684\u6269\u5c55\u3002 \u57fa\u4e8e Spiderpool \u5355 CNI \u73af\u5883 \u57fa\u4e8e Spiderpool \u548c Calico \u7684\u53cc CNI \u73af\u5883 \u57fa\u4e8e Spiderpool \u548c Cilium \u7684\u53cc CNI \u73af\u5883 \u83b7\u53d6\u96c6\u7fa4\u7684 Spidermultusconfig CR \u4e0e IPPool CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system macvlan-vlan0 1h kube-system macvlan-vlan100 1h kube-system macvlan-vlan200 1h ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true ... \u521b\u5efa\u5e94\u7528\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u901a\u8fc7\u5b83\u6307\u5b9a Spidermultusconfig CR: kube-system/macvlan-vlan0 \uff0c\u5e76\u901a\u8fc7\u8be5\u914d\u7f6e\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u7531 Macvlan \u914d\u7f6e\u7f51\u7edc\u7684\u9ed8\u8ba4\u7f51\u5361 (eth0) \u3002 ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9 IP \u4e0e\u5e94\u7528\u7684\u9ed8\u8ba4\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF \u9a8c\u8bc1\u5e94\u7528\u521b\u5efa\u6210\u529f\u3002 ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-7fdbb59666-4k5m7 1 /1 Running 0 9s 172 .18.40.223 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 3 : eth0@if339: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 0a:96:54:6f:76:b4 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.223/16 brd 172 .18.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 4a:8b:09:d9:4c:0a brd ff:ff:ff:ff:ff:ff \u83b7\u53d6\u96c6\u7fa4\u7684 Spidermultusconfig CR \u4e0e IPPool CR ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system calico 3m11s kube-system macvlan-vlan0 2m20s kube-system macvlan-vlan100 2m19s kube-system macvlan-vlan200 2m19s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... \u521b\u5efa\u5e94\u7528\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa\u4e00\u4e2a\u5177\u5907\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a \u9ed8\u8ba4\u7f51\u5361\uff08eth0\uff09\u7531\u96c6\u7fa4\u7f3a\u7701 CNI Calico \u914d\u7f6e\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u901a\u8fc7\u8be5\u6ce8\u89e3\u989d\u5916\u4e3a\u5e94\u7528\u989d\u5916\u518d\u521b\u5efa\u4e00\u5f20\u7531 Macvlan \u914d\u7f6e\u7f51\u7edc\u7684\u7f51\u5361 (net1) \u3002 ipam.spidernet.io/ippools \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9 IP \u4e0e\u5e94\u7528\u7684 net1 \u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF \u9a8c\u8bc1\u5e94\u7528\u521b\u5efa\u6210\u529f ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-bv6rm 1 /1 Running 0 12s 10 .243.104.211 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 1a:1e:e1:f3:f9:4b brd ff:ff:ff:ff:ff:ff inet 10 .243.104.211/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 56 :b4:3d:a6:d2:d1 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.154/16 brd 172 .18.255.255 scope global net1 \u83b7\u53d6\u96c6\u7fa4\u7684 Spidermultusconfig CR \u4e0e IPPool CR ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system cilium 5m32s kube-system macvlan-vlan0 5m12s kube-system macvlan-vlan100 5m17s kube-system macvlan-vlan200 5m18s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... \u521b\u5efa\u5e94\u7528\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa\u4e00\u4e2a\u5177\u5907\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a \u9ed8\u8ba4\u7f51\u5361\uff08eth0\uff09\u7531\u96c6\u7fa4\u7f3a\u7701 CNI Cilium \u914d\u7f6e\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u901a\u8fc7\u8be5\u6ce8\u89e3\u989d\u5916\u4e3a\u5e94\u7528\u989d\u5916\u518d\u521b\u5efa\u4e00\u5f20\u7531 Macvlan \u914d\u7f6e\u7f51\u7edc\u7684\u7f51\u5361 (net1) \u3002 ipam.spidernet.io/ippools \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9 IP \u4e0e\u5e94\u7528\u7684 net1 \u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF \u9a8c\u8bc1\u5e94\u7528\u521b\u5efa\u6210\u529f ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-ml8d9 1 /1 Running 0 58s 10 .244.102.212 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 26 :f1:88:f9:7d:d7 brd ff:ff:ff:ff:ff:ff inet 10 .244.102.212/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether ca:71:99:ec:ec:28 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.228/16 brd 172 .18.255.255 scope global net1 \u73b0\u5728\u60a8\u53ef\u4ee5\u57fa\u4e8e Kind \u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684 \u66f4\u591a\u529f\u80fd \u3002","title":"\u90e8\u7f72\u5e94\u7528"},{"location":"usage/install/get-started-kind-zh_CN/#_5","text":"\u5378\u8f7d Kind \u96c6\u7fa4 \u6267\u884c make clean \u5378\u8f7d Kind \u96c6\u7fa4\u3002 \u5220\u9664\u6d4b\u8bd5\u955c\u50cf ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"\u5378\u8f7d"},{"location":"usage/install/get-started-kind/","text":"Quick Start English | \u7b80\u4f53\u4e2d\u6587 Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Spiderpool provides a script to install the Kind cluster, you can use it to deploy a cluster that meets your needs, and test and experience Spiderpool. Prerequisites Get the Spiderpool stable version code to the local host and enter the root directory of the Spiderpool project. ~# LATEST_RELEASE_VERISON = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | grep '\"tag_name\":' | grep -v rc | grep -Eo \"([0-9]+\\.[0-9]+\\.[0-9])\" | sort -r | head -n 1 ) ~# curl -Lo /tmp/ $LATEST_RELEASE_VERISON .tar.gz https://github.com/spidernet-io/spiderpool/archive/refs/tags/v $LATEST_RELEASE_VERISON .tar.gz ~# tar -xvf /tmp/ $LATEST_RELEASE_VERISON .tar.gz -C /tmp/ ~# cd /tmp/spiderpool- $LATEST_RELEASE_VERISON Execute make dev-doctor to check whether the development tools on the local host meet the conditions for deploying Kind cluster and Spiderpool. Building a Spiderpool environment requires Kubectl, Kind, Docker, Helm, and yq tools. If they are missing on your machine, run test/scripts/install-tools.sh to install them. Quick Start If you are mainland user who is not available to access ghcr.io, Additional parameter -e E2E_CHINA_IMAGE_REGISTRY=true can be specified during installation to help you pull images faster. Spiderpool with Macvlan Dual CNIs with Spiderpool and Calico Dual CNIs with Spiderpool and Cilium In this scenario, POD cloud be assigned multiple macvlan interfaces, and communicate through Pod IP, clusterIP, nodePort, etc. Please refer to underlay network case for more details. The following command will create a single-CNI cluster with Macvlan\uff0cand it implements the ClusterIP by kube-proxy. ~# make setup_singleCni_macvlan In this scenario, you can experience the effect of Pod having dual CNI NICs. Please refer to dual CNIs case for more details. The following command will create a multi-CNI cluster with Calico and Spiderpool. In this environment, Calico serves as the default CNI and Pod could get a secondary underlay interface from Spiderpool. Calico works based on iptables datapath and implements service resolution based on kube-proxy. ~# make setup_dualCni_calico In this scenario, you can experience the effect of Pod having dual CNI NICs. Please refer to dual CNIs case for more details. The following command will create a multi-CNI cluster with Cilium and Spiderpool, In this environment, Cilium serves as the default CNI and Pod could get a secondary underlay interface from Spiderpool. Cilium's eBPF acceleration is enabled, kube-proxy is disabled, and service resolution is implemented based on eBPF. Confirm whether the operating system Kernel version number is >= 4.9.17. If the kernel is too low, the installation will fail. Kernel 5.10+ is recommended. ~# make setup_dualCni_cilium Check the environment Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl. ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config It should be possible to observe the following: ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectl get po -n kube-system | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application: ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> Deploy Applicatiion Through the above checks, everything is normal in the Kind cluster. In this chapter, we will introduce how to use Spiderpool in different environments. Spiderpool introduces the Spidermultusconfig CR to automate the management of Multus NetworkAttachmentDefinition CR and extend the capabilities of Multus CNI configurations. Spiderpool with Macvlan Dual CNIs with Spiderpool and Calico Dual CNIs with Spiderpool and Cilium Get the Spidermultusconfig CR and IPPool CR of the cluster ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system macvlan-vlan0 1h kube-system macvlan-vlan100 1h kube-system macvlan-vlan200 1h ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true ... Create an application. The following command will create a single NIC Deployment application: v1.multus-cni.io/default-network \uff1aSpecify Spidermultusconfig CR: kube-system/macvlan-vlan0 through it, and use this configuration to create a default NIC (eth0) configured by Macvlan for the application. ipam.spidernet.io/ippool \uff1aUsed to specify Spiderpool's IP pool. Spiderpool will automatically select an IP in the pool to bind to the application's default NIC. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF Verify that the application was created successfully. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-7fdbb59666-4k5m7 1 /1 Running 0 9s 172 .18.40.223 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 3 : eth0@if339: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 0a:96:54:6f:76:b4 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.223/16 brd 172 .18.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 4a:8b:09:d9:4c:0a brd ff:ff:ff:ff:ff:ff Get the Spidermultusconfig CR and IPPool CR of the cluster ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system calico 3m11s kube-system macvlan-vlan0 2m20s kube-system macvlan-vlan100 2m19s kube-system macvlan-vlan200 2m19s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... Create an application. The following command will create a Deployment application with two NICs. The default NIC(eth0) is configured by the cluster default CNI Calico. k8s.v1.cni.cncf.io/networks \uff1aUse this annotation to create an additional NIC (net1) configured by Macvlan for the application. ipam.spidernet.io/ippools \uff1aUsed to specify Spiderpool's IPPool. Spiderpool will automatically select an IP in the pool to bind to the application's net1 NIC cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF Verify that the application was created successfully. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-bv6rm 1 /1 Running 0 12s 10 .243.104.211 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 1a:1e:e1:f3:f9:4b brd ff:ff:ff:ff:ff:ff inet 10 .243.104.211/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 56 :b4:3d:a6:d2:d1 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.154/16 brd 172 .18.255.255 scope global net1 Get the Spidermultusconfig CR and IPPool CR of the cluster ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system cilium 5m32s kube-system macvlan-vlan0 5m12s kube-system macvlan-vlan100 5m17s kube-system macvlan-vlan200 5m18s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... Create an application. The following command will create a Deployment application with two NICs. The default NIC(eth0) is configured by the cluster default CNI Cilium. k8s.v1.cni.cncf.io/networks \uff1aUse this annotation to create an additional NIC (net1) configured by Macvlan for the application. ipam.spidernet.io/ippools \uff1aUsed to specify Spiderpool's IPPool. Spiderpool will automatically select an IP in the pool to bind to the application's net1 NIC cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF Verify that the application was created successfully. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-ml8d9 1 /1 Running 0 58s 10 .244.102.212 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 26 :f1:88:f9:7d:d7 brd ff:ff:ff:ff:ff:ff inet 10 .244.102.212/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether ca:71:99:ec:ec:28 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.228/16 brd 172 .18.255.255 scope global net1 Now you can test and experience Spiderpool's more features based on Kind. Uninstall Uninstall a Kind cluster Execute make clean to uninstall the Kind cluster. Delete test's images ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Quick Start"},{"location":"usage/install/get-started-kind/#quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Spiderpool provides a script to install the Kind cluster, you can use it to deploy a cluster that meets your needs, and test and experience Spiderpool.","title":"Quick Start"},{"location":"usage/install/get-started-kind/#prerequisites","text":"Get the Spiderpool stable version code to the local host and enter the root directory of the Spiderpool project. ~# LATEST_RELEASE_VERISON = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | grep '\"tag_name\":' | grep -v rc | grep -Eo \"([0-9]+\\.[0-9]+\\.[0-9])\" | sort -r | head -n 1 ) ~# curl -Lo /tmp/ $LATEST_RELEASE_VERISON .tar.gz https://github.com/spidernet-io/spiderpool/archive/refs/tags/v $LATEST_RELEASE_VERISON .tar.gz ~# tar -xvf /tmp/ $LATEST_RELEASE_VERISON .tar.gz -C /tmp/ ~# cd /tmp/spiderpool- $LATEST_RELEASE_VERISON Execute make dev-doctor to check whether the development tools on the local host meet the conditions for deploying Kind cluster and Spiderpool. Building a Spiderpool environment requires Kubectl, Kind, Docker, Helm, and yq tools. If they are missing on your machine, run test/scripts/install-tools.sh to install them.","title":"Prerequisites"},{"location":"usage/install/get-started-kind/#quick-start_1","text":"If you are mainland user who is not available to access ghcr.io, Additional parameter -e E2E_CHINA_IMAGE_REGISTRY=true can be specified during installation to help you pull images faster. Spiderpool with Macvlan Dual CNIs with Spiderpool and Calico Dual CNIs with Spiderpool and Cilium In this scenario, POD cloud be assigned multiple macvlan interfaces, and communicate through Pod IP, clusterIP, nodePort, etc. Please refer to underlay network case for more details. The following command will create a single-CNI cluster with Macvlan\uff0cand it implements the ClusterIP by kube-proxy. ~# make setup_singleCni_macvlan In this scenario, you can experience the effect of Pod having dual CNI NICs. Please refer to dual CNIs case for more details. The following command will create a multi-CNI cluster with Calico and Spiderpool. In this environment, Calico serves as the default CNI and Pod could get a secondary underlay interface from Spiderpool. Calico works based on iptables datapath and implements service resolution based on kube-proxy. ~# make setup_dualCni_calico In this scenario, you can experience the effect of Pod having dual CNI NICs. Please refer to dual CNIs case for more details. The following command will create a multi-CNI cluster with Cilium and Spiderpool, In this environment, Cilium serves as the default CNI and Pod could get a secondary underlay interface from Spiderpool. Cilium's eBPF acceleration is enabled, kube-proxy is disabled, and service resolution is implemented based on eBPF. Confirm whether the operating system Kernel version number is >= 4.9.17. If the kernel is too low, the installation will fail. Kernel 5.10+ is recommended. ~# make setup_dualCni_cilium","title":"Quick Start"},{"location":"usage/install/get-started-kind/#check-the-environment","text":"Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl. ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config It should be possible to observe the following: ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectl get po -n kube-system | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application: ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none>","title":"Check the environment"},{"location":"usage/install/get-started-kind/#deploy-applicatiion","text":"Through the above checks, everything is normal in the Kind cluster. In this chapter, we will introduce how to use Spiderpool in different environments. Spiderpool introduces the Spidermultusconfig CR to automate the management of Multus NetworkAttachmentDefinition CR and extend the capabilities of Multus CNI configurations. Spiderpool with Macvlan Dual CNIs with Spiderpool and Calico Dual CNIs with Spiderpool and Cilium Get the Spidermultusconfig CR and IPPool CR of the cluster ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system macvlan-vlan0 1h kube-system macvlan-vlan100 1h kube-system macvlan-vlan200 1h ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true ... Create an application. The following command will create a single NIC Deployment application: v1.multus-cni.io/default-network \uff1aSpecify Spidermultusconfig CR: kube-system/macvlan-vlan0 through it, and use this configuration to create a default NIC (eth0) configured by Macvlan for the application. ipam.spidernet.io/ippool \uff1aUsed to specify Spiderpool's IP pool. Spiderpool will automatically select an IP in the pool to bind to the application's default NIC. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF Verify that the application was created successfully. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-7fdbb59666-4k5m7 1 /1 Running 0 9s 172 .18.40.223 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 3 : eth0@if339: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 0a:96:54:6f:76:b4 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.223/16 brd 172 .18.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 4a:8b:09:d9:4c:0a brd ff:ff:ff:ff:ff:ff Get the Spidermultusconfig CR and IPPool CR of the cluster ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system calico 3m11s kube-system macvlan-vlan0 2m20s kube-system macvlan-vlan100 2m19s kube-system macvlan-vlan200 2m19s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... Create an application. The following command will create a Deployment application with two NICs. The default NIC(eth0) is configured by the cluster default CNI Calico. k8s.v1.cni.cncf.io/networks \uff1aUse this annotation to create an additional NIC (net1) configured by Macvlan for the application. ipam.spidernet.io/ippools \uff1aUsed to specify Spiderpool's IPPool. Spiderpool will automatically select an IP in the pool to bind to the application's net1 NIC cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF Verify that the application was created successfully. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-bv6rm 1 /1 Running 0 12s 10 .243.104.211 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 1a:1e:e1:f3:f9:4b brd ff:ff:ff:ff:ff:ff inet 10 .243.104.211/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 56 :b4:3d:a6:d2:d1 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.154/16 brd 172 .18.255.255 scope global net1 Get the Spidermultusconfig CR and IPPool CR of the cluster ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system cilium 5m32s kube-system macvlan-vlan0 5m12s kube-system macvlan-vlan100 5m17s kube-system macvlan-vlan200 5m18s ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 1 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 1 253 true ... Create an application. The following command will create a Deployment application with two NICs. The default NIC(eth0) is configured by the cluster default CNI Cilium. k8s.v1.cni.cncf.io/networks \uff1aUse this annotation to create an additional NIC (net1) configured by Macvlan for the application. ipam.spidernet.io/ippools \uff1aUsed to specify Spiderpool's IPPool. Spiderpool will automatically select an IP in the pool to bind to the application's net1 NIC cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippools: |- [{ \"interface\": \"net1\", \"ipv4\": [\"default-v4-ippool\"], \"ipv6\": [\"default-v6-ippool\"] }] k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0 spec: containers: - name: test-app image: alpine imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"sleep infinity\" EOF Verify that the application was created successfully. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-86dd478b-ml8d9 1 /1 Running 0 58s 10 .244.102.212 spider-worker <none> <none> ~# kubectl exec -ti test-app-7fdbb59666-4k5m7 -- ip a ... 4 : eth0@if148: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP qlen 1000 link/ether 26 :f1:88:f9:7d:d7 brd ff:ff:ff:ff:ff:ff inet 10 .244.102.212/32 scope global eth0 5 : net1@if347: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether ca:71:99:ec:ec:28 brd ff:ff:ff:ff:ff:ff inet 172 .18.40.228/16 brd 172 .18.255.255 scope global net1 Now you can test and experience Spiderpool's more features based on Kind.","title":"Deploy Applicatiion"},{"location":"usage/install/get-started-kind/#uninstall","text":"Uninstall a Kind cluster Execute make clean to uninstall the Kind cluster. Delete test's images ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Uninstall"},{"location":"usage/install/system-requirements-zh_CN/","text":"\u5b89\u88c5\u8981\u6c42 English | \u7b80\u4f53\u4e2d\u6587 \u4e3b\u673a\u8981\u6c42 x86-64 \u6216 arm64 \u76f8\u540c \u4f7f\u7528 ipvlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0clinux \u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2 Kubernetes \u8981\u6c42 \u4f7f\u7528 SpiderSubnet \u529f\u80fd\u8981\u6c42 Kubernetes \u7248\u672c\u4e0d\u4f4e\u4e8e v1.21 Spiderpool \u7ec4\u4ef6\u7684\u4e3b\u673a\u7f51\u7edc\u7aef\u53e3\u5360\u7528 \u7ec4\u4ef6 \u7aef\u53e3/\u534f\u8bae \u63cf\u8ff0 \u914d\u7f6e\u73af\u5883\u53d8\u91cf daemonset spiderpool-agent 5710/tcp pod \u5065\u5eb7\u68c0\u67e5\u7aef\u53e3 SPIDERPOOL_HEALTH_PORT daemonset spiderpool-agent 5711/tcp \u6307\u6807\u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86 \u6307\u6807\u529f\u80fd\uff09 SPIDERPOOL_METRIC_HTTP_PORT daemonset spiderpool-agent 5712/tcp gops \u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86debug\uff09 SPIDERPOOL_GOPS_LISTEN_PORT deployment spiderpool-controller 5720/tcp pod \u5065\u5eb7\u68c0\u67e5\u7aef\u53e3 SPIDERPOOL_HEALTH_PORT deployment spiderpool-controller 5711/tcp \u6307\u6807\u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86 \u6307\u6807\u529f\u80fd\uff09 SPIDERPOOL_METRIC_HTTP_PORT deployment spiderpool-controller 5722/tcp webhook \u7aef\u53e3 SPIDERPOOL_WEBHOOK_PORT deployment spiderpool-controller 5724/tcp gops \u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86debug\uff09 SPIDERPOOL_GOPS_LISTEN_PORT (\u53ef\u9009\u5b89\u88c5) SR-IOV \u7ec4\u4ef6\u7684\u4e3b\u673a\u7f51\u7edc\u7aef\u53e3\u5360\u7528 \u7ec4\u4ef6 \u7aef\u53e3/\u534f\u8bae \u63cf\u8ff0 \u914d\u7f6e\u73af\u5883\u53d8\u91cf daemonset network-resources-injector 5731/tcp webhook \u7aef\u53e3\uff0c\u8be5\u7aef\u53e3\u4f1a\u5360\u7528 \u8be5\u7ec4\u4ef6\u662f\u53ef\u9009\u5b89\u88c5 NA deployment operator-webhook 5732/tcp webhook \u7aef\u53e3\uff0c\u8be5\u7ec4\u4ef6\u662f\u53ef\u9009\u5b89\u88c5 NA","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/system-requirements-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/system-requirements-zh_CN/#_2","text":"x86-64 \u6216 arm64 \u76f8\u540c \u4f7f\u7528 ipvlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0clinux \u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2","title":"\u4e3b\u673a\u8981\u6c42"},{"location":"usage/install/system-requirements-zh_CN/#kubernetes","text":"\u4f7f\u7528 SpiderSubnet \u529f\u80fd\u8981\u6c42 Kubernetes \u7248\u672c\u4e0d\u4f4e\u4e8e v1.21","title":"Kubernetes \u8981\u6c42"},{"location":"usage/install/system-requirements-zh_CN/#spiderpool","text":"\u7ec4\u4ef6 \u7aef\u53e3/\u534f\u8bae \u63cf\u8ff0 \u914d\u7f6e\u73af\u5883\u53d8\u91cf daemonset spiderpool-agent 5710/tcp pod \u5065\u5eb7\u68c0\u67e5\u7aef\u53e3 SPIDERPOOL_HEALTH_PORT daemonset spiderpool-agent 5711/tcp \u6307\u6807\u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86 \u6307\u6807\u529f\u80fd\uff09 SPIDERPOOL_METRIC_HTTP_PORT daemonset spiderpool-agent 5712/tcp gops \u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86debug\uff09 SPIDERPOOL_GOPS_LISTEN_PORT deployment spiderpool-controller 5720/tcp pod \u5065\u5eb7\u68c0\u67e5\u7aef\u53e3 SPIDERPOOL_HEALTH_PORT deployment spiderpool-controller 5711/tcp \u6307\u6807\u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86 \u6307\u6807\u529f\u80fd\uff09 SPIDERPOOL_METRIC_HTTP_PORT deployment spiderpool-controller 5722/tcp webhook \u7aef\u53e3 SPIDERPOOL_WEBHOOK_PORT deployment spiderpool-controller 5724/tcp gops \u7aef\u53e3\uff08\u5982\u679c\u5f00\u542f\u4e86debug\uff09 SPIDERPOOL_GOPS_LISTEN_PORT","title":"Spiderpool \u7ec4\u4ef6\u7684\u4e3b\u673a\u7f51\u7edc\u7aef\u53e3\u5360\u7528"},{"location":"usage/install/system-requirements-zh_CN/#sr-iov","text":"\u7ec4\u4ef6 \u7aef\u53e3/\u534f\u8bae \u63cf\u8ff0 \u914d\u7f6e\u73af\u5883\u53d8\u91cf daemonset network-resources-injector 5731/tcp webhook \u7aef\u53e3\uff0c\u8be5\u7aef\u53e3\u4f1a\u5360\u7528 \u8be5\u7ec4\u4ef6\u662f\u53ef\u9009\u5b89\u88c5 NA deployment operator-webhook 5732/tcp webhook \u7aef\u53e3\uff0c\u8be5\u7ec4\u4ef6\u662f\u53ef\u9009\u5b89\u88c5 NA","title":"(\u53ef\u9009\u5b89\u88c5) SR-IOV \u7ec4\u4ef6\u7684\u4e3b\u673a\u7f51\u7edc\u7aef\u53e3\u5360\u7528"},{"location":"usage/install/system-requirements/","text":"System requirements English | \u7b80\u4f53\u4e2d\u6587 Node requirements x86-64, arm64 The system kernel version must be greater than 4.2 when using ipvlan as the cluster's CNI Kubernetes requirements The SpiderSubnet feature requires a minimum version of v1.21 . Spiderpool Requirements Of Host Ports Component Port/Protocol Description ENV Configuration daemonset spiderpool-agent 5710/tcp health-check port SPIDERPOOL_HEALTH_PORT daemonset spiderpool-agent 5711/tcp metrics port if metrics is enabled SPIDERPOOL_METRIC_HTTP_PORT daemonset spiderpool-agent 5712/tcp gops port if debugging is expected SPIDERPOOL_GOPS_LISTEN_PORT deployment spiderpool-controller 5720/tcp health-check port SPIDERPOOL_HEALTH_PORT deployment spiderpool-controller 5711/tcp metrics port if metrics is enabled SPIDERPOOL_METRIC_HTTP_PORT deployment spiderpool-controller 5722/tcp webhook port SPIDERPOOL_WEBHOOK_PORT deployment spiderpool-controller 5724/tcp gops port if debugging is expected SPIDERPOOL_GOPS_LISTEN_PORT (optional components) SR-IOV Requirements Of Host Ports Component Port/Protocol Description ENV Configuration daemonset network-resources-injector 5731/tcp webhook port NA deployment operator-webhook 5732/tcp webhook port NA","title":"System requirements"},{"location":"usage/install/system-requirements/#system-requirements","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"System requirements"},{"location":"usage/install/system-requirements/#node-requirements","text":"x86-64, arm64 The system kernel version must be greater than 4.2 when using ipvlan as the cluster's CNI","title":"Node requirements"},{"location":"usage/install/system-requirements/#kubernetes-requirements","text":"The SpiderSubnet feature requires a minimum version of v1.21 .","title":"Kubernetes requirements"},{"location":"usage/install/system-requirements/#spiderpool-requirements-of-host-ports","text":"Component Port/Protocol Description ENV Configuration daemonset spiderpool-agent 5710/tcp health-check port SPIDERPOOL_HEALTH_PORT daemonset spiderpool-agent 5711/tcp metrics port if metrics is enabled SPIDERPOOL_METRIC_HTTP_PORT daemonset spiderpool-agent 5712/tcp gops port if debugging is expected SPIDERPOOL_GOPS_LISTEN_PORT deployment spiderpool-controller 5720/tcp health-check port SPIDERPOOL_HEALTH_PORT deployment spiderpool-controller 5711/tcp metrics port if metrics is enabled SPIDERPOOL_METRIC_HTTP_PORT deployment spiderpool-controller 5722/tcp webhook port SPIDERPOOL_WEBHOOK_PORT deployment spiderpool-controller 5724/tcp gops port if debugging is expected SPIDERPOOL_GOPS_LISTEN_PORT","title":"Spiderpool Requirements Of Host Ports"},{"location":"usage/install/system-requirements/#optional-components-sr-iov-requirements-of-host-ports","text":"Component Port/Protocol Description ENV Configuration daemonset network-resources-injector 5731/tcp webhook port NA deployment operator-webhook 5732/tcp webhook port NA","title":"(optional components) SR-IOV Requirements Of Host Ports"},{"location":"usage/install/uninstall-zh_CN/","text":"\u5378\u8f7d\u6307\u5357 English | \u7b80\u4f53\u4e2d\u6587 \u672c\u5378\u8f7d\u6307\u5357\u9002\u7528\u4e8e\u5728 Kubernetes \u4e0a\u8fd0\u884c\u7684 Spiderpool\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u7591\u95ee\uff0c\u8bf7\u968f\u65f6\u901a\u8fc7 Spiderpool Community \u8054\u7cfb\u6211\u4eec\u3002 \u6ce8\u610f\u4e8b\u9879 \u5728\u6267\u884c\u5378\u8f7d\u4e4b\u524d\uff0c\u8bf7\u9605\u8bfb\u5b8c\u6574\u7684\u5378\u8f7d\u6307\u5357\u4ee5\u4e86\u89e3\u6240\u6709\u5fc5\u8981\u7684\u6b65\u9aa4\u3002 \u5378\u8f7d \u4e86\u89e3\u6b63\u5728\u8fd0\u884c\u5e94\u7528\uff0c\u7406\u89e3\u5378\u8f7d Spiderpool \u53ef\u80fd\u5bf9\u5176\u4ed6\u76f8\u5173\u7ec4\u4ef6\uff08\u5982\u4e2d\u95f4\u4ef6\uff09 \u4ea7\u751f\u7684\u5f71\u54cd\uff0c\u8bf7\u786e\u4fdd\u5b8c\u5168\u4e86\u89e3\u98ce\u9669\u540e\uff0c\u624d\u5f00\u59cb\u6267\u884c\u5378\u8f7d\u6b65\u9aa4\u3002 \u901a\u8fc7 helm ls \u67e5\u8be2\u96c6\u7fa4\u6240\u5b89\u88c5\u7684 Spiderpool helm ls -A | grep -i spiderpool \u901a\u8fc7 helm uninstall \u5378\u8f7d Spiderpool helm uninstall <spiderpool-name> --namespace <spiderpool-namespace> \u5c06 <spiderpool-name> \u66ff\u6362\u4e3a\u8981\u5378\u8f7d\u7684 Spiderpool \u7684\u540d\u79f0\uff0c\u5c06 <spiderpool-namespace> \u66ff\u6362\u4e3a Spiderpool \u6240\u5728\u7684\u547d\u540d\u7a7a\u95f4\u3002 v1.0.0 \u4ee5\u4e0a\u7248\u672c \u5728 v1.0.0 \u4e4b\u540e\u5f15\u5165\u4e86\u81ea\u52a8\u6e05\u7406 Spiderpool \u8d44\u6e90\u7684\u529f\u80fd\uff0c\u5b83\u901a\u8fc7 spiderpoolController.cleanup.enabled \u914d\u7f6e\u9879\u6765\u542f\u7528\uff0c\u8be5\u503c\u9ed8\u8ba4\u4e3a true \uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u65b9\u5f0f\u9a8c\u8bc1\u4e0e Spiderpool \u76f8\u5173\u7684\u8d44\u6e90\u6570\u91cf\u662f\u5426\u81ea\u52a8\u88ab\u6e05\u7406\u3002 kubectl get spidersubnets.spiderpool.spidernet.io -o name | wc -l kubectl get spiderips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderippools.spiderpool.spidernet.io -o name | wc -l kubectl get spiderreservedips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderendpoints.spiderpool.spidernet.io -o name | wc -l kubectl get spidercoordinators.spiderpool.spidernet.io -o name | wc -l v1.0.0 \u4ee5\u4e0b\u7248\u672c \u5728\u4f4e\u4e8e v1.0.0 \u7684\u7248\u672c\u4e2d\uff0c\u7531\u4e8e Spiderpool \u7684\u67d0\u4e9b CR \u8d44\u6e90\u4e2d\u5b58\u5728 finalizers \uff0c\u5bfc\u81f4 helm uninstall \u547d\u4ee4\u65e0\u6cd5\u6e05\u7406\u5e72\u51c0\uff0c\u60a8\u9700\u8981\u624b\u52a8\u6e05\u7406\u3002\u53ef\u83b7\u53d6\u5982\u4e0b\u6e05\u7406\u811a\u672c\u6765\u5b8c\u6210\u6e05\u7406\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh FAQ \u5220\u9664 Spiderpool \u672a\u4f7f\u7528 helm uninstall \u65b9\u5f0f\uff0c\u800c\u662f\u901a\u8fc7 kubectl delete <spiderpool \u90e8\u7f72\u7684 namespace> \uff0c\u5bfc\u81f4\u51fa\u73b0\u8d44\u6e90\u5378\u8f7d\u6b8b\u7559\uff0c\u4ece\u800c\u5f71\u54cd\u65b0\u7684\u5b89\u88c5\u3002\u60a8\u9700\u8981\u624b\u52a8\u6e05\u7406\u3002\u53ef\u83b7\u53d6\u5982\u4e0b\u6e05\u7406\u811a\u672c\u6765\u5b8c\u6210\u6e05\u7406\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"\u5378\u8f7d\u6307\u5357"},{"location":"usage/install/uninstall-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u5378\u8f7d\u6307\u5357\u9002\u7528\u4e8e\u5728 Kubernetes \u4e0a\u8fd0\u884c\u7684 Spiderpool\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u7591\u95ee\uff0c\u8bf7\u968f\u65f6\u901a\u8fc7 Spiderpool Community \u8054\u7cfb\u6211\u4eec\u3002","title":"\u5378\u8f7d\u6307\u5357"},{"location":"usage/install/uninstall-zh_CN/#_2","text":"\u5728\u6267\u884c\u5378\u8f7d\u4e4b\u524d\uff0c\u8bf7\u9605\u8bfb\u5b8c\u6574\u7684\u5378\u8f7d\u6307\u5357\u4ee5\u4e86\u89e3\u6240\u6709\u5fc5\u8981\u7684\u6b65\u9aa4\u3002","title":"\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/uninstall-zh_CN/#_3","text":"\u4e86\u89e3\u6b63\u5728\u8fd0\u884c\u5e94\u7528\uff0c\u7406\u89e3\u5378\u8f7d Spiderpool \u53ef\u80fd\u5bf9\u5176\u4ed6\u76f8\u5173\u7ec4\u4ef6\uff08\u5982\u4e2d\u95f4\u4ef6\uff09 \u4ea7\u751f\u7684\u5f71\u54cd\uff0c\u8bf7\u786e\u4fdd\u5b8c\u5168\u4e86\u89e3\u98ce\u9669\u540e\uff0c\u624d\u5f00\u59cb\u6267\u884c\u5378\u8f7d\u6b65\u9aa4\u3002 \u901a\u8fc7 helm ls \u67e5\u8be2\u96c6\u7fa4\u6240\u5b89\u88c5\u7684 Spiderpool helm ls -A | grep -i spiderpool \u901a\u8fc7 helm uninstall \u5378\u8f7d Spiderpool helm uninstall <spiderpool-name> --namespace <spiderpool-namespace> \u5c06 <spiderpool-name> \u66ff\u6362\u4e3a\u8981\u5378\u8f7d\u7684 Spiderpool \u7684\u540d\u79f0\uff0c\u5c06 <spiderpool-namespace> \u66ff\u6362\u4e3a Spiderpool \u6240\u5728\u7684\u547d\u540d\u7a7a\u95f4\u3002","title":"\u5378\u8f7d"},{"location":"usage/install/uninstall-zh_CN/#v100","text":"\u5728 v1.0.0 \u4e4b\u540e\u5f15\u5165\u4e86\u81ea\u52a8\u6e05\u7406 Spiderpool \u8d44\u6e90\u7684\u529f\u80fd\uff0c\u5b83\u901a\u8fc7 spiderpoolController.cleanup.enabled \u914d\u7f6e\u9879\u6765\u542f\u7528\uff0c\u8be5\u503c\u9ed8\u8ba4\u4e3a true \uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u65b9\u5f0f\u9a8c\u8bc1\u4e0e Spiderpool \u76f8\u5173\u7684\u8d44\u6e90\u6570\u91cf\u662f\u5426\u81ea\u52a8\u88ab\u6e05\u7406\u3002 kubectl get spidersubnets.spiderpool.spidernet.io -o name | wc -l kubectl get spiderips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderippools.spiderpool.spidernet.io -o name | wc -l kubectl get spiderreservedips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderendpoints.spiderpool.spidernet.io -o name | wc -l kubectl get spidercoordinators.spiderpool.spidernet.io -o name | wc -l","title":"v1.0.0 \u4ee5\u4e0a\u7248\u672c"},{"location":"usage/install/uninstall-zh_CN/#v100_1","text":"\u5728\u4f4e\u4e8e v1.0.0 \u7684\u7248\u672c\u4e2d\uff0c\u7531\u4e8e Spiderpool \u7684\u67d0\u4e9b CR \u8d44\u6e90\u4e2d\u5b58\u5728 finalizers \uff0c\u5bfc\u81f4 helm uninstall \u547d\u4ee4\u65e0\u6cd5\u6e05\u7406\u5e72\u51c0\uff0c\u60a8\u9700\u8981\u624b\u52a8\u6e05\u7406\u3002\u53ef\u83b7\u53d6\u5982\u4e0b\u6e05\u7406\u811a\u672c\u6765\u5b8c\u6210\u6e05\u7406\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"v1.0.0 \u4ee5\u4e0b\u7248\u672c"},{"location":"usage/install/uninstall-zh_CN/#faq","text":"\u5220\u9664 Spiderpool \u672a\u4f7f\u7528 helm uninstall \u65b9\u5f0f\uff0c\u800c\u662f\u901a\u8fc7 kubectl delete <spiderpool \u90e8\u7f72\u7684 namespace> \uff0c\u5bfc\u81f4\u51fa\u73b0\u8d44\u6e90\u5378\u8f7d\u6b8b\u7559\uff0c\u4ece\u800c\u5f71\u54cd\u65b0\u7684\u5b89\u88c5\u3002\u60a8\u9700\u8981\u624b\u52a8\u6e05\u7406\u3002\u53ef\u83b7\u53d6\u5982\u4e0b\u6e05\u7406\u811a\u672c\u6765\u5b8c\u6210\u6e05\u7406\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"FAQ"},{"location":"usage/install/uninstall/","text":"Uninstall Guide English | \u7b80\u4f53\u4e2d\u6587 This uninstall guide is intended for Spiderpool running on Kubernetes. If you have questions, feel free to ping us on Spiderpool Community . Warning Read the full uninstall guide to understand all the necessary steps before performing them. Uninstall Spiderpool Understand the running application and understand the impact that uninstalling Spiderpool may have on other related components (such as middleware). Please make sure you fully understand the risks before starting the uninstallation steps. Query the Spiderpool installed in the cluster through helm ls helm ls -A | grep -i spiderpool Uninstall Spiderpool via helm uninstall helm uninstall <spiderpool-name> --namespace <spiderpool-namespace> Replace <spiderpool-name> with the name of the Spiderpool you want to uninstall and <spiderpool-namespace> with the namespace of the Spiderpool. Above v1.0.0 The function of automatically cleaning Spiderpool resources was introduced after v1.0.0. It is enabled through the spiderpoolController.cleanup.enabled configuration item. The value defaults to true . You can verify whether the number of resources related to Spiderpool is automatically cleared as follows. kubectl get spidersubnets.spiderpool.spidernet.io -o name | wc -l kubectl get spiderips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderippools.spiderpool.spidernet.io -o name | wc -l kubectl get spiderreservedips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderendpoints.spiderpool.spidernet.io -o name | wc -l kubectl get spidercoordinators.spiderpool.spidernet.io -o name | wc -l Below v1.0.0 In versions lower than v1.0.0, Some CR resources having finalizers prevents complete cleanup via helm uninstall . You can download the cleaning script below to perform the necessary cleanup and avoid any unexpected errors during future deployments of Spiderpool. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh FAQ Spiderpool was not deleted using the helm uninstall method, but through kubectl delete <spiderpool deployed namespace> , which resulted in resource uninstallation residues, thus affecting the new installation. You need to clean it up manually. You can get the following cleanup script to complete the cleanup to ensure that there will be no unexpected errors when you deploy Spiderpool next time. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Uninstalling"},{"location":"usage/install/uninstall/#uninstall-guide","text":"English | \u7b80\u4f53\u4e2d\u6587 This uninstall guide is intended for Spiderpool running on Kubernetes. If you have questions, feel free to ping us on Spiderpool Community .","title":"Uninstall Guide"},{"location":"usage/install/uninstall/#warning","text":"Read the full uninstall guide to understand all the necessary steps before performing them.","title":"Warning"},{"location":"usage/install/uninstall/#uninstall-spiderpool","text":"Understand the running application and understand the impact that uninstalling Spiderpool may have on other related components (such as middleware). Please make sure you fully understand the risks before starting the uninstallation steps. Query the Spiderpool installed in the cluster through helm ls helm ls -A | grep -i spiderpool Uninstall Spiderpool via helm uninstall helm uninstall <spiderpool-name> --namespace <spiderpool-namespace> Replace <spiderpool-name> with the name of the Spiderpool you want to uninstall and <spiderpool-namespace> with the namespace of the Spiderpool.","title":"Uninstall Spiderpool"},{"location":"usage/install/uninstall/#above-v100","text":"The function of automatically cleaning Spiderpool resources was introduced after v1.0.0. It is enabled through the spiderpoolController.cleanup.enabled configuration item. The value defaults to true . You can verify whether the number of resources related to Spiderpool is automatically cleared as follows. kubectl get spidersubnets.spiderpool.spidernet.io -o name | wc -l kubectl get spiderips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderippools.spiderpool.spidernet.io -o name | wc -l kubectl get spiderreservedips.spiderpool.spidernet.io -o name | wc -l kubectl get spiderendpoints.spiderpool.spidernet.io -o name | wc -l kubectl get spidercoordinators.spiderpool.spidernet.io -o name | wc -l","title":"Above v1.0.0"},{"location":"usage/install/uninstall/#below-v100","text":"In versions lower than v1.0.0, Some CR resources having finalizers prevents complete cleanup via helm uninstall . You can download the cleaning script below to perform the necessary cleanup and avoid any unexpected errors during future deployments of Spiderpool. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Below v1.0.0"},{"location":"usage/install/uninstall/#faq","text":"Spiderpool was not deleted using the helm uninstall method, but through kubectl delete <spiderpool deployed namespace> , which resulted in resource uninstallation residues, thus affecting the new installation. You need to clean it up manually. You can get the following cleanup script to complete the cleanup to ensure that there will be no unexpected errors when you deploy Spiderpool next time. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"FAQ"},{"location":"usage/install/upgrade-zh_CN/","text":"\u5347\u7ea7\u6307\u5357 English | \u7b80\u4f53\u4e2d\u6587 \u672c\u5347\u7ea7\u6307\u5357\u9002\u7528\u4e8e\u5728 Kubernetes \u4e0a\u8fd0\u884c\u7684 Spiderpool\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u7591\u95ee\uff0c\u8bf7\u968f\u65f6\u901a\u8fc7 Spiderpool Community \u8054\u7cfb\u6211\u4eec\u3002 \u6ce8\u610f\u4e8b\u9879 \u5728\u6267\u884c\u5347\u7ea7\u4e4b\u524d\uff0c\u8bf7\u9605\u8bfb\u5b8c\u6574\u7684\u5347\u7ea7\u6307\u5357\u4ee5\u4e86\u89e3\u6240\u6709\u5fc5\u8981\u7684\u6b65\u9aa4\u3002 \u5728 Kubernetes \u8fdb\u884c Spiderpool \u5347\u7ea7\u65f6\uff0cKubernetes \u9996\u5148\u5c06\u7ec8\u6b62\u5df2\u6709 Pod\uff0c\u7136\u540e\u62c9\u53d6\u65b0\u7684\u955c\u50cf\u7248\u672c\uff0c\u6700\u540e\u4f7f\u7528\u65b0\u7684\u955c\u50cf\u542f\u52a8 Pod\u3002\u4e3a\u4e86\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u5e76\u9632\u6b62\u5347\u7ea7\u671f\u95f4\u53d1\u751f ErrImagePull \u9519\u8bef\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u63d0\u524d\u62c9\u53d6\u5bf9\u5e94\u7248\u672c\u7684\u955c\u50cf\u3002 # \u4ee5 docker \u4e3a\u4f8b\uff0c\u8bf7\u4fee\u6539 [upgraded-version] \u4e3a\u4f60\u5347\u7ea7\u7684\u7248\u672c\u3002 docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ] # \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u5927\u9646\u7528\u6237\uff0c\u53ef\u4ee5\u4f7f\u7528\u955c\u50cf\u6e90 ghcr.m.daocloud.io docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ] \u6b65\u9aa4 \u5efa\u8bae\u6bcf\u6b21\u90fd\u5347\u7ea7\u5230 Spiderpool \u7684\u6700\u65b0\u4e14\u88ab\u7ef4\u62a4\u7684\u8865\u4e01\u7248\u672c\u3002\u901a\u8fc7 Stable Releases \u4e86\u89e3\u53d7\u5230\u652f\u6301\u7684\u6700\u65b0\u8865\u4e01\u7248\u672c\u3002 \u4f7f\u7528 Helm \u5347\u7ea7 Spiderpool \u786e\u4fdd\u60a8\u5df2\u5b89\u88c5 Helm \u3002 \u8bbe\u7f6e Helm \u5b58\u50a8\u5e93\u5e76\u66f4\u65b0 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool \u5220\u9664 spiderpool-init Pod spiderpool-init Pod \u4f1a\u5e2e\u52a9\u521d\u59cb\u5316\u73af\u5883\u4fe1\u606f\uff0c\u6bcf\u6b21\u8fd0\u884c\u5b8c\u6bd5\u540e\u5176\u5904\u4e8e complete \u72b6\u6001\u3002\u5728 helm upgrade \u65f6\uff0c\u7531\u4e8e spiderpool-init \u672c\u8d28\u662f\u4e00\u4e2a Pod \uff0c\u5f53\u4f60\u8981\u5347\u7ea7\u5230\u7684\u7248\u672c\u53d1\u751f\u4e86\u8d44\u6e90\u53d8\u66f4\uff0c\u5c06\u4f1a Patch \u5931\u8d25\uff0c\u4ecb\u4e8e\u53ef\u80fd\u5e76\u4e0d\u6e05\u695a\u5177\u4f53\u7684\u53d8\u66f4\uff0c\u5efa\u8bae\u5347\u7ea7\u524d\u624b\u52a8\u5220\u9664\u4e00\u4e0b spiderpool-init Pod\uff0c\u907f\u514d\u51fa\u73b0 helm upgrade \u5931\u8d25\u7684\u60c5\u51b5\u3002 Error: UPGRADE FAILED: cannot patch \"spiderpool-init\" with kind Pod: Pod \"spiderpool-init\" is invalid: spec: Forbidden: pod updates may not change fields other than ` spec.containers [ * ] .image ` , ` spec.initContainers [ * ] .image ` , ` spec.activeDeadlineSeconds ` , ` spec.tolerations ` ( only additions to existing tolerations ) , ` spec.terminationGracePeriodSeconds ` ( allow it to be set to 1 if it was previously negative ) helm upgrade \u5347\u7ea7 # -n \u6307\u5b9a\u4f60 Spiderpool \u6240\u5728\u547d\u540d\u7a7a\u95f4\uff0c\u5e76\u4fee\u6539 [upgraded-version] \u4e3a\u4f60\u8981\u5347\u7ea7\u5230\u7684\u7248\u672c\u3002 helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] \u914d\u7f6e\u5347\u7ea7 \u60a8\u53ef\u4ee5\u901a\u8fc7 --set \u5728\u5347\u7ea7\u65f6\u53bb\u66f4\u65b0 Spiderpool \u914d\u7f6e\uff0c\u53ef\u7528\u7684 values \u53c2\u6570\uff0c\u8bf7\u67e5\u770b values \u8bf4\u660e\u6587\u6863\u3002 \u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5f00\u542f Spiderpool \u7684 SpiderSubnet \u529f\u80fd helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true \u540c\u65f6\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528 --reuse-values \u91cd\u7528\u4e0a\u4e00\u4e2a release \u7684\u503c\u5e76\u5408\u5e76\u6765\u81ea\u547d\u4ee4\u884c\u7684\u4efb\u4f55\u8986\u76d6\u3002\u4f46\u4ec5\u5f53 Spiderpool chart \u7248\u672c\u4fdd\u6301\u4e0d\u53d8\u65f6\uff0c\u624d\u53ef\u4ee5\u5b89\u5168\u5730\u4f7f\u7528 --reuse-values \u6807\u5fd7\uff0c\u4f8b\u5982\uff0c\u5f53\u4f7f\u7528 helm upgrade \u6765\u66f4\u6539 Spiderpool \u914d\u7f6e\u800c\u4e0d\u5347\u7ea7 Spiderpool \u7ec4\u4ef6\u3002 --reuse-values \u4f7f\u7528\uff0c\u53c2\u8003\u5982\u4e0b\u793a\u4f8b\uff1a helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true --reuse-values \u76f8\u53cd\uff0c\u5982\u679c Spiderpool chart \u7248\u672c\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u60a8\u60f3\u91cd\u7528\u73b0\u6709\u5b89\u88c5\u4e2d\u7684\u503c\uff0c\u8bf7\u5c06\u65e7\u503c\u4fdd\u5b58\u5728\u503c\u6587\u4ef6\u4e2d\uff0c\u68c0\u67e5\u8be5\u6587\u4ef6\u4e2d\u662f\u5426\u6709\u4efb\u4f55\u91cd\u547d\u540d\u6216\u5f03\u7528\u7684\u503c\uff0c\u7136\u540e\u5c06\u5176\u4f20\u9012\u7ed9 helm upgrade \u547d\u4ee4\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u68c0\u7d22\u5e76\u4fdd\u5b58\u73b0\u6709\u5b89\u88c5\u4e2d\u7684\u503c\uff1a helm get values spiderpool --namespace = kube-system -o yaml > old-values.yaml helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] -f old-values.yaml \u5347\u7ea7\u56de\u6eda \u6709\u65f6\u7531\u4e8e\u5347\u7ea7\u8fc7\u7a0b\u4e2d\u9057\u6f0f\u4e86\u67d0\u4e2a\u6b65\u9aa4\u6216\u51fa\u73b0\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u56de\u6eda\u5347\u7ea7\u3002 \u8981\u56de\u6eda\u8bf7\u53c2\u8003\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff1a helm history spiderpool --namespace = kube-system helm rollback spiderpool [ REVISION ] --namespace = kube-system \u7248\u672c\u5177\u4f53\u8bf4\u660e \u4e0b\u5217\u7684\u5347\u7ea7\u6ce8\u610f\u4e8b\u9879\uff0c\u5c06\u968f\u7740\u65b0\u7248\u672c\u7684\u53d1\u5e03\u6eda\u52a8\u66f4\u65b0\uff0c\u5b83\u4eec\u5c06\u5b58\u5728\u4f18\u5148\u7ea7\u5173\u7cfb\uff08\u4ece\u65e7\u5230\u65b0\uff09\uff0c\u60a8\u7684\u5f53\u524d\u7248\u672c\u6ee1\u8db3\u4efb\u4f55\u4e00\u9879\uff0c\u5728\u8fdb\u884c\u5347\u7ea7\u65f6\uff0c\u9700\u8981\u4f9d\u6b21\u68c0\u67e5\u4ece\u8be5\u9879\u5230\u6700\u65b0\u7684\u6bcf\u4e00\u4e2a\u6ce8\u610f\u4e8b\u9879\u3002 \u4f4e\u4e8e 0.3.6\uff08\u4e0d\u5305\u542b 0.3.6\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u5728\u4f4e\u4e8e 0.3.6 \u7684\u7248\u672c\u4e2d\uff0c\u4f7f\u7528\u4e86 - \u4f5c\u4e3a SpiderSubnet \u81ea\u52a8\u6c60\u540d\u79f0\u7684\u5206\u9694\u7b26\u3002\u6700\u7ec8\u5f88\u96be\u5c06\u5176\u89e3\u538b\u4ee5\u8ffd\u6eaf\u81ea\u52a8\u6c60\u6240\u5bf9\u5e94\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u547d\u540d\u7a7a\u95f4\u548c\u540d\u79f0\uff0c\u5728\u8fd9\u4e9b\u7248\u672c\u4e2d SpiderSubnet \u529f\u80fd\u662f\u5b58\u5728\u8bbe\u8ba1\u7f3a\u9677\u7684\uff0c\u5728\u6700\u65b0\u7684\u8865\u4e01\u7248\u672c\u4e2d\uff0c\u5bf9\u6b64\u505a\u4e86\u4fee\u6539\u4e0e\u4f18\u5316\uff0c\u5e76\u4e14\u5728 0.3.6 \u5f80\u540e\u7684\u7248\u672c\u4e2d\u652f\u6301\u4e86 SpiderSubnet \u529f\u80fd\u7684\u591a\u4e2a\u7f51\u7edc\u63a5\u53e3\u3002\u5982\u4e0a\u6240\u8ff0\uff0c\u65b0\u7248\u672c\u4e2d\u65b0\u5efa\u7684\u81ea\u52a8\u6c60\u540d\u79f0\u5df2\u53d1\u751f\u4e86\u6539\u53d8\uff0c\u4f8b\u5982\uff0c\u5e94\u7528 kube-system/test-app \u5bf9\u5e94\u7684 IPv4 \u81ea\u52a8\u6c60\u4e3a auto4-test-app-eth0-40371 \u3002 \u540c\u65f6\u81ea\u52a8\u6c60\u4e2d\u88ab\u6807\u8bb0\u4e86\u5982\u4e0b\u7684\u4e00\u4e9b label\u3002 metadata: labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172 -100-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: DaemonSet ipam.spidernet.io/owner-application-name: test-app ipam.spidernet.io/owner-application-namespace: kube-system ipam.spidernet.io/owner-application-uid: 2f78ccdd-398e-49e6-a85b-40371db6fdbd ipam.spidernet.io/owner-spider-subnet: vlan100-v4 spec: podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: DaemonSet ipam.spidernet.io/app-name: test-app ipam.spidernet.io/app-namespace: kube-system \u4f4e\u4e8e 0.3.6 \u5347\u7ea7\u6700\u65b0\u8865\u4e01\u7248\u672c\u5c5e\u4e8e\u4e0d\u517c\u5bb9\u5347\u7ea7\uff0c\u5982\u679c\u542f\u7528\u4e86 SpiderSubnet \u529f\u80fd\uff0c\u4e3a\u4f7f\u5b58\u91cf\u81ea\u52a8\u6c60\u53ef\u7528\uff0c\u9700\u8981\u4e3a\u5b58\u91cf\u7684\u81ea\u52a8\u6c60\u589e\u52a0\u5982\u4e0a\u6240\u8ff0\u7684\u4e00\u7cfb\u5217\u6807\u7b7e\uff0c\u64cd\u4f5c\u5982\u4e0b\uff1a kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-name\": \"test-app\"}}}' kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-namespace\": \"kube-system\"}}}' ... \u540c\u65f6 SpiderSubnet \u652f\u6301\u4e86\u591a\u7f51\u7edc\u63a5\u53e3\uff0c\u9700\u8981\u4e3a\u81ea\u52a8\u6c60\u589e\u52a0\u5bf9\u5e94\u7684\u7f51\u7edc\u63a5\u53e3 label \uff0c\u5982\u4e0b\uff1a kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' \u4f4e\u4e8e 0.4.0\uff08\u4e0d\u5305\u542b 0.4.0\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u7531\u4e8e\u67b6\u6784\u8c03\u6574\uff0c SpiderEndpoint.Status.OwnerControllerType \u5c5e\u6027\u4ece None \u66f4\u6539\u4e3a Pod \u3002 \u6545\u67e5\u627e\u6240\u6709 Status.OwnerControllerType \u4e3a None \u7684 SpiderEndpoint \u5bf9\u8c61\uff0c\u5c06 SpiderEndpoint.Status.OwnerControllerType \u5c5e\u6027\u4ece None \u66ff\u6362\u4e3a Pod \u3002 \u4f4e\u4e8e 0.5.0\uff08\u5305\u542b 0.5.0\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u5728\u9ad8\u4e8e 0.5.0 \u7684\u7248\u672c\u4e2d\uff0c\u65b0\u589e\u4e86 SpiderMultusConfig \u548c Coordinator \u529f\u80fd\u3002\u4f46\u7531\u4e8e helm upgrade \u5347\u7ea7\u65f6\uff0c\u65e0\u6cd5\u81ea\u52a8\u53bb\u5b89\u88c5\u5bf9\u5e94\u7684 CRD\uff1a spidercoordinators.spiderpool.spidernet.io \u548c spidermultusconfigs.spiderpool.spidernet.io \u3002\u6545\u5728\u5347\u7ea7\u524d\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u83b7\u53d6\u6700\u65b0\u7684\u7a33\u5b9a\u7248\u672c\uff0c\u5e76\u89e3\u538b chart \u5305\u5e76\u5e94\u7528\u6240\u6709 CRD\u3002 ~# helm search repo spiderpool --versions # \u8bf7\u66ff\u6362 [upgraded-version] \u4e3a\u8981\u5347\u7ea7\u5230\u7684\u7248\u672c\u3002 ~# helm fetch spiderpool/spiderpool --version [ upgraded-version ] ~# tar -xvf spiderpool- [ upgraded-version ] .tgz && cd spiderpool/crds ~# ls | grep '\\.yaml$' | xargs -I {} kubectl apply -f {} \u4f4e\u4e8e 0.7.3\uff08\u5305\u542b 0.7.3\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u5728 0.7.3 \u4ee5\u4e0b\u7248\u672c\u4e2d\uff0cSpiderpool \u4f1a\u542f\u7528\u4e00\u7ec4 DaemonSet: spiderpool-multus \u6765\u7ba1\u7406 Multus \u76f8\u5173\u914d\u7f6e\u3002\u5728\u66f4\u9ad8\u7248\u672c\u4e2d\uff0c\u5f03\u7528\u4e86\u8be5 DaemonSet\uff0c\u5e76\u4e14\u5c06 Multus \u7684\u914d\u7f6e\u79fb\u5230\u4e86 spiderpool-agent \u4e2d\u7eb3\u7ba1\uff0c\u540c\u65f6\u65b0\u589e\u4e86 \u5378\u8f7d\u65f6\u81ea\u52a8\u6e05\u7406 Muluts \u914d\u7f6e \u7684\u529f\u80fd\uff0c\u5b83\u9ed8\u8ba4\u662f\u542f\u7528\u7684\u3002\u5728\u5347\u7ea7\u65f6\u901a\u8fc7 --set multus.multusCNI.uninstall=false \u7981\u7528\u5b83\uff0c\u907f\u514d\u5728\u5347\u7ea7\u9636\u6bb5 CNI \u914d\u7f6e\u6587\u4ef6\u3001CRD \u7b49\u88ab\u5220\u9664\uff0c\u4ece\u800c\u5bfc\u81f4 Pod \u521b\u5efa\u5931\u8d25\u3002 \u4f4e\u4e8e 0.9.0 (\u4e0d\u5305\u542b 0.9.0) \u5347\u7ea7\u5230\u6700\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u7531\u4e8e\u5728 0.9.0 \u7684\u7248\u672c\u4e2d\uff0c\u6211\u4eec\u7ed9 SpiderCoordinator CRD \u8865\u5145\u4e86 txQueueLen \u5b57\u6bb5\uff0c\u4f46\u7531\u4e8e\u6267\u884c\u5347\u7ea7\u65f6 Helm \u4e0d\u652f\u6301\u5347\u7ea7\u6216\u5220\u9664 CRD\uff0c\u56e0\u6b64\u5728\u5347\u7ea7\u524d\u9700\u8981\u4f60\u624b\u52a8\u66f4\u65b0\u4e00\u4e0b CRD\u3002(\u5efa\u8bae\u8d8a\u8fc7 0.9.0 \u76f4\u63a5\u5347\u7ea7\u81f3 0.9.1 \u7248\u672c) \u4f4e\u4e8e 0.9.4 (\u5305\u542b 0.9.4) \u5347\u7ea7\u5230\u6700\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u5728 0.9.4 \u4ee5\u4e0b\u7684\u7248\u672c\u4e2d\uff0cstatefulSet \u5e94\u7528\u5728\u5feb\u901f\u6269\u7f29\u5bb9\u573a\u666f\u4e0b\uff0cSpiderpool GC \u53ef\u80fd\u4f1a\u9519\u8bef\u7684\u56de\u6536\u6389 IPPool \u4e2d\u7684 IP \u5730\u5740\uff0c\u5bfc\u81f4\u540c\u4e00\u4e2a IP \u88ab\u5206\u914d\u7ed9 K8S \u96c6\u7fa4\u7684\u591a\u4e2a Pod\uff0c\u4ece\u800c\u51fa\u73b0 IP \u5730\u5740\u51b2\u7a81\u3002\u8be5\u95ee\u9898\u5df2\u4fee\u590d\uff0c\u53c2\u8003 \u4fee\u590d \uff0c\u4f46\u5728\u5347\u7ea7\u540e\uff0c\u51b2\u7a81\u7684 IP \u5730\u5740\u5e76\u4e0d\u80fd\u81ea\u52a8\u88ab Spiderpool \u7ea0\u6b63\u56de\u6765\uff0c\u60a8\u9700\u8981\u901a\u8fc7\u624b\u52a8\u91cd\u542f\u51b2\u7a81 IP \u7684 Pod \u6765\u8f85\u52a9\u89e3\u51b3\uff0c\u5728\u65b0\u7248\u672c\u4e2d\u4e0d\u4f1a\u518d\u51fa\u73b0\u9519\u8bef GC IP \u800c\u5bfc\u81f4 IP \u51b2\u7a81\u7684\u95ee\u9898\u3002 \u4f4e\u4e8e 0.9.5 (\u4e0d\u5305\u542b 0.9.5) \u5347\u7ea7\u5230\u6700\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879 \u5728 0.9.5 \u4ee5\u4e0b\u7684\u7248\u672c\u4e2d\uff0cSpiderpool Charts values.yaml \u4e2d\u7684 spiderSubnet \u5b57\u6bb5\u53d1\u751f\u53d8\u66f4\uff0c\u7531 ipam.spidersubnet \u53d8\u66f4\u4e3a ipam.spiderSubnet \uff0c\u56e0\u6b64\uff0c\u60a8\u65e0\u6cd5\u53ef\u4ee5\u5b89\u5168\u5730\u4f7f\u7528 --reuse-values \u6807\u5fd7\u4ece 0.9.5 \u4ee5\u4e0b\u7248\u672c\u5347\u7ea7\u5230 0.9.5 \u53ca\u4ee5\u4e0a\u7248\u672c\u3002\u8bf7\u4fee\u6539 values.yaml \u6587\u4ef6\uff0c\u6216\u8005\u4f7f\u7528 --set ipam.spiderSubnet.enable=true \u6807\u5fd7\u6765\u8986\u76d6 values.yaml \u6587\u4ef6\u4e2d\u7684\u503c\u3002 \u66f4\u591a\u7248\u672c\u5347\u7ea7\u7684\u6ce8\u610f\u4e8b\u9879 TODO. FAQ \u7531\u4e8e\u60a8\u5bf9 Spiderpool \u9ad8\u53ef\u7528\u7684\u8981\u6c42\uff0c\u60a8\u5728\u5b89\u88c5\u65f6\u53ef\u80fd\u4f1a\u901a\u8fc7 --set spiderpoolController.replicas=5 \u8bbe\u7f6e spiderpool-controller Pod \u591a\u526f\u672c\uff0cspiderpool-controller \u7684 Pod \u4f1a\u9ed8\u8ba4\u5360\u7528\u8282\u70b9\u7684\u4e00\u4e9b\u7aef\u53e3\u5730\u5740\uff0c\u9ed8\u8ba4\u7aef\u53e3\u5360\u7528\u53c2\u8003 \u7cfb\u7edf\u914d\u7f6e \uff0c\u5982\u679c\u60a8\u7684\u526f\u672c\u6570\u4e0e\u8282\u70b9\u6570\u521a\u597d\u5c31\u76f8\u540c\uff0c\u90a3\u4e48\u5728\u5347\u7ea7\u65f6 Pod \u5c06\u4f1a\u56e0\u4e3a\u8282\u70b9\u65e0\u53ef\u7528\u7aef\u53e3\u800c\u542f\u52a8\u5931\u8d25\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u8fdb\u884c\u4fee\u6539\u3002 \u6267\u884c\u5347\u7ea7\u547d\u4ee4\u65f6\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u9644\u52a0 helm \u53c2\u6570 --set spiderpoolController.httpPort \u5bf9\u7aef\u53e3\u66f4\u6539\uff0c\u53ef\u4ee5\u901a\u8fc7 helm Values.yaml \u548c \u7cfb\u7edf\u914d\u7f6e \u67e5\u770b\u9700\u8981\u4fee\u6539\u7684\u7aef\u53e3\u3002 spiderpool-controller \u7684\u63a7\u5236\u5668\u7c7b\u578b\u662f Deployment , \u60a8\u53ef\u4ee5\u901a\u8fc7\u7f29\u51cf\u526f\u672c\u6570\uff0c\u8ba9 Pod \u6b63\u5e38\u542f\u52a8\u540e\uff0c\u518d\u6062\u590d\u526f\u672c\u6570\u3002","title":"\u5347\u7ea7\u6307\u5357"},{"location":"usage/install/upgrade-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u5347\u7ea7\u6307\u5357\u9002\u7528\u4e8e\u5728 Kubernetes \u4e0a\u8fd0\u884c\u7684 Spiderpool\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u7591\u95ee\uff0c\u8bf7\u968f\u65f6\u901a\u8fc7 Spiderpool Community \u8054\u7cfb\u6211\u4eec\u3002","title":"\u5347\u7ea7\u6307\u5357"},{"location":"usage/install/upgrade-zh_CN/#_2","text":"\u5728\u6267\u884c\u5347\u7ea7\u4e4b\u524d\uff0c\u8bf7\u9605\u8bfb\u5b8c\u6574\u7684\u5347\u7ea7\u6307\u5357\u4ee5\u4e86\u89e3\u6240\u6709\u5fc5\u8981\u7684\u6b65\u9aa4\u3002 \u5728 Kubernetes \u8fdb\u884c Spiderpool \u5347\u7ea7\u65f6\uff0cKubernetes \u9996\u5148\u5c06\u7ec8\u6b62\u5df2\u6709 Pod\uff0c\u7136\u540e\u62c9\u53d6\u65b0\u7684\u955c\u50cf\u7248\u672c\uff0c\u6700\u540e\u4f7f\u7528\u65b0\u7684\u955c\u50cf\u542f\u52a8 Pod\u3002\u4e3a\u4e86\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u5e76\u9632\u6b62\u5347\u7ea7\u671f\u95f4\u53d1\u751f ErrImagePull \u9519\u8bef\uff0c\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u63d0\u524d\u62c9\u53d6\u5bf9\u5e94\u7248\u672c\u7684\u955c\u50cf\u3002 # \u4ee5 docker \u4e3a\u4f8b\uff0c\u8bf7\u4fee\u6539 [upgraded-version] \u4e3a\u4f60\u5347\u7ea7\u7684\u7248\u672c\u3002 docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ] # \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u5927\u9646\u7528\u6237\uff0c\u53ef\u4ee5\u4f7f\u7528\u955c\u50cf\u6e90 ghcr.m.daocloud.io docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ]","title":"\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#_3","text":"\u5efa\u8bae\u6bcf\u6b21\u90fd\u5347\u7ea7\u5230 Spiderpool \u7684\u6700\u65b0\u4e14\u88ab\u7ef4\u62a4\u7684\u8865\u4e01\u7248\u672c\u3002\u901a\u8fc7 Stable Releases \u4e86\u89e3\u53d7\u5230\u652f\u6301\u7684\u6700\u65b0\u8865\u4e01\u7248\u672c\u3002","title":"\u6b65\u9aa4"},{"location":"usage/install/upgrade-zh_CN/#helm-spiderpool","text":"\u786e\u4fdd\u60a8\u5df2\u5b89\u88c5 Helm \u3002 \u8bbe\u7f6e Helm \u5b58\u50a8\u5e93\u5e76\u66f4\u65b0 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool \u5220\u9664 spiderpool-init Pod spiderpool-init Pod \u4f1a\u5e2e\u52a9\u521d\u59cb\u5316\u73af\u5883\u4fe1\u606f\uff0c\u6bcf\u6b21\u8fd0\u884c\u5b8c\u6bd5\u540e\u5176\u5904\u4e8e complete \u72b6\u6001\u3002\u5728 helm upgrade \u65f6\uff0c\u7531\u4e8e spiderpool-init \u672c\u8d28\u662f\u4e00\u4e2a Pod \uff0c\u5f53\u4f60\u8981\u5347\u7ea7\u5230\u7684\u7248\u672c\u53d1\u751f\u4e86\u8d44\u6e90\u53d8\u66f4\uff0c\u5c06\u4f1a Patch \u5931\u8d25\uff0c\u4ecb\u4e8e\u53ef\u80fd\u5e76\u4e0d\u6e05\u695a\u5177\u4f53\u7684\u53d8\u66f4\uff0c\u5efa\u8bae\u5347\u7ea7\u524d\u624b\u52a8\u5220\u9664\u4e00\u4e0b spiderpool-init Pod\uff0c\u907f\u514d\u51fa\u73b0 helm upgrade \u5931\u8d25\u7684\u60c5\u51b5\u3002 Error: UPGRADE FAILED: cannot patch \"spiderpool-init\" with kind Pod: Pod \"spiderpool-init\" is invalid: spec: Forbidden: pod updates may not change fields other than ` spec.containers [ * ] .image ` , ` spec.initContainers [ * ] .image ` , ` spec.activeDeadlineSeconds ` , ` spec.tolerations ` ( only additions to existing tolerations ) , ` spec.terminationGracePeriodSeconds ` ( allow it to be set to 1 if it was previously negative ) helm upgrade \u5347\u7ea7 # -n \u6307\u5b9a\u4f60 Spiderpool \u6240\u5728\u547d\u540d\u7a7a\u95f4\uff0c\u5e76\u4fee\u6539 [upgraded-version] \u4e3a\u4f60\u8981\u5347\u7ea7\u5230\u7684\u7248\u672c\u3002 helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ]","title":"\u4f7f\u7528 Helm \u5347\u7ea7 Spiderpool"},{"location":"usage/install/upgrade-zh_CN/#_4","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7 --set \u5728\u5347\u7ea7\u65f6\u53bb\u66f4\u65b0 Spiderpool \u914d\u7f6e\uff0c\u53ef\u7528\u7684 values \u53c2\u6570\uff0c\u8bf7\u67e5\u770b values \u8bf4\u660e\u6587\u6863\u3002 \u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5f00\u542f Spiderpool \u7684 SpiderSubnet \u529f\u80fd helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true \u540c\u65f6\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528 --reuse-values \u91cd\u7528\u4e0a\u4e00\u4e2a release \u7684\u503c\u5e76\u5408\u5e76\u6765\u81ea\u547d\u4ee4\u884c\u7684\u4efb\u4f55\u8986\u76d6\u3002\u4f46\u4ec5\u5f53 Spiderpool chart \u7248\u672c\u4fdd\u6301\u4e0d\u53d8\u65f6\uff0c\u624d\u53ef\u4ee5\u5b89\u5168\u5730\u4f7f\u7528 --reuse-values \u6807\u5fd7\uff0c\u4f8b\u5982\uff0c\u5f53\u4f7f\u7528 helm upgrade \u6765\u66f4\u6539 Spiderpool \u914d\u7f6e\u800c\u4e0d\u5347\u7ea7 Spiderpool \u7ec4\u4ef6\u3002 --reuse-values \u4f7f\u7528\uff0c\u53c2\u8003\u5982\u4e0b\u793a\u4f8b\uff1a helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true --reuse-values \u76f8\u53cd\uff0c\u5982\u679c Spiderpool chart \u7248\u672c\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u60a8\u60f3\u91cd\u7528\u73b0\u6709\u5b89\u88c5\u4e2d\u7684\u503c\uff0c\u8bf7\u5c06\u65e7\u503c\u4fdd\u5b58\u5728\u503c\u6587\u4ef6\u4e2d\uff0c\u68c0\u67e5\u8be5\u6587\u4ef6\u4e2d\u662f\u5426\u6709\u4efb\u4f55\u91cd\u547d\u540d\u6216\u5f03\u7528\u7684\u503c\uff0c\u7136\u540e\u5c06\u5176\u4f20\u9012\u7ed9 helm upgrade \u547d\u4ee4\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u68c0\u7d22\u5e76\u4fdd\u5b58\u73b0\u6709\u5b89\u88c5\u4e2d\u7684\u503c\uff1a helm get values spiderpool --namespace = kube-system -o yaml > old-values.yaml helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] -f old-values.yaml","title":"\u914d\u7f6e\u5347\u7ea7"},{"location":"usage/install/upgrade-zh_CN/#_5","text":"\u6709\u65f6\u7531\u4e8e\u5347\u7ea7\u8fc7\u7a0b\u4e2d\u9057\u6f0f\u4e86\u67d0\u4e2a\u6b65\u9aa4\u6216\u51fa\u73b0\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u56de\u6eda\u5347\u7ea7\u3002 \u8981\u56de\u6eda\u8bf7\u53c2\u8003\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff1a helm history spiderpool --namespace = kube-system helm rollback spiderpool [ REVISION ] --namespace = kube-system","title":"\u5347\u7ea7\u56de\u6eda"},{"location":"usage/install/upgrade-zh_CN/#_6","text":"\u4e0b\u5217\u7684\u5347\u7ea7\u6ce8\u610f\u4e8b\u9879\uff0c\u5c06\u968f\u7740\u65b0\u7248\u672c\u7684\u53d1\u5e03\u6eda\u52a8\u66f4\u65b0\uff0c\u5b83\u4eec\u5c06\u5b58\u5728\u4f18\u5148\u7ea7\u5173\u7cfb\uff08\u4ece\u65e7\u5230\u65b0\uff09\uff0c\u60a8\u7684\u5f53\u524d\u7248\u672c\u6ee1\u8db3\u4efb\u4f55\u4e00\u9879\uff0c\u5728\u8fdb\u884c\u5347\u7ea7\u65f6\uff0c\u9700\u8981\u4f9d\u6b21\u68c0\u67e5\u4ece\u8be5\u9879\u5230\u6700\u65b0\u7684\u6bcf\u4e00\u4e2a\u6ce8\u610f\u4e8b\u9879\u3002","title":"\u7248\u672c\u5177\u4f53\u8bf4\u660e"},{"location":"usage/install/upgrade-zh_CN/#036-036","text":"\u5728\u4f4e\u4e8e 0.3.6 \u7684\u7248\u672c\u4e2d\uff0c\u4f7f\u7528\u4e86 - \u4f5c\u4e3a SpiderSubnet \u81ea\u52a8\u6c60\u540d\u79f0\u7684\u5206\u9694\u7b26\u3002\u6700\u7ec8\u5f88\u96be\u5c06\u5176\u89e3\u538b\u4ee5\u8ffd\u6eaf\u81ea\u52a8\u6c60\u6240\u5bf9\u5e94\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u547d\u540d\u7a7a\u95f4\u548c\u540d\u79f0\uff0c\u5728\u8fd9\u4e9b\u7248\u672c\u4e2d SpiderSubnet \u529f\u80fd\u662f\u5b58\u5728\u8bbe\u8ba1\u7f3a\u9677\u7684\uff0c\u5728\u6700\u65b0\u7684\u8865\u4e01\u7248\u672c\u4e2d\uff0c\u5bf9\u6b64\u505a\u4e86\u4fee\u6539\u4e0e\u4f18\u5316\uff0c\u5e76\u4e14\u5728 0.3.6 \u5f80\u540e\u7684\u7248\u672c\u4e2d\u652f\u6301\u4e86 SpiderSubnet \u529f\u80fd\u7684\u591a\u4e2a\u7f51\u7edc\u63a5\u53e3\u3002\u5982\u4e0a\u6240\u8ff0\uff0c\u65b0\u7248\u672c\u4e2d\u65b0\u5efa\u7684\u81ea\u52a8\u6c60\u540d\u79f0\u5df2\u53d1\u751f\u4e86\u6539\u53d8\uff0c\u4f8b\u5982\uff0c\u5e94\u7528 kube-system/test-app \u5bf9\u5e94\u7684 IPv4 \u81ea\u52a8\u6c60\u4e3a auto4-test-app-eth0-40371 \u3002 \u540c\u65f6\u81ea\u52a8\u6c60\u4e2d\u88ab\u6807\u8bb0\u4e86\u5982\u4e0b\u7684\u4e00\u4e9b label\u3002 metadata: labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172 -100-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: DaemonSet ipam.spidernet.io/owner-application-name: test-app ipam.spidernet.io/owner-application-namespace: kube-system ipam.spidernet.io/owner-application-uid: 2f78ccdd-398e-49e6-a85b-40371db6fdbd ipam.spidernet.io/owner-spider-subnet: vlan100-v4 spec: podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: DaemonSet ipam.spidernet.io/app-name: test-app ipam.spidernet.io/app-namespace: kube-system \u4f4e\u4e8e 0.3.6 \u5347\u7ea7\u6700\u65b0\u8865\u4e01\u7248\u672c\u5c5e\u4e8e\u4e0d\u517c\u5bb9\u5347\u7ea7\uff0c\u5982\u679c\u542f\u7528\u4e86 SpiderSubnet \u529f\u80fd\uff0c\u4e3a\u4f7f\u5b58\u91cf\u81ea\u52a8\u6c60\u53ef\u7528\uff0c\u9700\u8981\u4e3a\u5b58\u91cf\u7684\u81ea\u52a8\u6c60\u589e\u52a0\u5982\u4e0a\u6240\u8ff0\u7684\u4e00\u7cfb\u5217\u6807\u7b7e\uff0c\u64cd\u4f5c\u5982\u4e0b\uff1a kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-name\": \"test-app\"}}}' kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-namespace\": \"kube-system\"}}}' ... \u540c\u65f6 SpiderSubnet \u652f\u6301\u4e86\u591a\u7f51\u7edc\u63a5\u53e3\uff0c\u9700\u8981\u4e3a\u81ea\u52a8\u6c60\u589e\u52a0\u5bf9\u5e94\u7684\u7f51\u7edc\u63a5\u53e3 label \uff0c\u5982\u4e0b\uff1a kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}'","title":"\u4f4e\u4e8e 0.3.6\uff08\u4e0d\u5305\u542b 0.3.6\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#040-040","text":"\u7531\u4e8e\u67b6\u6784\u8c03\u6574\uff0c SpiderEndpoint.Status.OwnerControllerType \u5c5e\u6027\u4ece None \u66f4\u6539\u4e3a Pod \u3002 \u6545\u67e5\u627e\u6240\u6709 Status.OwnerControllerType \u4e3a None \u7684 SpiderEndpoint \u5bf9\u8c61\uff0c\u5c06 SpiderEndpoint.Status.OwnerControllerType \u5c5e\u6027\u4ece None \u66ff\u6362\u4e3a Pod \u3002","title":"\u4f4e\u4e8e 0.4.0\uff08\u4e0d\u5305\u542b 0.4.0\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#050-050","text":"\u5728\u9ad8\u4e8e 0.5.0 \u7684\u7248\u672c\u4e2d\uff0c\u65b0\u589e\u4e86 SpiderMultusConfig \u548c Coordinator \u529f\u80fd\u3002\u4f46\u7531\u4e8e helm upgrade \u5347\u7ea7\u65f6\uff0c\u65e0\u6cd5\u81ea\u52a8\u53bb\u5b89\u88c5\u5bf9\u5e94\u7684 CRD\uff1a spidercoordinators.spiderpool.spidernet.io \u548c spidermultusconfigs.spiderpool.spidernet.io \u3002\u6545\u5728\u5347\u7ea7\u524d\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u83b7\u53d6\u6700\u65b0\u7684\u7a33\u5b9a\u7248\u672c\uff0c\u5e76\u89e3\u538b chart \u5305\u5e76\u5e94\u7528\u6240\u6709 CRD\u3002 ~# helm search repo spiderpool --versions # \u8bf7\u66ff\u6362 [upgraded-version] \u4e3a\u8981\u5347\u7ea7\u5230\u7684\u7248\u672c\u3002 ~# helm fetch spiderpool/spiderpool --version [ upgraded-version ] ~# tar -xvf spiderpool- [ upgraded-version ] .tgz && cd spiderpool/crds ~# ls | grep '\\.yaml$' | xargs -I {} kubectl apply -f {}","title":"\u4f4e\u4e8e 0.5.0\uff08\u5305\u542b 0.5.0\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#073-073","text":"\u5728 0.7.3 \u4ee5\u4e0b\u7248\u672c\u4e2d\uff0cSpiderpool \u4f1a\u542f\u7528\u4e00\u7ec4 DaemonSet: spiderpool-multus \u6765\u7ba1\u7406 Multus \u76f8\u5173\u914d\u7f6e\u3002\u5728\u66f4\u9ad8\u7248\u672c\u4e2d\uff0c\u5f03\u7528\u4e86\u8be5 DaemonSet\uff0c\u5e76\u4e14\u5c06 Multus \u7684\u914d\u7f6e\u79fb\u5230\u4e86 spiderpool-agent \u4e2d\u7eb3\u7ba1\uff0c\u540c\u65f6\u65b0\u589e\u4e86 \u5378\u8f7d\u65f6\u81ea\u52a8\u6e05\u7406 Muluts \u914d\u7f6e \u7684\u529f\u80fd\uff0c\u5b83\u9ed8\u8ba4\u662f\u542f\u7528\u7684\u3002\u5728\u5347\u7ea7\u65f6\u901a\u8fc7 --set multus.multusCNI.uninstall=false \u7981\u7528\u5b83\uff0c\u907f\u514d\u5728\u5347\u7ea7\u9636\u6bb5 CNI \u914d\u7f6e\u6587\u4ef6\u3001CRD \u7b49\u88ab\u5220\u9664\uff0c\u4ece\u800c\u5bfc\u81f4 Pod \u521b\u5efa\u5931\u8d25\u3002","title":"\u4f4e\u4e8e 0.7.3\uff08\u5305\u542b 0.7.3\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#090-090","text":"\u7531\u4e8e\u5728 0.9.0 \u7684\u7248\u672c\u4e2d\uff0c\u6211\u4eec\u7ed9 SpiderCoordinator CRD \u8865\u5145\u4e86 txQueueLen \u5b57\u6bb5\uff0c\u4f46\u7531\u4e8e\u6267\u884c\u5347\u7ea7\u65f6 Helm \u4e0d\u652f\u6301\u5347\u7ea7\u6216\u5220\u9664 CRD\uff0c\u56e0\u6b64\u5728\u5347\u7ea7\u524d\u9700\u8981\u4f60\u624b\u52a8\u66f4\u65b0\u4e00\u4e0b CRD\u3002(\u5efa\u8bae\u8d8a\u8fc7 0.9.0 \u76f4\u63a5\u5347\u7ea7\u81f3 0.9.1 \u7248\u672c)","title":"\u4f4e\u4e8e 0.9.0 (\u4e0d\u5305\u542b 0.9.0) \u5347\u7ea7\u5230\u6700\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#094-094","text":"\u5728 0.9.4 \u4ee5\u4e0b\u7684\u7248\u672c\u4e2d\uff0cstatefulSet \u5e94\u7528\u5728\u5feb\u901f\u6269\u7f29\u5bb9\u573a\u666f\u4e0b\uff0cSpiderpool GC \u53ef\u80fd\u4f1a\u9519\u8bef\u7684\u56de\u6536\u6389 IPPool \u4e2d\u7684 IP \u5730\u5740\uff0c\u5bfc\u81f4\u540c\u4e00\u4e2a IP \u88ab\u5206\u914d\u7ed9 K8S \u96c6\u7fa4\u7684\u591a\u4e2a Pod\uff0c\u4ece\u800c\u51fa\u73b0 IP \u5730\u5740\u51b2\u7a81\u3002\u8be5\u95ee\u9898\u5df2\u4fee\u590d\uff0c\u53c2\u8003 \u4fee\u590d \uff0c\u4f46\u5728\u5347\u7ea7\u540e\uff0c\u51b2\u7a81\u7684 IP \u5730\u5740\u5e76\u4e0d\u80fd\u81ea\u52a8\u88ab Spiderpool \u7ea0\u6b63\u56de\u6765\uff0c\u60a8\u9700\u8981\u901a\u8fc7\u624b\u52a8\u91cd\u542f\u51b2\u7a81 IP \u7684 Pod \u6765\u8f85\u52a9\u89e3\u51b3\uff0c\u5728\u65b0\u7248\u672c\u4e2d\u4e0d\u4f1a\u518d\u51fa\u73b0\u9519\u8bef GC IP \u800c\u5bfc\u81f4 IP \u51b2\u7a81\u7684\u95ee\u9898\u3002","title":"\u4f4e\u4e8e 0.9.4 (\u5305\u542b 0.9.4) \u5347\u7ea7\u5230\u6700\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#095-095","text":"\u5728 0.9.5 \u4ee5\u4e0b\u7684\u7248\u672c\u4e2d\uff0cSpiderpool Charts values.yaml \u4e2d\u7684 spiderSubnet \u5b57\u6bb5\u53d1\u751f\u53d8\u66f4\uff0c\u7531 ipam.spidersubnet \u53d8\u66f4\u4e3a ipam.spiderSubnet \uff0c\u56e0\u6b64\uff0c\u60a8\u65e0\u6cd5\u53ef\u4ee5\u5b89\u5168\u5730\u4f7f\u7528 --reuse-values \u6807\u5fd7\u4ece 0.9.5 \u4ee5\u4e0b\u7248\u672c\u5347\u7ea7\u5230 0.9.5 \u53ca\u4ee5\u4e0a\u7248\u672c\u3002\u8bf7\u4fee\u6539 values.yaml \u6587\u4ef6\uff0c\u6216\u8005\u4f7f\u7528 --set ipam.spiderSubnet.enable=true \u6807\u5fd7\u6765\u8986\u76d6 values.yaml \u6587\u4ef6\u4e2d\u7684\u503c\u3002","title":"\u4f4e\u4e8e 0.9.5 (\u4e0d\u5305\u542b 0.9.5) \u5347\u7ea7\u5230\u6700\u9ad8\u7248\u672c\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#_7","text":"TODO.","title":"\u66f4\u591a\u7248\u672c\u5347\u7ea7\u7684\u6ce8\u610f\u4e8b\u9879"},{"location":"usage/install/upgrade-zh_CN/#faq","text":"\u7531\u4e8e\u60a8\u5bf9 Spiderpool \u9ad8\u53ef\u7528\u7684\u8981\u6c42\uff0c\u60a8\u5728\u5b89\u88c5\u65f6\u53ef\u80fd\u4f1a\u901a\u8fc7 --set spiderpoolController.replicas=5 \u8bbe\u7f6e spiderpool-controller Pod \u591a\u526f\u672c\uff0cspiderpool-controller \u7684 Pod \u4f1a\u9ed8\u8ba4\u5360\u7528\u8282\u70b9\u7684\u4e00\u4e9b\u7aef\u53e3\u5730\u5740\uff0c\u9ed8\u8ba4\u7aef\u53e3\u5360\u7528\u53c2\u8003 \u7cfb\u7edf\u914d\u7f6e \uff0c\u5982\u679c\u60a8\u7684\u526f\u672c\u6570\u4e0e\u8282\u70b9\u6570\u521a\u597d\u5c31\u76f8\u540c\uff0c\u90a3\u4e48\u5728\u5347\u7ea7\u65f6 Pod \u5c06\u4f1a\u56e0\u4e3a\u8282\u70b9\u65e0\u53ef\u7528\u7aef\u53e3\u800c\u542f\u52a8\u5931\u8d25\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u8fdb\u884c\u4fee\u6539\u3002 \u6267\u884c\u5347\u7ea7\u547d\u4ee4\u65f6\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u9644\u52a0 helm \u53c2\u6570 --set spiderpoolController.httpPort \u5bf9\u7aef\u53e3\u66f4\u6539\uff0c\u53ef\u4ee5\u901a\u8fc7 helm Values.yaml \u548c \u7cfb\u7edf\u914d\u7f6e \u67e5\u770b\u9700\u8981\u4fee\u6539\u7684\u7aef\u53e3\u3002 spiderpool-controller \u7684\u63a7\u5236\u5668\u7c7b\u578b\u662f Deployment , \u60a8\u53ef\u4ee5\u901a\u8fc7\u7f29\u51cf\u526f\u672c\u6570\uff0c\u8ba9 Pod \u6b63\u5e38\u542f\u52a8\u540e\uff0c\u518d\u6062\u590d\u526f\u672c\u6570\u3002","title":"FAQ"},{"location":"usage/install/upgrade/","text":"Upgrade Guide English | \u7b80\u4f53\u4e2d\u6587 This upgrade guide is intended for Spiderpool running on Kubernetes. If you have questions, feel free to ping us on Spiderpool Community . Warning Read the full upgrade guide to understand all the necessary steps before performing them. When rolling out an upgrade with Kubernetes, Kubernetes will first terminate the pod followed by pulling the new image version and then finally spin up the new image. In order to reduce the downtime of the agent and to prevent ErrImagePull errors during upgrade. You can refer to the following command to pull the corresponding version of the image in advance. # Taking docker as an example, please modify [upgraded-version] to your upgraded version. docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ] # If you are mainland user who is not available to access ghcr.io, you can use the mirror source ghcr.m.daocloud.io docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ] Steps It is recommended to always upgrade to the latest and maintained patch version of Spiderpool. Check Stable Releases to learn about the latest supported patch versions. Upgrading Spiderpool via Helm Make sure you have Helm installed. Setup Helm repository and update helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool Remove spiderpool-init Pod spiderpool-init Pod will help initialize environment information, and it will be in complete state after each run. During helm upgrade , since spiderpool-init is essentially a Pod, patching some resources will fail. So delete it via kubectl delete spiderpool-init before upgrading. Error: UPGRADE FAILED: cannot patch \"spiderpool-init\" with kind Pod: Pod \"spiderpool-init\" is invalid: spec: Forbidden: pod updates may not change fields other than ` spec.containers [ * ] .image ` , ` spec.initContainers [ * ] .image ` , ` spec.activeDeadlineSeconds ` , ` spec.tolerations ` ( only additions to existing tolerations ) , ` spec.terminationGracePeriodSeconds ` ( allow it to be set to 1 if it was previously negative ) Upgrade via helm upgrade # -n specifies the namespace where your Spiderpool is located, and modify [upgraded-version] to the version you want to upgrade to. helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] Configuration upgrade You can use --set to update the Spiderpool configuration when upgrading. For available values parameters, please see the values documentation. The following example shows how to enable Spiderpool's SpiderSubnet function helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true You can also use --reuse-values to reuse the values from the previous release and merge any overrides from the command line. However, it is only safe to use the --reuse-values flag if the Spiderpool chart version remains unchanged, e.g. when using helm upgrade to change the Spiderpool configuration without upgrading the Spiderpool components. For --reuse-values usage, see the following example: helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true --reuse-values Conversely, if the Spiderpool chart version has changed and you want to reuse the values from the existing installation, save the old values in a values file, check that file for any renamed or deprecated values, and pass it to helm upgrade command, you can retrieve and save values from existing installations using. helm get values spiderpool --namespace = kube-system -o yaml > old-values.yaml helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] -f old-values.yaml Rolling Back Occasionally, it may be necessary to undo the rollout because a step was missed or something went wrong during upgrade. To undo the rollout run: helm history spiderpool --namespace = kube-system helm rollback spiderpool [ REVISION ] --namespace = kube-system Version Specific Notes The following upgrade notes will be updated on a rolling basis with the release of new versions. They will have a priority relationship (from old to new). If your current version meets any one of them, when upgrading, you need to check in order from that item to Latest on every note. Upgrading from a version below 0.3.6 (Excludes 0.3.6) to a higher version In versions lower than 0.3.6, - is used as a separator for SpiderSubnet delimiter for autopool names. It was ultimately difficult to extract it to trace the namespace and name of the application to which the autopool corresponded. The SpiderSubnet functionality in these releases was flawed by design, and has been modified and optimised in the latest patch releases, as well as supporting multiple network interfaces for the SpiderSubnet functionality in releases from 0.3.6 onwards. As mentioned above, the names of the new auto pools created in the new release have been changed, e.g., the IPv4 auto pool corresponding to application kube-system/test-app is auto4-test-app-eth0-40371 . At the same time, the auto pool is marked with some labels as follows. metadata: labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172 -100-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: DaemonSet ipam.spidernet.io/owner-application-name: test-app ipam.spidernet.io/owner-application-namespace: kube-system ipam.spidernet.io/owner-application-uid: 2f78ccdd-398e-49e6-a85b-40371db6fdbd ipam.spidernet.io/owner-spider-subnet: vlan100-v4 spec: podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: DaemonSet ipam.spidernet.io/app-name: test-app ipam.spidernet.io/app-namespace: kube-system Upgrading below 0.3.6 to the latest patch version is an incompatible upgrade. If the SpiderSubnet feature is enabled, you will need to add a series of tags as described above to the stock auto pool in order to make it available to the stock auto pool, as follows: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-name\": \"test-app\"}}}' kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-namespace\": \"kube-system\"}}}' ... SpiderSubnet supports multiple network interfaces, you need to add the corresponding network interface label for the auto pool as follows: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Upgrading from a version below 0.4.0 (Excludes 0.4.0) to a higher version Due to architecture adjustment, SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod . Therefore, find all SpiderEndpoint objects with Status.OwnerControllerType of None and replace the SpiderEndpoint.Status.OwnerControllerType property from None to Pod . Upgrading from a version below 0.5.0 (Includes 0.5.0) to a higher version In versions higher than 0.5.0, the SpiderMultusConfig and Coordinator functions are added. However, due to helm upgrade, the corresponding CRDs cannot be automatically installed: spidercoordinators.spiderpool.spidernet.io and spidermultusconfigs.spiderpool.spidernet.io . Therefore, before upgrading, you can obtain the latest stable version through the following commands, decompress the chart package and apply all CRDs. ~# helm search repo spiderpool --versions # Please replace [upgraded-version] with the version you want to upgrade to. ~# helm fetch spiderpool/spiderpool --version [ upgraded-version ] ~# tar -xvf spiderpool- [ upgraded-version ] .tgz && cd spiderpool/crds ~# ls | grep '\\.yaml$' | xargs -I {} kubectl apply -f {} Upgrading from a version below 0.7.3 (Includes 0.7.3) to a higher version In versions below 0.7.3, Spiderpool will enable a set of DaemonSet: spiderpool-multus to manage Multus related configurations. In later versions, the DaemonSet was deprecated, and the Muluts configuration was moved to spiderpool-agent for management. At the same time, the function of automatically cleaning up the Muluts configuration during uninstallation was added, which is enabled by default. Disable it by --set multus.multusCNI.uninstall=false when upgrading to avoid CNI configuration files, CRDs, etc. being deleted during the upgrade phase, causing Pod creation to fail. Upgrading from a version below 0.9.0 (Excludes 0.9.0) to a higher version Due to the addition of the txQueueLen field to the SpiderCoordinator CRD in version 0.9.0, you need to manually update the CRD before upgrading as Helm does not support upgrading or deleting CRDs during the upgrade process.(We suggest skipping version 0.9.0 and upgrading directly to version 0.9.1) Upgrading from a version below 0.9.4 (Includes 0.9.4) to a higher version In versions below 0.9.4, when statefulSet is rapidly scaling up or down, Spiderpool GC may mistakenly reclaim IP addresses in IPPool, causing the same IP to be assigned to multiple Pods in the K8S cluster, resulting in IP address conflicts. This issue has been fixed, see Fix , but after the upgrade, the conflicting IP addresses cannot be automatically corrected by Spiderpool. You need to manually restart the Pod with the conflicting IP to assist in resolving the issue. In the new version, there will no longer be an issue with IP conflicts caused by incorrect GC IPs. Upgrading from a version below 0.9.5 (Excludes 0.9.5) to a higher version In versions lower than 0.9.5, the spiderSubnet field in Spiderpool Charts values.yaml changed from ipam.spidersubnet to ipam.spiderSubnet , so you cannot safely use the --reuse-values flag to upgrade from versions < 0.9.5 to 0.9.5 and above. Please modify the values.yaml file or use the --set ipam.spiderSubnet.enable=true flag to override the value in the values.yaml file. More notes on version upgrades TODO. FAQ Due to your high availability requirements for Spiderpool, you may set multiple replicas of the spiderpool-controller Pod through --set spiderpoolController.replicas=5 during installation. The Pod of spiderpool-controller will occupy some port addresses of the node by default. The default port Please refer to System Configuration for occupancy. If your number of replicas is exactly the same as the number of nodes, then the Pod will fail to start because the node has no available ports during the upgrade. You can refer to the following Modifications can be made in two ways. When executing the upgrade command, you can change the port by appending the helm parameter --set spiderpoolController.httpPort , and you can change the port through helm Values.yaml and System Configuration to check the ports that need to be modified. The type of spiderpool-controller is Deployment . You can reduce the number of replicas and restore the number of replicas after the Pod starts normally.","title":"Upgrading"},{"location":"usage/install/upgrade/#upgrade-guide","text":"English | \u7b80\u4f53\u4e2d\u6587 This upgrade guide is intended for Spiderpool running on Kubernetes. If you have questions, feel free to ping us on Spiderpool Community .","title":"Upgrade Guide"},{"location":"usage/install/upgrade/#warning","text":"Read the full upgrade guide to understand all the necessary steps before performing them. When rolling out an upgrade with Kubernetes, Kubernetes will first terminate the pod followed by pulling the new image version and then finally spin up the new image. In order to reduce the downtime of the agent and to prevent ErrImagePull errors during upgrade. You can refer to the following command to pull the corresponding version of the image in advance. # Taking docker as an example, please modify [upgraded-version] to your upgraded version. docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ] # If you are mainland user who is not available to access ghcr.io, you can use the mirror source ghcr.m.daocloud.io docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-agent: [ upgraded-version ] docker pull ghcr.m.daocloud.io/spidernet-io/spiderpool/spiderpool-controller: [ upgraded-version ]","title":"Warning"},{"location":"usage/install/upgrade/#steps","text":"It is recommended to always upgrade to the latest and maintained patch version of Spiderpool. Check Stable Releases to learn about the latest supported patch versions.","title":"Steps"},{"location":"usage/install/upgrade/#upgrading-spiderpool-via-helm","text":"Make sure you have Helm installed. Setup Helm repository and update helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool Remove spiderpool-init Pod spiderpool-init Pod will help initialize environment information, and it will be in complete state after each run. During helm upgrade , since spiderpool-init is essentially a Pod, patching some resources will fail. So delete it via kubectl delete spiderpool-init before upgrading. Error: UPGRADE FAILED: cannot patch \"spiderpool-init\" with kind Pod: Pod \"spiderpool-init\" is invalid: spec: Forbidden: pod updates may not change fields other than ` spec.containers [ * ] .image ` , ` spec.initContainers [ * ] .image ` , ` spec.activeDeadlineSeconds ` , ` spec.tolerations ` ( only additions to existing tolerations ) , ` spec.terminationGracePeriodSeconds ` ( allow it to be set to 1 if it was previously negative ) Upgrade via helm upgrade # -n specifies the namespace where your Spiderpool is located, and modify [upgraded-version] to the version you want to upgrade to. helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ]","title":"Upgrading Spiderpool via Helm"},{"location":"usage/install/upgrade/#configuration-upgrade","text":"You can use --set to update the Spiderpool configuration when upgrading. For available values parameters, please see the values documentation. The following example shows how to enable Spiderpool's SpiderSubnet function helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true You can also use --reuse-values to reuse the values from the previous release and merge any overrides from the command line. However, it is only safe to use the --reuse-values flag if the Spiderpool chart version remains unchanged, e.g. when using helm upgrade to change the Spiderpool configuration without upgrading the Spiderpool components. For --reuse-values usage, see the following example: helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] --set ipam.spiderSubnet.enable = true --reuse-values Conversely, if the Spiderpool chart version has changed and you want to reuse the values from the existing installation, save the old values in a values file, check that file for any renamed or deprecated values, and pass it to helm upgrade command, you can retrieve and save values from existing installations using. helm get values spiderpool --namespace = kube-system -o yaml > old-values.yaml helm upgrade spiderpool spiderpool/spiderpool -n kube-system --version [ upgraded-version ] -f old-values.yaml","title":"Configuration upgrade"},{"location":"usage/install/upgrade/#rolling-back","text":"Occasionally, it may be necessary to undo the rollout because a step was missed or something went wrong during upgrade. To undo the rollout run: helm history spiderpool --namespace = kube-system helm rollback spiderpool [ REVISION ] --namespace = kube-system","title":"Rolling Back"},{"location":"usage/install/upgrade/#version-specific-notes","text":"The following upgrade notes will be updated on a rolling basis with the release of new versions. They will have a priority relationship (from old to new). If your current version meets any one of them, when upgrading, you need to check in order from that item to Latest on every note.","title":"Version Specific Notes"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-036-excludes-036-to-a-higher-version","text":"In versions lower than 0.3.6, - is used as a separator for SpiderSubnet delimiter for autopool names. It was ultimately difficult to extract it to trace the namespace and name of the application to which the autopool corresponded. The SpiderSubnet functionality in these releases was flawed by design, and has been modified and optimised in the latest patch releases, as well as supporting multiple network interfaces for the SpiderSubnet functionality in releases from 0.3.6 onwards. As mentioned above, the names of the new auto pools created in the new release have been changed, e.g., the IPv4 auto pool corresponding to application kube-system/test-app is auto4-test-app-eth0-40371 . At the same time, the auto pool is marked with some labels as follows. metadata: labels: ipam.spidernet.io/interface: eth0 ipam.spidernet.io/ip-version: IPv4 ipam.spidernet.io/ippool-cidr: 172 -100-0-0-16 ipam.spidernet.io/ippool-reclaim: \"true\" ipam.spidernet.io/owner-application-gv: apps_v1 ipam.spidernet.io/owner-application-kind: DaemonSet ipam.spidernet.io/owner-application-name: test-app ipam.spidernet.io/owner-application-namespace: kube-system ipam.spidernet.io/owner-application-uid: 2f78ccdd-398e-49e6-a85b-40371db6fdbd ipam.spidernet.io/owner-spider-subnet: vlan100-v4 spec: podAffinity: matchLabels: ipam.spidernet.io/app-api-group: apps ipam.spidernet.io/app-api-version: v1 ipam.spidernet.io/app-kind: DaemonSet ipam.spidernet.io/app-name: test-app ipam.spidernet.io/app-namespace: kube-system Upgrading below 0.3.6 to the latest patch version is an incompatible upgrade. If the SpiderSubnet feature is enabled, you will need to add a series of tags as described above to the stock auto pool in order to make it available to the stock auto pool, as follows: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-name\": \"test-app\"}}}' kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application-namespace\": \"kube-system\"}}}' ... SpiderSubnet supports multiple network interfaces, you need to add the corresponding network interface label for the auto pool as follows: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}'","title":"Upgrading from a version below 0.3.6 (Excludes 0.3.6) to a higher version"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-040-excludes-040-to-a-higher-version","text":"Due to architecture adjustment, SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod . Therefore, find all SpiderEndpoint objects with Status.OwnerControllerType of None and replace the SpiderEndpoint.Status.OwnerControllerType property from None to Pod .","title":"Upgrading from a version below 0.4.0 (Excludes 0.4.0) to a higher version"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-050-includes-050-to-a-higher-version","text":"In versions higher than 0.5.0, the SpiderMultusConfig and Coordinator functions are added. However, due to helm upgrade, the corresponding CRDs cannot be automatically installed: spidercoordinators.spiderpool.spidernet.io and spidermultusconfigs.spiderpool.spidernet.io . Therefore, before upgrading, you can obtain the latest stable version through the following commands, decompress the chart package and apply all CRDs. ~# helm search repo spiderpool --versions # Please replace [upgraded-version] with the version you want to upgrade to. ~# helm fetch spiderpool/spiderpool --version [ upgraded-version ] ~# tar -xvf spiderpool- [ upgraded-version ] .tgz && cd spiderpool/crds ~# ls | grep '\\.yaml$' | xargs -I {} kubectl apply -f {}","title":"Upgrading from a version below 0.5.0 (Includes 0.5.0) to a higher version"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-073-includes-073-to-a-higher-version","text":"In versions below 0.7.3, Spiderpool will enable a set of DaemonSet: spiderpool-multus to manage Multus related configurations. In later versions, the DaemonSet was deprecated, and the Muluts configuration was moved to spiderpool-agent for management. At the same time, the function of automatically cleaning up the Muluts configuration during uninstallation was added, which is enabled by default. Disable it by --set multus.multusCNI.uninstall=false when upgrading to avoid CNI configuration files, CRDs, etc. being deleted during the upgrade phase, causing Pod creation to fail.","title":"Upgrading from a version below 0.7.3 (Includes 0.7.3) to a higher version"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-090-excludes-090-to-a-higher-version","text":"Due to the addition of the txQueueLen field to the SpiderCoordinator CRD in version 0.9.0, you need to manually update the CRD before upgrading as Helm does not support upgrading or deleting CRDs during the upgrade process.(We suggest skipping version 0.9.0 and upgrading directly to version 0.9.1)","title":"Upgrading from a version below 0.9.0 (Excludes 0.9.0) to a higher version"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-094-includes-094-to-a-higher-version","text":"In versions below 0.9.4, when statefulSet is rapidly scaling up or down, Spiderpool GC may mistakenly reclaim IP addresses in IPPool, causing the same IP to be assigned to multiple Pods in the K8S cluster, resulting in IP address conflicts. This issue has been fixed, see Fix , but after the upgrade, the conflicting IP addresses cannot be automatically corrected by Spiderpool. You need to manually restart the Pod with the conflicting IP to assist in resolving the issue. In the new version, there will no longer be an issue with IP conflicts caused by incorrect GC IPs.","title":"Upgrading from a version below 0.9.4 (Includes 0.9.4) to a higher version"},{"location":"usage/install/upgrade/#upgrading-from-a-version-below-095-excludes-095-to-a-higher-version","text":"In versions lower than 0.9.5, the spiderSubnet field in Spiderpool Charts values.yaml changed from ipam.spidersubnet to ipam.spiderSubnet , so you cannot safely use the --reuse-values flag to upgrade from versions < 0.9.5 to 0.9.5 and above. Please modify the values.yaml file or use the --set ipam.spiderSubnet.enable=true flag to override the value in the values.yaml file.","title":"Upgrading from a version below 0.9.5 (Excludes 0.9.5) to a higher version"},{"location":"usage/install/upgrade/#more-notes-on-version-upgrades","text":"TODO.","title":"More notes on version upgrades"},{"location":"usage/install/upgrade/#faq","text":"Due to your high availability requirements for Spiderpool, you may set multiple replicas of the spiderpool-controller Pod through --set spiderpoolController.replicas=5 during installation. The Pod of spiderpool-controller will occupy some port addresses of the node by default. The default port Please refer to System Configuration for occupancy. If your number of replicas is exactly the same as the number of nodes, then the Pod will fail to start because the node has no available ports during the upgrade. You can refer to the following Modifications can be made in two ways. When executing the upgrade command, you can change the port by appending the helm parameter --set spiderpoolController.httpPort , and you can change the port through helm Values.yaml and System Configuration to check the ports that need to be modified. The type of spiderpool-controller is Deployment . You can reduce the number of replicas and restore the number of replicas after the Pod starts normally.","title":"FAQ"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/","text":"AI Cluster With Macvlan \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u672c\u8282\u4ecb\u7ecd\u5728\u5efa\u8bbe AI \u96c6\u7fa4\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u57fa\u4e8e Macvlan \u6280\u672f\u7ed9\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\uff0c\u9002\u7528\u5728 RoCE \u7f51\u7edc\u573a\u666f\u4e0b\u3002 \u57fa\u4e8e RDMA shared device plugin \uff0c\u7ed9\u5bb9\u5668\u63d2\u5165 Macvlan \u63a5\u53e3\uff0c\u80fd\u591f\u628a master \u63a5\u53e3\u7684 RDMA \u8bbe\u5907\u5171\u4eab\u7ed9\u5bb9\u5668\u4f7f\u7528\uff0c\u56e0\u6b64\uff1a RDMA system \u9700\u8981\u5de5\u4f5c\u5728 shared \u6a21\u5f0f\u4e0b\uff0c\u6240\u6709\u7684\u5bb9\u5668\u5171\u4eab\u4f7f\u7528\u5bbf\u4e3b\u673a\u4e0a\u7684 master \u7f51\u5361\u7684 RDMA \u8bbe\u5907\u3002\u5b83\u7684\u7279\u70b9\u662f\uff0c\u6bcf\u4e2a\u65b0\u542f\u52a8\u7684\u5bb9\u5668\u4e2d\uff0c\u5176 RDMA \u8bbe\u5907\u7684\u53ef\u7528 GID index \u603b\u662f\u5728\u9012\u589e\u53d8\u6362\u7684\uff0c\u4e0d\u662f\u56fa\u5b9a\u503c\u3002 \u5728 Infiniband \u7684 IPOIB \u7f51\u5361\u4e0a\u4e0d\u652f\u6301\u521b\u5efa Macvlan \u63a5\u53e3\uff0c\u56e0\u6b64\uff0c\u672c\u65b9\u6848\u53ea\u80fd\u9002\u7528\u5728 RoCE \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e0d\u80fd\u4f7f\u7528\u5728 infiniband \u7f51\u7edc\u573a\u666f\u4e0b\u3002 \u65b9\u6848 \u672c\u6587\u5c06\u4ee5\u5982\u4e0b\u5178\u578b\u7684 AI \u96c6\u7fa4\u62d3\u6251\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u642d\u5efa Spiderpool\u3002 \u56fe 1. AI \u96c6\u7fa4\u62d3\u6251 \u96c6\u7fa4\u7684\u7f51\u7edc\u89c4\u5212\u5982\u4e0b\uff1a \u5728\u8282\u70b9\u7684 eth0 \u7f51\u5361\u4e0a\u8fd0\u884c calico CNI\uff0c\u6765\u627f\u8f7d kubernetes \u6d41\u91cf\u3002AI workload \u5c06\u4f1a\u88ab\u5206\u914d\u4e00\u4e2a calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\u3002 \u8282\u70b9\u4e0a\u4f7f\u7528\u5177\u5907 RDMA \u529f\u80fd\u7684 Mellanox ConnectX5 \u7f51\u5361\u6765\u627f\u8f7d AI \u8ba1\u7b97\u7684 RDMA \u6d41\u91cf\uff0c\u7f51\u5361\u63a5\u5165\u5230 rail optimized \u7f51\u7edc\u4e2d\u3002AI workload \u5c06\u4f1a\u88ab\u989d\u5916\u5206\u914d\u6240\u6709 RDMA \u7f51\u5361\u7684 Macvlan \u865a\u62df\u5316\u63a5\u53e3\uff0c\u786e\u4fdd GPU \u7684\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u3002 \u5b89\u88c5\u8981\u6c42 \u53c2\u8003 Spiderpool\u5b89\u88c5\u8981\u6c42 \u4e3b\u673a\u4e0a\u51c6\u5907\u597d Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5\u597d Kubernetes \u96c6\u7fa4\uff0ckubelet \u5de5\u4f5c\u5728\u56fe 1 \u4e2d\u7684\u4e3b\u673a eth0 \u7f51\u5361\u4e0a \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\uff0c\u4f7f\u7528\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u4f5c\u4e3a calico \u7684\u6d41\u91cf\u8f6c\u53d1\u7f51\u5361\u3002 \u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip \u4e3b\u673a\u51c6\u5907 \u5b89\u88c5 RDMA \u7f51\u5361\u9a71\u52a8 \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u53ef\u4e0b\u8f7d NVIDIA OFED \u5b98\u65b9\u9a71\u52a8 \u8fdb\u884c\u4e3b\u673a\u5b89\u88c5\uff0c\u6267\u884c\u5982\u4e0b\u5b89\u88c5\u547d\u4ee4\uff1a mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u4e5f\u53ef\u57fa\u4e8e\u5bb9\u5668\u5316\u5b89\u88c5\u9a71\u52a8\uff0c\u5b9e\u73b0\u5bf9\u96c6\u7fa4\u4e3b\u673a\u4e0a\u6240\u6709 Mellanox \u7f51\u5361\u6279\u91cf\u5b89\u88c5\u9a71\u52a8\uff0c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u9700\u8981\u8bbf\u95ee\u56e0\u7279\u7f51\u83b7\u53d6\u4e00\u4e9b\u5b89\u88c5\u5305\u3002\u5f53\u6240\u6709\u7684 ofed pod \u8fdb\u5165 ready \u72b6\u6001\uff0c\u8868\u793a\u4e3b\u673a\u4e0a\u5df2\u7ecf\u5b8c\u6210\u4e86 OFED driver \u5b89\u88c5\u3002 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" \u786e\u8ba4\u7f51\u5361\u652f\u6301 Ethernet \u5de5\u4f5c\u6a21\u5f0f \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u63a5\u5165\u4e86 mellanox ConnectX 5 VPI \u7f51\u5361\uff0c\u67e5\u8be2 RDMA \u8bbe\u5907\uff0c\u786e\u8ba4\u7f51\u5361\u9a71\u52a8\u5b89\u88c5\u5b8c\u6210 $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... \u786e\u8ba4\u7f51\u5361\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Ethernet \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 RoCE \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet \u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Infiniband \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 Infiniband \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand \u5982\u679c\u7f51\u5361\u6ca1\u6709\u5de5\u4f5c\u5728\u9884\u671f\u7684\u6a21\u5f0f\u4e0b\uff0c\u8bf7\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u7f51\u5361\u652f\u6301\u914d\u7f6e LINK_TYPE \u53c2\u6570\uff0c\u5982\u679c\u6ca1\u6709\u8be5\u53c2\u6570\uff0c\u8bf7\u66f4\u6362\u652f\u6301\u7684\u7f51\u5361\u578b\u53f7 $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) \u5f00\u542f GPUDirect RMDA \u529f\u80fd \u5728\u5b89\u88c5\u6216\u4f7f\u7528 gpu-operator \u8fc7\u7a0b\u4e2d \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 nvidia-peermem \u5185\u6838\u6a21\u5757\uff0c\u542f\u7528 GPUDirect RMDA \u529f\u80fd\uff0c\u52a0\u901f GPU \u548c RDMA \u7f51\u5361\u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set gdrcopy.enabled=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 gdrcopy \u5185\u6838\u6a21\u5757\uff0c\u52a0\u901f GPU \u663e\u5b58 \u548c CPU \u5185\u5b58 \u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep gdrdrv gdrdrv 24576 0 \u786e\u8ba4\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u4e3a shared \u6a21\u5f0f\uff0c\u8fd9\u662f macvlan \u573a\u666f\u4e0b\u63d0\u4f9b RDMA \u8bbe\u5907\u7ed9\u5bb9\u5668\u7684\u8981\u6c42\u3002 # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on \u5b89\u88c5 Spiderpool \u4f7f\u7528 helm \u5b89\u88c5 Spiderpool\uff0c\u5e76\u542f\u7528 rdmaSharedDevicePlugin \u7ec4\u4ef6 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set rdma.rdmaSharedDevicePlugin.install = true \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabled --set spiderpoolAgent.prometheus.enabledRdmaMetric=true \u548c --set grafanaDashboard.install=true \u547d\u4ee4\u884c\u53c2\u6570\u53ef\u4ee5\u5f00\u542f RDMA metrics exporter \u548c Grafana dashboard\uff0c\u66f4\u591a\u53ef\u4ee5\u67e5\u770b RDMA metrics . \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b $ kubectl get pod -n spiderpool spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-rdma-shared-device-plugin-9xsm9 1 /1 Running 0 1m spiderpool-rdma-shared-device-plugin-nxvlx 1 /1 Running 0 1m \u914d\u7f6e k8s-rdma-shared-dev-plugin, \u8bc6\u522b\u51fa\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684 RDMA \u5171\u4eab\u8bbe\u5907\u8d44\u6e90 \u4fee\u6539\u5982\u4e0b configmap\uff0c\u521b\u5efa\u51fa 8 \u79cd RDMA \u5171\u4eab\u8bbe\u5907\uff0c\u5b83\u4eec\u5206\u522b\u4eb2\u548c\u6bcf\u4e00\u4e2a GPU \u8bbe\u5907\u3002configmap \u7684\u8be6\u7ec6\u914d\u7f6e\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u3002 $ kubectl edit configmap -n spiderpool spiderpool-rdma-shared-device-plugi .... config.json: | { \"periodicUpdateInterval\" : 300 , \"configList\" : [ { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu1\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp11s0f0np0\" ] } } , .... { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu8\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp18s0f0np0\" ] } } ] \u5b8c\u6210\u5982\u4e0a\u914d\u7f6e\u540e\uff0c\u53ef\u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u786e\u8ba4\u6bcf\u4e2a\u8282\u70b9\u90fd\u6b63\u786e\u8bc6\u522b\u5e76\u4e0a\u62a5\u4e86 8 \u79cd RDMA \u8bbe\u5907\u8d44\u6e90\u3002 $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/shared_cx5_gpu1\" : \"100\" , \"spidernet.io/shared_cx5_gpu2\" : \"100\" , ... \"spidernet.io/shared_cx5_gpu8\" : \"100\" , ... } } , ... ] \u521b\u5efa CNI \u914d\u7f6e\u548c\u5bf9\u5e94\u7684 ippool \u8d44\u6e90 \u5bf9\u4e8e Ethernet \u7f51\u7edc\uff0c\u8bf7\u4e3a\u6240\u6709\u7684 GPU \u4eb2\u548c\u7684 macvlan \u7f51\u5361\u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u5bf9\u5e94\u7684 IP \u5730\u5740\u6c60\u3002\u5982\u4e0b\u4f8b\u5b50\uff0c\u914d\u7f6e\u4e86 GPU1 \u4eb2\u548c\u7684\u7f51\u5361\u548c IP \u5730\u5740\u6c60\u3002 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-macvlan namespace: spiderpool spec: cniType: macvlan rdmaResourceName: spidernet.io/shared_cx5_gpu1 macvlan: master: [\"enp11s0f0np0\"] ippools: ipv4: [\"gpu1-net11\"] EOF \u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 \u5728\u6307\u5b9a\u8282\u70b9\u4e0a\u521b\u5efa\u4e00\u7ec4 DaemonSet \u5e94\u7528 \u5982\u4e0b\u4f8b\u5b50\uff0c\u901a\u8fc7 annotations v1.multus-cni.io/default-network \u6307\u5b9a\u4f7f\u7528 calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u7528\u4e8e\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\uff0cannotations k8s.v1.cni.cncf.io/networks \u63a5\u5165 8 \u4e2a GPU \u4eb2\u548c\u7f51\u5361\u7684\u7f51\u5361\uff0c\u7528\u4e8e RDMA \u901a\u4fe1\uff0c\u5e76\u914d\u7f6e 8 \u79cd RDMA resources \u8d44\u6e90 \u6ce8\uff1a\u53ef\u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u53c2\u8003 \u57fa\u4e8e Webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u8d44\u6e90 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # macvlan interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: |- [{\"name\":\"gpu1-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-macvlan\",\"namespace\":\"spiderpool\"}] # macvlan resource resources: limits: spidernet.io/shared_cx5_gpu1: 1 spidernet.io/shared_cx5_gpu2: 1 spidernet.io/shared_cx5_gpu3: 1 spidernet.io/shared_cx5_gpu4: 1 spidernet.io/shared_cx5_gpu5: 1 spidernet.io/shared_cx5_gpu6: 1 spidernet.io/shared_cx5_gpu7: 1 spidernet.io/shared_cx5_gpu8: 1 #nvidia.com/gpu: 1 EOF $ helm install rdma-tools spiderchart/rdma-tools -f ./values.yaml \u5728\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u4f1a\u5bf9 macvlan \u63a5\u53e3\u4e0a\u7684\u7f51\u5173\u8fdb\u884c\u8fde\u901a\u6027\u6d4b\u8bd5\uff0c\u5982\u679c\u5982\u4e0a\u5e94\u7528\u7684\u6240\u6709 POD \u90fd\u542f\u52a8\u6210\u529f\uff0c\u8bf4\u660e\u4e86\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684 VF \u8bbe\u5907\u7684\u8fde\u901a\u6027\u6210\u529f\uff0c\u53ef\u8fdb\u884c\u6b63\u5e38\u7684 RDMA \u901a\u4fe1\u3002 \u67e5\u770b\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u72b6\u6001 \u53ef\u8fdb\u5165\u4efb\u4e00\u4e00\u4e2a POD \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u786e\u8ba4\u5177\u5907 9 \u4e2a\u7f51\u5361\uff1a $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... \u67e5\u770b\u8def\u7531\u914d\u7f6e\uff0cSpiderpool \u4f1a\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u7f51\u5361\u8c03\u8c10\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7f51\u5361\u4e0a\u6536\u5230\u7684\u5916\u90e8\u8bf7\u6c42\u90fd\u4f1a\u4ece\u8be5\u7f51\u5361\u4e0a\u8fd4\u56de\u56de\u590d\u6d41\u91cf\uff1a root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 main \u8def\u7531\u4e2d\uff0c\u786e\u4fdd\u4e86 calico \u7f51\u7edc\u6d41\u91cf\u3001ClusterIP \u6d41\u91cf\u3001\u672c\u5730\u5bbf\u4e3b\u673a\u901a\u4fe1\u7b49\u6d41\u91cf\u90fd\u4f1a\u4ece calico \u7f51\u5361\u8f6c\u53d1 root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link \u786e\u8ba4\u5177\u5907 8 \u4e2a RDMA \u8bbe\u5907 root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1\uff1a # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115 \u57fa\u4e8e Webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90 \u5728\u4e0a\u8ff0\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 SR-IOV \u6280\u672f\u5728 RoCE \u548c Infiniband \u7f51\u7edc\u73af\u5883\u4e2d\u4e3a\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u914d\u7f6e\u591a\u7f51\u5361\u7684 AI \u5e94\u7528\u65f6\uff0c\u8fc7\u7a0b\u4f1a\u53d8\u5f97\u590d\u6742\u3002\u4e3a\u7b80\u5316\u8fd9\u4e2a\u8fc7\u7a0b\uff0cSpiderpool \u901a\u8fc7 annotations( cni.spidernet.io/rdma-resource-inject \u6216 cni.spidernet.io/network-resource-inject ) \u652f\u6301\u5bf9\u4e00\u7ec4\u7f51\u5361\u914d\u7f6e\u8fdb\u884c\u5206\u7c7b\u3002\u7528\u6237\u53ea\u9700\u8981\u4e3a\u5e94\u7528\u6dfb\u52a0\u4e0e\u7f51\u5361\u914d\u7f6e\u76f8\u540c\u7684\u6ce8\u89e3\uff0cSpiderpool \u5c31\u4f1a\u901a\u8fc7 webhook \u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165\u6240\u6709\u5177\u6709\u76f8\u540c\u6ce8\u89e3\u7684\u5bf9\u5e94\u7f51\u5361\u548c\u7f51\u7edc\u8d44\u6e90\u3002 cni.spidernet.io/rdma-resource-inject \u53ea\u9002\u7528\u4e8e AI \u573a\u666f\uff0c\u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u5361\u53ca RDMA Resources\uff1b cni.spidernet.io/network-resource-inject \u4e0d\u4f46\u53ef\u4ee5\u7528\u4e8e AI \u573a\u666f\uff0c\u4e5f\u652f\u6301 Underlay \u573a\u666f\u3002\u5728\u672a\u6765\u6211\u4eec\u5e0c\u671b\u90fd\u7edf\u4e00\u4f7f\u7528 cni.spidernet.io/network-resource-inject \u652f\u6301\u8fd9\u4e24\u79cd\u573a\u666f\u3002 \u8be5\u529f\u80fd\u4ec5\u652f\u6301 [ macvlan, ipvlan, sriov, ib-sriov, ipoib ] \u8fd9\u51e0\u79cd cniType \u7684\u7f51\u5361\u914d\u7f6e\u3002 \u5f53\u524d Spiderpool \u7684 webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u9ed8\u8ba4\u662f\u5173\u95ed\u7684\uff0c\u9700\u8981\u624b\u52a8\u5f00\u542f\u3002 ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true \u542f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u66f4\u65b0 configMap: spiderpool-config \u4e2d\u7684 podResourceInject \u5b57\u6bb5\u66f4\u65b0\u914d\u7f6e\u3002 \u901a\u8fc7 podResourceInject.namespacesExclude \u6307\u5b9a\u4e0d\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4 \u901a\u8fc7 podResourceInject.namespacesInclude \u6307\u5b9a\u9700\u8981\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u5982\u679c podResourceInject.namespacesExclude \u548c podResourceInject.namespacesInclude \u90fd\u6ca1\u6709\u6307\u5b9a\uff0c\u5219\u9ed8\u8ba4\u5bf9\u6240\u6709\u547d\u540d\u7a7a\u95f4\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u3002 \u5f53\u524d\uff0c\u5b8c\u6210\u914d\u7f6e\u53d8\u66f4\u540e\uff0c\u60a8\u9700\u8981\u91cd\u542f spiderpool-controller \u6765\u4f7f\u914d\u7f6e\u751f\u6548\u3002 \u5728\u521b\u5efa AI \u7b97\u529b\u7f51\u7edc\u7684\u6240\u6709 SpiderMultusConfig \u5b9e\u4f8b\u65f6\uff0c\u6dfb\u52a0 key \u4e3a \"cni.spidernet.io/rdma-resource-inject\" \u6216 \"cni.spidernet.io/network-resource-inject\" \u7684 annotation\uff0cvalue \u53ef\u81ea\u5b9a\u4e49\u4efb\u4f55\u503c apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : macvlan macvlan : master : [ \"enp11s0f0np0\" ] enableRdma : true rdmaResourceName : spidernet.io/gpu1rdma ippools : ipv4 : [ \"gpu1-net11\" ] \u521b\u5efa AI \u5e94\u7528\u65f6\uff0c\u4e3a\u5e94\u7528\u4e5f\u6dfb\u52a0\u76f8\u540c\u6ce8\u89e3: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network \u6ce8\u610f\uff1a\u4f7f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u65f6\uff0c\u4e0d\u80fd\u4e3a\u5e94\u7528\u6dfb\u52a0\u5176\u4ed6\u7f51\u7edc\u914d\u7f6e\u6ce8\u89e3(\u5982 k8s.v1.cni.cncf.io/networks \u548c ipam.spidernet.io ippools \u7b49)\uff0c\u5426\u5219\u4f1a\u5f71\u54cd\u8d44\u6e90\u81ea\u52a8\u6ce8\u5165\u529f\u80fd\u3002 \u5f53 Pod \u88ab\u521b\u5efa\u540e\uff0c\u53ef\u89c2\u6d4b\u5230 Pod \u88ab\u81ea\u52a8\u6ce8\u5165\u4e86\u7f51\u5361 annotation \u548c RDMA \u8d44\u6e90 ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"AI Cluster With Macvlan"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#ai-cluster-with-macvlan","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"AI Cluster With Macvlan"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#_1","text":"\u672c\u8282\u4ecb\u7ecd\u5728\u5efa\u8bbe AI \u96c6\u7fa4\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u57fa\u4e8e Macvlan \u6280\u672f\u7ed9\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\uff0c\u9002\u7528\u5728 RoCE \u7f51\u7edc\u573a\u666f\u4e0b\u3002 \u57fa\u4e8e RDMA shared device plugin \uff0c\u7ed9\u5bb9\u5668\u63d2\u5165 Macvlan \u63a5\u53e3\uff0c\u80fd\u591f\u628a master \u63a5\u53e3\u7684 RDMA \u8bbe\u5907\u5171\u4eab\u7ed9\u5bb9\u5668\u4f7f\u7528\uff0c\u56e0\u6b64\uff1a RDMA system \u9700\u8981\u5de5\u4f5c\u5728 shared \u6a21\u5f0f\u4e0b\uff0c\u6240\u6709\u7684\u5bb9\u5668\u5171\u4eab\u4f7f\u7528\u5bbf\u4e3b\u673a\u4e0a\u7684 master \u7f51\u5361\u7684 RDMA \u8bbe\u5907\u3002\u5b83\u7684\u7279\u70b9\u662f\uff0c\u6bcf\u4e2a\u65b0\u542f\u52a8\u7684\u5bb9\u5668\u4e2d\uff0c\u5176 RDMA \u8bbe\u5907\u7684\u53ef\u7528 GID index \u603b\u662f\u5728\u9012\u589e\u53d8\u6362\u7684\uff0c\u4e0d\u662f\u56fa\u5b9a\u503c\u3002 \u5728 Infiniband \u7684 IPOIB \u7f51\u5361\u4e0a\u4e0d\u652f\u6301\u521b\u5efa Macvlan \u63a5\u53e3\uff0c\u56e0\u6b64\uff0c\u672c\u65b9\u6848\u53ea\u80fd\u9002\u7528\u5728 RoCE \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e0d\u80fd\u4f7f\u7528\u5728 infiniband \u7f51\u7edc\u573a\u666f\u4e0b\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#_2","text":"\u672c\u6587\u5c06\u4ee5\u5982\u4e0b\u5178\u578b\u7684 AI \u96c6\u7fa4\u62d3\u6251\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u642d\u5efa Spiderpool\u3002 \u56fe 1. AI \u96c6\u7fa4\u62d3\u6251 \u96c6\u7fa4\u7684\u7f51\u7edc\u89c4\u5212\u5982\u4e0b\uff1a \u5728\u8282\u70b9\u7684 eth0 \u7f51\u5361\u4e0a\u8fd0\u884c calico CNI\uff0c\u6765\u627f\u8f7d kubernetes \u6d41\u91cf\u3002AI workload \u5c06\u4f1a\u88ab\u5206\u914d\u4e00\u4e2a calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\u3002 \u8282\u70b9\u4e0a\u4f7f\u7528\u5177\u5907 RDMA \u529f\u80fd\u7684 Mellanox ConnectX5 \u7f51\u5361\u6765\u627f\u8f7d AI \u8ba1\u7b97\u7684 RDMA \u6d41\u91cf\uff0c\u7f51\u5361\u63a5\u5165\u5230 rail optimized \u7f51\u7edc\u4e2d\u3002AI workload \u5c06\u4f1a\u88ab\u989d\u5916\u5206\u914d\u6240\u6709 RDMA \u7f51\u5361\u7684 Macvlan \u865a\u62df\u5316\u63a5\u53e3\uff0c\u786e\u4fdd GPU \u7684\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u3002","title":"\u65b9\u6848"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#_3","text":"\u53c2\u8003 Spiderpool\u5b89\u88c5\u8981\u6c42 \u4e3b\u673a\u4e0a\u51c6\u5907\u597d Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5\u597d Kubernetes \u96c6\u7fa4\uff0ckubelet \u5de5\u4f5c\u5728\u56fe 1 \u4e2d\u7684\u4e3b\u673a eth0 \u7f51\u5361\u4e0a \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\uff0c\u4f7f\u7528\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u4f5c\u4e3a calico \u7684\u6d41\u91cf\u8f6c\u53d1\u7f51\u5361\u3002 \u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#_4","text":"\u5b89\u88c5 RDMA \u7f51\u5361\u9a71\u52a8 \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u53ef\u4e0b\u8f7d NVIDIA OFED \u5b98\u65b9\u9a71\u52a8 \u8fdb\u884c\u4e3b\u673a\u5b89\u88c5\uff0c\u6267\u884c\u5982\u4e0b\u5b89\u88c5\u547d\u4ee4\uff1a mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u4e5f\u53ef\u57fa\u4e8e\u5bb9\u5668\u5316\u5b89\u88c5\u9a71\u52a8\uff0c\u5b9e\u73b0\u5bf9\u96c6\u7fa4\u4e3b\u673a\u4e0a\u6240\u6709 Mellanox \u7f51\u5361\u6279\u91cf\u5b89\u88c5\u9a71\u52a8\uff0c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u9700\u8981\u8bbf\u95ee\u56e0\u7279\u7f51\u83b7\u53d6\u4e00\u4e9b\u5b89\u88c5\u5305\u3002\u5f53\u6240\u6709\u7684 ofed pod \u8fdb\u5165 ready \u72b6\u6001\uff0c\u8868\u793a\u4e3b\u673a\u4e0a\u5df2\u7ecf\u5b8c\u6210\u4e86 OFED driver \u5b89\u88c5\u3002 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" \u786e\u8ba4\u7f51\u5361\u652f\u6301 Ethernet \u5de5\u4f5c\u6a21\u5f0f \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u63a5\u5165\u4e86 mellanox ConnectX 5 VPI \u7f51\u5361\uff0c\u67e5\u8be2 RDMA \u8bbe\u5907\uff0c\u786e\u8ba4\u7f51\u5361\u9a71\u52a8\u5b89\u88c5\u5b8c\u6210 $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... \u786e\u8ba4\u7f51\u5361\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Ethernet \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 RoCE \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet \u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Infiniband \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 Infiniband \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand \u5982\u679c\u7f51\u5361\u6ca1\u6709\u5de5\u4f5c\u5728\u9884\u671f\u7684\u6a21\u5f0f\u4e0b\uff0c\u8bf7\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u7f51\u5361\u652f\u6301\u914d\u7f6e LINK_TYPE \u53c2\u6570\uff0c\u5982\u679c\u6ca1\u6709\u8be5\u53c2\u6570\uff0c\u8bf7\u66f4\u6362\u652f\u6301\u7684\u7f51\u5361\u578b\u53f7 $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) \u5f00\u542f GPUDirect RMDA \u529f\u80fd \u5728\u5b89\u88c5\u6216\u4f7f\u7528 gpu-operator \u8fc7\u7a0b\u4e2d \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 nvidia-peermem \u5185\u6838\u6a21\u5757\uff0c\u542f\u7528 GPUDirect RMDA \u529f\u80fd\uff0c\u52a0\u901f GPU \u548c RDMA \u7f51\u5361\u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set gdrcopy.enabled=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 gdrcopy \u5185\u6838\u6a21\u5757\uff0c\u52a0\u901f GPU \u663e\u5b58 \u548c CPU \u5185\u5b58 \u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep gdrdrv gdrdrv 24576 0 \u786e\u8ba4\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u4e3a shared \u6a21\u5f0f\uff0c\u8fd9\u662f macvlan \u573a\u666f\u4e0b\u63d0\u4f9b RDMA \u8bbe\u5907\u7ed9\u5bb9\u5668\u7684\u8981\u6c42\u3002 # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on","title":"\u4e3b\u673a\u51c6\u5907"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#spiderpool","text":"\u4f7f\u7528 helm \u5b89\u88c5 Spiderpool\uff0c\u5e76\u542f\u7528 rdmaSharedDevicePlugin \u7ec4\u4ef6 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set rdma.rdmaSharedDevicePlugin.install = true \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabled --set spiderpoolAgent.prometheus.enabledRdmaMetric=true \u548c --set grafanaDashboard.install=true \u547d\u4ee4\u884c\u53c2\u6570\u53ef\u4ee5\u5f00\u542f RDMA metrics exporter \u548c Grafana dashboard\uff0c\u66f4\u591a\u53ef\u4ee5\u67e5\u770b RDMA metrics . \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b $ kubectl get pod -n spiderpool spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-rdma-shared-device-plugin-9xsm9 1 /1 Running 0 1m spiderpool-rdma-shared-device-plugin-nxvlx 1 /1 Running 0 1m \u914d\u7f6e k8s-rdma-shared-dev-plugin, \u8bc6\u522b\u51fa\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684 RDMA \u5171\u4eab\u8bbe\u5907\u8d44\u6e90 \u4fee\u6539\u5982\u4e0b configmap\uff0c\u521b\u5efa\u51fa 8 \u79cd RDMA \u5171\u4eab\u8bbe\u5907\uff0c\u5b83\u4eec\u5206\u522b\u4eb2\u548c\u6bcf\u4e00\u4e2a GPU \u8bbe\u5907\u3002configmap \u7684\u8be6\u7ec6\u914d\u7f6e\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u3002 $ kubectl edit configmap -n spiderpool spiderpool-rdma-shared-device-plugi .... config.json: | { \"periodicUpdateInterval\" : 300 , \"configList\" : [ { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu1\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp11s0f0np0\" ] } } , .... { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu8\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp18s0f0np0\" ] } } ] \u5b8c\u6210\u5982\u4e0a\u914d\u7f6e\u540e\uff0c\u53ef\u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u786e\u8ba4\u6bcf\u4e2a\u8282\u70b9\u90fd\u6b63\u786e\u8bc6\u522b\u5e76\u4e0a\u62a5\u4e86 8 \u79cd RDMA \u8bbe\u5907\u8d44\u6e90\u3002 $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/shared_cx5_gpu1\" : \"100\" , \"spidernet.io/shared_cx5_gpu2\" : \"100\" , ... \"spidernet.io/shared_cx5_gpu8\" : \"100\" , ... } } , ... ] \u521b\u5efa CNI \u914d\u7f6e\u548c\u5bf9\u5e94\u7684 ippool \u8d44\u6e90 \u5bf9\u4e8e Ethernet \u7f51\u7edc\uff0c\u8bf7\u4e3a\u6240\u6709\u7684 GPU \u4eb2\u548c\u7684 macvlan \u7f51\u5361\u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u5bf9\u5e94\u7684 IP \u5730\u5740\u6c60\u3002\u5982\u4e0b\u4f8b\u5b50\uff0c\u914d\u7f6e\u4e86 GPU1 \u4eb2\u548c\u7684\u7f51\u5361\u548c IP \u5730\u5740\u6c60\u3002 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-macvlan namespace: spiderpool spec: cniType: macvlan rdmaResourceName: spidernet.io/shared_cx5_gpu1 macvlan: master: [\"enp11s0f0np0\"] ippools: ipv4: [\"gpu1-net11\"] EOF","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#_5","text":"\u5728\u6307\u5b9a\u8282\u70b9\u4e0a\u521b\u5efa\u4e00\u7ec4 DaemonSet \u5e94\u7528 \u5982\u4e0b\u4f8b\u5b50\uff0c\u901a\u8fc7 annotations v1.multus-cni.io/default-network \u6307\u5b9a\u4f7f\u7528 calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u7528\u4e8e\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\uff0cannotations k8s.v1.cni.cncf.io/networks \u63a5\u5165 8 \u4e2a GPU \u4eb2\u548c\u7f51\u5361\u7684\u7f51\u5361\uff0c\u7528\u4e8e RDMA \u901a\u4fe1\uff0c\u5e76\u914d\u7f6e 8 \u79cd RDMA resources \u8d44\u6e90 \u6ce8\uff1a\u53ef\u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u53c2\u8003 \u57fa\u4e8e Webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u8d44\u6e90 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # macvlan interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: |- [{\"name\":\"gpu1-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-macvlan\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-macvlan\",\"namespace\":\"spiderpool\"}] # macvlan resource resources: limits: spidernet.io/shared_cx5_gpu1: 1 spidernet.io/shared_cx5_gpu2: 1 spidernet.io/shared_cx5_gpu3: 1 spidernet.io/shared_cx5_gpu4: 1 spidernet.io/shared_cx5_gpu5: 1 spidernet.io/shared_cx5_gpu6: 1 spidernet.io/shared_cx5_gpu7: 1 spidernet.io/shared_cx5_gpu8: 1 #nvidia.com/gpu: 1 EOF $ helm install rdma-tools spiderchart/rdma-tools -f ./values.yaml \u5728\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u4f1a\u5bf9 macvlan \u63a5\u53e3\u4e0a\u7684\u7f51\u5173\u8fdb\u884c\u8fde\u901a\u6027\u6d4b\u8bd5\uff0c\u5982\u679c\u5982\u4e0a\u5e94\u7528\u7684\u6240\u6709 POD \u90fd\u542f\u52a8\u6210\u529f\uff0c\u8bf4\u660e\u4e86\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684 VF \u8bbe\u5907\u7684\u8fde\u901a\u6027\u6210\u529f\uff0c\u53ef\u8fdb\u884c\u6b63\u5e38\u7684 RDMA \u901a\u4fe1\u3002 \u67e5\u770b\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u72b6\u6001 \u53ef\u8fdb\u5165\u4efb\u4e00\u4e00\u4e2a POD \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u786e\u8ba4\u5177\u5907 9 \u4e2a\u7f51\u5361\uff1a $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... \u67e5\u770b\u8def\u7531\u914d\u7f6e\uff0cSpiderpool \u4f1a\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u7f51\u5361\u8c03\u8c10\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7f51\u5361\u4e0a\u6536\u5230\u7684\u5916\u90e8\u8bf7\u6c42\u90fd\u4f1a\u4ece\u8be5\u7f51\u5361\u4e0a\u8fd4\u56de\u56de\u590d\u6d41\u91cf\uff1a root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 main \u8def\u7531\u4e2d\uff0c\u786e\u4fdd\u4e86 calico \u7f51\u7edc\u6d41\u91cf\u3001ClusterIP \u6d41\u91cf\u3001\u672c\u5730\u5bbf\u4e3b\u673a\u901a\u4fe1\u7b49\u6d41\u91cf\u90fd\u4f1a\u4ece calico \u7f51\u5361\u8f6c\u53d1 root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link \u786e\u8ba4\u5177\u5907 8 \u4e2a RDMA \u8bbe\u5907 root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1\uff1a # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115","title":"\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528"},{"location":"usage/install/ai/get-started-macvlan-zh_CN/#webhook-rdma","text":"\u5728\u4e0a\u8ff0\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 SR-IOV \u6280\u672f\u5728 RoCE \u548c Infiniband \u7f51\u7edc\u73af\u5883\u4e2d\u4e3a\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u914d\u7f6e\u591a\u7f51\u5361\u7684 AI \u5e94\u7528\u65f6\uff0c\u8fc7\u7a0b\u4f1a\u53d8\u5f97\u590d\u6742\u3002\u4e3a\u7b80\u5316\u8fd9\u4e2a\u8fc7\u7a0b\uff0cSpiderpool \u901a\u8fc7 annotations( cni.spidernet.io/rdma-resource-inject \u6216 cni.spidernet.io/network-resource-inject ) \u652f\u6301\u5bf9\u4e00\u7ec4\u7f51\u5361\u914d\u7f6e\u8fdb\u884c\u5206\u7c7b\u3002\u7528\u6237\u53ea\u9700\u8981\u4e3a\u5e94\u7528\u6dfb\u52a0\u4e0e\u7f51\u5361\u914d\u7f6e\u76f8\u540c\u7684\u6ce8\u89e3\uff0cSpiderpool \u5c31\u4f1a\u901a\u8fc7 webhook \u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165\u6240\u6709\u5177\u6709\u76f8\u540c\u6ce8\u89e3\u7684\u5bf9\u5e94\u7f51\u5361\u548c\u7f51\u7edc\u8d44\u6e90\u3002 cni.spidernet.io/rdma-resource-inject \u53ea\u9002\u7528\u4e8e AI \u573a\u666f\uff0c\u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u5361\u53ca RDMA Resources\uff1b cni.spidernet.io/network-resource-inject \u4e0d\u4f46\u53ef\u4ee5\u7528\u4e8e AI \u573a\u666f\uff0c\u4e5f\u652f\u6301 Underlay \u573a\u666f\u3002\u5728\u672a\u6765\u6211\u4eec\u5e0c\u671b\u90fd\u7edf\u4e00\u4f7f\u7528 cni.spidernet.io/network-resource-inject \u652f\u6301\u8fd9\u4e24\u79cd\u573a\u666f\u3002 \u8be5\u529f\u80fd\u4ec5\u652f\u6301 [ macvlan, ipvlan, sriov, ib-sriov, ipoib ] \u8fd9\u51e0\u79cd cniType \u7684\u7f51\u5361\u914d\u7f6e\u3002 \u5f53\u524d Spiderpool \u7684 webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u9ed8\u8ba4\u662f\u5173\u95ed\u7684\uff0c\u9700\u8981\u624b\u52a8\u5f00\u542f\u3002 ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true \u542f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u66f4\u65b0 configMap: spiderpool-config \u4e2d\u7684 podResourceInject \u5b57\u6bb5\u66f4\u65b0\u914d\u7f6e\u3002 \u901a\u8fc7 podResourceInject.namespacesExclude \u6307\u5b9a\u4e0d\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4 \u901a\u8fc7 podResourceInject.namespacesInclude \u6307\u5b9a\u9700\u8981\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u5982\u679c podResourceInject.namespacesExclude \u548c podResourceInject.namespacesInclude \u90fd\u6ca1\u6709\u6307\u5b9a\uff0c\u5219\u9ed8\u8ba4\u5bf9\u6240\u6709\u547d\u540d\u7a7a\u95f4\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u3002 \u5f53\u524d\uff0c\u5b8c\u6210\u914d\u7f6e\u53d8\u66f4\u540e\uff0c\u60a8\u9700\u8981\u91cd\u542f spiderpool-controller \u6765\u4f7f\u914d\u7f6e\u751f\u6548\u3002 \u5728\u521b\u5efa AI \u7b97\u529b\u7f51\u7edc\u7684\u6240\u6709 SpiderMultusConfig \u5b9e\u4f8b\u65f6\uff0c\u6dfb\u52a0 key \u4e3a \"cni.spidernet.io/rdma-resource-inject\" \u6216 \"cni.spidernet.io/network-resource-inject\" \u7684 annotation\uff0cvalue \u53ef\u81ea\u5b9a\u4e49\u4efb\u4f55\u503c apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : macvlan macvlan : master : [ \"enp11s0f0np0\" ] enableRdma : true rdmaResourceName : spidernet.io/gpu1rdma ippools : ipv4 : [ \"gpu1-net11\" ] \u521b\u5efa AI \u5e94\u7528\u65f6\uff0c\u4e3a\u5e94\u7528\u4e5f\u6dfb\u52a0\u76f8\u540c\u6ce8\u89e3: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network \u6ce8\u610f\uff1a\u4f7f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u65f6\uff0c\u4e0d\u80fd\u4e3a\u5e94\u7528\u6dfb\u52a0\u5176\u4ed6\u7f51\u7edc\u914d\u7f6e\u6ce8\u89e3(\u5982 k8s.v1.cni.cncf.io/networks \u548c ipam.spidernet.io ippools \u7b49)\uff0c\u5426\u5219\u4f1a\u5f71\u54cd\u8d44\u6e90\u81ea\u52a8\u6ce8\u5165\u529f\u80fd\u3002 \u5f53 Pod \u88ab\u521b\u5efa\u540e\uff0c\u53ef\u89c2\u6d4b\u5230 Pod \u88ab\u81ea\u52a8\u6ce8\u5165\u4e86\u7f51\u5361 annotation \u548c RDMA \u8d44\u6e90 ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"\u57fa\u4e8e Webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90"},{"location":"usage/install/ai/get-started-macvlan/","text":"AI Cluster With Macvlan English | \u7b80\u4f53\u4e2d\u6587 Introduction This section explains how to provide RDMA communication capabilities to containers using Macvlan technology in the context of building an AI cluster, applicable in RoCE network scenarios. By using RDMA shared device plugin , a Macvlan interface can be attached to a container, allowing the RDMA device on the master interface to be shared with the container. Therefore: The RDMA system needs to operate in shared mode, where all containers share the RDMA device of the host's master network interface. A key characteristic of this setup is that in each newly launched container, the available GID index of the RDMA device continuously increments and is not a fixed value. Macvlan interfaces cannot be created on an Infiniband IPOIB network card, so this solution is only applicable in RoCE network scenarios and cannot be used in Infiniband network scenarios. Solution This article will introduce how to set up Spiderpool using the following typical AI cluster topology as an example. Figure 1: AI Cluster Topology The network planning for the cluster is as follows: The calico CNI runs on the eth0 network card of the nodes to carry Kubernetes traffic. The AI workload will be assigned a default calico network interface for control plane communication. The nodes use Mellanox ConnectX5 network cards with RDMA functionality to carry the RDMA traffic for AI computation. The network cards are connected to a rail-optimized network. The AI workload will be additionally assigned Macvlan virtualized interfaces for all RDMA network cards to ensure high-speed network communication for the GPUs. Installation Requirements Refer to the Spiderpool Installation Requirements . Prepare the Helm binary on the host. Install a Kubernetes cluster with kubelet running on the host\u2019s eth0 network card as shown in Figure 1 . Install Calico as the default CNI for the cluster, using the host\u2019s eth0 network card for Calico\u2019s traffic forwarding. If not installed, refer to the official documentation or use the following commands to install: $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip Host Preparation Install the RDMA network card driver. For Mellanox network cards, you can download the NVIDIA OFED official driver and install it on the host using the following installation command: mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all For Mellanox network cards, you can also perform a containerized installation to batch install drivers on all Mellanox network cards in the cluster hosts. Run the following command. Note that this process requires internet access to fetch some installation packages. When all the OFED pods enter the ready state, it indicates that the OFED driver installation on the hosts is complete: $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" Verify that the network card supports Ethernet operating modes. In this example environment, the host is equipped with Mellanox ConnectX 5 VPI network cards. Query the RDMA devices to confirm that the network card driver is installed correctly. $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... Verify the network card's operating mode. The following output indicates that the network card is operating in Ethernet mode and can achieve RoCE communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet The following output indicates that the network card is operating in Infiniband mode and can achieve Infiniband communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand If the network card is not operating in the expected mode, enter the following command to verify that the network card supports configuring the LINK_TYPE parameter. If the parameter is not available, please switch to a supported network card model: $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) Enable GPUDirect RDMA The installation of the gpu-operator : Enable the Helm installation options: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true . The gpu-operator will install the nvidia-peermem kernel module, enabling GPUDirect RDMA functionality to accelerate data transfer performance between the GPU and RDMA network cards. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 Enable the Helm installation option: --set gdrcopy.enabled=true . The gpu-operator will install the gdrcopy kernel module to accelerate data transfer performance between GPU memory and CPU memory. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep gdrdrv gdrdrv 24576 0 Set the RDMA subsystem on the host to shared mode, allowing containers to independently use shared RDMA device. # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on Install Spiderpool Use Helm to install Spiderpool and enable the rdmaSharedDevicePlugin: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set rdma.rdmaSharedDevicePlugin.install = true If you are a user in China, you can specify the helm option --set global.imageRegistryOverride=ghcr.m.daocloud.io to use a domestic image source. Set --set spiderpoolAgent.prometheus.enabled --set spiderpoolAgent.prometheus.enabledRdmaMetric=true and --set grafanaDashboard.install=true flag to enable the RDMA metrics exporter and Grafana dashboard. Refer to RDMA metrics . After completion, the installed components are as follows: $ kubectl get pod -n spiderpool spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-rdma-shared-device-plugin-9xsm9 1 /1 Running 0 1m spiderpool-rdma-shared-device-plugin-nxvlx 1 /1 Running 0 1m Configure k8s-rdma-shared-dev-plugin Modify the following ConfigMap to create eight types of RDMA shared devices, each associated with a specific GPU device. For detailed configuration of the ConfigMap, refer to the official documentation . $ kubectl edit configmap -n spiderpool spiderpool-rdma-shared-device-plugi .... config.json: | { \"periodicUpdateInterval\" : 300 , \"configList\" : [ { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu1\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp11s0f0np0\" ] } } , .... { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu8\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp18s0f0np0\" ] } } ] After completing the above configuration, you can check the available resources on the node to confirm that each node has correctly recognized and reported the eight types of RDMA device resources. $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/shared_cx5_gpu1\" : \"100\" , \"spidernet.io/shared_cx5_gpu2\" : \"100\" , ... \"spidernet.io/shared_cx5_gpu8\" : \"100\" , ... } } , ... ] Create CNI configuration and proper IP pool resources For Ethernet networks, please configure the Macvlan network interfaces associated with all GPUs and create corresponding IP address pools. The example below shows the configuration for the network interface and IP address pool associated with GPU1. $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-macvlan namespace: spiderpool spec: cniType: macvlan macvlan: master: [\"enp11s0f0np0\"] ippools: ipv4: [\"gpu1-net11\"] EOF Create a Test Application Create a DaemonSet application on specified nodes. In the following example, the annotation field v1.multus-cni.io/default-network specifies the use of the default Calico network card for control plane communication. The annotation field k8s.v1.cni.cncf.io/networks connects to the 8 network cards affinitized to the GPU for RDMA communication, and configures 8 types of RDMA resources. NOTICE: It support auto inject RDMA resources for application, see Auto inject RDMA Resources $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringE xecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: | - [{ \"name\" : \"gpu1-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu2-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu3-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu4-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu5-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu6-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu7-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu8-macvlan\" , \"namespace\" : \"spiderpool\" }] # macvlan resource resources: requests: spidernet.io/shared_cx5_gpu1: 1 spidernet.io/shared_cx5_gpu2: 1 spidernet.io/shared_cx5_gpu3: 1 spidernet.io/shared_cx5_gpu4: 1 spidernet.io/shared_cx5_gpu5: 1 spidernet.io/shared_cx5_gpu6: 1 spidernet.io/shared_cx5_gpu7: 1 spidernet.io/shared_cx5_gpu8: 1 #nvidia.com/gpu: 1 During the creation of the network namespace for the container, Spiderpool will perform connectivity tests on the gateway of the macvlan interface. If all PODs of the above application start successfully, it indicates successful connectivity of the network cards on each node, allowing normal RDMA communication. Check the network namespace status of the container. You can enter the network namespace of any POD to confirm that it has 9 network cards. $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... Check the routing configuration. Spiderpool will automatically tune policy routes for each network card, ensuring that external requests received on each card are returned through the same card. root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 In the main routing table, ensure that Calico network traffic, ClusterIP traffic, and local host communication traffic are all forwarded through the Calico network card. root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link Confirm that there are 8 RDMA devices. root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... Confirm that RDMA data transmission is functioning properly between Pods across nodes. Open a terminal, enter a Pod, and start the service: # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat Open another terminal, enter another Pod, and access the service: # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115 Auto Inject RDMA Resources Based on Webhook In the steps above, we demonstrated how to use SR-IOV technology to provide RDMA communication capabilities for containers in RoCE and Infiniband network environments. However, the process can become complex when configuring AI applications with multiple network cards. To simplify this process, Spiderpool supports classifying a set of network card configurations through annotations ( cni.spidernet.io/rdma-resource-inject or cni.spidernet.io/network-resource-inject ). Users only need to add the same annotation to the application, and Spiderpool will automatically inject all corresponding network cards and network resources with the same annotation into the application through a webhook. cni.spidernet.io/rdma-resource-inject annotation is only applicable to AI scenarios, automatically injecting RDMA network cards and RDMA resources. cni.spidernet.io/network-resource-inject annotation can be used not only for AI scenarios but also supports underlay scenarios. In the future, we hope to uniformly use cni.spidernet.io/network-resource-inject to support both of these scenarios. This feature only supports network card configurations with cniType of [macvlan, ipvlan, sriov, ib-sriov, ipoib]. Currently, Spiderpool's webhook for automatically injecting RDMA network resources is disabled by default and needs to be enabled manually. ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true After enabling the webhook automatic injection of network resources, you can update the configuration by updating the podResourceInject field in configMap: spiderpool-config. Specify namespaces that do not require RDMA network resource injection through podResourceInject.namespacesExclude . Specify namespaces that require RDMA network resource injection through podResourceInject.namespacesInclude . If neither podResourceInject.namespacesExclude nor podResourceInject.namespacesInclude is specified, RDMA network resource injection is performed for all namespaces by default. Currently, after completing the configuration change, you need to restart the spiderpool-controller for the configuration to take effect. When creating all SpiderMultusConfig instances for AI computing networks, add an annotation with the key \"cni.spidernet.io/rdma-resource-inject\" (or \"cni.spidernet.io/network-resource-inject\") and a customizable value. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : macvlan macvlan : master : [ \"enp11s0f0np0\" ] enableRdma : true rdmaResourceName : spidernet.io/gpu1rdma ippools : ipv4 : [ \"gpu1-net11\" ] When creating an AI application, add the same annotation to the application: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network Note: When using the webhook automatic injection of network resources feature, do not add other network configuration annotations (such as k8s.v1.cni.cncf.io/networks and ipam.spidernet.io/ippools ) to the application, as it will affect the automatic injection of resources. Once the Pod is created, you can observe that the Pod has been automatically injected with network card annotations and RDMA resources. ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"AI Cluster with Macvlan"},{"location":"usage/install/ai/get-started-macvlan/#ai-cluster-with-macvlan","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"AI Cluster With Macvlan"},{"location":"usage/install/ai/get-started-macvlan/#introduction","text":"This section explains how to provide RDMA communication capabilities to containers using Macvlan technology in the context of building an AI cluster, applicable in RoCE network scenarios. By using RDMA shared device plugin , a Macvlan interface can be attached to a container, allowing the RDMA device on the master interface to be shared with the container. Therefore: The RDMA system needs to operate in shared mode, where all containers share the RDMA device of the host's master network interface. A key characteristic of this setup is that in each newly launched container, the available GID index of the RDMA device continuously increments and is not a fixed value. Macvlan interfaces cannot be created on an Infiniband IPOIB network card, so this solution is only applicable in RoCE network scenarios and cannot be used in Infiniband network scenarios.","title":"Introduction"},{"location":"usage/install/ai/get-started-macvlan/#solution","text":"This article will introduce how to set up Spiderpool using the following typical AI cluster topology as an example. Figure 1: AI Cluster Topology The network planning for the cluster is as follows: The calico CNI runs on the eth0 network card of the nodes to carry Kubernetes traffic. The AI workload will be assigned a default calico network interface for control plane communication. The nodes use Mellanox ConnectX5 network cards with RDMA functionality to carry the RDMA traffic for AI computation. The network cards are connected to a rail-optimized network. The AI workload will be additionally assigned Macvlan virtualized interfaces for all RDMA network cards to ensure high-speed network communication for the GPUs.","title":"Solution"},{"location":"usage/install/ai/get-started-macvlan/#installation-requirements","text":"Refer to the Spiderpool Installation Requirements . Prepare the Helm binary on the host. Install a Kubernetes cluster with kubelet running on the host\u2019s eth0 network card as shown in Figure 1 . Install Calico as the default CNI for the cluster, using the host\u2019s eth0 network card for Calico\u2019s traffic forwarding. If not installed, refer to the official documentation or use the following commands to install: $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip","title":"Installation Requirements"},{"location":"usage/install/ai/get-started-macvlan/#host-preparation","text":"Install the RDMA network card driver. For Mellanox network cards, you can download the NVIDIA OFED official driver and install it on the host using the following installation command: mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all For Mellanox network cards, you can also perform a containerized installation to batch install drivers on all Mellanox network cards in the cluster hosts. Run the following command. Note that this process requires internet access to fetch some installation packages. When all the OFED pods enter the ready state, it indicates that the OFED driver installation on the hosts is complete: $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" Verify that the network card supports Ethernet operating modes. In this example environment, the host is equipped with Mellanox ConnectX 5 VPI network cards. Query the RDMA devices to confirm that the network card driver is installed correctly. $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... Verify the network card's operating mode. The following output indicates that the network card is operating in Ethernet mode and can achieve RoCE communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet The following output indicates that the network card is operating in Infiniband mode and can achieve Infiniband communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand If the network card is not operating in the expected mode, enter the following command to verify that the network card supports configuring the LINK_TYPE parameter. If the parameter is not available, please switch to a supported network card model: $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) Enable GPUDirect RDMA The installation of the gpu-operator : Enable the Helm installation options: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true . The gpu-operator will install the nvidia-peermem kernel module, enabling GPUDirect RDMA functionality to accelerate data transfer performance between the GPU and RDMA network cards. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 Enable the Helm installation option: --set gdrcopy.enabled=true . The gpu-operator will install the gdrcopy kernel module to accelerate data transfer performance between GPU memory and CPU memory. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep gdrdrv gdrdrv 24576 0 Set the RDMA subsystem on the host to shared mode, allowing containers to independently use shared RDMA device. # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on","title":"Host Preparation"},{"location":"usage/install/ai/get-started-macvlan/#install-spiderpool","text":"Use Helm to install Spiderpool and enable the rdmaSharedDevicePlugin: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set rdma.rdmaSharedDevicePlugin.install = true If you are a user in China, you can specify the helm option --set global.imageRegistryOverride=ghcr.m.daocloud.io to use a domestic image source. Set --set spiderpoolAgent.prometheus.enabled --set spiderpoolAgent.prometheus.enabledRdmaMetric=true and --set grafanaDashboard.install=true flag to enable the RDMA metrics exporter and Grafana dashboard. Refer to RDMA metrics . After completion, the installed components are as follows: $ kubectl get pod -n spiderpool spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m spiderpool-rdma-shared-device-plugin-9xsm9 1 /1 Running 0 1m spiderpool-rdma-shared-device-plugin-nxvlx 1 /1 Running 0 1m Configure k8s-rdma-shared-dev-plugin Modify the following ConfigMap to create eight types of RDMA shared devices, each associated with a specific GPU device. For detailed configuration of the ConfigMap, refer to the official documentation . $ kubectl edit configmap -n spiderpool spiderpool-rdma-shared-device-plugi .... config.json: | { \"periodicUpdateInterval\" : 300 , \"configList\" : [ { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu1\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp11s0f0np0\" ] } } , .... { \"resourcePrefix\" : \"spidernet.io\" , \"resourceName\" : \"shared_cx5_gpu8\" , \"rdmaHcaMax\" : 100 , \"selectors\" : { \"ifNames\" : [ \"enp18s0f0np0\" ] } } ] After completing the above configuration, you can check the available resources on the node to confirm that each node has correctly recognized and reported the eight types of RDMA device resources. $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/shared_cx5_gpu1\" : \"100\" , \"spidernet.io/shared_cx5_gpu2\" : \"100\" , ... \"spidernet.io/shared_cx5_gpu8\" : \"100\" , ... } } , ... ] Create CNI configuration and proper IP pool resources For Ethernet networks, please configure the Macvlan network interfaces associated with all GPUs and create corresponding IP address pools. The example below shows the configuration for the network interface and IP address pool associated with GPU1. $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-macvlan namespace: spiderpool spec: cniType: macvlan macvlan: master: [\"enp11s0f0np0\"] ippools: ipv4: [\"gpu1-net11\"] EOF","title":"Install Spiderpool"},{"location":"usage/install/ai/get-started-macvlan/#create-a-test-application","text":"Create a DaemonSet application on specified nodes. In the following example, the annotation field v1.multus-cni.io/default-network specifies the use of the default Calico network card for control plane communication. The annotation field k8s.v1.cni.cncf.io/networks connects to the 8 network cards affinitized to the GPU for RDMA communication, and configures 8 types of RDMA resources. NOTICE: It support auto inject RDMA resources for application, see Auto inject RDMA Resources $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringE xecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: | - [{ \"name\" : \"gpu1-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu2-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu3-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu4-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu5-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu6-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu7-macvlan\" , \"namespace\" : \"spiderpool\" } , { \"name\" : \"gpu8-macvlan\" , \"namespace\" : \"spiderpool\" }] # macvlan resource resources: requests: spidernet.io/shared_cx5_gpu1: 1 spidernet.io/shared_cx5_gpu2: 1 spidernet.io/shared_cx5_gpu3: 1 spidernet.io/shared_cx5_gpu4: 1 spidernet.io/shared_cx5_gpu5: 1 spidernet.io/shared_cx5_gpu6: 1 spidernet.io/shared_cx5_gpu7: 1 spidernet.io/shared_cx5_gpu8: 1 #nvidia.com/gpu: 1 During the creation of the network namespace for the container, Spiderpool will perform connectivity tests on the gateway of the macvlan interface. If all PODs of the above application start successfully, it indicates successful connectivity of the network cards on each node, allowing normal RDMA communication. Check the network namespace status of the container. You can enter the network namespace of any POD to confirm that it has 9 network cards. $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... Check the routing configuration. Spiderpool will automatically tune policy routes for each network card, ensuring that external requests received on each card are returned through the same card. root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 In the main routing table, ensure that Calico network traffic, ClusterIP traffic, and local host communication traffic are all forwarded through the Calico network card. root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link Confirm that there are 8 RDMA devices. root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... Confirm that RDMA data transmission is functioning properly between Pods across nodes. Open a terminal, enter a Pod, and start the service: # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat Open another terminal, enter another Pod, and access the service: # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115","title":"Create a Test Application"},{"location":"usage/install/ai/get-started-macvlan/#auto-inject-rdma-resources-based-on-webhook","text":"In the steps above, we demonstrated how to use SR-IOV technology to provide RDMA communication capabilities for containers in RoCE and Infiniband network environments. However, the process can become complex when configuring AI applications with multiple network cards. To simplify this process, Spiderpool supports classifying a set of network card configurations through annotations ( cni.spidernet.io/rdma-resource-inject or cni.spidernet.io/network-resource-inject ). Users only need to add the same annotation to the application, and Spiderpool will automatically inject all corresponding network cards and network resources with the same annotation into the application through a webhook. cni.spidernet.io/rdma-resource-inject annotation is only applicable to AI scenarios, automatically injecting RDMA network cards and RDMA resources. cni.spidernet.io/network-resource-inject annotation can be used not only for AI scenarios but also supports underlay scenarios. In the future, we hope to uniformly use cni.spidernet.io/network-resource-inject to support both of these scenarios. This feature only supports network card configurations with cniType of [macvlan, ipvlan, sriov, ib-sriov, ipoib]. Currently, Spiderpool's webhook for automatically injecting RDMA network resources is disabled by default and needs to be enabled manually. ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true After enabling the webhook automatic injection of network resources, you can update the configuration by updating the podResourceInject field in configMap: spiderpool-config. Specify namespaces that do not require RDMA network resource injection through podResourceInject.namespacesExclude . Specify namespaces that require RDMA network resource injection through podResourceInject.namespacesInclude . If neither podResourceInject.namespacesExclude nor podResourceInject.namespacesInclude is specified, RDMA network resource injection is performed for all namespaces by default. Currently, after completing the configuration change, you need to restart the spiderpool-controller for the configuration to take effect. When creating all SpiderMultusConfig instances for AI computing networks, add an annotation with the key \"cni.spidernet.io/rdma-resource-inject\" (or \"cni.spidernet.io/network-resource-inject\") and a customizable value. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : macvlan macvlan : master : [ \"enp11s0f0np0\" ] enableRdma : true rdmaResourceName : spidernet.io/gpu1rdma ippools : ipv4 : [ \"gpu1-net11\" ] When creating an AI application, add the same annotation to the application: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network Note: When using the webhook automatic injection of network resources feature, do not add other network configuration annotations (such as k8s.v1.cni.cncf.io/networks and ipam.spidernet.io/ippools ) to the application, as it will affect the automatic injection of resources. Once the Pod is created, you can observe that the Pod has been automatically injected with network card annotations and RDMA resources. ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"Auto Inject RDMA Resources Based on Webhook"},{"location":"usage/install/ai/get-started-sriov-zh_CN/","text":"AI Cluster With SR-IOV \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u672c\u8282\u4ecb\u7ecd\u5728\u5efa\u8bbe AI \u96c6\u7fa4\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u57fa\u4e8e SR-IOV \u6280\u672f\u7ed9\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\uff0c\u5b83\u9002\u7528\u5728 RoCE \u548c Infiniband \u7f51\u7edc\u573a\u666f\u4e0b\u3002 Spiderpool \u4f7f\u7528\u4e86 sriov-network-operator \u4e3a\u5bb9\u5668\u63d0\u4f9b\u4e86\u57fa\u4e8e SR-IOV \u63a5\u53e3\u7684 RDMA \u8bbe\u5907\uff1a Linux \u7684 RDMA \u5b50\u7cfb\u7edf\uff0c\u53ef\u4e24\u79cd\u5728\u5171\u4eab\u6a21\u5f0f\u6216\u72ec\u5360\u6a21\u5f0f\u4e0b\uff1a \u5171\u4eab\u6a21\u5f0f\uff0c\u5bb9\u5668\u4e2d\u4f1a\u770b\u5230 PF \u63a5\u53e3\u7684\u6240\u6709 VF \u8bbe\u5907\u7684 RDMA \u8bbe\u5907\uff0c\u4f46\u53ea\u6709\u5206\u914d\u7ed9\u672c\u5bb9\u5668\u7684 VF \u624d\u5177\u5907\u4ece 0 \u5f00\u59cb\u7684 GID Index\u3002 \u72ec\u5360\u6a21\u5f0f\uff0c\u5bb9\u5668\u4e2d\u53ea\u4f1a\u770b\u5230\u5206\u914d\u7ed9\u81ea\u8eab VF \u7684 RDMA \u8bbe\u5907\uff0c\u4e0d\u4f1a\u770b\u89c1 PF \u548c \u5176\u5b83 VF \u7684 RDMA \u8bbe\u5907\u3002 \u5728\u4e0d\u540c\u7684\u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4f7f\u7528\u4e86\u4e0d\u540c\u7684 CNI Infiniband \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4f7f\u7528 IB-SRIOV CNI \u7ed9 POD \u63d0\u4f9b SR-IOV \u7f51\u5361\u3002 RoCE \u7f51\u7edc\u573a\u666f\u4e0b\uff0c \u4f7f\u7528\u4e86 SR-IOV CNI \u6765\u66b4\u9732\u5bbf\u4e3b\u673a\u4e0a\u7684 RDMA \u7f51\u5361\u7ed9 Pod \u4f7f\u7528\uff0c\u66b4\u9732 RDMA \u8d44\u6e90\u3002\u53ef\u989d\u5916\u4f7f\u7528 RDMA CNI \u6765\u5b8c\u6210 RDMA \u8bbe\u5907\u9694\u79bb\u3002 \u65b9\u6848 \u672c\u6587\u5c06\u4ee5\u5982\u4e0b\u5178\u578b\u7684 AI \u96c6\u7fa4\u62d3\u6251\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u642d\u5efa Spiderpool \u56fe1 AI \u96c6\u7fa4\u62d3\u6251 \u96c6\u7fa4\u7684\u7f51\u7edc\u89c4\u5212\u5982\u4e0b\uff1a \u5728\u8282\u70b9\u7684 eth0 \u7f51\u5361\u4e0a\u8fd0\u884c calico CNI\uff0c\u6765\u627f\u8f7d kubernetes \u6d41\u91cf\u3002AI workload \u5c06\u4f1a\u88ab\u5206\u914d\u4e00\u4e2a calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\u3002 \u8282\u70b9\u4e0a\u4f7f\u7528\u5177\u5907 RDMA \u529f\u80fd\u7684 Mellanox ConnectX5 \u7f51\u5361\u6765\u627f\u8f7d AI \u8ba1\u7b97\u7684 RDMA \u6d41\u91cf\uff0c\u7f51\u5361\u63a5\u5165\u5230 rail optimized \u7f51\u7edc\u4e2d\u3002AI workload \u5c06\u4f1a\u88ab\u989d\u5916\u5206\u914d\u6240\u6709 RDMA \u7f51\u5361\u7684 SR-IOV \u865a\u62df\u5316\u63a5\u53e3\uff0c\u786e\u4fdd GPU \u7684\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u3002 \u5b89\u88c5\u8981\u6c42 \u53c2\u8003 Spiderpool\u5b89\u88c5\u8981\u6c42 \u4e3b\u673a\u4e0a\u51c6\u5907\u597d Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5\u597d Kubernetes \u96c6\u7fa4\uff0ckubelet \u5de5\u4f5c\u5728\u56fe 1 \u4e2d\u7684\u4e3b\u673a eth0 \u7f51\u5361\u4e0a \u5728 Infiniband \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u786e\u4fdd OpenSM \u5b50\u7f51\u7ba1\u7406\u5668\u5de5\u4f5c\u6b63\u5e38 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\uff0c\u4f7f\u7528\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u4f5c\u4e3a calico \u7684\u6d41\u91cf\u8f6c\u53d1\u7f51\u5361\u3002 \u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip \u4e3b\u673a\u51c6\u5907 \u5b89\u88c5 RDMA \u7f51\u5361\u9a71\u52a8 \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u53ef\u4e0b\u8f7d NVIDIA OFED \u5b98\u65b9\u9a71\u52a8 \u8fdb\u884c\u4e3b\u673a\u5b89\u88c5\uff0c\u6267\u884c\u5982\u4e0b\u5b89\u88c5\u547d\u4ee4 mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u4e5f\u53ef\u57fa\u4e8e\u5bb9\u5668\u5316\u5b89\u88c5\uff0c\u5b9e\u73b0\u5bf9\u96c6\u7fa4\u4e3b\u673a\u4e0a\u6240\u6709 Mellanox \u7f51\u5361\u6279\u91cf\u5b89\u88c5\u9a71\u52a8\uff0c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u9700\u8981\u8bbf\u95ee\u56e0\u7279\u7f51\u83b7\u53d6\u4e00\u4e9b\u5b89\u88c5\u5305\u3002\u5f53\u6240\u6709\u7684 ofed pod \u8fdb\u5165 ready \u72b6\u6001\uff0c\u8868\u793a\u4e3b\u673a\u4e0a\u5df2\u7ecf\u5b8c\u6210\u4e86 OFED driver \u5b89\u88c5 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" \u82e5\u5e0c\u671b RDMA \u7cfb\u7edf\u5de5\u4f5c\u5728\u72ec\u5360\u6a21\u5f0f\u4e0b\uff0c\u5fc5\u987b\u81f3\u5c11\u6ee1\u8db3\u4ee5\u4e0b\u6761\u4ef6\u4e4b\u4e00\uff1a (1\uff09 \u57fa\u4e8e 5.3.0 \u6216\u66f4\u65b0\u7248\u672c\u7684 Linux \u5185\u6838\uff0c\u7cfb\u7edf\u4e2d\u52a0\u8f7d\u7684 RDMA \u6a21\u5757\uff0crdma \u6838\u5fc3\u5305\u63d0\u4f9b\u4e86\u5728\u7cfb\u7edf\u542f\u52a8\u65f6\u81ea\u52a8\u52a0\u8f7d\u76f8\u5173\u6a21\u5757\u7684\u65b9\u6cd5 (2\uff09 \u9700\u8981 Mellanox OFED 4.7 \u7248\u6216\u66f4\u65b0\u7248\u672c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u4f7f\u7528\u57fa\u4e8e 5.3.0 \u6216\u66f4\u65b0\u7248\u672c\u7684\u5185\u6838\u3002 \u786e\u8ba4\u7f51\u5361\u652f\u6301 Infiniband \u6216 Ethernet \u5de5\u4f5c\u6a21\u5f0f \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u63a5\u5165\u4e86 mellanox ConnectX 5 VPI \u7f51\u5361\uff0c\u67e5\u8be2 RDMA \u8bbe\u5907\uff0c\u786e\u8ba4\u7f51\u5361\u9a71\u52a8\u5b89\u88c5\u5b8c\u6210 $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... \u786e\u8ba4\u7f51\u5361\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Ethernet \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 RoCE \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet \u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Infiniband \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 Infiniband \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand \u5982\u679c\u7f51\u5361\u6ca1\u6709\u5de5\u4f5c\u5728\u9884\u671f\u7684\u6a21\u5f0f\u4e0b\uff0c\u8bf7\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u7f51\u5361\u652f\u6301\u914d\u7f6e LINK_TYPE \u53c2\u6570\uff0c\u5982\u679c\u6ca1\u6709\u8be5\u53c2\u6570\uff0c\u8bf7\u66f4\u6362\u652f\u6301\u7684\u7f51\u5361\u578b\u53f7 $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) \u5f00\u542f GPUDirect RMDA \u529f\u80fd \u5728\u5b89\u88c5\u6216\u4f7f\u7528 gpu-operator \u8fc7\u7a0b\u4e2d a. \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 nvidia-peermem \u5185\u6838\u6a21\u5757\uff0c\u542f\u7528 GPUDirect RMDA \u529f\u80fd\uff0c\u52a0\u901f GPU \u548c RDMA \u7f51\u5361\u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 b. \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set gdrcopy.enabled=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 gdrcopy \u5185\u6838\u6a21\u5757\uff0c\u52a0\u901f GPU \u663e\u5b58 \u548c CPU \u5185\u5b58 \u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep gdrdrv gdrdrv 24576 0 \u82e5\u5e0c\u671b RDMA \u7cfb\u7edf\u5de5\u4f5c\u5728\u72ec\u5360\u6a21\u5f0f\u4e0b\uff0c\u8bf7\u8bbe\u7f6e\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u4e3a exclusive \u6a21\u5f0f\uff0c\u4f7f\u5f97\u5bb9\u5668\u80fd\u591f\u72ec\u7acb\u4f7f\u7528 RDMA \u8bbe\u5907\u8fc7\u7a0b\uff0c\u907f\u514d\u4e0e\u5176\u4ed6\u5bb9\u5668\u5171\u4eab # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on # Persist the exclusive mode to remain effective after a reboot $ echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf # Switch the current operating mode to exclusive mode. If the setting fails, please reboot the host $ rdma system set netns exclusive # Verify the successful switch to exclusive mode $ rdma system netns exclusive copy-on-fork on \u5b89\u88c5 Spiderpool \u4f7f\u7528 helm \u5b89\u88c5 Spiderpool\uff0c\u5e76\u542f\u7528 SR-IOV \u7ec4\u4ef6 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set sriov.install = true \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabled --set spiderpoolAgent.prometheus.enabledRdmaMetric=true \u548c --set grafanaDashboard.install=true \u547d\u4ee4\u884c\u53c2\u6570\u53ef\u4ee5\u5f00\u542f RDMA metrics exporter \u548c Grafana dashboard\uff0c\u66f4\u591a\u53ef\u4ee5\u67e5\u770b RDMA metrics . \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 1m spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m \u914d\u7f6e SR-IOV operator, \u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u521b\u5efa\u51fa VF \u8bbe\u5907 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u67e5\u8be2\u4e3b\u673a\u4e0a\u7f51\u5361\u8bbe\u5907\u7684 PCIE \u4fe1\u606f\u3002\u786e\u8ba4\u5982\u4e0b\u8f93\u51fa\u7684\u8bbe\u5907\u53f7 [15b3:1017] \u51fa\u73b0\u5728 sriov-network-operator \u652f\u6301\u7f51\u5361\u578b\u53f7\u8303\u56f4 $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] .... SRIOV VF \u6570\u91cf\u51b3\u5b9a\u4e86\u4e00\u4e2a\u7f51\u5361\u80fd\u540c\u65f6\u4e3a\u591a\u5c11\u4e2a POD \u63d0\u4f9b\u7f51\u5361\uff0c\u4e0d\u540c\u578b\u53f7\u7684\u7f51\u5361\u7684\u6709\u4e0d\u540c\u7684\u6700\u5927 VF \u6570\u91cf\u4e0a\u9650\uff0cMellanox \u7684 ConnectX \u7f51\u5361\u5e38\u89c1\u578b\u53f7\u7684\u6700\u5927 VF \u4e0a\u9650\u662f 127 \u3002 \u5982\u4e0b\u793a\u4f8b\uff0c\u8bbe\u7f6e\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684 GPU1 \u548c GPU2 \u7684\u7f51\u5361\uff0c\u6bcf\u4e2a\u7f51\u5361\u914d\u7f6e\u51fa 12 \u4e2a VF \u8bbe\u5907\u3002\u8bf7\u53c2\u8003\u5982\u4e0b\uff0c\u4e3a\u4e3b\u673a\u4e0a\u6bcf\u4e2a\u4eb2\u548c GPU \u7684\u7f51\u5361\u914d\u7f6e SriovNetworkNodePolicy\uff0c\u8fd9\u6837\uff0c\u5c06\u6709 8 \u4e2a SRIOV resource \u4ee5\u4f9b\u4f7f\u7528\u3002 # \u5bf9\u4e8e ethernet \u7f51\u7edc\uff0c\u8bbe\u7f6e LINK_TYPE=eth\uff0c \u5bf9\u4e8e Infiniband \u7f51\u7edc\uff0c\u8bbe\u7f6e LINK_TYPE=ib $ LINK_TYPE = eth $ cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu1-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu1sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true --- apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu2-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu2sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true EOF \u521b\u5efa SriovNetworkNodePolicy \u914d\u7f6e\u540e\uff0c\u6bcf\u4e2a\u8282\u70b9\u4e0a\u5c06\u4f1a\u542f\u52a8 sriov-device-plugin \uff0c\u8d1f\u8d23\u4e0a\u62a5 VF \u8bbe\u5907\u8d44\u6e90 $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 2m spiderpool-agent-9sllh 1 /1 Running 0 2m spiderpool-agent-h92bv 1 /1 Running 0 2m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 2m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 2m spiderpool-init 0 /1 Completed 0 2m sriov-device-plugin-x2g6b 1 /1 Running 0 1m sriov-device-plugin-z4gjt 1 /1 Running 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m ....... \u521b\u5efa SriovNetworkNodePolicy \u914d\u7f6e\u540e\uff0cSR-IOV operator \u4f1a\u987a\u5e8f\u5730\u5728\u6bcf\u4e00\u4e2a\u8282\u70b9\u4e0a\u9a71\u9010 POD\uff0c\u914d\u7f6e\u7f51\u5361\u9a71\u52a8\u4e2d\u7684 VF \u8bbe\u7f6e\uff0c\u7136\u540e\u91cd\u542f\u4e3b\u673a\u3002\u56e0\u6b64\uff0c\u4f1a\u89c2\u6d4b\u5230\u96c6\u7fa4\u4e2d\u7684\u8282\u70b9\u4f1a\u987a\u5e8f\u8fdb\u5165 SchedulingDisabled \u72b6\u6001\uff0c\u5e76\u88ab\u91cd\u542f\u3002 $ kubectl get node NAME STATUS ROLES AGE VERSION ai-10-1-16-1 Ready worker 2d15h v1.28.9 ai-10-1-16-2 Ready,SchedulingDisabled worker 2d15h v1.28.9 ....... \u6240\u6709\u8282\u70b9\u5b8c\u6210 VF \u914d\u7f6e\u7684\u8fc7\u7a0b\uff0c\u53ef\u80fd\u9700\u8981\u6570\u5206\u949f\uff0c\u53ef\u4ee5\u89c2\u5bdf sriovnetworknodestates \u4e2d\u7684 status \u662f\u5426\u8fdb\u5165 Succeeded \u72b6\u6001\uff0c\u8868\u793a\u914d\u7f6e\u5b8c\u6210 $ kubectl get sriovnetworknodestates -A NAMESPACE NAME SYNC STATUS DESIRED SYNC STATE CURRENT SYNC STATE AGE spiderpool ai-10-1-16-1 Succeeded Idle Idle 4d6h spiderpool ai-10-1-16-2 Succeeded Idle Idle 4d6h ....... \u5bf9\u4e8e\u914d\u7f6e\u6210\u529f\u7684\u8282\u70b9\uff0c\u53ef\u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u5305\u542b\u4e86\u4e0a\u62a5\u7684 SR-IOV \u8bbe\u5907\u8d44\u6e90 $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/gpu1sriov\" : \"12\" , \"spidernet.io/gpu2sriov\" : \"12\" , ... } } , ... ] \u521b\u5efa CNI \u914d\u7f6e\u548c\u5bf9\u5e94\u7684 ippool \u8d44\u6e90 (1) \u5bf9\u4e8e Infiniband \u7f51\u7edc\uff0c\u8bf7\u4e3a\u6240\u6709\u7684 GPU \u4eb2\u548c\u7684 SR-IOV \u7f51\u5361\u914d\u7f6e IB-SRIOV CNI \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u5bf9\u5e94\u7684 IP \u5730\u5740\u6c60 \u3002 \u5982\u4e0b\u4f8b\u5b50\uff0c\u914d\u7f6e\u4e86 GPU1 \u4eb2\u548c\u7684\u7f51\u5361\u548c IP \u5730\u5740\u6c60 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: resourceName: spidernet.io/gpu1sriov rdmaIsolation: true ippools: ipv4: [\"gpu1-net91\"] EOF (2) \u5bf9\u4e8e Ethernet \u7f51\u7edc\uff0c\u8bf7\u4e3a\u6240\u6709\u7684 GPU \u4eb2\u548c\u7684 SR-IOV \u7f51\u5361\u914d\u7f6e SR-IOV CNI \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u5bf9\u5e94\u7684 IP \u5730\u5740\u6c60 \u3002 \u5982\u4e0b\u4f8b\u5b50\uff0c\u914d\u7f6e\u4e86 GPU1 \u4eb2\u548c\u7684\u7f51\u5361\u548c IP \u5730\u5740\u6c60 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: sriov sriov: resourceName: spidernet.io/gpu1sriov enableRdma: true ippools: ipv4: [\"gpu1-net11\"] EOF \u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 \u5728\u6307\u5b9a\u8282\u70b9\u4e0a\u521b\u5efa\u4e00\u7ec4 DaemonSet \u5e94\u7528\uff0c\u6d4b\u8bd5\u6307\u5b9a\u8282\u70b9\u4e0a\u7684 SR-IOV \u8bbe\u5907\u7684\u53ef\u7528\u6027 \u5982\u4e0b\u4f8b\u5b50\uff0c\u901a\u8fc7 annotations v1.multus-cni.io/default-network \u6307\u5b9a\u4f7f\u7528 calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u7528\u4e8e\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\uff0cannotations k8s.v1.cni.cncf.io/networks \u63a5\u5165 8 \u4e2a GPU \u4eb2\u548c\u7f51\u5361\u7684 VF \u7f51\u5361\uff0c\u7528\u4e8e RDMA \u901a\u4fe1\uff0c\u5e76\u914d\u7f6e 8 \u79cd RDMA resources \u8d44\u6e90 \u6ce8\uff1a\u652f\u6301\u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u53c2\u8003 \u57fa\u4e8e Webhook \u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # sriov interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] # sriov resource resources: limits: spidernet.io/gpu1sriov: 1 spidernet.io/gpu2sriov: 1 spidernet.io/gpu3sriov: 1 spidernet.io/gpu4sriov: 1 spidernet.io/gpu5sriov: 1 spidernet.io/gpu6sriov: 1 spidernet.io/gpu7sriov: 1 spidernet.io/gpu8sriov: 1 #nvidia.com/gpu: 1 EOF $ helm install rdma-tools spiderchart/rdma-tools -f ./values.yaml \u5728\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u4f1a\u5bf9 sriov \u63a5\u53e3\u4e0a\u7684\u7f51\u5173\u8fdb\u884c\u8fde\u901a\u6027\u6d4b\u8bd5\uff0c\u5982\u679c\u5982\u4e0a\u5e94\u7528\u7684\u6240\u6709 POD \u90fd\u542f\u52a8\u6210\u529f\uff0c\u8bf4\u660e\u4e86\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684 VF \u8bbe\u5907\u7684\u8fde\u901a\u6027\u6210\u529f\uff0c\u53ef\u8fdb\u884c\u6b63\u5e38\u7684 RDMA \u901a\u4fe1\u3002 \u67e5\u770b\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u72b6\u6001 \u53ef\u8fdb\u5165\u4efb\u4e00\u4e00\u4e2a POD \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u786e\u8ba4\u5177\u5907 9 \u4e2a\u7f51\u5361 $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... \u67e5\u770b\u8def\u7531\u914d\u7f6e\uff0cSpiderpool \u4f1a\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u7f51\u5361\u8c03\u8c10\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7f51\u5361\u4e0a\u6536\u5230\u7684\u5916\u90e8\u8bf7\u6c42\u90fd\u4f1a\u4ece\u8be5\u7f51\u5361\u4e0a\u8fd4\u56de\u56de\u590d\u6d41\u91cf root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 main \u8def\u7531\u4e2d\uff0c\u786e\u4fdd\u4e86 calico \u7f51\u7edc\u6d41\u91cf\u3001ClusterIP \u6d41\u91cf\u3001\u672c\u5730\u5bbf\u4e3b\u673a\u901a\u4fe1\u7b49\u6d41\u91cf\u90fd\u4f1a\u4ece calico \u7f51\u5361\u8f6c\u53d1 root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link \u786e\u8ba4\u5177\u5907 8 \u4e2a RDMA \u8bbe\u5907 root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1 # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115 \uff08\u53ef\u9009\uff09Infiniband \u7f51\u7edc\u4e0b\u5bf9\u63a5 UFM \u5bf9\u4e8e\u4f7f\u7528\u4e86 Infiniband \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5982\u679c\u7f51\u7edc\u4e2d\u6709 UFM \u7ba1\u7406\u5e73\u53f0 \uff0c\u53ef\u4f7f\u7528 ib-kubernetes \u63d2\u4ef6\uff0c\u5b83\u4ee5 daemonset \u5f62\u5f0f\u8fd0\u884c\uff0c\u76d1\u63a7\u6240\u6709\u4f7f\u7528 SRIOV \u7f51\u5361\u7684\u5bb9\u5668\uff0c\u628a VF \u8bbe\u5907\u7684 Pkey \u548c GUID \u4e0a\u62a5\u7ed9 UFM \u3002 \u5728 UFM \u4e3b\u673a\u4e0a\u521b\u5efa\u901a\u4fe1\u6240\u9700\u8981\u7684\u8bc1\u4e66\uff1a # replace to right address $ UFM_ADDRESS = 172 .16.10.10 $ openssl req -x509 -newkey rsa:4096 -keyout ufm.key -out ufm.crt -days 365 -subj '/CN=${UFM_ADDRESS}' # Copy the certificate files to the UFM certificate directory: $ cp ufm.key /etc/pki/tls/private/ufmlocalhost.key $ cp ufm.crt /etc/pki/tls/certs/ufmlocalhost.crt # For containerized UFM deployment, restart the container service $ docker restart ufm # For host-based UFM deployment, restart the UFM service $ systemctl restart ufmd \u5728 kubernetes \u96c6\u7fa4\u4e0a\uff0c\u521b\u5efa ib-kubernetes \u6240\u9700\u7684\u901a\u4fe1\u8bc1\u4e66\u3002\u628a UFM \u4e3b\u673a\u4e0a\u751f\u6210\u7684 ufm.crt \u6587\u4ef6\u4f20\u8f93\u81f3 kubernetes \u8282\u70b9\u4e0a\uff0c\u5e76\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u8bc1\u4e66 # replace to right user $ UFM_USERNAME = admin # replace to right password $ UFM_PASSWORD = 12345 # replace to right address $ UFM_ADDRESS = \"172.16.10.10\" $ kubectl create secret generic ib-kubernetes-ufm-secret --namespace = \"kube-system\" \\ --from-literal = UFM_USER = \" ${ UFM_USERNAME } \" \\ --from-literal = UFM_PASSWORD = \" ${ UFM_PASSWORD } \" \\ --from-literal = UFM_ADDRESS = \" ${ UFM_ADDRESS } \" \\ --from-file = UFM_CERTIFICATE = ufm.crt \u5728 kubernetes \u96c6\u7fa4\u4e0a\u5b89\u88c5 ib-kubernetes git clone https://github.com/Mellanox/ib-kubernetes.git && cd ib-kubernetes $ kubectl create -f deployment/ib-kubernetes-configmap.yaml kubectl create -f deployment/ib-kubernetes.yaml \u5728 Infiniband \u7f51\u7edc\u4e0b\uff0c\u521b\u5efa Spiderpool \u7684 SpiderMultusConfig \u65f6\uff0c\u53ef\u914d\u7f6e pkey\uff0c\u4f7f\u7528\u8be5\u914d\u7f6e\u521b\u5efa\u7684 POD \u5c06\u751f\u6548 pkey \u914d\u7f6e\uff0c\u4e14\u88ab ib-kubernetes \u540c\u6b65\u7ed9 UFM $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ib-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: pkey: 1000 ... EOF Note: Each node in an Infiniband Kubernetes deployment may be associated with up to 128 PKeys due to kernel limitation \u57fa\u4e8e Webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90 \u5728\u4e0a\u8ff0\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 SR-IOV \u6280\u672f\u5728 RoCE \u548c Infiniband \u7f51\u7edc\u73af\u5883\u4e2d\u4e3a\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u914d\u7f6e\u591a\u7f51\u5361\u7684 AI \u5e94\u7528\u65f6\uff0c\u8fc7\u7a0b\u4f1a\u53d8\u5f97\u590d\u6742\u3002\u4e3a\u7b80\u5316\u8fd9\u4e2a\u8fc7\u7a0b\uff0cSpiderpool \u901a\u8fc7 annotations( cni.spidernet.io/rdma-resource-inject \u6216 cni.spidernet.io/network-resource-inject ) \u652f\u6301\u5bf9\u4e00\u7ec4\u7f51\u5361\u914d\u7f6e\u8fdb\u884c\u5206\u7c7b\u3002\u7528\u6237\u53ea\u9700\u8981\u4e3a\u5e94\u7528\u6dfb\u52a0\u4e0e\u7f51\u5361\u914d\u7f6e\u76f8\u540c\u7684\u6ce8\u89e3\uff0cSpiderpool \u5c31\u4f1a\u901a\u8fc7 webhook \u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165\u6240\u6709\u5177\u6709\u76f8\u540c\u6ce8\u89e3\u7684\u5bf9\u5e94\u7f51\u5361\u548c\u7f51\u7edc\u8d44\u6e90\u3002 cni.spidernet.io/rdma-resource-inject \u53ea\u9002\u7528\u4e8e AI \u573a\u666f\uff0c\u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u5361\u53ca RDMA Resources\uff1b cni.spidernet.io/network-resource-inject \u4e0d\u4f46\u53ef\u4ee5\u7528\u4e8e AI \u573a\u666f\uff0c\u4e5f\u652f\u6301 Underlay \u573a\u666f\u3002\u5728\u672a\u6765\u6211\u4eec\u5e0c\u671b\u90fd\u7edf\u4e00\u4f7f\u7528 cni.spidernet.io/network-resource-inject \u652f\u6301\u8fd9\u4e24\u79cd\u573a\u666f\u3002 \u8be5\u529f\u80fd\u4ec5\u652f\u6301 [ macvlan, ipvlan, sriov, ib-sriov, ipoib ] \u8fd9\u51e0\u79cd cniType \u7684\u7f51\u5361\u914d\u7f6e\u3002 \u5f53\u524d Spiderpool \u7684 webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u9ed8\u8ba4\u662f\u5173\u95ed\u7684\uff0c\u9700\u8981\u624b\u52a8\u5f00\u542f\u3002 ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true \u542f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u66f4\u65b0 configMap: spiderpool-config \u4e2d\u7684 podResourceInject \u5b57\u6bb5\u66f4\u65b0\u914d\u7f6e\u3002 \u901a\u8fc7 podResourceInject.namespacesExclude \u6307\u5b9a\u4e0d\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4 \u901a\u8fc7 podResourceInject.namespacesInclude \u6307\u5b9a\u9700\u8981\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u5982\u679c podResourceInject.namespacesExclude \u548c podResourceInject.namespacesInclude \u90fd\u6ca1\u6709\u6307\u5b9a\uff0c\u5219\u9ed8\u8ba4\u5bf9\u6240\u6709\u547d\u540d\u7a7a\u95f4\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u3002 \u5f53\u524d\uff0c\u5b8c\u6210\u914d\u7f6e\u53d8\u66f4\u540e\uff0c\u60a8\u9700\u8981\u91cd\u542f spiderpool-controller \u6765\u4f7f\u914d\u7f6e\u751f\u6548\u3002 \u5728\u521b\u5efa AI \u7b97\u529b\u7f51\u7edc\u7684\u6240\u6709 SpiderMultusConfig \u5b9e\u4f8b\u65f6\uff0c\u6dfb\u52a0 key \u4e3a \"cni.spidernet.io/rdma-resource-inject\" \u6216 \"cni.spidernet.io/network-resource-inject\" \u7684 annotation\uff0cvalue \u53ef\u81ea\u5b9a\u4e49\u4efb\u4f55\u503c apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : sriov sriov : resourceName : spidernet.io/gpu1rdma enableRdma : true ippools : ipv4 : [ \"gpu1-net11\" ] \u521b\u5efa AI \u5e94\u7528\u65f6\uff0c\u4e3a\u5e94\u7528\u4e5f\u6dfb\u52a0\u76f8\u540c\u6ce8\u89e3: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network \u6ce8\u610f\uff1a\u4f7f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u65f6\uff0c\u4e0d\u80fd\u4e3a\u5e94\u7528\u6dfb\u52a0\u5176\u4ed6\u7f51\u7edc\u914d\u7f6e\u6ce8\u89e3(\u5982 k8s.v1.cni.cncf.io/networks \u548c ipam.spidernet.io ippools \u7b49)\uff0c\u5426\u5219\u4f1a\u5f71\u54cd\u8d44\u6e90\u81ea\u52a8\u6ce8\u5165\u529f\u80fd\u3002 \u5f53 Pod \u88ab\u521b\u5efa\u540e\uff0c\u53ef\u89c2\u6d4b\u5230 Pod \u88ab\u81ea\u52a8\u6ce8\u5165\u4e86\u7f51\u5361 annotation \u548c RDMA \u8d44\u6e90 ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"AI Cluster With SR-IOV"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#ai-cluster-with-sr-iov","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"AI Cluster With SR-IOV"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#_1","text":"\u672c\u8282\u4ecb\u7ecd\u5728\u5efa\u8bbe AI \u96c6\u7fa4\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u57fa\u4e8e SR-IOV \u6280\u672f\u7ed9\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\uff0c\u5b83\u9002\u7528\u5728 RoCE \u548c Infiniband \u7f51\u7edc\u573a\u666f\u4e0b\u3002 Spiderpool \u4f7f\u7528\u4e86 sriov-network-operator \u4e3a\u5bb9\u5668\u63d0\u4f9b\u4e86\u57fa\u4e8e SR-IOV \u63a5\u53e3\u7684 RDMA \u8bbe\u5907\uff1a Linux \u7684 RDMA \u5b50\u7cfb\u7edf\uff0c\u53ef\u4e24\u79cd\u5728\u5171\u4eab\u6a21\u5f0f\u6216\u72ec\u5360\u6a21\u5f0f\u4e0b\uff1a \u5171\u4eab\u6a21\u5f0f\uff0c\u5bb9\u5668\u4e2d\u4f1a\u770b\u5230 PF \u63a5\u53e3\u7684\u6240\u6709 VF \u8bbe\u5907\u7684 RDMA \u8bbe\u5907\uff0c\u4f46\u53ea\u6709\u5206\u914d\u7ed9\u672c\u5bb9\u5668\u7684 VF \u624d\u5177\u5907\u4ece 0 \u5f00\u59cb\u7684 GID Index\u3002 \u72ec\u5360\u6a21\u5f0f\uff0c\u5bb9\u5668\u4e2d\u53ea\u4f1a\u770b\u5230\u5206\u914d\u7ed9\u81ea\u8eab VF \u7684 RDMA \u8bbe\u5907\uff0c\u4e0d\u4f1a\u770b\u89c1 PF \u548c \u5176\u5b83 VF \u7684 RDMA \u8bbe\u5907\u3002 \u5728\u4e0d\u540c\u7684\u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4f7f\u7528\u4e86\u4e0d\u540c\u7684 CNI Infiniband \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4f7f\u7528 IB-SRIOV CNI \u7ed9 POD \u63d0\u4f9b SR-IOV \u7f51\u5361\u3002 RoCE \u7f51\u7edc\u573a\u666f\u4e0b\uff0c \u4f7f\u7528\u4e86 SR-IOV CNI \u6765\u66b4\u9732\u5bbf\u4e3b\u673a\u4e0a\u7684 RDMA \u7f51\u5361\u7ed9 Pod \u4f7f\u7528\uff0c\u66b4\u9732 RDMA \u8d44\u6e90\u3002\u53ef\u989d\u5916\u4f7f\u7528 RDMA CNI \u6765\u5b8c\u6210 RDMA \u8bbe\u5907\u9694\u79bb\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#_2","text":"\u672c\u6587\u5c06\u4ee5\u5982\u4e0b\u5178\u578b\u7684 AI \u96c6\u7fa4\u62d3\u6251\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u642d\u5efa Spiderpool \u56fe1 AI \u96c6\u7fa4\u62d3\u6251 \u96c6\u7fa4\u7684\u7f51\u7edc\u89c4\u5212\u5982\u4e0b\uff1a \u5728\u8282\u70b9\u7684 eth0 \u7f51\u5361\u4e0a\u8fd0\u884c calico CNI\uff0c\u6765\u627f\u8f7d kubernetes \u6d41\u91cf\u3002AI workload \u5c06\u4f1a\u88ab\u5206\u914d\u4e00\u4e2a calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\u3002 \u8282\u70b9\u4e0a\u4f7f\u7528\u5177\u5907 RDMA \u529f\u80fd\u7684 Mellanox ConnectX5 \u7f51\u5361\u6765\u627f\u8f7d AI \u8ba1\u7b97\u7684 RDMA \u6d41\u91cf\uff0c\u7f51\u5361\u63a5\u5165\u5230 rail optimized \u7f51\u7edc\u4e2d\u3002AI workload \u5c06\u4f1a\u88ab\u989d\u5916\u5206\u914d\u6240\u6709 RDMA \u7f51\u5361\u7684 SR-IOV \u865a\u62df\u5316\u63a5\u53e3\uff0c\u786e\u4fdd GPU \u7684\u9ad8\u901f\u7f51\u7edc\u901a\u4fe1\u3002","title":"\u65b9\u6848"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#_3","text":"\u53c2\u8003 Spiderpool\u5b89\u88c5\u8981\u6c42 \u4e3b\u673a\u4e0a\u51c6\u5907\u597d Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5\u597d Kubernetes \u96c6\u7fa4\uff0ckubelet \u5de5\u4f5c\u5728\u56fe 1 \u4e2d\u7684\u4e3b\u673a eth0 \u7f51\u5361\u4e0a \u5728 Infiniband \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u786e\u4fdd OpenSM \u5b50\u7f51\u7ba1\u7406\u5668\u5de5\u4f5c\u6b63\u5e38 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\uff0c\u4f7f\u7528\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u4f5c\u4e3a calico \u7684\u6d41\u91cf\u8f6c\u53d1\u7f51\u5361\u3002 \u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#_4","text":"\u5b89\u88c5 RDMA \u7f51\u5361\u9a71\u52a8 \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u53ef\u4e0b\u8f7d NVIDIA OFED \u5b98\u65b9\u9a71\u52a8 \u8fdb\u884c\u4e3b\u673a\u5b89\u88c5\uff0c\u6267\u884c\u5982\u4e0b\u5b89\u88c5\u547d\u4ee4 mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all \u5bf9\u4e8e Mellanox \u7f51\u5361\uff0c\u4e5f\u53ef\u57fa\u4e8e\u5bb9\u5668\u5316\u5b89\u88c5\uff0c\u5b9e\u73b0\u5bf9\u96c6\u7fa4\u4e3b\u673a\u4e0a\u6240\u6709 Mellanox \u7f51\u5361\u6279\u91cf\u5b89\u88c5\u9a71\u52a8\uff0c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u9700\u8981\u8bbf\u95ee\u56e0\u7279\u7f51\u83b7\u53d6\u4e00\u4e9b\u5b89\u88c5\u5305\u3002\u5f53\u6240\u6709\u7684 ofed pod \u8fdb\u5165 ready \u72b6\u6001\uff0c\u8868\u793a\u4e3b\u673a\u4e0a\u5df2\u7ecf\u5b8c\u6210\u4e86 OFED driver \u5b89\u88c5 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" \u82e5\u5e0c\u671b RDMA \u7cfb\u7edf\u5de5\u4f5c\u5728\u72ec\u5360\u6a21\u5f0f\u4e0b\uff0c\u5fc5\u987b\u81f3\u5c11\u6ee1\u8db3\u4ee5\u4e0b\u6761\u4ef6\u4e4b\u4e00\uff1a (1\uff09 \u57fa\u4e8e 5.3.0 \u6216\u66f4\u65b0\u7248\u672c\u7684 Linux \u5185\u6838\uff0c\u7cfb\u7edf\u4e2d\u52a0\u8f7d\u7684 RDMA \u6a21\u5757\uff0crdma \u6838\u5fc3\u5305\u63d0\u4f9b\u4e86\u5728\u7cfb\u7edf\u542f\u52a8\u65f6\u81ea\u52a8\u52a0\u8f7d\u76f8\u5173\u6a21\u5757\u7684\u65b9\u6cd5 (2\uff09 \u9700\u8981 Mellanox OFED 4.7 \u7248\u6216\u66f4\u65b0\u7248\u672c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u4f7f\u7528\u57fa\u4e8e 5.3.0 \u6216\u66f4\u65b0\u7248\u672c\u7684\u5185\u6838\u3002 \u786e\u8ba4\u7f51\u5361\u652f\u6301 Infiniband \u6216 Ethernet \u5de5\u4f5c\u6a21\u5f0f \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u63a5\u5165\u4e86 mellanox ConnectX 5 VPI \u7f51\u5361\uff0c\u67e5\u8be2 RDMA \u8bbe\u5907\uff0c\u786e\u8ba4\u7f51\u5361\u9a71\u52a8\u5b89\u88c5\u5b8c\u6210 $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... \u786e\u8ba4\u7f51\u5361\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Ethernet \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 RoCE \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet \u5982\u4e0b\u8f93\u51fa\u8868\u793a\u7f51\u5361\u5de5\u4f5c\u5728 Infiniband \u6a21\u5f0f\u4e0b\uff0c\u53ef\u5b9e\u73b0 Infiniband \u901a\u4fe1 $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand \u5982\u679c\u7f51\u5361\u6ca1\u6709\u5de5\u4f5c\u5728\u9884\u671f\u7684\u6a21\u5f0f\u4e0b\uff0c\u8bf7\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u7f51\u5361\u652f\u6301\u914d\u7f6e LINK_TYPE \u53c2\u6570\uff0c\u5982\u679c\u6ca1\u6709\u8be5\u53c2\u6570\uff0c\u8bf7\u66f4\u6362\u652f\u6301\u7684\u7f51\u5361\u578b\u53f7 $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) \u5f00\u542f GPUDirect RMDA \u529f\u80fd \u5728\u5b89\u88c5\u6216\u4f7f\u7528 gpu-operator \u8fc7\u7a0b\u4e2d a. \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 nvidia-peermem \u5185\u6838\u6a21\u5757\uff0c\u542f\u7528 GPUDirect RMDA \u529f\u80fd\uff0c\u52a0\u901f GPU \u548c RDMA \u7f51\u5361\u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 b. \u5f00\u542f helm \u5b89\u88c5\u9009\u9879: --set gdrcopy.enabled=true \uff0cgpu-operator \u4f1a\u5b89\u88c5 gdrcopy \u5185\u6838\u6a21\u5757\uff0c\u52a0\u901f GPU \u663e\u5b58 \u548c CPU \u5185\u5b58 \u4e4b\u95f4\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53ef\u5728\u4e3b\u673a\u4e0a\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u7684\u5185\u6838\u6a21\u5757 $ lsmod | grep gdrdrv gdrdrv 24576 0 \u82e5\u5e0c\u671b RDMA \u7cfb\u7edf\u5de5\u4f5c\u5728\u72ec\u5360\u6a21\u5f0f\u4e0b\uff0c\u8bf7\u8bbe\u7f6e\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u4e3a exclusive \u6a21\u5f0f\uff0c\u4f7f\u5f97\u5bb9\u5668\u80fd\u591f\u72ec\u7acb\u4f7f\u7528 RDMA \u8bbe\u5907\u8fc7\u7a0b\uff0c\u907f\u514d\u4e0e\u5176\u4ed6\u5bb9\u5668\u5171\u4eab # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on # Persist the exclusive mode to remain effective after a reboot $ echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf # Switch the current operating mode to exclusive mode. If the setting fails, please reboot the host $ rdma system set netns exclusive # Verify the successful switch to exclusive mode $ rdma system netns exclusive copy-on-fork on","title":"\u4e3b\u673a\u51c6\u5907"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#spiderpool","text":"\u4f7f\u7528 helm \u5b89\u88c5 Spiderpool\uff0c\u5e76\u542f\u7528 SR-IOV \u7ec4\u4ef6 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set sriov.install = true \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u8bbe\u7f6e --set spiderpoolAgent.prometheus.enabled --set spiderpoolAgent.prometheus.enabledRdmaMetric=true \u548c --set grafanaDashboard.install=true \u547d\u4ee4\u884c\u53c2\u6570\u53ef\u4ee5\u5f00\u542f RDMA metrics exporter \u548c Grafana dashboard\uff0c\u66f4\u591a\u53ef\u4ee5\u67e5\u770b RDMA metrics . \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 1m spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m \u914d\u7f6e SR-IOV operator, \u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u521b\u5efa\u51fa VF \u8bbe\u5907 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u67e5\u8be2\u4e3b\u673a\u4e0a\u7f51\u5361\u8bbe\u5907\u7684 PCIE \u4fe1\u606f\u3002\u786e\u8ba4\u5982\u4e0b\u8f93\u51fa\u7684\u8bbe\u5907\u53f7 [15b3:1017] \u51fa\u73b0\u5728 sriov-network-operator \u652f\u6301\u7f51\u5361\u578b\u53f7\u8303\u56f4 $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] .... SRIOV VF \u6570\u91cf\u51b3\u5b9a\u4e86\u4e00\u4e2a\u7f51\u5361\u80fd\u540c\u65f6\u4e3a\u591a\u5c11\u4e2a POD \u63d0\u4f9b\u7f51\u5361\uff0c\u4e0d\u540c\u578b\u53f7\u7684\u7f51\u5361\u7684\u6709\u4e0d\u540c\u7684\u6700\u5927 VF \u6570\u91cf\u4e0a\u9650\uff0cMellanox \u7684 ConnectX \u7f51\u5361\u5e38\u89c1\u578b\u53f7\u7684\u6700\u5927 VF \u4e0a\u9650\u662f 127 \u3002 \u5982\u4e0b\u793a\u4f8b\uff0c\u8bbe\u7f6e\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684 GPU1 \u548c GPU2 \u7684\u7f51\u5361\uff0c\u6bcf\u4e2a\u7f51\u5361\u914d\u7f6e\u51fa 12 \u4e2a VF \u8bbe\u5907\u3002\u8bf7\u53c2\u8003\u5982\u4e0b\uff0c\u4e3a\u4e3b\u673a\u4e0a\u6bcf\u4e2a\u4eb2\u548c GPU \u7684\u7f51\u5361\u914d\u7f6e SriovNetworkNodePolicy\uff0c\u8fd9\u6837\uff0c\u5c06\u6709 8 \u4e2a SRIOV resource \u4ee5\u4f9b\u4f7f\u7528\u3002 # \u5bf9\u4e8e ethernet \u7f51\u7edc\uff0c\u8bbe\u7f6e LINK_TYPE=eth\uff0c \u5bf9\u4e8e Infiniband \u7f51\u7edc\uff0c\u8bbe\u7f6e LINK_TYPE=ib $ LINK_TYPE = eth $ cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu1-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu1sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true --- apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu2-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu2sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true EOF \u521b\u5efa SriovNetworkNodePolicy \u914d\u7f6e\u540e\uff0c\u6bcf\u4e2a\u8282\u70b9\u4e0a\u5c06\u4f1a\u542f\u52a8 sriov-device-plugin \uff0c\u8d1f\u8d23\u4e0a\u62a5 VF \u8bbe\u5907\u8d44\u6e90 $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 2m spiderpool-agent-9sllh 1 /1 Running 0 2m spiderpool-agent-h92bv 1 /1 Running 0 2m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 2m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 2m spiderpool-init 0 /1 Completed 0 2m sriov-device-plugin-x2g6b 1 /1 Running 0 1m sriov-device-plugin-z4gjt 1 /1 Running 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m ....... \u521b\u5efa SriovNetworkNodePolicy \u914d\u7f6e\u540e\uff0cSR-IOV operator \u4f1a\u987a\u5e8f\u5730\u5728\u6bcf\u4e00\u4e2a\u8282\u70b9\u4e0a\u9a71\u9010 POD\uff0c\u914d\u7f6e\u7f51\u5361\u9a71\u52a8\u4e2d\u7684 VF \u8bbe\u7f6e\uff0c\u7136\u540e\u91cd\u542f\u4e3b\u673a\u3002\u56e0\u6b64\uff0c\u4f1a\u89c2\u6d4b\u5230\u96c6\u7fa4\u4e2d\u7684\u8282\u70b9\u4f1a\u987a\u5e8f\u8fdb\u5165 SchedulingDisabled \u72b6\u6001\uff0c\u5e76\u88ab\u91cd\u542f\u3002 $ kubectl get node NAME STATUS ROLES AGE VERSION ai-10-1-16-1 Ready worker 2d15h v1.28.9 ai-10-1-16-2 Ready,SchedulingDisabled worker 2d15h v1.28.9 ....... \u6240\u6709\u8282\u70b9\u5b8c\u6210 VF \u914d\u7f6e\u7684\u8fc7\u7a0b\uff0c\u53ef\u80fd\u9700\u8981\u6570\u5206\u949f\uff0c\u53ef\u4ee5\u89c2\u5bdf sriovnetworknodestates \u4e2d\u7684 status \u662f\u5426\u8fdb\u5165 Succeeded \u72b6\u6001\uff0c\u8868\u793a\u914d\u7f6e\u5b8c\u6210 $ kubectl get sriovnetworknodestates -A NAMESPACE NAME SYNC STATUS DESIRED SYNC STATE CURRENT SYNC STATE AGE spiderpool ai-10-1-16-1 Succeeded Idle Idle 4d6h spiderpool ai-10-1-16-2 Succeeded Idle Idle 4d6h ....... \u5bf9\u4e8e\u914d\u7f6e\u6210\u529f\u7684\u8282\u70b9\uff0c\u53ef\u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u5305\u542b\u4e86\u4e0a\u62a5\u7684 SR-IOV \u8bbe\u5907\u8d44\u6e90 $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/gpu1sriov\" : \"12\" , \"spidernet.io/gpu2sriov\" : \"12\" , ... } } , ... ] \u521b\u5efa CNI \u914d\u7f6e\u548c\u5bf9\u5e94\u7684 ippool \u8d44\u6e90 (1) \u5bf9\u4e8e Infiniband \u7f51\u7edc\uff0c\u8bf7\u4e3a\u6240\u6709\u7684 GPU \u4eb2\u548c\u7684 SR-IOV \u7f51\u5361\u914d\u7f6e IB-SRIOV CNI \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u5bf9\u5e94\u7684 IP \u5730\u5740\u6c60 \u3002 \u5982\u4e0b\u4f8b\u5b50\uff0c\u914d\u7f6e\u4e86 GPU1 \u4eb2\u548c\u7684\u7f51\u5361\u548c IP \u5730\u5740\u6c60 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: resourceName: spidernet.io/gpu1sriov rdmaIsolation: true ippools: ipv4: [\"gpu1-net91\"] EOF (2) \u5bf9\u4e8e Ethernet \u7f51\u7edc\uff0c\u8bf7\u4e3a\u6240\u6709\u7684 GPU \u4eb2\u548c\u7684 SR-IOV \u7f51\u5361\u914d\u7f6e SR-IOV CNI \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u5bf9\u5e94\u7684 IP \u5730\u5740\u6c60 \u3002 \u5982\u4e0b\u4f8b\u5b50\uff0c\u914d\u7f6e\u4e86 GPU1 \u4eb2\u548c\u7684\u7f51\u5361\u548c IP \u5730\u5740\u6c60 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: sriov sriov: resourceName: spidernet.io/gpu1sriov enableRdma: true ippools: ipv4: [\"gpu1-net11\"] EOF","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#_5","text":"\u5728\u6307\u5b9a\u8282\u70b9\u4e0a\u521b\u5efa\u4e00\u7ec4 DaemonSet \u5e94\u7528\uff0c\u6d4b\u8bd5\u6307\u5b9a\u8282\u70b9\u4e0a\u7684 SR-IOV \u8bbe\u5907\u7684\u53ef\u7528\u6027 \u5982\u4e0b\u4f8b\u5b50\uff0c\u901a\u8fc7 annotations v1.multus-cni.io/default-network \u6307\u5b9a\u4f7f\u7528 calico \u7684\u7f3a\u7701\u7f51\u5361\uff0c\u7528\u4e8e\u8fdb\u884c\u63a7\u5236\u9762\u901a\u4fe1\uff0cannotations k8s.v1.cni.cncf.io/networks \u63a5\u5165 8 \u4e2a GPU \u4eb2\u548c\u7f51\u5361\u7684 VF \u7f51\u5361\uff0c\u7528\u4e8e RDMA \u901a\u4fe1\uff0c\u5e76\u914d\u7f6e 8 \u79cd RDMA resources \u8d44\u6e90 \u6ce8\uff1a\u652f\u6301\u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u53c2\u8003 \u57fa\u4e8e Webhook \u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90 $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # sriov interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] # sriov resource resources: limits: spidernet.io/gpu1sriov: 1 spidernet.io/gpu2sriov: 1 spidernet.io/gpu3sriov: 1 spidernet.io/gpu4sriov: 1 spidernet.io/gpu5sriov: 1 spidernet.io/gpu6sriov: 1 spidernet.io/gpu7sriov: 1 spidernet.io/gpu8sriov: 1 #nvidia.com/gpu: 1 EOF $ helm install rdma-tools spiderchart/rdma-tools -f ./values.yaml \u5728\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u4f1a\u5bf9 sriov \u63a5\u53e3\u4e0a\u7684\u7f51\u5173\u8fdb\u884c\u8fde\u901a\u6027\u6d4b\u8bd5\uff0c\u5982\u679c\u5982\u4e0a\u5e94\u7528\u7684\u6240\u6709 POD \u90fd\u542f\u52a8\u6210\u529f\uff0c\u8bf4\u660e\u4e86\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684 VF \u8bbe\u5907\u7684\u8fde\u901a\u6027\u6210\u529f\uff0c\u53ef\u8fdb\u884c\u6b63\u5e38\u7684 RDMA \u901a\u4fe1\u3002 \u67e5\u770b\u5bb9\u5668\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u72b6\u6001 \u53ef\u8fdb\u5165\u4efb\u4e00\u4e00\u4e2a POD \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u786e\u8ba4\u5177\u5907 9 \u4e2a\u7f51\u5361 $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... \u67e5\u770b\u8def\u7531\u914d\u7f6e\uff0cSpiderpool \u4f1a\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u7f51\u5361\u8c03\u8c10\u7b56\u7565\u8def\u7531\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7f51\u5361\u4e0a\u6536\u5230\u7684\u5916\u90e8\u8bf7\u6c42\u90fd\u4f1a\u4ece\u8be5\u7f51\u5361\u4e0a\u8fd4\u56de\u56de\u590d\u6d41\u91cf root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 main \u8def\u7531\u4e2d\uff0c\u786e\u4fdd\u4e86 calico \u7f51\u7edc\u6d41\u91cf\u3001ClusterIP \u6d41\u91cf\u3001\u672c\u5730\u5bbf\u4e3b\u673a\u901a\u4fe1\u7b49\u6d41\u91cf\u90fd\u4f1a\u4ece calico \u7f51\u5361\u8f6c\u53d1 root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link \u786e\u8ba4\u5177\u5907 8 \u4e2a RDMA \u8bbe\u5907 root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1 # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115","title":"\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#infiniband-ufm","text":"\u5bf9\u4e8e\u4f7f\u7528\u4e86 Infiniband \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5982\u679c\u7f51\u7edc\u4e2d\u6709 UFM \u7ba1\u7406\u5e73\u53f0 \uff0c\u53ef\u4f7f\u7528 ib-kubernetes \u63d2\u4ef6\uff0c\u5b83\u4ee5 daemonset \u5f62\u5f0f\u8fd0\u884c\uff0c\u76d1\u63a7\u6240\u6709\u4f7f\u7528 SRIOV \u7f51\u5361\u7684\u5bb9\u5668\uff0c\u628a VF \u8bbe\u5907\u7684 Pkey \u548c GUID \u4e0a\u62a5\u7ed9 UFM \u3002 \u5728 UFM \u4e3b\u673a\u4e0a\u521b\u5efa\u901a\u4fe1\u6240\u9700\u8981\u7684\u8bc1\u4e66\uff1a # replace to right address $ UFM_ADDRESS = 172 .16.10.10 $ openssl req -x509 -newkey rsa:4096 -keyout ufm.key -out ufm.crt -days 365 -subj '/CN=${UFM_ADDRESS}' # Copy the certificate files to the UFM certificate directory: $ cp ufm.key /etc/pki/tls/private/ufmlocalhost.key $ cp ufm.crt /etc/pki/tls/certs/ufmlocalhost.crt # For containerized UFM deployment, restart the container service $ docker restart ufm # For host-based UFM deployment, restart the UFM service $ systemctl restart ufmd \u5728 kubernetes \u96c6\u7fa4\u4e0a\uff0c\u521b\u5efa ib-kubernetes \u6240\u9700\u7684\u901a\u4fe1\u8bc1\u4e66\u3002\u628a UFM \u4e3b\u673a\u4e0a\u751f\u6210\u7684 ufm.crt \u6587\u4ef6\u4f20\u8f93\u81f3 kubernetes \u8282\u70b9\u4e0a\uff0c\u5e76\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u8bc1\u4e66 # replace to right user $ UFM_USERNAME = admin # replace to right password $ UFM_PASSWORD = 12345 # replace to right address $ UFM_ADDRESS = \"172.16.10.10\" $ kubectl create secret generic ib-kubernetes-ufm-secret --namespace = \"kube-system\" \\ --from-literal = UFM_USER = \" ${ UFM_USERNAME } \" \\ --from-literal = UFM_PASSWORD = \" ${ UFM_PASSWORD } \" \\ --from-literal = UFM_ADDRESS = \" ${ UFM_ADDRESS } \" \\ --from-file = UFM_CERTIFICATE = ufm.crt \u5728 kubernetes \u96c6\u7fa4\u4e0a\u5b89\u88c5 ib-kubernetes git clone https://github.com/Mellanox/ib-kubernetes.git && cd ib-kubernetes $ kubectl create -f deployment/ib-kubernetes-configmap.yaml kubectl create -f deployment/ib-kubernetes.yaml \u5728 Infiniband \u7f51\u7edc\u4e0b\uff0c\u521b\u5efa Spiderpool \u7684 SpiderMultusConfig \u65f6\uff0c\u53ef\u914d\u7f6e pkey\uff0c\u4f7f\u7528\u8be5\u914d\u7f6e\u521b\u5efa\u7684 POD \u5c06\u751f\u6548 pkey \u914d\u7f6e\uff0c\u4e14\u88ab ib-kubernetes \u540c\u6b65\u7ed9 UFM $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ib-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: pkey: 1000 ... EOF Note: Each node in an Infiniband Kubernetes deployment may be associated with up to 128 PKeys due to kernel limitation","title":"\uff08\u53ef\u9009\uff09Infiniband \u7f51\u7edc\u4e0b\u5bf9\u63a5 UFM"},{"location":"usage/install/ai/get-started-sriov-zh_CN/#webhook-rdma","text":"\u5728\u4e0a\u8ff0\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 SR-IOV \u6280\u672f\u5728 RoCE \u548c Infiniband \u7f51\u7edc\u73af\u5883\u4e2d\u4e3a\u5bb9\u5668\u63d0\u4f9b RDMA \u901a\u4fe1\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u914d\u7f6e\u591a\u7f51\u5361\u7684 AI \u5e94\u7528\u65f6\uff0c\u8fc7\u7a0b\u4f1a\u53d8\u5f97\u590d\u6742\u3002\u4e3a\u7b80\u5316\u8fd9\u4e2a\u8fc7\u7a0b\uff0cSpiderpool \u901a\u8fc7 annotations( cni.spidernet.io/rdma-resource-inject \u6216 cni.spidernet.io/network-resource-inject ) \u652f\u6301\u5bf9\u4e00\u7ec4\u7f51\u5361\u914d\u7f6e\u8fdb\u884c\u5206\u7c7b\u3002\u7528\u6237\u53ea\u9700\u8981\u4e3a\u5e94\u7528\u6dfb\u52a0\u4e0e\u7f51\u5361\u914d\u7f6e\u76f8\u540c\u7684\u6ce8\u89e3\uff0cSpiderpool \u5c31\u4f1a\u901a\u8fc7 webhook \u81ea\u52a8\u4e3a\u5e94\u7528\u6ce8\u5165\u6240\u6709\u5177\u6709\u76f8\u540c\u6ce8\u89e3\u7684\u5bf9\u5e94\u7f51\u5361\u548c\u7f51\u7edc\u8d44\u6e90\u3002 cni.spidernet.io/rdma-resource-inject \u53ea\u9002\u7528\u4e8e AI \u573a\u666f\uff0c\u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u5361\u53ca RDMA Resources\uff1b cni.spidernet.io/network-resource-inject \u4e0d\u4f46\u53ef\u4ee5\u7528\u4e8e AI \u573a\u666f\uff0c\u4e5f\u652f\u6301 Underlay \u573a\u666f\u3002\u5728\u672a\u6765\u6211\u4eec\u5e0c\u671b\u90fd\u7edf\u4e00\u4f7f\u7528 cni.spidernet.io/network-resource-inject \u652f\u6301\u8fd9\u4e24\u79cd\u573a\u666f\u3002 \u8be5\u529f\u80fd\u4ec5\u652f\u6301 [ macvlan, ipvlan, sriov, ib-sriov, ipoib ] \u8fd9\u51e0\u79cd cniType \u7684\u7f51\u5361\u914d\u7f6e\u3002 \u5f53\u524d Spiderpool \u7684 webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90\uff0c\u9ed8\u8ba4\u662f\u5173\u95ed\u7684\uff0c\u9700\u8981\u624b\u52a8\u5f00\u542f\u3002 ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true \u542f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u66f4\u65b0 configMap: spiderpool-config \u4e2d\u7684 podResourceInject \u5b57\u6bb5\u66f4\u65b0\u914d\u7f6e\u3002 \u901a\u8fc7 podResourceInject.namespacesExclude \u6307\u5b9a\u4e0d\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4 \u901a\u8fc7 podResourceInject.namespacesInclude \u6307\u5b9a\u9700\u8981\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u5982\u679c podResourceInject.namespacesExclude \u548c podResourceInject.namespacesInclude \u90fd\u6ca1\u6709\u6307\u5b9a\uff0c\u5219\u9ed8\u8ba4\u5bf9\u6240\u6709\u547d\u540d\u7a7a\u95f4\u8fdb\u884c RDMA \u7f51\u7edc\u8d44\u6e90\u6ce8\u5165\u3002 \u5f53\u524d\uff0c\u5b8c\u6210\u914d\u7f6e\u53d8\u66f4\u540e\uff0c\u60a8\u9700\u8981\u91cd\u542f spiderpool-controller \u6765\u4f7f\u914d\u7f6e\u751f\u6548\u3002 \u5728\u521b\u5efa AI \u7b97\u529b\u7f51\u7edc\u7684\u6240\u6709 SpiderMultusConfig \u5b9e\u4f8b\u65f6\uff0c\u6dfb\u52a0 key \u4e3a \"cni.spidernet.io/rdma-resource-inject\" \u6216 \"cni.spidernet.io/network-resource-inject\" \u7684 annotation\uff0cvalue \u53ef\u81ea\u5b9a\u4e49\u4efb\u4f55\u503c apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : sriov sriov : resourceName : spidernet.io/gpu1rdma enableRdma : true ippools : ipv4 : [ \"gpu1-net11\" ] \u521b\u5efa AI \u5e94\u7528\u65f6\uff0c\u4e3a\u5e94\u7528\u4e5f\u6dfb\u52a0\u76f8\u540c\u6ce8\u89e3: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network \u6ce8\u610f\uff1a\u4f7f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u65f6\uff0c\u4e0d\u80fd\u4e3a\u5e94\u7528\u6dfb\u52a0\u5176\u4ed6\u7f51\u7edc\u914d\u7f6e\u6ce8\u89e3(\u5982 k8s.v1.cni.cncf.io/networks \u548c ipam.spidernet.io ippools \u7b49)\uff0c\u5426\u5219\u4f1a\u5f71\u54cd\u8d44\u6e90\u81ea\u52a8\u6ce8\u5165\u529f\u80fd\u3002 \u5f53 Pod \u88ab\u521b\u5efa\u540e\uff0c\u53ef\u89c2\u6d4b\u5230 Pod \u88ab\u81ea\u52a8\u6ce8\u5165\u4e86\u7f51\u5361 annotation \u548c RDMA \u8d44\u6e90 ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"\u57fa\u4e8e Webhook \u81ea\u52a8\u6ce8\u5165 RDMA \u7f51\u7edc\u8d44\u6e90"},{"location":"usage/install/ai/get-started-sriov/","text":"AI Cluster With SR-IOV English | \u7b80\u4f53\u4e2d\u6587 Introduction This section explains how to provide RDMA communication capabilities to containers using SR-IOV technology in the context of building an AI cluster. This approach is applicable in both RoCE and Infiniband network scenarios. Spiderpool uses the sriov-network-operator to provide containers with RDMA devices based on SR-IOV interfaces: The Linux RDMA subsystem can operate in two modes: shared mode or exclusive mode: In shared mode, the container can see the RDMA devices of all VF devices on the PF interface, but only the VF assigned to the container will have a GID Index starting from 0. In exclusive mode, the container will only see the RDMA device of the VF assigned to it, without visibility of the PF or other VF RDMA devices. Different CNIs are used for different network scenarios: In Infiniband network scenarios, the IB-SRIOV CNI is used to provide SR-IOV network interfaces to the POD. In RoCE network scenarios, the SR-IOV CNI is used to expose the RDMA network interface on the host to the Pod, thereby exposing RDMA resources. Additionally, the RDMA CNI can be used to achieve RDMA device isolation. Solution This article will introduce how to set up Spiderpool using the following typical AI cluster topology as an example. Figure 1: AI Cluster Topology The network planning for the cluster is as follows: The calico CNI runs on the eth0 network card of the nodes to carry Kubernetes traffic. The AI workload will be assigned a default calico network interface for control plane communication. The nodes use Mellanox ConnectX5 network cards with RDMA functionality to carry the RDMA traffic for AI computation. The network cards are connected to a rail-optimized network. The AI workload will be additionally assigned SR-IOV virtualized interfaces for all RDMA network cards to ensure high-speed network communication for the GPUs. Installation Requirements Refer to the Spiderpool Installation Requirements . Prepare the Helm binary on the host. In Infiniband network scenarios, ensure that the OpenSM subnet manager is functioning properly. Install a Kubernetes cluster with kubelet running on the host\u2019s eth0 network card as shown in Figure 1. Install Calico as the default CNI for the cluster, using the host\u2019s eth0 network card for Calico\u2019s traffic forwarding. If not installed, refer to the official documentation or use the following commands to install: $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip Host Preparation Install the RDMA network card driver. For Mellanox network cards, you can download the NVIDIA OFED official driver and install it on the host using the following installation command: mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all For Mellanox network cards, you can also perform a containerized installation to batch install drivers on all Mellanox network cards in the cluster hosts. Run the following command. Note that this process requires internet access to fetch some installation packages. When all the OFED pods enter the ready state, it indicates that the OFED driver installation on the hosts is complete: $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" If you want the RDMA system to operate in exclusive mode, at least one of the following conditions must be met: (1) The system must be based on the Linux kernel version 5.3.0 or later, with the RDMA module loaded. The RDMA core package provides a method to automatically load the relevant modules at system startup. (2) Mellanox OFED version 4.7 or later is required. In this case, it is not necessary to use a kernel based on version 5.3.0 or later. Verify that the network card supports Infiniband or Ethernet operating modes. In this example environment, the host is equipped with Mellanox ConnectX 5 VPI network cards. Query the RDMA devices to confirm that the network card driver is installed correctly. $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... Verify the network card's operating mode. The following output indicates that the network card is operating in Ethernet mode and can achieve RoCE communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet The following output indicates that the network card is operating in Infiniband mode and can achieve Infiniband communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand If the network card is not operating in the expected mode, enter the following command to verify that the network card supports configuring the LINK_TYPE parameter. If the parameter is not available, please switch to a supported network card model: $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) Enable GPUDirect RDMA The installation of the gpu-operator : a. Enable the Helm installation options: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true . The gpu-operator will install the nvidia-peermem kernel module, enabling GPUDirect RDMA functionality to accelerate data transfer performance between the GPU and RDMA network cards. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 b. Enable the Helm installation option: --set gdrcopy.enabled=true . The gpu-operator will install the gdrcopy kernel module to accelerate data transfer performance between GPU memory and CPU memory. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep gdrdrv gdrdrv 24576 0 Set the RDMA subsystem on the host to exclusive mode under infiniband network, allowing containers to independently use RDMA devices and avoiding sharing with other containers. # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on # Persist the exclusive mode to remain effective after a reboot $ echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf # Switch the current operating mode to exclusive mode. If the setting fails, please reboot the host $ rdma system set netns exclusive # Verify the successful switch to exclusive mode $ rdma system netns exclusive copy-on-fork on Install Spiderpool Use Helm to install Spiderpool and enable the SR-IOV component: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set sriov.install = true If you are a user in China, you can specify the helm option --set global.imageRegistryOverride=ghcr.m.daocloud.io to use a domestic image source. After completion, the installed components are as follows: $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 1m spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m Configure the SR-IOV Operator to Create VF Devices on Each Host Use the following command to query the PCIe information of the network card devices on the host. Confirm that the device ID [15b3:1017] appears in the supported network card models list of the sriov-network-operator . $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] .... The number of SR-IOV VFs (Virtual Functions) determines how many PODs a network card can simultaneously support. Different models of network cards have different maximum VF limits. For example, Mellanox's ConnectX series network cards typically have a maximum VF limit of 127. In the following example, we set up the network cards of GPU1 and GPU2 on each node, configuring 12 VFs for each card. Refer to the following configuration to set up the SriovNetworkNodePolicy for each network card associated with a GPU on the host. This setup will provide 8 SR-IOV resources for use. # For Ethernet networks, set LINK_TYPE=eth. For Infiniband networks, set LINK_TYPE=ib $ LINK_TYPE = eth $ cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu1-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu1sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true --- apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu2-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu2sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true EOF After creating the SriovNetworkNodePolicy configuration, the sriov-device-plugin will be started on each node, responsible for reporting VF device resources. $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 2m spiderpool-agent-9sllh 1 /1 Running 0 2m spiderpool-agent-h92bv 1 /1 Running 0 2m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 2m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 2m spiderpool-init 0 /1 Completed 0 2m sriov-device-plugin-x2g6b 1 /1 Running 0 1m sriov-device-plugin-z4gjt 1 /1 Running 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m ....... Once the SriovNetworkNodePolicy configuration is created, the SR-IOV operator will sequentially evict PODs on each node, configure the VF settings in the network card driver, and then reboot the host. Consequently, you will observe the nodes in the cluster sequentially entering the SchedulingDisabled state and being rebooted. $ kubectl get node NAME STATUS ROLES AGE VERSION ai-10-1-16-1 Ready worker 2d15h v1.28.9 ai-10-1-16-2 Ready,SchedulingDisabled worker 2d15h v1.28.9 ....... It may take several minutes for all nodes to complete the VF configuration process. You can monitor the sriovnetworknodestates status to see if it has entered the Succeeded state, indicating that the configuration is complete. $ kubectl get sriovnetworknodestates -A NAMESPACE NAME SYNC STATUS DESIRED SYNC STATE CURRENT SYNC STATE AGE spiderpool ai-10-1-16-1 Succeeded Idle Idle 4d6h spiderpool ai-10-1-16-2 Succeeded Idle Idle 4d6h ....... For nodes that have successfully configured VFs, you can check the available resources of the node, including the reported SR-IOV device resources. $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/gpu1sriov\" : \"12\" , \"spidernet.io/gpu2sriov\" : \"12\" , ... } } , ... ] Create CNI Configuration and Corresponding IP Pool Resources a. For Infiniband Networks, configure the IB-SRIOV CNI for all GPU-affinitized SR-IOV network cards and create the corresponding IP address pool. The following example configures the network card and IP address pool for GPU1 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: resourceName: spidernet.io/gpu1sriov ippools: ipv4: [\"gpu1-net91\"] EOF b. For Ethernet Networks, configure the SR-IOV CNI for all GPU-affinitized SR-IOV network cards and create the corresponding IP address pool. The following example configures the network card and IP address pool for GPU1 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: sriov sriov: resourceName: spidernet.io/gpu1sriov enableRdma: true ippools: ipv4: [\"gpu1-net11\"] EOF Create a Test Application Create a DaemonSet application on a specified node to test the availability of SR-IOV devices on that node. In the following example, the annotation field v1.multus-cni.io/default-network specifies the use of the default Calico network card for control plane communication. The annotation field k8s.v1.cni.cncf.io/networks connects to the 8 VF network cards affinitized to the GPU for RDMA communication, and configures 8 types of RDMA resources. NOTICE: It support auto inject RDMA resources for application, see Auto inject RDMA Resources $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # sriov interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] # sriov resource resources: limits: spidernet.io/gpu1sriov: 1 spidernet.io/gpu2sriov: 1 spidernet.io/gpu3sriov: 1 spidernet.io/gpu4sriov: 1 spidernet.io/gpu5sriov: 1 spidernet.io/gpu6sriov: 1 spidernet.io/gpu7sriov: 1 spidernet.io/gpu8sriov: 1 #nvidia.com/gpu: 1 EOF $ helm install rdma-tools spiderchart/rdma-tools -f ./values.yaml During the creation of the network namespace for the container, Spiderpool will perform connectivity tests on the gateway of the SR-IOV interface. If all PODs of the above application start successfully, it indicates successful connectivity of the VF devices on each node, allowing normal RDMA communication. Check the network namespace status of the container. You can enter the network namespace of any POD to confirm that it has 9 network cards. $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... Check the routing configuration. Spiderpool will automatically tune policy routes for each network card, ensuring that external requests received on each card are returned through the same card. root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 In the main routing table, ensure that Calico network traffic, ClusterIP traffic, and local host communication traffic are all forwarded through the Calico network card. root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link Confirm that there are 8 RDMA devices. root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... Confirm that RDMA data transmission is functioning properly between Pods across nodes. Open a terminal, enter a Pod, and start the service: # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat Open another terminal, enter another Pod, and access the service: # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115 (Optional) Integrate with UFM on Infiniband Networks For clusters using Infiniband networks, if there is a UFM management platform in the network, you can use the ib-kubernetes plugin . This plugin runs as a daemonset, monitoring all containers using SRIOV network cards and reporting the Pkey and GUID of VF devices to UFM. Create the necessary certificates for communication on the UFM host: # replace to right address $ UFM_ADDRESS = 172 .16.10.10 $ openssl req -x509 -newkey rsa:4096 -keyout ufm.key -out ufm.crt -days 365 -subj '/CN=${UFM_ADDRESS}' # Copy the certificate files to the UFM certificate directory: $ cp ufm.key /etc/pki/tls/private/ufmlocalhost.key $ cp ufm.crt /etc/pki/tls/certs/ufmlocalhost.crt # For containerized UFM deployment, restart the container service $ docker restart ufm # For host-based UFM deployment, restart the UFM service $ systemctl restart ufmd On the Kubernetes cluster, create the communication certificates required by ib-kubernetes. Transfer the ufm.crt file generated on the UFM host to the Kubernetes nodes, and use the following command to create the certificate: # replace to right user $ UFM_USERNAME = admin # replace to right password $ UFM_PASSWORD = 12345 # replace to right address $ UFM_ADDRESS = \"172.16.10.10\" $ kubectl create secret generic ib-kubernetes-ufm-secret --namespace = \"kube-system\" \\ --from-literal = UFM_USER = \" ${ UFM_USERNAME } \" \\ --from-literal = UFM_PASSWORD = \" ${ UFM_PASSWORD } \" \\ --from-literal = UFM_ADDRESS = \" ${ UFM_ADDRESS } \" \\ --from-file = UFM_CERTIFICATE = ufm.crt Install ib-kubernetes on the Kubernetes cluster git clone https://github.com/Mellanox/ib-kubernetes.git && cd ib-kubernetes $ kubectl create -f deployment/ib-kubernetes-configmap.yaml kubectl create -f deployment/ib-kubernetes.yaml On Infiniband networks, when creating Spiderpool's SpiderMultusConfig, you can configure the Pkey. Pods created with this configuration will use the Pkey settings and be synchronized with UFM by ib-kubernetes $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ib-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: pkey: 1000 ... EOF Note: Each node in an Infiniband Kubernetes deployment may be associated with up to 128 PKeys due to kernel limitation Auto Inject RDMA Resources Based on Webhook In the steps above, we demonstrated how to use SR-IOV technology to provide RDMA communication capabilities for containers in RoCE and Infiniband network environments. However, the process can become complex when configuring AI applications with multiple network cards. To simplify this process, Spiderpool supports classifying a set of network card configurations through annotations ( cni.spidernet.io/rdma-resource-inject or cni.spidernet.io/network-resource-inject ). Users only need to add the same annotation to the application, and Spiderpool will automatically inject all corresponding network cards and network resources with the same annotation into the application through a webhook. cni.spidernet.io/rdma-resource-inject annotation is only applicable to AI scenarios, automatically injecting RDMA network cards and RDMA resources. cni.spidernet.io/network-resource-inject annotation can be used not only for AI scenarios but also supports underlay scenarios. In the future, we hope to uniformly use cni.spidernet.io/network-resource-inject to support both of these scenarios. This feature only supports network card configurations with cniType of [macvlan, ipvlan, sriov, ib-sriov, ipoib]. Currently, Spiderpool's webhook for automatically injecting RDMA network resources is disabled by default and needs to be enabled manually. ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true After enabling the webhook automatic injection of network resources, you can update the configuration by updating the podResourceInject field in configMap: spiderpool-config. Specify namespaces that do not require RDMA network resource injection through podResourceInject.namespacesExclude . Specify namespaces that require RDMA network resource injection through podResourceInject.namespacesInclude . If neither podResourceInject.namespacesExclude nor podResourceInject.namespacesInclude is specified, RDMA network resource injection is performed for all namespaces by default. Currently, after completing the configuration change, you need to restart the spiderpool-controller for the configuration to take effect. When creating all SpiderMultusConfig instances for AI computing networks, add an annotation with the key \"cni.spidernet.io/rdma-resource-inject\" (or \"cni.spidernet.io/network-resource-inject\") and a customizable value. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : sriov sriov : resourceName : spidernet.io/gpu1rdma enableRdma : true ippools : ipv4 : [ \"gpu1-net11\" ] When creating an AI application, add the same annotation to the application: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network Note: When using the webhook automatic injection of network resources feature, do not add other network configuration annotations (such as k8s.v1.cni.cncf.io/networks and ipam.spidernet.io/ippools ) to the application, as it will affect the automatic injection of resources. Once the Pod is created, you can observe that the Pod has been automatically injected with network card annotations and RDMA resources. ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"AI Cluster with Sriov"},{"location":"usage/install/ai/get-started-sriov/#ai-cluster-with-sr-iov","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"AI Cluster With SR-IOV"},{"location":"usage/install/ai/get-started-sriov/#introduction","text":"This section explains how to provide RDMA communication capabilities to containers using SR-IOV technology in the context of building an AI cluster. This approach is applicable in both RoCE and Infiniband network scenarios. Spiderpool uses the sriov-network-operator to provide containers with RDMA devices based on SR-IOV interfaces: The Linux RDMA subsystem can operate in two modes: shared mode or exclusive mode: In shared mode, the container can see the RDMA devices of all VF devices on the PF interface, but only the VF assigned to the container will have a GID Index starting from 0. In exclusive mode, the container will only see the RDMA device of the VF assigned to it, without visibility of the PF or other VF RDMA devices. Different CNIs are used for different network scenarios: In Infiniband network scenarios, the IB-SRIOV CNI is used to provide SR-IOV network interfaces to the POD. In RoCE network scenarios, the SR-IOV CNI is used to expose the RDMA network interface on the host to the Pod, thereby exposing RDMA resources. Additionally, the RDMA CNI can be used to achieve RDMA device isolation.","title":"Introduction"},{"location":"usage/install/ai/get-started-sriov/#solution","text":"This article will introduce how to set up Spiderpool using the following typical AI cluster topology as an example. Figure 1: AI Cluster Topology The network planning for the cluster is as follows: The calico CNI runs on the eth0 network card of the nodes to carry Kubernetes traffic. The AI workload will be assigned a default calico network interface for control plane communication. The nodes use Mellanox ConnectX5 network cards with RDMA functionality to carry the RDMA traffic for AI computation. The network cards are connected to a rail-optimized network. The AI workload will be additionally assigned SR-IOV virtualized interfaces for all RDMA network cards to ensure high-speed network communication for the GPUs.","title":"Solution"},{"location":"usage/install/ai/get-started-sriov/#installation-requirements","text":"Refer to the Spiderpool Installation Requirements . Prepare the Helm binary on the host. In Infiniband network scenarios, ensure that the OpenSM subnet manager is functioning properly. Install a Kubernetes cluster with kubelet running on the host\u2019s eth0 network card as shown in Figure 1. Install Calico as the default CNI for the cluster, using the host\u2019s eth0 network card for Calico\u2019s traffic forwarding. If not installed, refer to the official documentation or use the following commands to install: $ kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml $ kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP_AUTODETECTION_METHOD = kubernetes-internal-ip # set calico to work on host eth0 $ kubectl set env daemonset -n kube-system calico-node IP6_AUTODETECTION_METHOD = kubernetes-internal-ip","title":"Installation Requirements"},{"location":"usage/install/ai/get-started-sriov/#host-preparation","text":"Install the RDMA network card driver. For Mellanox network cards, you can download the NVIDIA OFED official driver and install it on the host using the following installation command: mount /root/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt /mnt/mlnxofedinstall --all For Mellanox network cards, you can also perform a containerized installation to batch install drivers on all Mellanox network cards in the cluster hosts. Run the following command. Note that this process requires internet access to fetch some installation packages. When all the OFED pods enter the ready state, it indicates that the OFED driver installation on the hosts is complete: $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo ofed # pelase replace the following values with your actual environment # for china user, it could set `--set image.registry=nvcr.m.daocloud.io` to use a domestic registry $ helm install ofed-driver spiderchart/ofed-driver -n kube-system \\ --set image.OSName = \"ubuntu\" \\ --set image.OSVer = \"22.04\" \\ --set image.Arch = \"amd64\" If you want the RDMA system to operate in exclusive mode, at least one of the following conditions must be met: (1) The system must be based on the Linux kernel version 5.3.0 or later, with the RDMA module loaded. The RDMA core package provides a method to automatically load the relevant modules at system startup. (2) Mellanox OFED version 4.7 or later is required. In this case, it is not necessary to use a kernel based on version 5.3.0 or later. Verify that the network card supports Infiniband or Ethernet operating modes. In this example environment, the host is equipped with Mellanox ConnectX 5 VPI network cards. Query the RDMA devices to confirm that the network card driver is installed correctly. $ rdma link link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 ....... Verify the network card's operating mode. The following output indicates that the network card is operating in Ethernet mode and can achieve RoCE communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: Ethernet The following output indicates that the network card is operating in Infiniband mode and can achieve Infiniband communication: $ ibstat mlx5_0 | grep \"Link layer\" Link layer: InfiniBand If the network card is not operating in the expected mode, enter the following command to verify that the network card supports configuring the LINK_TYPE parameter. If the parameter is not available, please switch to a supported network card model: $ mst start # check the card's PCIE $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] ....... # check whether the network card supports parameters LINK_TYPE $ mlxconfig -d 86 :00.0 q | grep LINK_TYPE LINK_TYPE_P1 IB ( 1 ) Enable GPUDirect RDMA The installation of the gpu-operator : a. Enable the Helm installation options: --set driver.rdma.enabled=true --set driver.rdma.useHostMofed=true . The gpu-operator will install the nvidia-peermem kernel module, enabling GPUDirect RDMA functionality to accelerate data transfer performance between the GPU and RDMA network cards. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep nvidia_peermem nvidia_peermem 16384 0 b. Enable the Helm installation option: --set gdrcopy.enabled=true . The gpu-operator will install the gdrcopy kernel module to accelerate data transfer performance between GPU memory and CPU memory. Enter the following command on the host to confirm the successful installation of the kernel module: $ lsmod | grep gdrdrv gdrdrv 24576 0 Set the RDMA subsystem on the host to exclusive mode under infiniband network, allowing containers to independently use RDMA devices and avoiding sharing with other containers. # Check the current operating mode (the Linux RDMA subsystem operates in shared mode by default): $ rdma system netns shared copy-on-fork on # Persist the exclusive mode to remain effective after a reboot $ echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf # Switch the current operating mode to exclusive mode. If the setting fails, please reboot the host $ rdma system set netns exclusive # Verify the successful switch to exclusive mode $ rdma system netns exclusive copy-on-fork on","title":"Host Preparation"},{"location":"usage/install/ai/get-started-sriov/#install-spiderpool","text":"Use Helm to install Spiderpool and enable the SR-IOV component: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool kubectl create namespace spiderpool helm install spiderpool spiderpool/spiderpool -n spiderpool --set sriov.install = true If you are a user in China, you can specify the helm option --set global.imageRegistryOverride=ghcr.m.daocloud.io to use a domestic image source. After completion, the installed components are as follows: $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 1m spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m Configure the SR-IOV Operator to Create VF Devices on Each Host Use the following command to query the PCIe information of the network card devices on the host. Confirm that the device ID [15b3:1017] appears in the supported network card models list of the sriov-network-operator . $ lspci -nn | grep Mellanox 86 :00.0 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] 86 :00.1 Infiniband controller [ 0207 ] : Mellanox Technologies MT27800 Family [ ConnectX-5 ] [ 15b3:1017 ] .... The number of SR-IOV VFs (Virtual Functions) determines how many PODs a network card can simultaneously support. Different models of network cards have different maximum VF limits. For example, Mellanox's ConnectX series network cards typically have a maximum VF limit of 127. In the following example, we set up the network cards of GPU1 and GPU2 on each node, configuring 12 VFs for each card. Refer to the following configuration to set up the SriovNetworkNodePolicy for each network card associated with a GPU on the host. This setup will provide 8 SR-IOV resources for use. # For Ethernet networks, set LINK_TYPE=eth. For Infiniband networks, set LINK_TYPE=ib $ LINK_TYPE = eth $ cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu1-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu1sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true --- apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: gpu2-nic-policy namespace: spiderpool spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: gpu2sriov priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" vendor: \"15b3\" rootDevices: - 0000:86:00.0 linkType: ${LINK_TYPE} deviceType: netdevice isRdma: true EOF After creating the SriovNetworkNodePolicy configuration, the sriov-device-plugin will be started on each node, responsible for reporting VF device resources. $ kubectl get pod -n spiderpool operator-webhook-sgkxp 1 /1 Running 0 2m spiderpool-agent-9sllh 1 /1 Running 0 2m spiderpool-agent-h92bv 1 /1 Running 0 2m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 2m spiderpool-sriov-operator-65b59cd75d-89wtg 1 /1 Running 0 2m spiderpool-init 0 /1 Completed 0 2m sriov-device-plugin-x2g6b 1 /1 Running 0 1m sriov-device-plugin-z4gjt 1 /1 Running 0 1m sriov-network-config-daemon-8h576 1 /1 Running 0 1m sriov-network-config-daemon-n629x 1 /1 Running 0 1m ....... Once the SriovNetworkNodePolicy configuration is created, the SR-IOV operator will sequentially evict PODs on each node, configure the VF settings in the network card driver, and then reboot the host. Consequently, you will observe the nodes in the cluster sequentially entering the SchedulingDisabled state and being rebooted. $ kubectl get node NAME STATUS ROLES AGE VERSION ai-10-1-16-1 Ready worker 2d15h v1.28.9 ai-10-1-16-2 Ready,SchedulingDisabled worker 2d15h v1.28.9 ....... It may take several minutes for all nodes to complete the VF configuration process. You can monitor the sriovnetworknodestates status to see if it has entered the Succeeded state, indicating that the configuration is complete. $ kubectl get sriovnetworknodestates -A NAMESPACE NAME SYNC STATUS DESIRED SYNC STATE CURRENT SYNC STATE AGE spiderpool ai-10-1-16-1 Succeeded Idle Idle 4d6h spiderpool ai-10-1-16-2 Succeeded Idle Idle 4d6h ....... For nodes that have successfully configured VFs, you can check the available resources of the node, including the reported SR-IOV device resources. $ kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\" : \"ai-10-1-16-1\" , \"allocable\" : { \"cpu\" : \"40\" , \"pods\" : \"110\" , \"spidernet.io/gpu1sriov\" : \"12\" , \"spidernet.io/gpu2sriov\" : \"12\" , ... } } , ... ] Create CNI Configuration and Corresponding IP Pool Resources a. For Infiniband Networks, configure the IB-SRIOV CNI for all GPU-affinitized SR-IOV network cards and create the corresponding IP address pool. The following example configures the network card and IP address pool for GPU1 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: resourceName: spidernet.io/gpu1sriov ippools: ipv4: [\"gpu1-net91\"] EOF b. For Ethernet Networks, configure the SR-IOV CNI for all GPU-affinitized SR-IOV network cards and create the corresponding IP address pool. The following example configures the network card and IP address pool for GPU1 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: gpu1-net11 spec: gateway: 172.16.11.254 subnet: 172.16.11.0/16 ips: - 172.16.11.1-172.16.11.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: gpu1-sriov namespace: spiderpool spec: cniType: sriov sriov: resourceName: spidernet.io/gpu1sriov enableRdma: true ippools: ipv4: [\"gpu1-net11\"] EOF","title":"Install Spiderpool"},{"location":"usage/install/ai/get-started-sriov/#create-a-test-application","text":"Create a DaemonSet application on a specified node to test the availability of SR-IOV devices on that node. In the following example, the annotation field v1.multus-cni.io/default-network specifies the use of the default Calico network card for control plane communication. The annotation field k8s.v1.cni.cncf.io/networks connects to the 8 VF network cards affinitized to the GPU for RDMA communication, and configures 8 types of RDMA resources. NOTICE: It support auto inject RDMA resources for application, see Auto inject RDMA Resources $ helm repo add spiderchart https://spidernet-io.github.io/charts $ helm repo update $ helm search repo rdma-tools # run daemonset on worker1 and worker2 $ cat <<EOF > values.yaml # for china user , it could add these to use a domestic registry #image: # registry: ghcr.m.daocloud.io # just run daemonset in nodes 'worker1' and 'worker2' affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker1 - worker2 # sriov interfaces extraAnnotations: k8s.v1.cni.cncf.io/networks: |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] # sriov resource resources: limits: spidernet.io/gpu1sriov: 1 spidernet.io/gpu2sriov: 1 spidernet.io/gpu3sriov: 1 spidernet.io/gpu4sriov: 1 spidernet.io/gpu5sriov: 1 spidernet.io/gpu6sriov: 1 spidernet.io/gpu7sriov: 1 spidernet.io/gpu8sriov: 1 #nvidia.com/gpu: 1 EOF $ helm install rdma-tools spiderchart/rdma-tools -f ./values.yaml During the creation of the network namespace for the container, Spiderpool will perform connectivity tests on the gateway of the SR-IOV interface. If all PODs of the above application start successfully, it indicates successful connectivity of the VF devices on each node, allowing normal RDMA communication. Check the network namespace status of the container. You can enter the network namespace of any POD to confirm that it has 9 network cards. $ kubectl exec -it rdma-tools-4v8t8 bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. root@rdma-tools-4v8t8:/# ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 3 : eth0@if356: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000 link/ether ca:39:52:fc:61:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.119.164/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c839:52ff:fefc:61cd/64 scope link valid_lft forever preferred_lft forever 269 : net1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 3a:97:49:35:79:95 brd ff:ff:ff:ff:ff:ff inet 172 .16.11.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::3897:49ff:fe35:7995/64 scope link valid_lft forever preferred_lft forever 239 : net2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 1e:b6:13:0e:2a:d5 brd ff:ff:ff:ff:ff:ff inet 172 .16.12.10/24 brd 10 .1.19.255 scope global net1 valid_lft forever preferred_lft forever inet6 fe80::1cb6:13ff:fe0e:2ad5/64 scope link valid_lft forever preferred_lft forever ..... Check the routing configuration. Spiderpool will automatically tune policy routes for each network card, ensuring that external requests received on each card are returned through the same card. root@rdma-tools-4v8t8:/# ip rule 0 : from all lookup local 32762 : from 172 .16.11.10 lookup 107 32763 : from 172 .16.12.10 lookup 106 32764 : from 172 .16.13.10 lookup 105 32765 : from 172 .16.14.10 lookup 104 32765 : from 172 .16.15.10 lookup 103 32765 : from 172 .16.16.10 lookup 102 32765 : from 172 .16.17.10 lookup 101 32765 : from 172 .16.18.10 lookup 100 32766 : from all lookup main 32767 : from all lookup default root@rdma-tools-4v8t8:/# ip route show table 100 default via 172 .16.11.254 dev net1 In the main routing table, ensure that Calico network traffic, ClusterIP traffic, and local host communication traffic are all forwarded through the Calico network card. root@rdma-tools-4v8t8:/# ip r show table main default via 169 .254.1.1 dev eth0 172 .16.11.0/24 dev net1 proto kernel scope link src 172 .16.11.10 172 .16.12.0/24 dev net2 proto kernel scope link src 172 .16.12.10 172 .16.13.0/24 dev net3 proto kernel scope link src 172 .16.13.10 172 .16.14.0/24 dev net4 proto kernel scope link src 172 .16.14.10 172 .16.15.0/24 dev net5 proto kernel scope link src 172 .16.15.10 172 .16.16.0/24 dev net6 proto kernel scope link src 172 .16.16.10 172 .16.17.0/24 dev net7 proto kernel scope link src 172 .16.17.10 172 .16.18.0/24 dev net8 proto kernel scope link src 172 .16.18.10 10 .233.0.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.64.0/18 via 10 .1.20.4 dev eth0 src 10 .233.119.164 10 .233.119.128 dev eth0 scope link src 10 .233.119.164 169 .254.0.0/16 via 10 .1.20.4 dev eth0 src 10 .233.119.164 169 .254.1.1 dev eth0 scope link Confirm that there are 8 RDMA devices. root@rdma-tools-4v8t8:/# rdma link link mlx5_27/1 state ACTIVE physical_state LINK_UP netdev net2 link mlx5_54/1 state ACTIVE physical_state LINK_UP netdev net1 link mlx5_67/1 state ACTIVE physical_state LINK_UP netdev net4 link mlx5_98/1 state ACTIVE physical_state LINK_UP netdev net3 ..... Confirm that RDMA data transmission is functioning properly between Pods across nodes. Open a terminal, enter a Pod, and start the service: # see 8 RDMA devices assigned to the Pod $ rdma link # Start an RDMA service $ ib_read_lat Open another terminal, enter another Pod, and access the service: # You should be able to see all RDMA network cards on the host $ rdma link # Successfully access the RDMA service of the other Pod $ ib_read_lat 172 .91.0.115","title":"Create a Test Application"},{"location":"usage/install/ai/get-started-sriov/#optional-integrate-with-ufm-on-infiniband-networks","text":"For clusters using Infiniband networks, if there is a UFM management platform in the network, you can use the ib-kubernetes plugin . This plugin runs as a daemonset, monitoring all containers using SRIOV network cards and reporting the Pkey and GUID of VF devices to UFM. Create the necessary certificates for communication on the UFM host: # replace to right address $ UFM_ADDRESS = 172 .16.10.10 $ openssl req -x509 -newkey rsa:4096 -keyout ufm.key -out ufm.crt -days 365 -subj '/CN=${UFM_ADDRESS}' # Copy the certificate files to the UFM certificate directory: $ cp ufm.key /etc/pki/tls/private/ufmlocalhost.key $ cp ufm.crt /etc/pki/tls/certs/ufmlocalhost.crt # For containerized UFM deployment, restart the container service $ docker restart ufm # For host-based UFM deployment, restart the UFM service $ systemctl restart ufmd On the Kubernetes cluster, create the communication certificates required by ib-kubernetes. Transfer the ufm.crt file generated on the UFM host to the Kubernetes nodes, and use the following command to create the certificate: # replace to right user $ UFM_USERNAME = admin # replace to right password $ UFM_PASSWORD = 12345 # replace to right address $ UFM_ADDRESS = \"172.16.10.10\" $ kubectl create secret generic ib-kubernetes-ufm-secret --namespace = \"kube-system\" \\ --from-literal = UFM_USER = \" ${ UFM_USERNAME } \" \\ --from-literal = UFM_PASSWORD = \" ${ UFM_PASSWORD } \" \\ --from-literal = UFM_ADDRESS = \" ${ UFM_ADDRESS } \" \\ --from-file = UFM_CERTIFICATE = ufm.crt Install ib-kubernetes on the Kubernetes cluster git clone https://github.com/Mellanox/ib-kubernetes.git && cd ib-kubernetes $ kubectl create -f deployment/ib-kubernetes-configmap.yaml kubectl create -f deployment/ib-kubernetes.yaml On Infiniband networks, when creating Spiderpool's SpiderMultusConfig, you can configure the Pkey. Pods created with this configuration will use the Pkey settings and be synchronized with UFM by ib-kubernetes $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ib-sriov namespace: spiderpool spec: cniType: ib-sriov ibsriov: pkey: 1000 ... EOF Note: Each node in an Infiniband Kubernetes deployment may be associated with up to 128 PKeys due to kernel limitation","title":"(Optional) Integrate with UFM on Infiniband Networks"},{"location":"usage/install/ai/get-started-sriov/#auto-inject-rdma-resources-based-on-webhook","text":"In the steps above, we demonstrated how to use SR-IOV technology to provide RDMA communication capabilities for containers in RoCE and Infiniband network environments. However, the process can become complex when configuring AI applications with multiple network cards. To simplify this process, Spiderpool supports classifying a set of network card configurations through annotations ( cni.spidernet.io/rdma-resource-inject or cni.spidernet.io/network-resource-inject ). Users only need to add the same annotation to the application, and Spiderpool will automatically inject all corresponding network cards and network resources with the same annotation into the application through a webhook. cni.spidernet.io/rdma-resource-inject annotation is only applicable to AI scenarios, automatically injecting RDMA network cards and RDMA resources. cni.spidernet.io/network-resource-inject annotation can be used not only for AI scenarios but also supports underlay scenarios. In the future, we hope to uniformly use cni.spidernet.io/network-resource-inject to support both of these scenarios. This feature only supports network card configurations with cniType of [macvlan, ipvlan, sriov, ib-sriov, ipoib]. Currently, Spiderpool's webhook for automatically injecting RDMA network resources is disabled by default and needs to be enabled manually. ~# helm upgrade --install spiderpool spiderpool/spiderpool --namespace spiderpool --create-namespace --reuse-values --set spiderpoolController.podResourceInject.enabled = true After enabling the webhook automatic injection of network resources, you can update the configuration by updating the podResourceInject field in configMap: spiderpool-config. Specify namespaces that do not require RDMA network resource injection through podResourceInject.namespacesExclude . Specify namespaces that require RDMA network resource injection through podResourceInject.namespacesInclude . If neither podResourceInject.namespacesExclude nor podResourceInject.namespacesInclude is specified, RDMA network resource injection is performed for all namespaces by default. Currently, after completing the configuration change, you need to restart the spiderpool-controller for the configuration to take effect. When creating all SpiderMultusConfig instances for AI computing networks, add an annotation with the key \"cni.spidernet.io/rdma-resource-inject\" (or \"cni.spidernet.io/network-resource-inject\") and a customizable value. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : gpu1-net11 spec : gateway : 172.16.11.254 subnet : 172.16.11.0/16 ips : - 172.16.11.1-172.16.11.200 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : gpu1-sriov namespace : spiderpool annotations : cni.spidernet.io/rdma-resource-inject : rdma-network spec : cniType : sriov sriov : resourceName : spidernet.io/gpu1rdma enableRdma : true ippools : ipv4 : [ \"gpu1-net11\" ] When creating an AI application, add the same annotation to the application: ... spec : template : metadata : annotations : cni.spidernet.io/rdma-resource-inject : rdma-network Note: When using the webhook automatic injection of network resources feature, do not add other network configuration annotations (such as k8s.v1.cni.cncf.io/networks and ipam.spidernet.io/ippools ) to the application, as it will affect the automatic injection of resources. Once the Pod is created, you can observe that the Pod has been automatically injected with network card annotations and RDMA resources. ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"gpu1-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu2-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu3-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu4-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu5-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu6-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu7-sriov\",\"namespace\":\"spiderpool\"}, {\"name\":\"gpu8-sriov\",\"namespace\":\"spiderpool\"}] .... resources : limits : spidernet.io/gpu1rdma : 1 spidernet.io/gpu2rdma : 1 spidernet.io/gpu3rdma : 1 spidernet.io/gpu4rdma : 1 spidernet.io/gpu5rdma : 1 spidernet.io/gpu6rdma : 1 spidernet.io/gpu7rdma : 1 spidernet.io/gpu8rdma : 1","title":"Auto Inject RDMA Resources Based on Webhook"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/","text":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u7684\u8282\u70b9\u62d3\u6251\u529f\u80fd\u53ef\u4ee5\u5c06 IPPool \u4e0e\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u4e2a\u7f51\u5361\u7684\u53ef\u7528 IP \u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u8fd8\u5177\u5907\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\u3002 Spiderpool \u80fd\u57fa\u4e8e IPVlan Underlay CNI \u5728\u963f\u91cc\u4e91\u73af\u5883\u4e0a\u8fd0\u884c\uff0c\u5e76\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001IPvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u5904\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d IPVlan CNI \u8fdb\u884c\u89e3\u51b3\u3002IPVlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 IPvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002 \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u963f\u91cc\u4e91\u73af\u5883 \u51c6\u5907\u4e00\u5957\u963f\u91cc\u4e91\u73af\u5883\uff0c\u7ed9\u865a\u62df\u673a\u5206\u914d 2 \u4e2a\u7f51\u5361\uff0c\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe\uff1a \u5b9e\u4f8b\uff08\u865a\u62df\u673a\uff09\u662f\u80fd\u591f\u4e3a\u60a8\u7684\u4e1a\u52a1\u63d0\u4f9b\u8ba1\u7b97\u670d\u52a1\u7684\u6700\u5c0f\u5355\u4f4d\uff0c\u4e0d\u540c\u7684\u5b9e\u4f8b\u89c4\u683c\u53ef\u521b\u5efa\u7f51\u5361\u6570\u548c\u6bcf\u5f20\u7f51\u5361\u53ef\u5206\u914d\u7684\u8f85\u52a9 IP \u6570\u5b58\u5728\u5dee\u5f02\uff0c\u6839\u636e\u4e1a\u52a1\u573a\u666f\u548c\u4f7f\u7528\u573a\u666f\uff0c\u53c2\u8003\u963f\u91cc\u4e91 \u5b9e\u4f8b\u89c4\u683c\u65cf \u9009\u62e9\u5bf9\u5e94\u89c4\u683c\u8fdb\u884c\u521b\u5efa\u5b9e\u4f8b\u3002 \u5982\u679c\u6709 IPv6 \u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u53c2\u8003\u963f\u91cc\u4e91 \u914d\u7f6e IPv6 \u5730\u5740 \u3002 \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 IPVlan, \u4f60\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 IPVlan\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a IPvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u4e0e eth1 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m \u521b\u5efa IPPools Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u8be5\u5b57\u6bb5\u7684\u503c\u5fc5\u987b\u8bbe\u7f6e\u3002\u7531\u4e8e\u963f\u91cc\u4e91\u9650\u5236\u4e86\u8282\u70b9\u53ef\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u60a8\u53ef\u4ee5\u4ece\u963f\u91cc\u4e91\u7684\u5f39\u6027\u7f51\u5361\u754c\u9762\u83b7\u53d6\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( eth0\u3001eth1 )\u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u7ec4 DaemonSet \u5e94\u7528\u548c 1 \u4e2a type \u4e3a ClusterIP \u7684 service \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u5206\u522b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\uff0c\u5e94\u7528\u7684 IP \u5747\u5728\u671f\u671b\u7684 IP \u6c60\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true \u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027 \u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes \u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027 \u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee \u963f\u91cc\u4e91\u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u73af\u5883\u4e0b\u6784\u5efa\u4e00\u4e2a\u516c\u7f51\u6216\u79c1\u7f51\u6d41\u91cf\u7684\u51fa\u5165\u53e3\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u6d4b\u8bd5\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 \u5982\u679c\u5e0c\u671b\u901a\u8fc7 IPv6 \u5730\u5740\u5b9e\u73b0\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\uff0c\u4f60\u9700\u8981\u901a\u8fc7 IPv6 \u7f51\u5173\uff0c\u4e3a Pod \u6240\u5206\u914d\u5230\u7684 IPv6 \u5730\u5740 \u5f00\u901a\u516c\u7f51\u5e26\u5bbd \uff0c\u5c06\u79c1\u7f51 IPv6 \u8f6c\u6362\u4e3a\u516c\u7f51 IPv6 \u5730\u5740\u3002\u914d\u7f6e\u5982\u4e0b\u3002 \u6d4b\u8bd5 IPv6 \u8bbf\u95ee\u5982\u4e0b\uff1a ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms \u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee \u90e8\u7f72 Cloud Controller Manager CCM\uff08Cloud Controller Manager\uff09\u662f\u963f\u91cc\u4e91\u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e\u963f\u91cc\u4e91\u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e CCM \u6587\u6863 \u5b8c\u6210 CCM \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u5206\u522b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4ece\u800c\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5404\u81ea\u7684 providerID \u3002 http://100.100.100.200/latest/meta-data \u662f\u963f\u91cc\u4e91 CLI \u63d0\u4f9b\u83b7\u53d6\u5b9e\u4f8b\u5143\u6570\u636e\u7684 API \u5165\u53e3\uff0c\u5728\u4e0b\u5217\u793a\u4f8b\u4e2d\u65e0\u9700\u4fee\u6539\u5b83\u3002\u66f4\u591a\u7528\u6cd5\u53ef\u53c2\u8003 \u5b9e\u4f8b\u5143\u6570\u636e ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* \u5728\u96c6\u7fa4\u7684 master \u8282\u70b9\u901a\u8fc7 kubectl patch \u547d\u4ee4\u4e3a\u96c6\u7fa4\u4e2d\u7684 \u6bcf\u4e2a\u8282\u70b9 \u8865\u5145\u5404\u81ea\u7684 providerID \uff0c\u8be5\u6b65\u9aa4\u5fc5\u987b\u88ab\u6267\u884c\uff0c\u5426\u5219\u5bf9\u5e94\u8282\u70b9\u7684 CCM Pod \u5c06\u65e0\u6cd5\u8fd0\u884c\u3002 ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # \u5c06 <NODE_NAME> \u4e0e <provider_id> \u66ff\u6362\u4e3a\u5bf9\u5e94\u503c\u3002 \u521b\u5efa\u963f\u91cc\u4e91\u7684 RAM \u7528\u6237\uff0c\u5e76\u6388\u6743\u3002 RAM \u7528\u6237\u662f RAM \u4e2d\u7684\u4e00\u79cd\u5b9e\u4f53\u8eab\u4efd\uff0c\u4ee3\u8868\u9700\u8981\u8bbf\u95ee\u963f\u91cc\u4e91\u7684\u4eba\u5458\u6216\u5e94\u7528\u7a0b\u5e8f\u3002\u901a\u8fc7\u53c2\u9605 RAM \u8bbf\u95ee\u63a7\u5236 \u521b\u5efa RAM \u7528\u6237\uff0c\u5e76\u6388\u4e8e\u9700\u8981\u8bbf\u95ee\u8d44\u6e90\u7684\u6743\u9650\u3002 \u4e3a\u786e\u4fdd\u540e\u7eed\u6b65\u9aa4\u4e2d\u6240\u4f7f\u7528\u7684 RAM \u7528\u6237\u5177\u5907\u8db3\u591f\u7684\u6743\u9650\uff0c\u8bf7\u4e0e\u672c\u6587\u4fdd\u6301\u4e00\u81f4\uff0c\u7ed9\u4e88 RAM \u7528\u6237 AdministratorAccess \u548c AliyunSLBFullAccess \u6743\u9650\u3002 \u83b7\u53d6 RAM \u7528\u6237\u7684 AccessKey & AccessKeySecret \u767b\u5f55 RAM \u7528\u6237\uff0c\u8bbf\u95ee \u7528\u6237\u4e2d\u5fc3 \uff0c\u83b7\u53d6\u5bf9\u5e94 RAM \u7528\u7684 AccessKey & AccessKeySecret\u3002 \u521b\u5efa CCM \u7684 Cloud ConfigMap\u3002 \u5c06\u6b65\u9aa4 3 \u83b7\u53d6\u7684 AccessKey & AccessKeySecret\uff0c\u53c2\u8003\u4e0b\u5217\u65b9\u5f0f\u5199\u5165\u73af\u5883\u53d8\u91cf\u3002 ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u5b8c\u6210\u521b\u5efa cloud-config\u3002 accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF \u83b7\u53d6 Yaml \uff0c\u5e76\u901a\u8fc7 kubectl apply -f cloud-controller-manager.yaml \u65b9\u5f0f\u5b89\u88c5 CCM\uff0c\u672c\u6587\u4e2d\u5b89\u88c5\u7684\u7248\u672c\u4e3a v2.5.0 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u83b7\u53d6 cloud-controller-manager.yaml\uff0c\u5e76\u66ff\u6362\u5176\u4e2d <<cluster_cidr>> \u4e3a\u60a8\u771f\u5b9e\u96c6\u7fa4\u7684 cluster CIDR \uff1b\u60a8\u53ef\u4ee5\u901a\u8fc7 kubectl cluster-info dump | grep -m1 cluster-cidr \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u7684 cluster CIDR \u3002 ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml \u68c0\u67e5 CCM \u5b89\u88c5\u5b8c\u6210\u3002 ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s \u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3 \u5982\u4e0b\u7684 Yaml \u5c06\u521b\u5efa spec.type \u4e3a LoadBalancer \u7684 2 \u7ec4 service\uff0c\u4e00\u7ec4\u4e3a tcp \uff08\u56db\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u4e00\u7ec4\u4e3a http \uff08\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\u3002 service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port \uff1aCCM \u63d0\u4f9b\u7684\u521b\u5efa\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\u6ce8\u89e3\u3002\u53ef\u4ee5\u901a\u8fc7\u5b83\u81ea\u5b9a\u4e49\u66b4\u9732\u7aef\u53e3\u3002\u66f4\u591a\u7528\u6cd5\u53c2\u8003 CCM \u4f7f\u7528\u6587\u6863 \u3002 .spec.externalTrafficPolicy \uff1a\u8868\u793a\u6b64 Service \u662f\u5426\u5e0c\u671b\u5c06\u5916\u90e8\u6d41\u91cf\u8def\u7531\u5230\u8282\u70b9\u672c\u5730\u6216\u96c6\u7fa4\u8303\u56f4\u7684\u7aef\u70b9\u3002\u5b83\u6709\u4e24\u4e2a\u53ef\u7528\u9009\u9879\uff1aCluster\uff08\u9ed8\u8ba4\uff09\u548c Local\u3002\u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP\uff0c\u4f46\u516c\u6709\u4e91\u81ea\u5efa\u96c6\u7fa4\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5e73\u53f0\u7684 Loadbalancer \u7ec4\u4ef6\u8fdb\u884c nodePort \u8f6c\u53d1\u65f6\uff0c\u4f1a\u51fa\u73b0\u8bbf\u95ee\u4e0d\u901a\u3002\u9488\u5bf9\u8be5\u95ee\u9898 Spiderpool \u63d0\u4f9b\u4e86 coordinator \u63d2\u4ef6\uff0c\u8be5\u63d2\u4ef6\u901a\u8fc7 iptables \u5728\u6570\u636e\u5305\u4e2d\u6253\u6807\u8bb0\uff0c\u786e\u8ba4\u4ece veth0 \u8fdb\u5165\u7684\u6570\u636e\u7684\u56de\u590d\u5305\u4ecd\u4ece veth0 \u8f6c\u53d1\uff0c\u8fdb\u800c\u89e3\u51b3\u5728\u8be5\u6a21\u5f0f\u4e0b nodeport \u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM \u5c06\u81ea\u52a8\u5728 IaaS \u5c42\u521b\u5efa\u56db\u5c42\u4e0e\u4e03\u5c42\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u53ef\u4ee5\u901a\u8fc7\u963f\u91cc\u4e91\u754c\u9762\u8fdb\u884c\u67e5\u770b\uff0c\u5982\u4e0b\uff1a \u9a8c\u8bc1\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee \u5728\u516c\u7f51\u7684\u673a\u5668\u4e0a\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u7684 \u516c\u7f51 IP + \u7aef\u53e3 \u5b9e\u73b0\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee # \u8bbf\u95ee\u56db\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # \u8bbf\u95ee\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes \u963f\u91cc\u4e91\u7684 CCM \u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u7684\u5165\u53e3\u8bbf\u95ee\u65f6\uff0c\u5176\u4e0d\u652f\u6301\u540e\u7aef service \u7684 spec.ipFamilies \u8bbe\u7f6e\u4e3a IPv6 \u3002 ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6. \u603b\u7ed3 Spiderpool \u80fd\u591f\u8fd0\u884c\u5728\u963f\u91cc\u4e91\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\u3002","title":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_2","text":"\u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_3","text":"Spiderpool \u7684\u8282\u70b9\u62d3\u6251\u529f\u80fd\u53ef\u4ee5\u5c06 IPPool \u4e0e\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u4e2a\u7f51\u5361\u7684\u53ef\u7528 IP \u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u8fd8\u5177\u5907\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\u3002 Spiderpool \u80fd\u57fa\u4e8e IPVlan Underlay CNI \u5728\u963f\u91cc\u4e91\u73af\u5883\u4e0a\u8fd0\u884c\uff0c\u5e76\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001IPvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u5904\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d IPVlan CNI \u8fdb\u884c\u89e3\u51b3\u3002IPVlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 IPvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_4","text":"\u5b89\u88c5\u8981\u6c42 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_5","text":"","title":"\u6b65\u9aa4"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_6","text":"\u51c6\u5907\u4e00\u5957\u963f\u91cc\u4e91\u73af\u5883\uff0c\u7ed9\u865a\u62df\u673a\u5206\u914d 2 \u4e2a\u7f51\u5361\uff0c\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe\uff1a \u5b9e\u4f8b\uff08\u865a\u62df\u673a\uff09\u662f\u80fd\u591f\u4e3a\u60a8\u7684\u4e1a\u52a1\u63d0\u4f9b\u8ba1\u7b97\u670d\u52a1\u7684\u6700\u5c0f\u5355\u4f4d\uff0c\u4e0d\u540c\u7684\u5b9e\u4f8b\u89c4\u683c\u53ef\u521b\u5efa\u7f51\u5361\u6570\u548c\u6bcf\u5f20\u7f51\u5361\u53ef\u5206\u914d\u7684\u8f85\u52a9 IP \u6570\u5b58\u5728\u5dee\u5f02\uff0c\u6839\u636e\u4e1a\u52a1\u573a\u666f\u548c\u4f7f\u7528\u573a\u666f\uff0c\u53c2\u8003\u963f\u91cc\u4e91 \u5b9e\u4f8b\u89c4\u683c\u65cf \u9009\u62e9\u5bf9\u5e94\u89c4\u683c\u8fdb\u884c\u521b\u5efa\u5b9e\u4f8b\u3002 \u5982\u679c\u6709 IPv6 \u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u53c2\u8003\u963f\u91cc\u4e91 \u914d\u7f6e IPv6 \u5730\u5740 \u3002 \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a","title":"\u963f\u91cc\u4e91\u73af\u5883"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 IPVlan, \u4f60\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 IPVlan\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a IPvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u4e0e eth1 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#ippools","text":"Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u8be5\u5b57\u6bb5\u7684\u503c\u5fc5\u987b\u8bbe\u7f6e\u3002\u7531\u4e8e\u963f\u91cc\u4e91\u9650\u5236\u4e86\u8282\u70b9\u53ef\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u60a8\u53ef\u4ee5\u4ece\u963f\u91cc\u4e91\u7684\u5f39\u6027\u7f51\u5361\u754c\u9762\u83b7\u53d6\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( eth0\u3001eth1 )\u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF","title":"\u521b\u5efa IPPools"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_7","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u7ec4 DaemonSet \u5e94\u7528\u548c 1 \u4e2a type \u4e3a ClusterIP \u7684 service \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u5206\u522b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\uff0c\u5e94\u7528\u7684 IP \u5747\u5728\u671f\u671b\u7684 IP \u6c60\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_8","text":"\u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes","title":"\u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_9","text":"","title":"\u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#pod","text":"\u963f\u91cc\u4e91\u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u73af\u5883\u4e0b\u6784\u5efa\u4e00\u4e2a\u516c\u7f51\u6216\u79c1\u7f51\u6d41\u91cf\u7684\u51fa\u5165\u53e3\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u6d4b\u8bd5\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 \u5982\u679c\u5e0c\u671b\u901a\u8fc7 IPv6 \u5730\u5740\u5b9e\u73b0\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\uff0c\u4f60\u9700\u8981\u901a\u8fc7 IPv6 \u7f51\u5173\uff0c\u4e3a Pod \u6240\u5206\u914d\u5230\u7684 IPv6 \u5730\u5740 \u5f00\u901a\u516c\u7f51\u5e26\u5bbd \uff0c\u5c06\u79c1\u7f51 IPv6 \u8f6c\u6362\u4e3a\u516c\u7f51 IPv6 \u5730\u5740\u3002\u914d\u7f6e\u5982\u4e0b\u3002 \u6d4b\u8bd5 IPv6 \u8bbf\u95ee\u5982\u4e0b\uff1a ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms","title":"\u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_10","text":"","title":"\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#cloud-controller-manager","text":"CCM\uff08Cloud Controller Manager\uff09\u662f\u963f\u91cc\u4e91\u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e\u963f\u91cc\u4e91\u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e CCM \u6587\u6863 \u5b8c\u6210 CCM \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u5206\u522b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4ece\u800c\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5404\u81ea\u7684 providerID \u3002 http://100.100.100.200/latest/meta-data \u662f\u963f\u91cc\u4e91 CLI \u63d0\u4f9b\u83b7\u53d6\u5b9e\u4f8b\u5143\u6570\u636e\u7684 API \u5165\u53e3\uff0c\u5728\u4e0b\u5217\u793a\u4f8b\u4e2d\u65e0\u9700\u4fee\u6539\u5b83\u3002\u66f4\u591a\u7528\u6cd5\u53ef\u53c2\u8003 \u5b9e\u4f8b\u5143\u6570\u636e ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* \u5728\u96c6\u7fa4\u7684 master \u8282\u70b9\u901a\u8fc7 kubectl patch \u547d\u4ee4\u4e3a\u96c6\u7fa4\u4e2d\u7684 \u6bcf\u4e2a\u8282\u70b9 \u8865\u5145\u5404\u81ea\u7684 providerID \uff0c\u8be5\u6b65\u9aa4\u5fc5\u987b\u88ab\u6267\u884c\uff0c\u5426\u5219\u5bf9\u5e94\u8282\u70b9\u7684 CCM Pod \u5c06\u65e0\u6cd5\u8fd0\u884c\u3002 ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # \u5c06 <NODE_NAME> \u4e0e <provider_id> \u66ff\u6362\u4e3a\u5bf9\u5e94\u503c\u3002 \u521b\u5efa\u963f\u91cc\u4e91\u7684 RAM \u7528\u6237\uff0c\u5e76\u6388\u6743\u3002 RAM \u7528\u6237\u662f RAM \u4e2d\u7684\u4e00\u79cd\u5b9e\u4f53\u8eab\u4efd\uff0c\u4ee3\u8868\u9700\u8981\u8bbf\u95ee\u963f\u91cc\u4e91\u7684\u4eba\u5458\u6216\u5e94\u7528\u7a0b\u5e8f\u3002\u901a\u8fc7\u53c2\u9605 RAM \u8bbf\u95ee\u63a7\u5236 \u521b\u5efa RAM \u7528\u6237\uff0c\u5e76\u6388\u4e8e\u9700\u8981\u8bbf\u95ee\u8d44\u6e90\u7684\u6743\u9650\u3002 \u4e3a\u786e\u4fdd\u540e\u7eed\u6b65\u9aa4\u4e2d\u6240\u4f7f\u7528\u7684 RAM \u7528\u6237\u5177\u5907\u8db3\u591f\u7684\u6743\u9650\uff0c\u8bf7\u4e0e\u672c\u6587\u4fdd\u6301\u4e00\u81f4\uff0c\u7ed9\u4e88 RAM \u7528\u6237 AdministratorAccess \u548c AliyunSLBFullAccess \u6743\u9650\u3002 \u83b7\u53d6 RAM \u7528\u6237\u7684 AccessKey & AccessKeySecret \u767b\u5f55 RAM \u7528\u6237\uff0c\u8bbf\u95ee \u7528\u6237\u4e2d\u5fc3 \uff0c\u83b7\u53d6\u5bf9\u5e94 RAM \u7528\u7684 AccessKey & AccessKeySecret\u3002 \u521b\u5efa CCM \u7684 Cloud ConfigMap\u3002 \u5c06\u6b65\u9aa4 3 \u83b7\u53d6\u7684 AccessKey & AccessKeySecret\uff0c\u53c2\u8003\u4e0b\u5217\u65b9\u5f0f\u5199\u5165\u73af\u5883\u53d8\u91cf\u3002 ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u5b8c\u6210\u521b\u5efa cloud-config\u3002 accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF \u83b7\u53d6 Yaml \uff0c\u5e76\u901a\u8fc7 kubectl apply -f cloud-controller-manager.yaml \u65b9\u5f0f\u5b89\u88c5 CCM\uff0c\u672c\u6587\u4e2d\u5b89\u88c5\u7684\u7248\u672c\u4e3a v2.5.0 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u83b7\u53d6 cloud-controller-manager.yaml\uff0c\u5e76\u66ff\u6362\u5176\u4e2d <<cluster_cidr>> \u4e3a\u60a8\u771f\u5b9e\u96c6\u7fa4\u7684 cluster CIDR \uff1b\u60a8\u53ef\u4ee5\u901a\u8fc7 kubectl cluster-info dump | grep -m1 cluster-cidr \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u7684 cluster CIDR \u3002 ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml \u68c0\u67e5 CCM \u5b89\u88c5\u5b8c\u6210\u3002 ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s","title":"\u90e8\u7f72 Cloud Controller Manager"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#loadbalancer","text":"\u5982\u4e0b\u7684 Yaml \u5c06\u521b\u5efa spec.type \u4e3a LoadBalancer \u7684 2 \u7ec4 service\uff0c\u4e00\u7ec4\u4e3a tcp \uff08\u56db\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u4e00\u7ec4\u4e3a http \uff08\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\u3002 service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port \uff1aCCM \u63d0\u4f9b\u7684\u521b\u5efa\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\u6ce8\u89e3\u3002\u53ef\u4ee5\u901a\u8fc7\u5b83\u81ea\u5b9a\u4e49\u66b4\u9732\u7aef\u53e3\u3002\u66f4\u591a\u7528\u6cd5\u53c2\u8003 CCM \u4f7f\u7528\u6587\u6863 \u3002 .spec.externalTrafficPolicy \uff1a\u8868\u793a\u6b64 Service \u662f\u5426\u5e0c\u671b\u5c06\u5916\u90e8\u6d41\u91cf\u8def\u7531\u5230\u8282\u70b9\u672c\u5730\u6216\u96c6\u7fa4\u8303\u56f4\u7684\u7aef\u70b9\u3002\u5b83\u6709\u4e24\u4e2a\u53ef\u7528\u9009\u9879\uff1aCluster\uff08\u9ed8\u8ba4\uff09\u548c Local\u3002\u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP\uff0c\u4f46\u516c\u6709\u4e91\u81ea\u5efa\u96c6\u7fa4\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5e73\u53f0\u7684 Loadbalancer \u7ec4\u4ef6\u8fdb\u884c nodePort \u8f6c\u53d1\u65f6\uff0c\u4f1a\u51fa\u73b0\u8bbf\u95ee\u4e0d\u901a\u3002\u9488\u5bf9\u8be5\u95ee\u9898 Spiderpool \u63d0\u4f9b\u4e86 coordinator \u63d2\u4ef6\uff0c\u8be5\u63d2\u4ef6\u901a\u8fc7 iptables \u5728\u6570\u636e\u5305\u4e2d\u6253\u6807\u8bb0\uff0c\u786e\u8ba4\u4ece veth0 \u8fdb\u5165\u7684\u6570\u636e\u7684\u56de\u590d\u5305\u4ecd\u4ece veth0 \u8f6c\u53d1\uff0c\u8fdb\u800c\u89e3\u51b3\u5728\u8be5\u6a21\u5f0f\u4e0b nodeport \u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM \u5c06\u81ea\u52a8\u5728 IaaS \u5c42\u521b\u5efa\u56db\u5c42\u4e0e\u4e03\u5c42\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u53ef\u4ee5\u901a\u8fc7\u963f\u91cc\u4e91\u754c\u9762\u8fdb\u884c\u67e5\u770b\uff0c\u5982\u4e0b\uff1a","title":"\u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_11","text":"\u5728\u516c\u7f51\u7684\u673a\u5668\u4e0a\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u7684 \u516c\u7f51 IP + \u7aef\u53e3 \u5b9e\u73b0\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee # \u8bbf\u95ee\u56db\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # \u8bbf\u95ee\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes \u963f\u91cc\u4e91\u7684 CCM \u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u7684\u5165\u53e3\u8bbf\u95ee\u65f6\uff0c\u5176\u4e0d\u652f\u6301\u540e\u7aef service \u7684 spec.ipFamilies \u8bbe\u7f6e\u4e3a IPv6 \u3002 ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6.","title":"\u9a8c\u8bc1\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_12","text":"Spiderpool \u80fd\u591f\u8fd0\u884c\u5728\u963f\u91cc\u4e91\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\u3002","title":"\u603b\u7ed3"},{"location":"usage/install/cloud/get-started-alibaba/","text":"Running On Alibaba Cloud English | \u7b80\u4f53\u4e2d\u6587 Introduction With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using Underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized Underlay solutions for public clouds. This page introduces Spiderpool , an Underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios. Features Spiderpool's node topology function can bind IP pools to the available IPs of each network card on each node, and also achieve the validity of MAC addresses. Spiderpool can run on the Alibaba Cloud environment based on IPVlan Underlay CNI, and ensures that the east-west and north-south traffic of the cluster are normal. Its implementation principle is as follows: When using Underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and IPvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based Underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with IPVlan CNI . The IPVlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating IPVlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved. Prerequisites System requirements The system kernel version must be greater than 4.2 when using IPVlan as the cluster's CNI. Helm is installed. Steps Alibaba Cloud Environment Prepare an Alibaba Cloud environment with virtual machines that have 2 network interfaces. Assign a set of auxiliary private IP addresses to each network interface, as shown in the picture: An instance (virtual machine) is the smallest unit that can provide computing services for your business. Different instance specifications vary in the number of network cards that can be created and the number of auxiliary IPs that can be assigned to each network card. For more information on specific business and usage scenarios, refer to Alibaba Cloud Instance Specification Family to select the corresponding specification to create an instance. If you have IPv6 requirements, you can refer to Alibaba Cloud Configuring IPv6 Addresses . Utilize the configured VMs to build a Kubernetes cluster. The available IP addresses for the nodes and the network topology of the cluster are depicted below: Install Spiderpool Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" If IPVlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install IPVlan in your cluster. If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Here is an example of creating an IPvlan SpiderMultusConfig configuration: IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two IPvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth0 and eth1 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m Create IP Pools The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips \uff1athis field must not be empty. Due to Alibaba Cloud's limitations on available IP addresses for nodes, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . You can obtain this information from the Elastic Network Interface page in the Alibaba Cloud console. Based on the provided information, use the following YAML configuration to create a SpiderIPPool for each network interface (eth0 and eth1) on every node. These SpiderIPPools will assign IP addresses to Pods running on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF Create Applications In the following example YAML, there are 2 sets of DaemonSet applications and 1 service with a type of ClusterIP: v1.multus-cni.io/default-network : specify the subnet that each application will use. In the example, the applications are assigned different subnets. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF Check the status of the running Pods: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool automatically assigns IP addresses to the applications, ensuring that the assigned IPs are within the expected IP pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true Test East-West Connectivity Test communication between Pods and their hosts: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms Test communication between Pods across different nodes and subnets: ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms Test communication between Pods and ClusterIP services: ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes Test North-South Connectivity Test egress traffic from Pods to external destinations Alibaba Cloud's NAT Gateway provides an ingress and egress gateway for public or private network traffic within a VPC environment. By utilizing NAT Gateway, the cluster can have egress connectivity. Please refer to the NAT Gateway documentation for creating a NAT Gateway as depicted in the picture: Test egress traffic from Pods ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 If you want to access the traffic egress of Pods in the cluster through IPv6 addresses, you need to activate public network bandwidth for the IPv6 address assigned to the Pod through the IPv6 gateway and convert the private IPv6 to a public IPv6 address. The configuration is as follows. Test Pod egress traffic over IPv6: ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms Load Balancer Traffic Ingress Access Deploy Cloud Controller Manager Cloud Controller Manager (CCM) is an Alibaba Cloud's component that enables integration between Kubernetes and Alibaba Cloud services. We will use CCM along with Alibaba Cloud infrastructure to facilitate load balancer traffic ingress access. Follow the steps below and refer to the CCM documentation for deploying CCM. Configure providerID on Cluster Nodes On each node in the cluster, run the following command to obtain the providerID for each node. http://100.100.100.200/latest/meta-data is the API entry point provided by Alibaba Cloud CLI for retrieving instance metadata. You don't need to modify it in the provided example. For more information, please refer to ECS instance metadata . ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* On the master node of the cluster, use the kubectl patch command to add the providerID for each node in the cluster. This step is necessary to ensure the proper functioning of the CCM Pod on each corresponding node. Failure to run this step will result in the CCM Pod being unable to run correctly. ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # Replace <NODE_NAME> and <provider_id> with corresponding values. Create an Alibaba Cloud RAM user and grant authorization. A RAM user is an entity within Alibaba Cloud's Resource Access Management (RAM) that represents individuals or applications requiring access to Alibaba Cloud resources. Refer to Overview of RAM users to create a RAM user and assign the necessary permissions for accessing resources. To ensure that the RAM user used in the subsequent steps has sufficient privileges, grant the AdministratorAccess and AliyunSLBFullAccess permissions to the RAM user, following the instructions provided here. Obtain the AccessKey & AccessKeySecret for the RAM user. Log in to the RAM User account and go to User Center to retrieve the corresponding AccessKey & AccessKeySecret for the RAM User. Create the Cloud ConfigMap for CCM. Use the following method to write the AccessKey & AccessKeySecret obtained in step 3 as environment variables. ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** Run the following command to create cloud-config: accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF Retrieve the YAML file and install CCM by running the command kubectl apply -f cloud-controller-manager.yaml . The version of CCM being installed here is v2.5.0. Use the following command to obtain the cloud-controller-manager.yaml file and replace <<cluster_cidr>> with the actual cluster CIDR; You can view the cluster CIDR of the cluster through the kubectl cluster-info dump | grep -m1 cluster-cidr command. ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml Verify if CCM is installed. ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s Create Load Balancer Ingress for Applications The following YAML will create two sets of services, one for TCP (layer 4 load balancing) and one for HTTP (layer 7 load balancing), with spec.type set to LoadBalancer . service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port : this annotation provided by CCM allows you to customize the exposed ports for layer 7 load balancing. For more information, refer to the CCM Usage Documentation . .spec.externalTrafficPolicy : indicates whether the service prefers to route external traffic to local or cluster-wide endpoints. It has two options: Cluster (default) and Local. Setting .spec.externalTrafficPolicy to Local preserves the client source IP. However, when a self-built public cloud cluster uses the platform's Loadbalancer component for nodePort forwarding in this mode, access will be blocked. In response to this problem, Spiderpool provides the coordinator plug-in, which uses iptables to mark the data packets to confirm that the reply packets of data entering from veth0 are still forwarded from veth0, thus solving the problem of nodeport being unable to access in this mode. ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF After the creation is complete, you can view the following: ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM will automatically create layer 4 and layer 7 load balancers at its IaaS services. You can easily access and manage them through the Alibaba Cloud console, as shown below: Verify Load Balancer Traffic Ingress Access On a public machine, access the load balancer's public IP + port to test the traffic ingress: # Access layer 4 load balancing $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # Access layer 7 load balancing $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes Alibaba Cloud's CCM implements ingress access for load balancing traffic, and it does not support setting the spec.ipFamilies of the backend service to IPv6. ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6. Summary Spiderpool is successfully running in an Alibaba Cloud cluster, ensuring normal east-west and north-south traffic.","title":"Alibaba Cloud"},{"location":"usage/install/cloud/get-started-alibaba/#running-on-alibaba-cloud","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Alibaba Cloud"},{"location":"usage/install/cloud/get-started-alibaba/#introduction","text":"With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using Underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized Underlay solutions for public clouds. This page introduces Spiderpool , an Underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios.","title":"Introduction"},{"location":"usage/install/cloud/get-started-alibaba/#features","text":"Spiderpool's node topology function can bind IP pools to the available IPs of each network card on each node, and also achieve the validity of MAC addresses. Spiderpool can run on the Alibaba Cloud environment based on IPVlan Underlay CNI, and ensures that the east-west and north-south traffic of the cluster are normal. Its implementation principle is as follows: When using Underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and IPvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based Underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with IPVlan CNI . The IPVlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating IPVlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved.","title":"Features"},{"location":"usage/install/cloud/get-started-alibaba/#prerequisites","text":"System requirements The system kernel version must be greater than 4.2 when using IPVlan as the cluster's CNI. Helm is installed.","title":"Prerequisites"},{"location":"usage/install/cloud/get-started-alibaba/#steps","text":"","title":"Steps"},{"location":"usage/install/cloud/get-started-alibaba/#alibaba-cloud-environment","text":"Prepare an Alibaba Cloud environment with virtual machines that have 2 network interfaces. Assign a set of auxiliary private IP addresses to each network interface, as shown in the picture: An instance (virtual machine) is the smallest unit that can provide computing services for your business. Different instance specifications vary in the number of network cards that can be created and the number of auxiliary IPs that can be assigned to each network card. For more information on specific business and usage scenarios, refer to Alibaba Cloud Instance Specification Family to select the corresponding specification to create an instance. If you have IPv6 requirements, you can refer to Alibaba Cloud Configuring IPv6 Addresses . Utilize the configured VMs to build a Kubernetes cluster. The available IP addresses for the nodes and the network topology of the cluster are depicted below:","title":"Alibaba Cloud Environment"},{"location":"usage/install/cloud/get-started-alibaba/#install-spiderpool","text":"Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" If IPVlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install IPVlan in your cluster. If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus.","title":"Install Spiderpool"},{"location":"usage/install/cloud/get-started-alibaba/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Here is an example of creating an IPvlan SpiderMultusConfig configuration: IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two IPvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth0 and eth1 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m","title":"Install CNI"},{"location":"usage/install/cloud/get-started-alibaba/#create-ip-pools","text":"The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips \uff1athis field must not be empty. Due to Alibaba Cloud's limitations on available IP addresses for nodes, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . You can obtain this information from the Elastic Network Interface page in the Alibaba Cloud console. Based on the provided information, use the following YAML configuration to create a SpiderIPPool for each network interface (eth0 and eth1) on every node. These SpiderIPPools will assign IP addresses to Pods running on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF","title":"Create IP Pools"},{"location":"usage/install/cloud/get-started-alibaba/#create-applications","text":"In the following example YAML, there are 2 sets of DaemonSet applications and 1 service with a type of ClusterIP: v1.multus-cni.io/default-network : specify the subnet that each application will use. In the example, the applications are assigned different subnets. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF Check the status of the running Pods: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool automatically assigns IP addresses to the applications, ensuring that the assigned IPs are within the expected IP pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true","title":"Create Applications"},{"location":"usage/install/cloud/get-started-alibaba/#test-east-west-connectivity","text":"Test communication between Pods and their hosts: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms Test communication between Pods across different nodes and subnets: ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms Test communication between Pods and ClusterIP services: ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes","title":"Test East-West Connectivity"},{"location":"usage/install/cloud/get-started-alibaba/#test-north-south-connectivity","text":"","title":"Test North-South Connectivity"},{"location":"usage/install/cloud/get-started-alibaba/#test-egress-traffic-from-pods-to-external-destinations","text":"Alibaba Cloud's NAT Gateway provides an ingress and egress gateway for public or private network traffic within a VPC environment. By utilizing NAT Gateway, the cluster can have egress connectivity. Please refer to the NAT Gateway documentation for creating a NAT Gateway as depicted in the picture: Test egress traffic from Pods ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 If you want to access the traffic egress of Pods in the cluster through IPv6 addresses, you need to activate public network bandwidth for the IPv6 address assigned to the Pod through the IPv6 gateway and convert the private IPv6 to a public IPv6 address. The configuration is as follows. Test Pod egress traffic over IPv6: ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms","title":"Test egress traffic from Pods to external destinations"},{"location":"usage/install/cloud/get-started-alibaba/#load-balancer-traffic-ingress-access","text":"","title":"Load Balancer Traffic Ingress Access"},{"location":"usage/install/cloud/get-started-alibaba/#deploy-cloud-controller-manager","text":"Cloud Controller Manager (CCM) is an Alibaba Cloud's component that enables integration between Kubernetes and Alibaba Cloud services. We will use CCM along with Alibaba Cloud infrastructure to facilitate load balancer traffic ingress access. Follow the steps below and refer to the CCM documentation for deploying CCM. Configure providerID on Cluster Nodes On each node in the cluster, run the following command to obtain the providerID for each node. http://100.100.100.200/latest/meta-data is the API entry point provided by Alibaba Cloud CLI for retrieving instance metadata. You don't need to modify it in the provided example. For more information, please refer to ECS instance metadata . ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* On the master node of the cluster, use the kubectl patch command to add the providerID for each node in the cluster. This step is necessary to ensure the proper functioning of the CCM Pod on each corresponding node. Failure to run this step will result in the CCM Pod being unable to run correctly. ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # Replace <NODE_NAME> and <provider_id> with corresponding values. Create an Alibaba Cloud RAM user and grant authorization. A RAM user is an entity within Alibaba Cloud's Resource Access Management (RAM) that represents individuals or applications requiring access to Alibaba Cloud resources. Refer to Overview of RAM users to create a RAM user and assign the necessary permissions for accessing resources. To ensure that the RAM user used in the subsequent steps has sufficient privileges, grant the AdministratorAccess and AliyunSLBFullAccess permissions to the RAM user, following the instructions provided here. Obtain the AccessKey & AccessKeySecret for the RAM user. Log in to the RAM User account and go to User Center to retrieve the corresponding AccessKey & AccessKeySecret for the RAM User. Create the Cloud ConfigMap for CCM. Use the following method to write the AccessKey & AccessKeySecret obtained in step 3 as environment variables. ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** Run the following command to create cloud-config: accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF Retrieve the YAML file and install CCM by running the command kubectl apply -f cloud-controller-manager.yaml . The version of CCM being installed here is v2.5.0. Use the following command to obtain the cloud-controller-manager.yaml file and replace <<cluster_cidr>> with the actual cluster CIDR; You can view the cluster CIDR of the cluster through the kubectl cluster-info dump | grep -m1 cluster-cidr command. ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml Verify if CCM is installed. ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s","title":"Deploy Cloud Controller Manager"},{"location":"usage/install/cloud/get-started-alibaba/#create-load-balancer-ingress-for-applications","text":"The following YAML will create two sets of services, one for TCP (layer 4 load balancing) and one for HTTP (layer 7 load balancing), with spec.type set to LoadBalancer . service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port : this annotation provided by CCM allows you to customize the exposed ports for layer 7 load balancing. For more information, refer to the CCM Usage Documentation . .spec.externalTrafficPolicy : indicates whether the service prefers to route external traffic to local or cluster-wide endpoints. It has two options: Cluster (default) and Local. Setting .spec.externalTrafficPolicy to Local preserves the client source IP. However, when a self-built public cloud cluster uses the platform's Loadbalancer component for nodePort forwarding in this mode, access will be blocked. In response to this problem, Spiderpool provides the coordinator plug-in, which uses iptables to mark the data packets to confirm that the reply packets of data entering from veth0 are still forwarded from veth0, thus solving the problem of nodeport being unable to access in this mode. ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF After the creation is complete, you can view the following: ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM will automatically create layer 4 and layer 7 load balancers at its IaaS services. You can easily access and manage them through the Alibaba Cloud console, as shown below:","title":"Create Load Balancer Ingress for Applications"},{"location":"usage/install/cloud/get-started-alibaba/#verify-load-balancer-traffic-ingress-access","text":"On a public machine, access the load balancer's public IP + port to test the traffic ingress: # Access layer 4 load balancing $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # Access layer 7 load balancing $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes Alibaba Cloud's CCM implements ingress access for load balancing traffic, and it does not support setting the spec.ipFamilies of the backend service to IPv6. ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6.","title":"Verify Load Balancer Traffic Ingress Access"},{"location":"usage/install/cloud/get-started-alibaba/#summary","text":"Spiderpool is successfully running in an Alibaba Cloud cluster, ensuring normal east-west and north-south traffic.","title":"Summary"},{"location":"usage/install/cloud/get-started-aws-zh_CN/","text":"AWS \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002 \u4e3a\u4ec0\u4e48\u9009\u62e9 Spiderpool Spiderpool \u662f\u4e00\u4e2a Kubernetes \u7684 Underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u589e\u5f3a\u4e86 Macvlan CNI\u3001IPvlan CNI \u548c SR-IOV CNI \u7684\u529f\u80fd\uff0c\u6ee1\u8db3\u4e86\u5404\u79cd\u7f51\u7edc\u9700\u6c42\uff0c\u4f7f\u5f97 Underlay \u7f51\u7edc\u65b9\u6848\u53ef\u5e94\u7528\u5728**\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883**\u4e2d\uff0c\u53ef\u4e3a\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u3001\u4f4e\u5ef6\u65f6\u5e94\u7528\u5e26\u6765\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\u3002 aws-vpc-cni \u53ef\u4ee5\u4f7f\u7528 AWS \u4e0a\u7684\u5f39\u6027\u7f51\u7edc\u63a5\u53e3\u5728 Kubernetes \u4e2d\u5b9e\u73b0 Pod \u7f51\u7edc\u901a\u4fe1\u7684\u7f51\u7edc\u63d2\u4ef6\u3002 aws-vpc-cni \u662f AWS \u4e3a\u516c\u6709\u4e91\u63d0\u4f9b\u7684\u4e00\u79cd Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b83\u4e0d\u80fd\u6ee1\u8db3\u590d\u6742\u7684\u7f51\u7edc\u9700\u6c42\uff0c\u5982\u4e0b\u662f Spiderpool \u4e0e aws-vpc-cni \u5728 AWS \u4e91\u73af\u5883\u4e0a\u4f7f\u7528\u7684\u4e00\u4e9b\u529f\u80fd\u5bf9\u6bd4\uff0c\u5728\u540e\u7eed\u7ae0\u8282\u4f1a\u6f14\u793a Spiderpool \u7684\u76f8\u5173\u529f\u80fd\uff1a \u529f\u80fd\u6bd4\u8f83 aws-vpc-cni Spiderpool + IPvlan \u591a Underlay \u7f51\u5361 \u274c \u2705 (\u591a\u4e2a\u8de8\u5b50\u7f51\u7684 Underlay \u7f51\u5361) \u81ea\u5b9a\u4e49\u8def\u7531 \u274c \u2705 route \u53cc CNI \u534f\u540c \u652f\u6301\u591a CNI \u7f51\u5361\u4f46\u4e0d\u652f\u6301\u8def\u7531\u8c03\u534f \u2705 \u7f51\u7edc\u7b56\u7565 \u2705 aws-network-policy-agent \u2705 cilium-chaining clusterIP \u2705 (kube-proxy) \u2705 ( kube-proxy \u548c ebpf \u4e24\u79cd\u65b9\u5f0f) Bandwidth \u274c \u2705 Bandwidth \u7ba1\u7406 metrics \u2705 \u2705 \u53cc\u6808 \u652f\u6301\u5355IPv4\u3001IPv6\uff0c\u4e0d\u652f\u6301\u53cc\u6808 \u652f\u6301\u5355 IPv4\u3001IPv6, \u53cc\u6808 \u53ef\u89c2\u6d4b\u6027 \u274c \u2705(\u642d\u914d cilium hubble, \u5185\u6838>=4.19.57) \u591a\u96c6\u7fa4 \u65e0 \u2705 Submariner \u642d\u914dAWS 4/7\u5c42\u8d1f\u8f7d\u5747\u8861 \u2705 \u2705 \u5185\u6838\u9650\u5236 \u65e0 >= 4.2 (IPvlan \u5185\u6838\u9650\u5236) \u8f6c\u53d1\u539f\u7406 underlay \u7eaf\u8def\u7531 3 \u5c42\u8f6c\u53d1 IPvlan 2 \u5c42 \u7ec4\u64ad, \u591a\u64ad \u274c \u2705 \u8de8 vpc \u8bbf\u95ee \u2705 \u2705 \u9879\u76ee\u529f\u80fd Spiderpool \u80fd\u57fa\u4e8e ipvlan Underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u4e0a\uff0c\u5e76\u5b9e\u73b0\u6709\u8282\u70b9\u62d3\u6251\u3001\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001ipvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d ipvlan CNI \u8fdb\u884c\u89e3\u51b3\u3002ipvlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 ipvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002 \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u4f7f\u7528 ipvlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u4e86\u89e3 AWS VPC \u516c\u6709 & \u79c1\u6709\u5b50\u7f51 \u57fa\u7840\u77e5\u8bc6\u3002 \u5728 AWS VPC \u4e0b\u521b\u5efa\u7684\u5b50\u7f51\uff0c\u5982\u679c\u8bbe\u7f6e\u4e86\u51fa\u53e3\u8def\u7531 0.0.0.0/0, ::/0 \u7684\u4e0b\u4e00\u8df3\u4e3a Internet Gateway\uff0c\u5219\u8be5\u5b50\u7f51\u5c31\u96b6\u5c5e\u4e8e \u516c\u6709\u5b50\u7f51 \uff0c\u5426\u5219\u5c31\u662f \u79c1\u6709\u5b50\u7f51 \u3002 \u6b65\u9aa4 AWS \u73af\u5883 \u5728 VPC \u4e0b\u521b\u5efa\u516c\u6709\u5b50\u7f51\u4ee5\u53ca\u79c1\u6709\u5b50\u7f51\uff0c\u5e76\u5728\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u865a\u62df\u673a\uff0c\u5982\u56fe\uff1a \u672c\u4f8b\u4f1a\u5728\u540c\u4e00\u4e2a VPC \u4e0b\u5148\u521b\u5efa 1 \u4e2a\u516c\u6709\u5b50\u7f51\u4ee5\u53ca 2 \u4e2a\u79c1\u6709\u5b50\u7f51(\u8bf7\u5c06\u5b50\u7f51\u90e8\u7f72\u5728\u4e0d\u540c\u7684\u53ef\u7528\u533a)\uff0c\u63a5\u7740\u4f1a\u5728\u516c\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u4e00\u4e2a AWS EC2 \u5b9e\u4f8b\u4f5c\u4e3a\u8df3\u677f\u673a\uff0c\u7136\u540e\u4f1a\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u5bf9\u5e94\u7684 AWS EC2 \u5b9e\u4f8b\u7528\u4e8e\u90e8\u7f72 Kubernetes \u96c6\u7fa4\u3002 \u521b\u5efa\u5b9e\u4f8b\u65f6\u7ed9\u7f51\u5361\u7ed1\u5b9a IPv4 \u548c IPv6 \u5730\u5740\uff0c\u5982\u56fe\uff1a \u7ed9\u5b9e\u4f8b\u4eec\u7684\u6bcf\u5f20\u7f51\u5361\u5747\u7ed1\u5b9a\u4e00\u4e9b IP \u524d\u7f00\u59d4\u6258 \u7528\u4e8e\u7ed9 pod \u5206\u914d IP \u5730\u5740\uff0c\u5982\u56fe: IP \u524d\u7f00\u59d4\u6258\u7c7b\u4f3c\u7f51\u5361\u8f85\u52a9 IP \u53ef\u5c06 IPv4(/28) \u548c IPv6(/80) \u5730\u5740\u5757\u7ed1\u5b9a\u7ed9\u5b9e\u4f8b\u3002\u53ef\u7ed1\u5b9a\u524d\u7f00\u59d4\u6258\u6570\u91cf\u53ef\u53c2\u8003 AWS EC2 \u5b9e\u4f8b\u89c4\u683c \uff0c\u5b9e\u4f8b\u53ef\u7ed1\u5b9a\u524d\u7f00\u59d4\u6258\u6570\u91cf\u7b49\u540c\u4e8e\u5b9e\u4f8b\u7684\u7f51\u5361\u4e2d\u53ef\u7ed1\u5b9a\u8f85\u52a9 IP \u6570\u91cf\u3002\u6b64\u4f8b\u4e2d\uff0c\u6211\u4eec\u9009\u62e9\u7ed9\u5b9e\u4f8b\u7ed1\u5b9a1\u5f20\u7f51\u5361\u4ee5\u53ca1\u4e2a IP \u524d\u7f00\u59d4\u6258\u3002 | Node | ens5 primary IPv4 IP | ens5 primary IPv6 IP | ens5 IPv4 prefix | ens5 IPv6 prefix | | --------- | ---------------------- | ----------------------- | ------------------- | ----------------------------- | | master | 172 .31.22.228 | 2406 :da1e:c4:ed01::10 | 172 .31.28.16/28 | 2406 :da1e:c4:ed01:c57d::/80 | | worker1 | 180 .17.16.17 | 2406 :da1e:c4:ed02::10 | 172 .31.32.176/28 | 2406 :da1e:c4:ed02:7a2e::/80 | \u521b\u5efa AWS NAT \u7f51\u5173\uff0cAWS \u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u79c1\u6709\u5b50\u7f51\u4e2d\u7684\u5b9e\u4f8b\u8fde\u63a5\u5230 VPC \u5916\u90e8\u7684\u670d\u52a1\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u5728\u4e0a\u8ff0\u7684\u516c\u6709\u5b50\u7f51 public-172-31-0-0 \u4e0b\u521b\u5efa NAT \u7f51\u5173\uff0c\u5e76\u4e3a\u79c1\u6709\u5b50\u7f51\u7684\u8def\u7531\u8868\u914d\u7f6e 0.0.0.0/0 \u51fa\u53e3\u8def\u7531\u7684\u4e0b\u4e00\u8df3\u4e3a\u8be5 NAT \u7f51\u5173\u3002(\u6ce8\u610f IPv6 \u662f\u7531 AWS \u5206\u914d\u7684\u5168\u5c40\u552f\u4e00\u7684\u5730\u5740\uff0c\u53ef\u76f4\u63a5\u501f\u52a9 Internet Gateway \u8bbf\u95ee\u4e92\u8054\u7f51) \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-ens5\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u82e5\u76ee\u5f55\u4e0b\u4e0d\u5b58\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u6839\u636e\u524d\u9762\u521b\u5efa AWS EC2 \u5b9e\u4f8b\u865a\u62df\u673a\u8fc7\u7a0b\u4e2d\u521b\u5efa\u7684\u7f51\u5361\u60c5\u51b5\uff0c\u4e3a\u865a\u62df\u673a\u7684\u6bcf\u4e2a\u7528\u4e8e\u8fd0\u884c ipvlan CNI \u7684\u7f51\u5361\u521b\u5efa\u5982\u4e0b SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE = \"ens5\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e00\u4e2a ipvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth5 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d \u521b\u5efa IP \u6c60 Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u8be5\u5b57\u6bb5\u9650\u5236\u5f53\u524d SpiderIPPool\u8d44\u6e90\u4ec5\u9002\u7528\u4e8e\u54ea\u4e9b\u8282\u70b9\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53\u8be5\u5b57\u6bb5\u4e3a\u7a7a\u65f6\uff0c\u8868\u660e\u5f53\u524d Spiderpool \u8d44\u6e90\u9002\u7528\u4e8e\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u6839\u636e\u4e0a\u6587 AWS EC2 \u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca IP \u524d\u7f00\u59d4\u6258\u5730\u5740\u7b49\u4fe1\u606f\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u6240\u5c5e\u7684\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u4e14\u5bf9\u5e94\u552f\u4e00\u7684\u4e00\u5f20\u5b9e\u4f8b\u7f51\u5361\u3002 \u7ed3\u5408\u4e0a\u6587 AWS \u73af\u5883 \u6bcf\u53f0\u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca\u5bf9\u5e94\u7684 IP \u524d\u7f00\u59d4\u6258\u5730\u5740\u4fe1\u606f\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u7f51\u5361 ens5 \u5206\u522b\u521b\u5efa\u4e86 IPv4 \u548c IPv6 \u7684 SpiderIPPool \u5b9e\u4f8b\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4 spec: subnet: 172.31.16.0/20 ips: - 172.31.28.16-172.31.28.31 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v6 spec: subnet: 2406:da1e:c4:ed01::/64 ips: - 2406:da1e:c4:ed01:c57d::0-2406:da1e:c4:ed01:c57d::f gateway: 2406:da1e:c4:ed01::1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4 spec: subnet: 172.31.32.0/24 ips: - 172.31.32.176-172.31.32.191 gateway: 172.31.32.1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v6 spec: subnet: 2406:da1e:c4:ed02::/64 ips: - 2406:da1e:c4:ed02:7a2e::0-2406:da1e:c4:ed02:7a2e::f gateway: 2406:da1e:c4:ed02::1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u7684 CNI \u914d\u7f6e\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u9009\u62e9\u4f7f\u7528\u5bf9\u5e94\u4e8e\u5bbf\u4e3b\u673a ens5 \u7684 ipvlan \u914d\u7f6e\uff0c\u5e76\u6839\u636e\u6211\u4eec\u7684\u7f3a\u7701 SpiderIPPool \u8d44\u6e90\u9ed8\u8ba4\u6311\u9009\u5176\u5bf9\u5e94\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb spec: selector: matchLabels: run: nginx-lb replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-lb spec: containers: - name: nginx-lb image: nginx ports: - containerPort: 80 EOF \u67e5\u770b Pod \u7684\u8fd0\u884c\u72b6\u6001\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0c\u6211\u4eec\u4e24\u4e2a\u8282\u70b9\u4e0a\u90fd\u8fd0\u884c\u4e86 1 \u4e2a Pod \u4e14\u4f7f\u7528\u7684 IP \u90fd\u5bf9\u5e94\u5bbf\u4e3b\u673a\u7684\u7b2c\u4e00\u5f20\u7f51\u5361\u7684 IP \u524d\u7f00\u59d4\u6258\u5730\u5740: ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-64fbbb5fd8-q5wjm 1 /1 Running 0 10s 172 .31.32.184 worker1 <none> <none> nginx-lb-64fbbb5fd8-wkzf6 1 /1 Running 0 10s 172 .31.28.31 master <none> <none> \u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027 \u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a export NODE_MASTER_IP=172.31.18.11 export NODE_WORKER1_IP=172.31.32.18 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 ${NODE_MASTER_IP} ~# kubectl exec -it nginx-lb-64fbbb5fd8-q5wjm -- ping -c 1 ${NODE_WORKER1_IP} \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 172.31.32.184 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping6 -c 1 2406:da1e:c4:ed02:7a2e::d \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I ${CLUSTER_IP} \u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027 \u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee \u501f\u52a9\u4e0a\u6587\u6211\u4eec\u521b\u5efa\u7684 AWS NAT \u7f51\u5173 \uff0c\u6211\u4eec\u7684 VPC \u79c1\u7f51\u5df2\u53ef\u5b9e\u73b0\u8bbf\u95ee\u4e92\u8054\u7f51\u3002 kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I www.baidu.com \u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee(\u53ef\u9009) \u90e8\u7f72 AWS Load Balancer Controller AWS \u57fa\u7840\u4ea7\u54c1 \u8d1f\u8f7d\u5747\u8861 \u62e5\u6709 NLB (Network Load Balancer) \u548c ALB(Application Load Balancer) \u4e24\u79cd\u6a21\u5f0f\u5206\u522b\u5bf9\u5e94 Layer4 \u4e0e Layer7\u3002 aws-load-balancer-controller \u662f AWS \u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e AWS \u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u53ef\u5b9e\u73b0 kubernetes Service LoadBalancer \u548c Ingress \u529f\u80fd\u3002\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408 AWS \u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5\u6f14\u793a\uff0c \u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e aws-load-balancer-controller \u6587\u6863 \u5b8c\u6210 aws-load-balancer-controller \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u4e3a Kubernetes \u4e0a\u7684\u6bcf\u4e2a Node \u8bbe\u7f6e\u4e0a providerID \uff0c\u60a8\u53ef\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0: - \u53ef\u76f4\u63a5\u5728 AWS EC2 dashboard \u4e2d\u627e\u5230\u5b9e\u4f8b\u7684Instance ID. - \u4f7f\u7528 AWS CLI \u6765\u67e5\u8be2Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . \u4e3a AWS EC2 \u5b9e\u4f8b\u6240\u4f7f\u7528\u7684 IAM role \u8865\u5145 policy \u4ecb\u4e8e aws-load-balancer-controller \u8fd0\u884c\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u4e14\u9700\u8981\u8bbf\u95ee AWS \u7684 NLB/ALB APIs\uff0c\u56e0\u6b64\u9700\u8981 AWS IAM \u5173\u4e8e NLB/ALB \u76f8\u5173\u8bf7\u6c42\u7684\u6388\u6743\u3002\u53c8\u56e0\u6211\u4eec\u662f\u81ea\u5efa\u96c6\u7fa4\uff0c\u6211\u4eec\u9700\u8981\u501f\u7528\u8282\u70b9\u81ea\u8eab\u7684 IAM Role \u6765\u5b9e\u73b0\u6388\u6743\uff0c\u8be6\u60c5\u53ef\u770b aws-load-balancer-controller IAM \u3002 curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json \u4f7f\u7528\u5982\u4e0a\u83b7\u53d6\u7684 json \u5185\u5bb9\uff0c\u5728 AWS IAM Dashboard \u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684policy\uff0c\u5e76\u5c06\u8be5 policy \u4e0e\u60a8\u5f53\u524d\u865a\u62df\u673a\u5b9e\u4f8b\u7684 IAM Role \u8fdb\u884c\u5173\u8054\u3002 \u4e3a\u60a8 AWS EC2 \u5b9e\u4f8b\u6240\u5728\u7684\u53ef\u7528\u533a\u521b\u5efa\u4e00\u4e2a public subnet \u5e76\u6253\u4e0a\u53ef\u81ea\u52a8\u53d1\u73b0\u7684 tag. ALB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 2 \u4e2a\u8de8\u53ef\u7528\u533a\u7684\u5b50\u7f51\uff0c\u5bf9\u4e8e NLB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 1 \u4e2a\u5b50\u7f51\u3002\u8be6\u60c5\u8bf7\u770b \u5b50\u7f51\u81ea\u52a8\u53d1\u73b0 \u3002 \u5bf9\u4e8e\u516c\u7f51\u8bbf\u95ee\u7684 LB\uff0c\u60a8\u9700\u8981\u4e3a\u5b9e\u4f8b\u6240\u5728\u53ef\u7528\u533a\u7684 public subnet \u6253\u4e0a tag: kubernetes.io/role/elb:1 \uff0c\u5bf9\u4e8e VPC \u95f4\u8bbf\u95ee\u7684 LB\uff0c\u8bf7\u521b\u5efa private subnet \u5e76\u6253\u4e0a tag: kubernetes.io/role/internal-elb:1 \uff0c\u8bf7\u7ed3\u5408 AWS \u73af\u5883 \u6765\u521b\u5efa\u6240\u9700\u7684\u5b50\u7f51\uff1a \u9488\u5bf9\u56e0\u7279\u7f51\u66b4\u9732\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa public subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u6211\u4eec\u7684 Main \u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002(\u6ce8\u610f Main \u8def\u7531\u8868\u7684 0.0.0.0/0 \u8def\u7531\u7684\u4e0b\u4e00\u8df3\u9ed8\u8ba4\u4e3a Internet \u7f51\u5173\uff0c\u82e5\u4e22\u5931\u8bf7\u81ea\u884c\u521b\u5efa\u8be5\u8def\u7531\u89c4\u5219)\u3002 \u5728 AWS VPC Dashboard Route tables \u680f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8def\u7531\u8868\u5e76\u914d\u7f6e 0.0.0.0/0 \u7684\u8def\u7531\u4e0b\u4e00\u8df3\u4e3a NAT \u7f51\u5173\uff0c::/0 \u8def\u7531\u4e0b\u4e00\u8df3\u4e3a Internet \u7f51\u5173\u3002 \u9488\u5bf9 VPC \u95f4\u8bbf\u95ee\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa private subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002 \u4f7f\u7528 helm \u5b89\u88c5aws-load-balancer-controller(\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5) helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> \u68c0\u67e5 aws-load-balancer-controller \u5b89\u88c5\u5b8c\u6210 ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s \u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3 \u4e0a\u6587\u4e2d\u5df2\u521b\u5efa \u5e94\u7528 , \u73b0\u5728\u6211\u4eec\u4e3a\u5b83\u521b\u5efa\u4e00\u4e2a kubernetes Service LoadBalancer \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack \u6ce8\u89e3): cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb labels: run: nginx-lb annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb EOF \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a NLB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 NLB \u8fd8\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/load-balancer-source-ranges \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90 IP\u3002\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 \u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-scheme \u9009\u62e9\u6b64 NLB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true \u63d0\u4f9b\u4e86\u5ba2\u6237\u7aef\u6e90 IP \u4fdd\u7559\u529f\u80fd\u3002 \u4e3a\u5e94\u7528\u521b\u5efa Ingress \u8bbf\u95ee\u5165\u53e3 \u63a5\u4e0b\u6765\u6211\u4eec\u521b\u5efa\u4e00\u4e2akubernetes Ingress \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 alb.ingress.kubernetes.io/ip-address-type: dualstack \u6ce8\u89e3): apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a ALB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 ALB \u4e5f\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 alb.ingress.kubernetes.io/target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u4f7f\u7528 ALB \u7684 instance \u6a21\u5f0f\u9700\u8981\u6307\u5b9a service \u4e3a NodePort \u6a21\u5f0f\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/inbound-cidrs \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90IP\u3002(\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 alb.ingress.kubernetes.io/ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 )\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/scheme \u9009\u62e9\u6b64 ALB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u82e5\u60f3\u6574\u5408\u591a\u4e2a Ingress \u8d44\u6e90\u5171\u4eab\u540c\u4e00\u4e2a\u5165\u53e3\uff0c\u53ef\u914d\u7f6e\u6ce8\u89e3 alb.ingress.kubernetes.io/group.name \u6765\u663e\u793a\u6307\u5b9a\u4e00\u4e2a\u540d\u5b57\u3002\uff08\u6ce8\u610f\uff0c\u9ed8\u8ba4\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u7684 Ingresses \u8d44\u6e90\u5e76\u4e0d\u5c5e\u4e8e\u4efb\u4f55 IngressGroup\uff0c\u7cfb\u7edf\u4f1a\u5c06\u5176\u89c6\u4e3a\u7531 Ingress \u672c\u8eab\u7ec4\u6210\u7684 \"\u9690\u5f0f IngressGroup\"\uff09 \u5982\u679c\u60f3\u6307\u5b9a Ingress \u7684 host\uff0c\u9700\u8981\u642d\u914d externalDNS \u4f7f\u7528\u3002\u8be6\u60c5\u8bf7\u67e5\u770b \u914d\u7f6e externalDNS \u3002","title":"AWS \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#aws","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"AWS \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_1","text":"\u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#spiderpool","text":"Spiderpool \u662f\u4e00\u4e2a Kubernetes \u7684 Underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u589e\u5f3a\u4e86 Macvlan CNI\u3001IPvlan CNI \u548c SR-IOV CNI \u7684\u529f\u80fd\uff0c\u6ee1\u8db3\u4e86\u5404\u79cd\u7f51\u7edc\u9700\u6c42\uff0c\u4f7f\u5f97 Underlay \u7f51\u7edc\u65b9\u6848\u53ef\u5e94\u7528\u5728**\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883**\u4e2d\uff0c\u53ef\u4e3a\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u3001\u4f4e\u5ef6\u65f6\u5e94\u7528\u5e26\u6765\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\u3002 aws-vpc-cni \u53ef\u4ee5\u4f7f\u7528 AWS \u4e0a\u7684\u5f39\u6027\u7f51\u7edc\u63a5\u53e3\u5728 Kubernetes \u4e2d\u5b9e\u73b0 Pod \u7f51\u7edc\u901a\u4fe1\u7684\u7f51\u7edc\u63d2\u4ef6\u3002 aws-vpc-cni \u662f AWS \u4e3a\u516c\u6709\u4e91\u63d0\u4f9b\u7684\u4e00\u79cd Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b83\u4e0d\u80fd\u6ee1\u8db3\u590d\u6742\u7684\u7f51\u7edc\u9700\u6c42\uff0c\u5982\u4e0b\u662f Spiderpool \u4e0e aws-vpc-cni \u5728 AWS \u4e91\u73af\u5883\u4e0a\u4f7f\u7528\u7684\u4e00\u4e9b\u529f\u80fd\u5bf9\u6bd4\uff0c\u5728\u540e\u7eed\u7ae0\u8282\u4f1a\u6f14\u793a Spiderpool \u7684\u76f8\u5173\u529f\u80fd\uff1a \u529f\u80fd\u6bd4\u8f83 aws-vpc-cni Spiderpool + IPvlan \u591a Underlay \u7f51\u5361 \u274c \u2705 (\u591a\u4e2a\u8de8\u5b50\u7f51\u7684 Underlay \u7f51\u5361) \u81ea\u5b9a\u4e49\u8def\u7531 \u274c \u2705 route \u53cc CNI \u534f\u540c \u652f\u6301\u591a CNI \u7f51\u5361\u4f46\u4e0d\u652f\u6301\u8def\u7531\u8c03\u534f \u2705 \u7f51\u7edc\u7b56\u7565 \u2705 aws-network-policy-agent \u2705 cilium-chaining clusterIP \u2705 (kube-proxy) \u2705 ( kube-proxy \u548c ebpf \u4e24\u79cd\u65b9\u5f0f) Bandwidth \u274c \u2705 Bandwidth \u7ba1\u7406 metrics \u2705 \u2705 \u53cc\u6808 \u652f\u6301\u5355IPv4\u3001IPv6\uff0c\u4e0d\u652f\u6301\u53cc\u6808 \u652f\u6301\u5355 IPv4\u3001IPv6, \u53cc\u6808 \u53ef\u89c2\u6d4b\u6027 \u274c \u2705(\u642d\u914d cilium hubble, \u5185\u6838>=4.19.57) \u591a\u96c6\u7fa4 \u65e0 \u2705 Submariner \u642d\u914dAWS 4/7\u5c42\u8d1f\u8f7d\u5747\u8861 \u2705 \u2705 \u5185\u6838\u9650\u5236 \u65e0 >= 4.2 (IPvlan \u5185\u6838\u9650\u5236) \u8f6c\u53d1\u539f\u7406 underlay \u7eaf\u8def\u7531 3 \u5c42\u8f6c\u53d1 IPvlan 2 \u5c42 \u7ec4\u64ad, \u591a\u64ad \u274c \u2705 \u8de8 vpc \u8bbf\u95ee \u2705 \u2705","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9 Spiderpool"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_2","text":"Spiderpool \u80fd\u57fa\u4e8e ipvlan Underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u4e0a\uff0c\u5e76\u5b9e\u73b0\u6709\u8282\u70b9\u62d3\u6251\u3001\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001ipvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d ipvlan CNI \u8fdb\u884c\u89e3\u51b3\u3002ipvlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 ipvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_3","text":"\u5b89\u88c5\u8981\u6c42 \u4f7f\u7528 ipvlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u4e86\u89e3 AWS VPC \u516c\u6709 & \u79c1\u6709\u5b50\u7f51 \u57fa\u7840\u77e5\u8bc6\u3002 \u5728 AWS VPC \u4e0b\u521b\u5efa\u7684\u5b50\u7f51\uff0c\u5982\u679c\u8bbe\u7f6e\u4e86\u51fa\u53e3\u8def\u7531 0.0.0.0/0, ::/0 \u7684\u4e0b\u4e00\u8df3\u4e3a Internet Gateway\uff0c\u5219\u8be5\u5b50\u7f51\u5c31\u96b6\u5c5e\u4e8e \u516c\u6709\u5b50\u7f51 \uff0c\u5426\u5219\u5c31\u662f \u79c1\u6709\u5b50\u7f51 \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#aws_1","text":"\u5728 VPC \u4e0b\u521b\u5efa\u516c\u6709\u5b50\u7f51\u4ee5\u53ca\u79c1\u6709\u5b50\u7f51\uff0c\u5e76\u5728\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u865a\u62df\u673a\uff0c\u5982\u56fe\uff1a \u672c\u4f8b\u4f1a\u5728\u540c\u4e00\u4e2a VPC \u4e0b\u5148\u521b\u5efa 1 \u4e2a\u516c\u6709\u5b50\u7f51\u4ee5\u53ca 2 \u4e2a\u79c1\u6709\u5b50\u7f51(\u8bf7\u5c06\u5b50\u7f51\u90e8\u7f72\u5728\u4e0d\u540c\u7684\u53ef\u7528\u533a)\uff0c\u63a5\u7740\u4f1a\u5728\u516c\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u4e00\u4e2a AWS EC2 \u5b9e\u4f8b\u4f5c\u4e3a\u8df3\u677f\u673a\uff0c\u7136\u540e\u4f1a\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u5bf9\u5e94\u7684 AWS EC2 \u5b9e\u4f8b\u7528\u4e8e\u90e8\u7f72 Kubernetes \u96c6\u7fa4\u3002 \u521b\u5efa\u5b9e\u4f8b\u65f6\u7ed9\u7f51\u5361\u7ed1\u5b9a IPv4 \u548c IPv6 \u5730\u5740\uff0c\u5982\u56fe\uff1a \u7ed9\u5b9e\u4f8b\u4eec\u7684\u6bcf\u5f20\u7f51\u5361\u5747\u7ed1\u5b9a\u4e00\u4e9b IP \u524d\u7f00\u59d4\u6258 \u7528\u4e8e\u7ed9 pod \u5206\u914d IP \u5730\u5740\uff0c\u5982\u56fe: IP \u524d\u7f00\u59d4\u6258\u7c7b\u4f3c\u7f51\u5361\u8f85\u52a9 IP \u53ef\u5c06 IPv4(/28) \u548c IPv6(/80) \u5730\u5740\u5757\u7ed1\u5b9a\u7ed9\u5b9e\u4f8b\u3002\u53ef\u7ed1\u5b9a\u524d\u7f00\u59d4\u6258\u6570\u91cf\u53ef\u53c2\u8003 AWS EC2 \u5b9e\u4f8b\u89c4\u683c \uff0c\u5b9e\u4f8b\u53ef\u7ed1\u5b9a\u524d\u7f00\u59d4\u6258\u6570\u91cf\u7b49\u540c\u4e8e\u5b9e\u4f8b\u7684\u7f51\u5361\u4e2d\u53ef\u7ed1\u5b9a\u8f85\u52a9 IP \u6570\u91cf\u3002\u6b64\u4f8b\u4e2d\uff0c\u6211\u4eec\u9009\u62e9\u7ed9\u5b9e\u4f8b\u7ed1\u5b9a1\u5f20\u7f51\u5361\u4ee5\u53ca1\u4e2a IP \u524d\u7f00\u59d4\u6258\u3002 | Node | ens5 primary IPv4 IP | ens5 primary IPv6 IP | ens5 IPv4 prefix | ens5 IPv6 prefix | | --------- | ---------------------- | ----------------------- | ------------------- | ----------------------------- | | master | 172 .31.22.228 | 2406 :da1e:c4:ed01::10 | 172 .31.28.16/28 | 2406 :da1e:c4:ed01:c57d::/80 | | worker1 | 180 .17.16.17 | 2406 :da1e:c4:ed02::10 | 172 .31.32.176/28 | 2406 :da1e:c4:ed02:7a2e::/80 | \u521b\u5efa AWS NAT \u7f51\u5173\uff0cAWS \u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u79c1\u6709\u5b50\u7f51\u4e2d\u7684\u5b9e\u4f8b\u8fde\u63a5\u5230 VPC \u5916\u90e8\u7684\u670d\u52a1\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u5728\u4e0a\u8ff0\u7684\u516c\u6709\u5b50\u7f51 public-172-31-0-0 \u4e0b\u521b\u5efa NAT \u7f51\u5173\uff0c\u5e76\u4e3a\u79c1\u6709\u5b50\u7f51\u7684\u8def\u7531\u8868\u914d\u7f6e 0.0.0.0/0 \u51fa\u53e3\u8def\u7531\u7684\u4e0b\u4e00\u8df3\u4e3a\u8be5 NAT \u7f51\u5173\u3002(\u6ce8\u610f IPv6 \u662f\u7531 AWS \u5206\u914d\u7684\u5168\u5c40\u552f\u4e00\u7684\u5730\u5740\uff0c\u53ef\u76f4\u63a5\u501f\u52a9 Internet Gateway \u8bbf\u95ee\u4e92\u8054\u7f51) \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a","title":"AWS \u73af\u5883"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#spiderpool_1","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-ens5\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u82e5\u76ee\u5f55\u4e0b\u4e0d\u5b58\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u6839\u636e\u524d\u9762\u521b\u5efa AWS EC2 \u5b9e\u4f8b\u865a\u62df\u673a\u8fc7\u7a0b\u4e2d\u521b\u5efa\u7684\u7f51\u5361\u60c5\u51b5\uff0c\u4e3a\u865a\u62df\u673a\u7684\u6bcf\u4e2a\u7528\u4e8e\u8fd0\u884c ipvlan CNI \u7684\u7f51\u5361\u521b\u5efa\u5982\u4e0b SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE = \"ens5\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e00\u4e2a ipvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth5 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#ip","text":"Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u8be5\u5b57\u6bb5\u9650\u5236\u5f53\u524d SpiderIPPool\u8d44\u6e90\u4ec5\u9002\u7528\u4e8e\u54ea\u4e9b\u8282\u70b9\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53\u8be5\u5b57\u6bb5\u4e3a\u7a7a\u65f6\uff0c\u8868\u660e\u5f53\u524d Spiderpool \u8d44\u6e90\u9002\u7528\u4e8e\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u6839\u636e\u4e0a\u6587 AWS EC2 \u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca IP \u524d\u7f00\u59d4\u6258\u5730\u5740\u7b49\u4fe1\u606f\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u6240\u5c5e\u7684\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u4e14\u5bf9\u5e94\u552f\u4e00\u7684\u4e00\u5f20\u5b9e\u4f8b\u7f51\u5361\u3002 \u7ed3\u5408\u4e0a\u6587 AWS \u73af\u5883 \u6bcf\u53f0\u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca\u5bf9\u5e94\u7684 IP \u524d\u7f00\u59d4\u6258\u5730\u5740\u4fe1\u606f\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u7f51\u5361 ens5 \u5206\u522b\u521b\u5efa\u4e86 IPv4 \u548c IPv6 \u7684 SpiderIPPool \u5b9e\u4f8b\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4 spec: subnet: 172.31.16.0/20 ips: - 172.31.28.16-172.31.28.31 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v6 spec: subnet: 2406:da1e:c4:ed01::/64 ips: - 2406:da1e:c4:ed01:c57d::0-2406:da1e:c4:ed01:c57d::f gateway: 2406:da1e:c4:ed01::1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4 spec: subnet: 172.31.32.0/24 ips: - 172.31.32.176-172.31.32.191 gateway: 172.31.32.1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v6 spec: subnet: 2406:da1e:c4:ed02::/64 ips: - 2406:da1e:c4:ed02:7a2e::0-2406:da1e:c4:ed02:7a2e::f gateway: 2406:da1e:c4:ed02::1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] EOF","title":"\u521b\u5efa IP \u6c60"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_5","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u7684 CNI \u914d\u7f6e\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u9009\u62e9\u4f7f\u7528\u5bf9\u5e94\u4e8e\u5bbf\u4e3b\u673a ens5 \u7684 ipvlan \u914d\u7f6e\uff0c\u5e76\u6839\u636e\u6211\u4eec\u7684\u7f3a\u7701 SpiderIPPool \u8d44\u6e90\u9ed8\u8ba4\u6311\u9009\u5176\u5bf9\u5e94\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb spec: selector: matchLabels: run: nginx-lb replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-lb spec: containers: - name: nginx-lb image: nginx ports: - containerPort: 80 EOF \u67e5\u770b Pod \u7684\u8fd0\u884c\u72b6\u6001\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0c\u6211\u4eec\u4e24\u4e2a\u8282\u70b9\u4e0a\u90fd\u8fd0\u884c\u4e86 1 \u4e2a Pod \u4e14\u4f7f\u7528\u7684 IP \u90fd\u5bf9\u5e94\u5bbf\u4e3b\u673a\u7684\u7b2c\u4e00\u5f20\u7f51\u5361\u7684 IP \u524d\u7f00\u59d4\u6258\u5730\u5740: ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-64fbbb5fd8-q5wjm 1 /1 Running 0 10s 172 .31.32.184 worker1 <none> <none> nginx-lb-64fbbb5fd8-wkzf6 1 /1 Running 0 10s 172 .31.28.31 master <none> <none>","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_6","text":"\u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a export NODE_MASTER_IP=172.31.18.11 export NODE_WORKER1_IP=172.31.32.18 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 ${NODE_MASTER_IP} ~# kubectl exec -it nginx-lb-64fbbb5fd8-q5wjm -- ping -c 1 ${NODE_WORKER1_IP} \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 172.31.32.184 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping6 -c 1 2406:da1e:c4:ed02:7a2e::d \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I ${CLUSTER_IP}","title":"\u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_7","text":"","title":"\u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#pod","text":"\u501f\u52a9\u4e0a\u6587\u6211\u4eec\u521b\u5efa\u7684 AWS NAT \u7f51\u5173 \uff0c\u6211\u4eec\u7684 VPC \u79c1\u7f51\u5df2\u53ef\u5b9e\u73b0\u8bbf\u95ee\u4e92\u8054\u7f51\u3002 kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I www.baidu.com","title":"\u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_8","text":"","title":"\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee(\u53ef\u9009)"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#aws-load-balancer-controller","text":"AWS \u57fa\u7840\u4ea7\u54c1 \u8d1f\u8f7d\u5747\u8861 \u62e5\u6709 NLB (Network Load Balancer) \u548c ALB(Application Load Balancer) \u4e24\u79cd\u6a21\u5f0f\u5206\u522b\u5bf9\u5e94 Layer4 \u4e0e Layer7\u3002 aws-load-balancer-controller \u662f AWS \u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e AWS \u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u53ef\u5b9e\u73b0 kubernetes Service LoadBalancer \u548c Ingress \u529f\u80fd\u3002\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408 AWS \u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5\u6f14\u793a\uff0c \u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e aws-load-balancer-controller \u6587\u6863 \u5b8c\u6210 aws-load-balancer-controller \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u4e3a Kubernetes \u4e0a\u7684\u6bcf\u4e2a Node \u8bbe\u7f6e\u4e0a providerID \uff0c\u60a8\u53ef\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0: - \u53ef\u76f4\u63a5\u5728 AWS EC2 dashboard \u4e2d\u627e\u5230\u5b9e\u4f8b\u7684Instance ID. - \u4f7f\u7528 AWS CLI \u6765\u67e5\u8be2Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . \u4e3a AWS EC2 \u5b9e\u4f8b\u6240\u4f7f\u7528\u7684 IAM role \u8865\u5145 policy \u4ecb\u4e8e aws-load-balancer-controller \u8fd0\u884c\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u4e14\u9700\u8981\u8bbf\u95ee AWS \u7684 NLB/ALB APIs\uff0c\u56e0\u6b64\u9700\u8981 AWS IAM \u5173\u4e8e NLB/ALB \u76f8\u5173\u8bf7\u6c42\u7684\u6388\u6743\u3002\u53c8\u56e0\u6211\u4eec\u662f\u81ea\u5efa\u96c6\u7fa4\uff0c\u6211\u4eec\u9700\u8981\u501f\u7528\u8282\u70b9\u81ea\u8eab\u7684 IAM Role \u6765\u5b9e\u73b0\u6388\u6743\uff0c\u8be6\u60c5\u53ef\u770b aws-load-balancer-controller IAM \u3002 curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json \u4f7f\u7528\u5982\u4e0a\u83b7\u53d6\u7684 json \u5185\u5bb9\uff0c\u5728 AWS IAM Dashboard \u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684policy\uff0c\u5e76\u5c06\u8be5 policy \u4e0e\u60a8\u5f53\u524d\u865a\u62df\u673a\u5b9e\u4f8b\u7684 IAM Role \u8fdb\u884c\u5173\u8054\u3002 \u4e3a\u60a8 AWS EC2 \u5b9e\u4f8b\u6240\u5728\u7684\u53ef\u7528\u533a\u521b\u5efa\u4e00\u4e2a public subnet \u5e76\u6253\u4e0a\u53ef\u81ea\u52a8\u53d1\u73b0\u7684 tag. ALB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 2 \u4e2a\u8de8\u53ef\u7528\u533a\u7684\u5b50\u7f51\uff0c\u5bf9\u4e8e NLB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 1 \u4e2a\u5b50\u7f51\u3002\u8be6\u60c5\u8bf7\u770b \u5b50\u7f51\u81ea\u52a8\u53d1\u73b0 \u3002 \u5bf9\u4e8e\u516c\u7f51\u8bbf\u95ee\u7684 LB\uff0c\u60a8\u9700\u8981\u4e3a\u5b9e\u4f8b\u6240\u5728\u53ef\u7528\u533a\u7684 public subnet \u6253\u4e0a tag: kubernetes.io/role/elb:1 \uff0c\u5bf9\u4e8e VPC \u95f4\u8bbf\u95ee\u7684 LB\uff0c\u8bf7\u521b\u5efa private subnet \u5e76\u6253\u4e0a tag: kubernetes.io/role/internal-elb:1 \uff0c\u8bf7\u7ed3\u5408 AWS \u73af\u5883 \u6765\u521b\u5efa\u6240\u9700\u7684\u5b50\u7f51\uff1a \u9488\u5bf9\u56e0\u7279\u7f51\u66b4\u9732\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa public subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u6211\u4eec\u7684 Main \u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002(\u6ce8\u610f Main \u8def\u7531\u8868\u7684 0.0.0.0/0 \u8def\u7531\u7684\u4e0b\u4e00\u8df3\u9ed8\u8ba4\u4e3a Internet \u7f51\u5173\uff0c\u82e5\u4e22\u5931\u8bf7\u81ea\u884c\u521b\u5efa\u8be5\u8def\u7531\u89c4\u5219)\u3002 \u5728 AWS VPC Dashboard Route tables \u680f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8def\u7531\u8868\u5e76\u914d\u7f6e 0.0.0.0/0 \u7684\u8def\u7531\u4e0b\u4e00\u8df3\u4e3a NAT \u7f51\u5173\uff0c::/0 \u8def\u7531\u4e0b\u4e00\u8df3\u4e3a Internet \u7f51\u5173\u3002 \u9488\u5bf9 VPC \u95f4\u8bbf\u95ee\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa private subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002 \u4f7f\u7528 helm \u5b89\u88c5aws-load-balancer-controller(\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5) helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> \u68c0\u67e5 aws-load-balancer-controller \u5b89\u88c5\u5b8c\u6210 ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s","title":"\u90e8\u7f72 AWS Load Balancer Controller"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#loadbalancer","text":"\u4e0a\u6587\u4e2d\u5df2\u521b\u5efa \u5e94\u7528 , \u73b0\u5728\u6211\u4eec\u4e3a\u5b83\u521b\u5efa\u4e00\u4e2a kubernetes Service LoadBalancer \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack \u6ce8\u89e3): cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb labels: run: nginx-lb annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb EOF \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a NLB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 NLB \u8fd8\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/load-balancer-source-ranges \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90 IP\u3002\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 \u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-scheme \u9009\u62e9\u6b64 NLB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true \u63d0\u4f9b\u4e86\u5ba2\u6237\u7aef\u6e90 IP \u4fdd\u7559\u529f\u80fd\u3002","title":"\u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#ingress","text":"\u63a5\u4e0b\u6765\u6211\u4eec\u521b\u5efa\u4e00\u4e2akubernetes Ingress \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 alb.ingress.kubernetes.io/ip-address-type: dualstack \u6ce8\u89e3): apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a ALB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 ALB \u4e5f\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 alb.ingress.kubernetes.io/target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u4f7f\u7528 ALB \u7684 instance \u6a21\u5f0f\u9700\u8981\u6307\u5b9a service \u4e3a NodePort \u6a21\u5f0f\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/inbound-cidrs \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90IP\u3002(\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 alb.ingress.kubernetes.io/ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 )\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/scheme \u9009\u62e9\u6b64 ALB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u82e5\u60f3\u6574\u5408\u591a\u4e2a Ingress \u8d44\u6e90\u5171\u4eab\u540c\u4e00\u4e2a\u5165\u53e3\uff0c\u53ef\u914d\u7f6e\u6ce8\u89e3 alb.ingress.kubernetes.io/group.name \u6765\u663e\u793a\u6307\u5b9a\u4e00\u4e2a\u540d\u5b57\u3002\uff08\u6ce8\u610f\uff0c\u9ed8\u8ba4\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u7684 Ingresses \u8d44\u6e90\u5e76\u4e0d\u5c5e\u4e8e\u4efb\u4f55 IngressGroup\uff0c\u7cfb\u7edf\u4f1a\u5c06\u5176\u89c6\u4e3a\u7531 Ingress \u672c\u8eab\u7ec4\u6210\u7684 \"\u9690\u5f0f IngressGroup\"\uff09 \u5982\u679c\u60f3\u6307\u5b9a Ingress \u7684 host\uff0c\u9700\u8981\u642d\u914d externalDNS \u4f7f\u7528\u3002\u8be6\u60c5\u8bf7\u67e5\u770b \u914d\u7f6e externalDNS \u3002","title":"\u4e3a\u5e94\u7528\u521b\u5efa Ingress \u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-aws/","text":"Running On AWS English | \u7b80\u4f53\u4e2d\u6587 Introduction With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized underlay solutions for public clouds. This page introduces Spiderpool , an underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios. Why Spiderpool Spiderpool is an underlay and RDMA network solution for the Kubernetes. It enhances the capabilities of Macvlan CNI, IPvlan CNI, SR-IOV CNI fulfills various networking needs, and supports to run on bare metal, virtual machine, and public cloud environments . Spiderpool delivers exceptional network performance. aws-vpc-cni Networking plugin for pod networking in Kubernetes using Elastic Network Interfaces on AWS. aws-vpc-cni is an underlay network solution provided by AWS for public cloud, but it cannot meet complex network requirements. The following is a comparison of some functions between spiderpool and aws-cni. The related functions of Spiderpool will be demonstrated in subsequent chapters\uff1a Feature comparison aws-vpc-cni Spiderpool + IPvlan Multiple Underlay NICs \u274c \u2705 (Multiple Underlay NICs across subnets) Custom routing \u274c \u2705 route Dual CNI collaboration Supports multiple CNI NIC but does not support routing coordination \u2705 support RDMA network policy \u2705 aws-network-policy-agent \u2705 cilium-chaining clusterIP \u2705 (kube-proxy) \u2705 (kube-proxy and ebpf two methods) Bandwidth \u274c \u2705 Bandwidth management metrics \u2705 \u2705 Dual stack IPv4 only, IPv6 only, dual stack is not supported IPv4 only, IPv6 only, dual stack Observability \u274c \u2705(with cilium hubble, kernel>=4.19.57) Multi-cluster \u274c \u2705 Submariner Paired with AWS layer 4/7 load balancing \u2705 \u2705 Kernel limit None >= 4.2 (IPvlan kernel limit) Forwarding principle underlay pure routing layer 3 forwarding IPvlan layer 2 multicast \u274c \u2705 Cross vpc access \u2705 \u2705 Features Spiderpool can operate in public cloud environments using the ipvlan underlay CNI and provide features such as node topology and MAC address validity resolution. Here is how it works: When using underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and ipvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with ipvlan CNI . The ipvlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating ipvlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved. Prerequisites System requirements The system kernel version must be greater than 4.2 when using ipvlan as the cluster's CNI. Helm is installed. Understand the basics of AWS VPC Public and Private Subnets . In an AWS VPC, a subnet is categorized as a public subnet if it has an outbound route configured with the Internet Gateway as the next hop for destinations 0.0.0.0/0 or ::/0. Otherwise, a subnet is considered a private subnet if it lacks this specific outbound routing configuration. Steps AWS Environment Create a public subnet and multiple private subnets within a VPC, and deploy virtual machines in the private subnets as shown in the following picture: We will create one public subnet and two private subnets within the same VPC. Each private subnet should be deployed in a different availability zone. A EC2 instance as a jump server will be created in the public subnet for secure access. Additionally, two AWS EC2 instances will be created in the respective different private subnets to set up the Kubernetes cluster. Bind IPv4 and IPv6 addresses to the network interface when creating an instance, as the picture below: Bind IP prefix delegation to each network interface of the instances in which we can use it to allocate IP address for pod: IP prefix delegation just like the secondary IP address that could bind a CIDR range for instance. The number of IP prefix delegation can be referenced from AWS EC2 instance specifications . The instance can bind the same number of prefix delegations as the number of secondary IPs that can be bound to the instance's network interface. In this example, we choose to bind 1 network interface and 1 IP prefix delegation to the instance. | Node | ens5 primary IP | ens5 secondary IPs | ens6 primary IP | ens6 secondary IPs | | --------- | ----------------- | --------------------------- | ----------------- | --------------------------- | | master | 172 .31.22.228 | 172 .31.16.4-172.31.16.8 | 210 .22.16.10 | 210 .22.16.11-210.22.16.15 | | worker1 | 180 .17.16.17 | 180 .17.16.11-180.17.16.15 | 210 .22.32.10 | 210 .22.32.11-210.22.32.15 | Create an AWS NAT gateway to allow instances in the VPC's private subnets to connect to external services. The NAT gateway serves as an outbound traffic gateway for the cluster. Follow the NAT gateway documentation to create a NAT gateway: Create a NAT gateway in the public subnet, public-172-31-0-0 , and configure the route table of the private subnets to set the next-hop of the outbound route 0.0.0.0/0 to NAT gateway. (IPv6 addresses provided by AWS are globally unique and can access the internet directly via the Internet Gateway). Use the configured virtual machines to establish a Kubernetes cluster. The available IP addresses for the nodes and the network topology diagram of the cluster are shown below: Install Spiderpool Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-ens5\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Based on the network interface configuration created during the process of setting up the AWS EC2 instances, here is an example configuration of SpiderMultusConfig for each network interface used to run ipvlan CNI: IPVLAN_MASTER_INTERFACE = \"ens5\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF This case uses the given configuration to create one ipvlan SpiderMultusConfig instances. This resource will automatically generate corresponding Multus NetworkAttachmentDefinition CR for the host's eth5 network interface. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d Create IP Pools The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips : based on the information provided about the network interfaces and IP prefix delegation addresses of the AWS EC2 instances, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . Each value should correspond to a unique instance network interface. Taking into account the network interfaces and associated IP prefix delegation information for each instance in the AWS environment , the following YAML is used to create IPv4 and IPv6 SpiderIPPool resources for network interface ens5 on each node. These pools will provide IP addresses for Pods on different nodes: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4 spec: subnet: 172.31.16.0/20 ips: - 172.31.28.16-172.31.28.31 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v6 spec: subnet: 2406:da1e:c4:ed01::/64 ips: - 2406:da1e:c4:ed01:c57d::0-2406:da1e:c4:ed01:c57d::f gateway: 2406:da1e:c4:ed01::1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4 spec: subnet: 172.31.32.0/24 ips: - 172.31.32.176-172.31.32.191 gateway: 172.31.32.1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v6 spec: subnet: 2406:da1e:c4:ed02::/64 ips: - 2406:da1e:c4:ed02:7a2e::0-2406:da1e:c4:ed02:7a2e::f gateway: 2406:da1e:c4:ed02::1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] EOF Create Applications The following YAML example creates a Deployment application with the following configuration: v1.multus-cni.io/default-network : specify the CNI configuration for the application. In this example, the application is configured to use the ipvlan configuration associated with the ens5 interface of the host machine. The subnet is selected automatically according to the default SpiderIPPool resource. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb spec: selector: matchLabels: run: nginx-lb replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-lb spec: containers: - name: nginx-lb image: nginx ports: - containerPort: 80 EOF By checking the running status of the Pods, you can observe that one Pod is running on each node, and the Pods are assigned the IP prefix delegation addresses of the first network interface of their respective host machines: ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-64fbbb5fd8-q5wjm 1 /1 Running 0 10s 172 .31.32.184 worker1 <none> <none> nginx-lb-64fbbb5fd8-wkzf6 1 /1 Running 0 10s 172 .31.28.31 master <none> <none> Test East-West Connectivity Test communication between Pods and their hosts: export NODE_MASTER_IP=172.31.18.11 export NODE_WORKER1_IP=172.31.32.18 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 ${NODE_MASTER_IP} ~# kubectl exec -it nginx-lb-64fbbb5fd8-q5wjm -- ping -c 1 ${NODE_WORKER1_IP} Test communication between Pods across different nodes and subnets: ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 172.31.32.184 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping6 -c 1 2406:da1e:c4:ed02:7a2e::d Test communication between Pods and ClusterIP: ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I ${CLUSTER_IP} Test North-South Connectivity Test egress traffic from Pods to external destinations With the AWS NAT gateway created in the previous section, our VPC's private network can now be accessed from the internet. kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I www.baidu.com Load Balancer Ingress Access (Optional) Deploy AWS Load Balancer Controller The AWS Load Balancer product offers two modes: NLB (Network Load Balancer) and ALB (Application Load Balancer), corresponding to Layer 4 and Layer 7, respectively. The aws-load-balancer-controller is an AWS-provided component that integrates Kubernetes with AWS Load Balancer, enabling Kubernetes Service LoadBalancer and Ingress functionality. We will use this component to facilitate load balancing ingress access with AWS infrastructure. The installation demo is based on version v2.6 . You can follow the steps below and refer to the aws-load-balancer-controller documentation for aws-load-balancer-controller deployment: Configure providerID for cluster nodes. It is necessary to set the providerID for each Node in Kubernetes. You can achieve this in either of the following ways: Find the Instance ID for each instance directly in the AWS EC2 dashboard. Use the AWS CLI to query the Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . Add necessary IAM role policy for AWS EC2 instances The aws-load-balancer-controller runs on each node and requires access to AWS NLB/ALB APIs. Therefore, it needs authorization to make requests related to NLB/ALB through AWS IAM. As we are deploying a self-managed cluster, we need to leverage the IAM Role of the nodes themselves to grant this authorization. For more details, refer to the aws-load-balancer-controller IAM . curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json Create a new policy in the AWS IAM Dashboard by using the obtained JSON content and associate it with the IAM Role of your virtual machine instance. Create a public subnet for the availability zone where your AWS EC2 instances are located and apply an auto-discoverable tag. For ALB, you need at least two subnets across different availability zones. For NLB, at least one subnet is required. Refer to the Subnet Discovery for more details. To enable LB with public access, add the kubernetes.io/role/elb:1 tag to the public subnet in the availability zone where the instances reside. Regarding cross-VPC access for LB, create a private subnet and apply the kubernetes.io/role/internal-elb:1 tag. Use the AWS environment to create the necessary subnets: To create a public subnet for an internet-exposed load balancer, go to the AWS VPC Dashboard, select \"Create subnet\" in the Subnets section, and choose the same availability zone as the EC2 instance, and associate the subnet with the Main route table (make sure the default 0.0.0.0/0 route in the Main route table has the Internet Gateway as the next hop; if not, create this route rule). Create a new route table in the AWS VPC Dashboard and configure the 0.0.0.0/0 route with the NAT Gateway as the next hop, and the ::/0 route with the Internet Gateway as the next hop. To create a private subnet for LB with cross-VPC access, go to the AWS VPC Dashboard Subnets section, select \"Create subnet,\" choose the same availability zone as the EC2 instance, and associate it with the route table created in the previous step. Install aws-load-balancer-controller v2.6 using Helm. helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> Check if aws-load-balancer-controller has been installed already ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s Create a LoadBalancer for Application Access To provide access to the application created in the previous section Create Applications , we will create a Kubernetes Service of type LoadBalancer. If you have a dual-stack requirement, add the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack : cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb labels: run: nginx-lb annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb EOF As shown in the AWS EC2 Load Balancing dashboard, an NLB has been created and is accessible. NLB also supports creating LB in instance mode by just modifying the annotation service.beta.kubernetes.io/aws-load-balancer-nlb-target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. Use the annotation service.beta.kubernetes.io/load-balancer-source-ranges to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation service.beta.kubernetes.io/aws-load-balancer-scheme to specify whether the NLB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. The annotation service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true enables client source IP preservation ability. Create an Ingress for Application Access Next, we will create a Kubernetes Ingress resource. If you have a dual-stack requirement, add the annotation alb.ingress.kubernetes.io/ip-address-type: dualstack : apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 As shown in the AWS EC2 Load Balancing dashboard, an ALB has been created and is accessible. ALB also supports creating LB in instance mode by just modifying the annotation alb.ingress.kubernetes.io/target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. When using ALB in instance mode, specify the service as NodePort mode. Use the annotation alb.ingress.kubernetes.io/inbound-cidrs to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation alb.ingress.kubernetes.io/ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation alb.ingress.kubernetes.io/scheme to specify whether the ALB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. To integrate multiple Ingress resources and share the same entry point, configure the annotation alb.ingress.kubernetes.io/group.name to specify a name. Ingress resources without this annotation are treated as an \"implicit IngressGroup\" composed by the Ingress itself. To specify the host for the Ingress, refer to Configuring externalDNS to enable it.","title":"AWS Cloud"},{"location":"usage/install/cloud/get-started-aws/#running-on-aws","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On AWS"},{"location":"usage/install/cloud/get-started-aws/#introduction","text":"With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized underlay solutions for public clouds. This page introduces Spiderpool , an underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios.","title":"Introduction"},{"location":"usage/install/cloud/get-started-aws/#why-spiderpool","text":"Spiderpool is an underlay and RDMA network solution for the Kubernetes. It enhances the capabilities of Macvlan CNI, IPvlan CNI, SR-IOV CNI fulfills various networking needs, and supports to run on bare metal, virtual machine, and public cloud environments . Spiderpool delivers exceptional network performance. aws-vpc-cni Networking plugin for pod networking in Kubernetes using Elastic Network Interfaces on AWS. aws-vpc-cni is an underlay network solution provided by AWS for public cloud, but it cannot meet complex network requirements. The following is a comparison of some functions between spiderpool and aws-cni. The related functions of Spiderpool will be demonstrated in subsequent chapters\uff1a Feature comparison aws-vpc-cni Spiderpool + IPvlan Multiple Underlay NICs \u274c \u2705 (Multiple Underlay NICs across subnets) Custom routing \u274c \u2705 route Dual CNI collaboration Supports multiple CNI NIC but does not support routing coordination \u2705 support RDMA network policy \u2705 aws-network-policy-agent \u2705 cilium-chaining clusterIP \u2705 (kube-proxy) \u2705 (kube-proxy and ebpf two methods) Bandwidth \u274c \u2705 Bandwidth management metrics \u2705 \u2705 Dual stack IPv4 only, IPv6 only, dual stack is not supported IPv4 only, IPv6 only, dual stack Observability \u274c \u2705(with cilium hubble, kernel>=4.19.57) Multi-cluster \u274c \u2705 Submariner Paired with AWS layer 4/7 load balancing \u2705 \u2705 Kernel limit None >= 4.2 (IPvlan kernel limit) Forwarding principle underlay pure routing layer 3 forwarding IPvlan layer 2 multicast \u274c \u2705 Cross vpc access \u2705 \u2705","title":"Why Spiderpool"},{"location":"usage/install/cloud/get-started-aws/#features","text":"Spiderpool can operate in public cloud environments using the ipvlan underlay CNI and provide features such as node topology and MAC address validity resolution. Here is how it works: When using underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and ipvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with ipvlan CNI . The ipvlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating ipvlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved.","title":"Features"},{"location":"usage/install/cloud/get-started-aws/#prerequisites","text":"System requirements The system kernel version must be greater than 4.2 when using ipvlan as the cluster's CNI. Helm is installed. Understand the basics of AWS VPC Public and Private Subnets . In an AWS VPC, a subnet is categorized as a public subnet if it has an outbound route configured with the Internet Gateway as the next hop for destinations 0.0.0.0/0 or ::/0. Otherwise, a subnet is considered a private subnet if it lacks this specific outbound routing configuration.","title":"Prerequisites"},{"location":"usage/install/cloud/get-started-aws/#steps","text":"","title":"Steps"},{"location":"usage/install/cloud/get-started-aws/#aws-environment","text":"Create a public subnet and multiple private subnets within a VPC, and deploy virtual machines in the private subnets as shown in the following picture: We will create one public subnet and two private subnets within the same VPC. Each private subnet should be deployed in a different availability zone. A EC2 instance as a jump server will be created in the public subnet for secure access. Additionally, two AWS EC2 instances will be created in the respective different private subnets to set up the Kubernetes cluster. Bind IPv4 and IPv6 addresses to the network interface when creating an instance, as the picture below: Bind IP prefix delegation to each network interface of the instances in which we can use it to allocate IP address for pod: IP prefix delegation just like the secondary IP address that could bind a CIDR range for instance. The number of IP prefix delegation can be referenced from AWS EC2 instance specifications . The instance can bind the same number of prefix delegations as the number of secondary IPs that can be bound to the instance's network interface. In this example, we choose to bind 1 network interface and 1 IP prefix delegation to the instance. | Node | ens5 primary IP | ens5 secondary IPs | ens6 primary IP | ens6 secondary IPs | | --------- | ----------------- | --------------------------- | ----------------- | --------------------------- | | master | 172 .31.22.228 | 172 .31.16.4-172.31.16.8 | 210 .22.16.10 | 210 .22.16.11-210.22.16.15 | | worker1 | 180 .17.16.17 | 180 .17.16.11-180.17.16.15 | 210 .22.32.10 | 210 .22.32.11-210.22.32.15 | Create an AWS NAT gateway to allow instances in the VPC's private subnets to connect to external services. The NAT gateway serves as an outbound traffic gateway for the cluster. Follow the NAT gateway documentation to create a NAT gateway: Create a NAT gateway in the public subnet, public-172-31-0-0 , and configure the route table of the private subnets to set the next-hop of the outbound route 0.0.0.0/0 to NAT gateway. (IPv6 addresses provided by AWS are globally unique and can access the internet directly via the Internet Gateway). Use the configured virtual machines to establish a Kubernetes cluster. The available IP addresses for the nodes and the network topology diagram of the cluster are shown below:","title":"AWS Environment"},{"location":"usage/install/cloud/get-started-aws/#install-spiderpool","text":"Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-ens5\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus.","title":"Install Spiderpool"},{"location":"usage/install/cloud/get-started-aws/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Based on the network interface configuration created during the process of setting up the AWS EC2 instances, here is an example configuration of SpiderMultusConfig for each network interface used to run ipvlan CNI: IPVLAN_MASTER_INTERFACE = \"ens5\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF This case uses the given configuration to create one ipvlan SpiderMultusConfig instances. This resource will automatically generate corresponding Multus NetworkAttachmentDefinition CR for the host's eth5 network interface. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE kube-system ipvlan-ens5 8d","title":"Install CNI"},{"location":"usage/install/cloud/get-started-aws/#create-ip-pools","text":"The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips : based on the information provided about the network interfaces and IP prefix delegation addresses of the AWS EC2 instances, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . Each value should correspond to a unique instance network interface. Taking into account the network interfaces and associated IP prefix delegation information for each instance in the AWS environment , the following YAML is used to create IPv4 and IPv6 SpiderIPPool resources for network interface ens5 on each node. These pools will provide IP addresses for Pods on different nodes: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4 spec: subnet: 172.31.16.0/20 ips: - 172.31.28.16-172.31.28.31 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v6 spec: subnet: 2406:da1e:c4:ed01::/64 ips: - 2406:da1e:c4:ed01:c57d::0-2406:da1e:c4:ed01:c57d::f gateway: 2406:da1e:c4:ed01::1 default: true nodeName: [\"master\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4 spec: subnet: 172.31.32.0/24 ips: - 172.31.32.176-172.31.32.191 gateway: 172.31.32.1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v6 spec: subnet: 2406:da1e:c4:ed02::/64 ips: - 2406:da1e:c4:ed02:7a2e::0-2406:da1e:c4:ed02:7a2e::f gateway: 2406:da1e:c4:ed02::1 default: true nodeName: [\"worker1\"] multusName: [\"kube-system/ipvlan-ens5\"] EOF","title":"Create IP Pools"},{"location":"usage/install/cloud/get-started-aws/#create-applications","text":"The following YAML example creates a Deployment application with the following configuration: v1.multus-cni.io/default-network : specify the CNI configuration for the application. In this example, the application is configured to use the ipvlan configuration associated with the ens5 interface of the host machine. The subnet is selected automatically according to the default SpiderIPPool resource. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb spec: selector: matchLabels: run: nginx-lb replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-lb spec: containers: - name: nginx-lb image: nginx ports: - containerPort: 80 EOF By checking the running status of the Pods, you can observe that one Pod is running on each node, and the Pods are assigned the IP prefix delegation addresses of the first network interface of their respective host machines: ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-64fbbb5fd8-q5wjm 1 /1 Running 0 10s 172 .31.32.184 worker1 <none> <none> nginx-lb-64fbbb5fd8-wkzf6 1 /1 Running 0 10s 172 .31.28.31 master <none> <none>","title":"Create Applications"},{"location":"usage/install/cloud/get-started-aws/#test-east-west-connectivity","text":"Test communication between Pods and their hosts: export NODE_MASTER_IP=172.31.18.11 export NODE_WORKER1_IP=172.31.32.18 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 ${NODE_MASTER_IP} ~# kubectl exec -it nginx-lb-64fbbb5fd8-q5wjm -- ping -c 1 ${NODE_WORKER1_IP} Test communication between Pods across different nodes and subnets: ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping -c 1 172.31.32.184 ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- ping6 -c 1 2406:da1e:c4:ed02:7a2e::d Test communication between Pods and ClusterIP: ~# kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I ${CLUSTER_IP}","title":"Test East-West Connectivity"},{"location":"usage/install/cloud/get-started-aws/#test-north-south-connectivity","text":"","title":"Test North-South Connectivity"},{"location":"usage/install/cloud/get-started-aws/#test-egress-traffic-from-pods-to-external-destinations","text":"With the AWS NAT gateway created in the previous section, our VPC's private network can now be accessed from the internet. kubectl exec -it nginx-lb-64fbbb5fd8-wkzf6 -- curl -I www.baidu.com","title":"Test egress traffic from Pods to external destinations"},{"location":"usage/install/cloud/get-started-aws/#load-balancer-ingress-access-optional","text":"","title":"Load Balancer Ingress Access (Optional)"},{"location":"usage/install/cloud/get-started-aws/#deploy-aws-load-balancer-controller","text":"The AWS Load Balancer product offers two modes: NLB (Network Load Balancer) and ALB (Application Load Balancer), corresponding to Layer 4 and Layer 7, respectively. The aws-load-balancer-controller is an AWS-provided component that integrates Kubernetes with AWS Load Balancer, enabling Kubernetes Service LoadBalancer and Ingress functionality. We will use this component to facilitate load balancing ingress access with AWS infrastructure. The installation demo is based on version v2.6 . You can follow the steps below and refer to the aws-load-balancer-controller documentation for aws-load-balancer-controller deployment: Configure providerID for cluster nodes. It is necessary to set the providerID for each Node in Kubernetes. You can achieve this in either of the following ways: Find the Instance ID for each instance directly in the AWS EC2 dashboard. Use the AWS CLI to query the Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . Add necessary IAM role policy for AWS EC2 instances The aws-load-balancer-controller runs on each node and requires access to AWS NLB/ALB APIs. Therefore, it needs authorization to make requests related to NLB/ALB through AWS IAM. As we are deploying a self-managed cluster, we need to leverage the IAM Role of the nodes themselves to grant this authorization. For more details, refer to the aws-load-balancer-controller IAM . curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json Create a new policy in the AWS IAM Dashboard by using the obtained JSON content and associate it with the IAM Role of your virtual machine instance. Create a public subnet for the availability zone where your AWS EC2 instances are located and apply an auto-discoverable tag. For ALB, you need at least two subnets across different availability zones. For NLB, at least one subnet is required. Refer to the Subnet Discovery for more details. To enable LB with public access, add the kubernetes.io/role/elb:1 tag to the public subnet in the availability zone where the instances reside. Regarding cross-VPC access for LB, create a private subnet and apply the kubernetes.io/role/internal-elb:1 tag. Use the AWS environment to create the necessary subnets: To create a public subnet for an internet-exposed load balancer, go to the AWS VPC Dashboard, select \"Create subnet\" in the Subnets section, and choose the same availability zone as the EC2 instance, and associate the subnet with the Main route table (make sure the default 0.0.0.0/0 route in the Main route table has the Internet Gateway as the next hop; if not, create this route rule). Create a new route table in the AWS VPC Dashboard and configure the 0.0.0.0/0 route with the NAT Gateway as the next hop, and the ::/0 route with the Internet Gateway as the next hop. To create a private subnet for LB with cross-VPC access, go to the AWS VPC Dashboard Subnets section, select \"Create subnet,\" choose the same availability zone as the EC2 instance, and associate it with the route table created in the previous step. Install aws-load-balancer-controller v2.6 using Helm. helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> Check if aws-load-balancer-controller has been installed already ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s","title":"Deploy AWS Load Balancer Controller"},{"location":"usage/install/cloud/get-started-aws/#create-a-loadbalancer-for-application-access","text":"To provide access to the application created in the previous section Create Applications , we will create a Kubernetes Service of type LoadBalancer. If you have a dual-stack requirement, add the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack : cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb labels: run: nginx-lb annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb EOF As shown in the AWS EC2 Load Balancing dashboard, an NLB has been created and is accessible. NLB also supports creating LB in instance mode by just modifying the annotation service.beta.kubernetes.io/aws-load-balancer-nlb-target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. Use the annotation service.beta.kubernetes.io/load-balancer-source-ranges to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation service.beta.kubernetes.io/aws-load-balancer-scheme to specify whether the NLB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. The annotation service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true enables client source IP preservation ability.","title":"Create a LoadBalancer for Application Access"},{"location":"usage/install/cloud/get-started-aws/#create-an-ingress-for-application-access","text":"Next, we will create a Kubernetes Ingress resource. If you have a dual-stack requirement, add the annotation alb.ingress.kubernetes.io/ip-address-type: dualstack : apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"kube-system/ipvlan-ens5\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 As shown in the AWS EC2 Load Balancing dashboard, an ALB has been created and is accessible. ALB also supports creating LB in instance mode by just modifying the annotation alb.ingress.kubernetes.io/target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. When using ALB in instance mode, specify the service as NodePort mode. Use the annotation alb.ingress.kubernetes.io/inbound-cidrs to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation alb.ingress.kubernetes.io/ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation alb.ingress.kubernetes.io/scheme to specify whether the ALB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. To integrate multiple Ingress resources and share the same entry point, configure the annotation alb.ingress.kubernetes.io/group.name to specify a name. Ingress resources without this annotation are treated as an \"implicit IngressGroup\" composed by the Ingress itself. To specify the host for the Ingress, refer to Configuring externalDNS to enable it.","title":"Create an Ingress for Application Access"},{"location":"usage/install/cloud/get-started-openstack-zh_CN/","text":"openstack \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English","title":"openstack \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-openstack-zh_CN/#openstack","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"openstack \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-openstack/","text":"Running On Openstack English | \u7b80\u4f53\u4e2d\u6587","title":"OpenStack"},{"location":"usage/install/cloud/get-started-openstack/#running-on-openstack","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Openstack"},{"location":"usage/install/cloud/get-started-vmware-zh_CN/","text":"vmware \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English","title":"vmware \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-vmware-zh_CN/#vmware","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"vmware \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-vmware/","text":"Running On Vmware English | \u7b80\u4f53\u4e2d\u6587","title":"VWware vSphere"},{"location":"usage/install/cloud/get-started-vmware/#running-on-vmware","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Vmware"},{"location":"usage/install/overlay/get-started-calico-zh_cn/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Calico \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Calico \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0c \u5b83\u7684\u597d\u5904\u662f\uff1a \u96c6\u7fa4\u5916\u90e8\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Pod \u7684 Underlay IP \u8bbf\u95ee Pod, \u800c\u65e0\u9700\u501f\u52a9 NodePort \u7684\u65b9\u5f0f\u66b4\u9732 Pod\u3002 \u53ef\u4e3a Pod \u5355\u72ec\u63a5\u5165\u4e00\u5f20 Underlay \u7f51\u5361\uff0c\u4f7f Pod \u5355\u72ec\u63a5\u5165\u5b58\u50a8\u7b49\u4e13\u7528\u7f51\u7edc\uff0c\u4fdd\u969c\u72ec\u7acb\u5e26\u5bbd\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u53ef\u901a\u8fc7\u8def\u7531\u8c03\u8c10\uff0c\u57fa\u4e8e Calico \u5b9e\u73b0 Pod \u7684 Underlay IP \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u786e\u4fdd Pod \u6570\u636e\u5305\u8bbf\u95ee\u65f6\u7684\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u8def\u5f84\u95ee\u9898\u800c\u5bfc\u81f4\u8def\u7531\u5668\u4e22\u5305\u3002 \u53ef\u57fa\u4e8e Pod \u7684 annotation: ipam.spidernet.io/default-route-nic \u7075\u6d3b\u6307\u5b9a Pod \u9ed8\u8ba4\u8def\u7531\u7684\u6240\u5728\u7f51\u5361\u3002 \u6ce8: \u672c\u6587\u5c06\u4f7f\u7528\u7b80\u5199 NAD \u6765\u4ee3\u6307 Multus CRD NetworkAttachmentDefinition \uff0cNAD \u4e3a\u5176\u9996\u5b57\u6bcd\u7b80\u5199 \u5b89\u88c5\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5 Spiderpool \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u7b49 CNI \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Calico \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# calicoctl get ippools NAME CIDR SELECTOR default-ipv4-ippool 10 .222.64.0/18 all () default-ipv4-ippool1 10 .223.64.0/18 all () ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 \u76ee\u524d Spiderpool \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u89e3\u51b3\u5f02\u5e38\u60c5\u51b5: \u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap, \u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002 \u521b\u5efa SpiderIPPool \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 ips \u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u521b\u5efa SpiderMultusConfig \u672c\u6587\u901a\u8fc7 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.spiderpoolConfigPools.IPv4IPPool \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u4fdd\u6301\u4e00\u81f4\u3002 \u5982\u679c\u9700\u8981\u4e3a Pod \u6dfb\u52a0\u591a\u5f20 Underlay \u7f51\u5361\u7684\u53ef\u4ee5\u53c2\u8003 Multi-Underlay-NIC \u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready\uff0c\u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d IP\u3001\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .6.212.132 lookup 101 32762 : from 10 .233.73.210 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 169 .254.1.1 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 169 .254.1.1 dev eth0 \u4ee5\u4e0a\u8868\u9879\u89e3\u91ca: Pod \u4e2d\u5206\u914d\u4e86 Calico(eth0) \u548c Macvlan(net1) \u4e24\u5f20\u7f51\u5361, IPv4 \u5730\u5740\u5206\u522b\u662f: 10.233.73.210 \u548c 10.6.212.145 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod\u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.132 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 eth0\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728\u5176\u4ed6\u7f51\u5361(\u5982 net1)\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: net1\" \u5b9e\u73b0\u3002 \u5bf9\u4e8e\u9ed8\u8ba4\u8def\u7531\u5728 eth0 \u7684\u573a\u666f\uff0cpod \u4e2d\u4f1a\u5b58\u5728\u4e00\u6761 table \u4e3a 100 \u7684\u7b56\u7565\u8def\u7531\uff0c \u8be5\u8def\u7531\u786e\u4fdd\u4ece eth0 \u63a5\u6536\u7684\u6d41\u91cf\u4ece eth0 \u8f6c\u53d1\uff0c\u9632\u6b62\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\u4e22\u5305\u3002 \u4e0b\u9762\u6d4b\u8bd5 Pod \u57fa\u672c\u7f51\u7edc\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 Pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Calico \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Calico \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0c \u5b83\u7684\u597d\u5904\u662f\uff1a \u96c6\u7fa4\u5916\u90e8\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Pod \u7684 Underlay IP \u8bbf\u95ee Pod, \u800c\u65e0\u9700\u501f\u52a9 NodePort \u7684\u65b9\u5f0f\u66b4\u9732 Pod\u3002 \u53ef\u4e3a Pod \u5355\u72ec\u63a5\u5165\u4e00\u5f20 Underlay \u7f51\u5361\uff0c\u4f7f Pod \u5355\u72ec\u63a5\u5165\u5b58\u50a8\u7b49\u4e13\u7528\u7f51\u7edc\uff0c\u4fdd\u969c\u72ec\u7acb\u5e26\u5bbd\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u53ef\u901a\u8fc7\u8def\u7531\u8c03\u8c10\uff0c\u57fa\u4e8e Calico \u5b9e\u73b0 Pod \u7684 Underlay IP \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u786e\u4fdd Pod \u6570\u636e\u5305\u8bbf\u95ee\u65f6\u7684\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u8def\u5f84\u95ee\u9898\u800c\u5bfc\u81f4\u8def\u7531\u5668\u4e22\u5305\u3002 \u53ef\u57fa\u4e8e Pod \u7684 annotation: ipam.spidernet.io/default-route-nic \u7075\u6d3b\u6307\u5b9a Pod \u9ed8\u8ba4\u8def\u7531\u7684\u6240\u5728\u7f51\u5361\u3002 \u6ce8: \u672c\u6587\u5c06\u4f7f\u7528\u7b80\u5199 NAD \u6765\u4ee3\u6307 Multus CRD NetworkAttachmentDefinition \uff0cNAD \u4e3a\u5176\u9996\u5b57\u6bcd\u7b80\u5199","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#_1","text":"\u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm \u4e8c\u8fdb\u5236","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spiderpool","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u7b49 CNI \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Calico \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# calicoctl get ippools NAME CIDR SELECTOR default-ipv4-ippool 10 .222.64.0/18 all () default-ipv4-ippool1 10 .223.64.0/18 all () ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 \u76ee\u524d Spiderpool \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u89e3\u51b3\u5f02\u5e38\u60c5\u51b5: \u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap, \u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spiderippool","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 ips \u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002","title":"\u521b\u5efa SpiderIPPool"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spidermultusconfig","text":"\u672c\u6587\u901a\u8fc7 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.spiderpoolConfigPools.IPv4IPPool \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u4fdd\u6301\u4e00\u81f4\u3002 \u5982\u679c\u9700\u8981\u4e3a Pod \u6dfb\u52a0\u591a\u5f20 Underlay \u7f51\u5361\u7684\u53ef\u4ee5\u53c2\u8003 Multi-Underlay-NIC \u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"\u521b\u5efa SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready\uff0c\u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d IP\u3001\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .6.212.132 lookup 101 32762 : from 10 .233.73.210 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 169 .254.1.1 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 169 .254.1.1 dev eth0 \u4ee5\u4e0a\u8868\u9879\u89e3\u91ca: Pod \u4e2d\u5206\u914d\u4e86 Calico(eth0) \u548c Macvlan(net1) \u4e24\u5f20\u7f51\u5361, IPv4 \u5730\u5740\u5206\u522b\u662f: 10.233.73.210 \u548c 10.6.212.145 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod\u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.132 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 eth0\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728\u5176\u4ed6\u7f51\u5361(\u5982 net1)\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: net1\" \u5b9e\u73b0\u3002 \u5bf9\u4e8e\u9ed8\u8ba4\u8def\u7531\u5728 eth0 \u7684\u573a\u666f\uff0cpod \u4e2d\u4f1a\u5b58\u5728\u4e00\u6761 table \u4e3a 100 \u7684\u7b56\u7565\u8def\u7531\uff0c \u8be5\u8def\u7531\u786e\u4fdd\u4ece eth0 \u63a5\u6536\u7684\u6d41\u91cf\u4ece eth0 \u8f6c\u53d1\uff0c\u9632\u6b62\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\u4e22\u5305\u3002 \u4e0b\u9762\u6d4b\u8bd5 Pod \u57fa\u672c\u7f51\u7edc\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 Pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/overlay/get-started-calico/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Calico serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . This setup enables Pod's east-west traffic to be forwarded through the Calico-created NIC (eth0). The advantages offered by Spiderpool's solution are: External clients outside the cluster can directly access Pods through their Underlay IP without the need for exposing Pods using the NodePort approach. Pods can individually connect to an Underlay network interface, allowing them to access dedicated networks such as storage with guaranteed independent bandwidth. When Pods have multiple network interfaces attached, such as Calico and Macvlan, route tuning can be applied to address issues related to Underlay IP access to ClusterIP based on the Calico setup. When Pods have multiple network interfaces attached, such as Calico and Macvlan, subnet route tuning is performed to ensure consistent round-trip paths for Pod data packet access, avoiding routing issues that may lead to packet loss. It is possible to flexibly specify the network interface for the default route of Pods based on the Pod's annotation: ipam.spidernet.io/default-route-nic. This document will use the abbreviation NAD to refer to the Multus CRD NetworkAttachmentDefinition, with NAD being its acronym. Prerequisites System requirements A ready Kubernetes cluster. Calico has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm binary Install Spiderpool Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present, Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can take either of the following approaches: Manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Once created, Spiderpool will automatically synchronize its status. Create SpiderIPPool The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. If you need to add multiple underlay NICs to a Pod, you can refer to Multi-Underlay-NIC . Create SpiderMultusConfig The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 \u3002 Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' Create an application Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller Enter the Pod and use the command ip to view information such as IP addresses and routes within the Pod: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .6.212.132 lookup 101 32762 : from 10 .233.73.210 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 169 .254.1.1 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 169 .254.1.1 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (Calico) and net1 (Macvlan), having IPv4 addresses of 10.233.73.210 and 10.6.212.145, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, it will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in eth0. To reserve it in net1, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: net1\". If the default route is eth0, a policy-based route with table 100 exists in the pod. This route ensures that traffic received from eth0 is forwarded from eth0 to prevent packet loss caused by inconsistent forward and return paths. To test the basic network connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Calico"},{"location":"usage/install/overlay/get-started-calico/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Calico serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . This setup enables Pod's east-west traffic to be forwarded through the Calico-created NIC (eth0). The advantages offered by Spiderpool's solution are: External clients outside the cluster can directly access Pods through their Underlay IP without the need for exposing Pods using the NodePort approach. Pods can individually connect to an Underlay network interface, allowing them to access dedicated networks such as storage with guaranteed independent bandwidth. When Pods have multiple network interfaces attached, such as Calico and Macvlan, route tuning can be applied to address issues related to Underlay IP access to ClusterIP based on the Calico setup. When Pods have multiple network interfaces attached, such as Calico and Macvlan, subnet route tuning is performed to ensure consistent round-trip paths for Pod data packet access, avoiding routing issues that may lead to packet loss. It is possible to flexibly specify the network interface for the default route of Pods based on the Pod's annotation: ipam.spidernet.io/default-route-nic. This document will use the abbreviation NAD to refer to the Multus CRD NetworkAttachmentDefinition, with NAD being its acronym.","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico/#prerequisites","text":"System requirements A ready Kubernetes cluster. Calico has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm binary","title":"Prerequisites"},{"location":"usage/install/overlay/get-started-calico/#install-spiderpool","text":"Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present, Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can take either of the following approaches: Manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Once created, Spiderpool will automatically synchronize its status.","title":"Install Spiderpool"},{"location":"usage/install/overlay/get-started-calico/#create-spiderippool","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. If you need to add multiple underlay NICs to a Pod, you can refer to Multi-Underlay-NIC .","title":"Create SpiderIPPool"},{"location":"usage/install/overlay/get-started-calico/#create-spidermultusconfig","text":"The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 \u3002 Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"Create SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-calico/#create-an-application","text":"Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller Enter the Pod and use the command ip to view information such as IP addresses and routes within the Pod: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .6.212.132 lookup 101 32762 : from 10 .233.73.210 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 169 .254.1.1 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 169 .254.1.1 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (Calico) and net1 (Macvlan), having IPv4 addresses of 10.233.73.210 and 10.6.212.145, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, it will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in eth0. To reserve it in net1, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: net1\". If the default route is eth0, a policy-based route with table 100 exists in the pod. This route ensures that traffic received from eth0 is forwarded from eth0 to prevent packet loss caused by inconsistent forward and return paths. To test the basic network connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Create an application"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/","text":"Cilium Quick Start English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Cilium \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u5f20\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0\u4ee5\u4e0b\u6548\u679c: \u96c6\u7fa4\u5916\u90e8\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Pod \u7684 Underlay IP \u8bbf\u95ee Pod, \u800c\u65e0\u9700\u501f\u52a9 NodePort \u7684\u65b9\u5f0f\u66b4\u9732 Pod\u3002 \u53ef\u4e3a Pod \u5355\u72ec\u63a5\u5165\u4e00\u5f20 Underlay \u7f51\u5361\uff0c\u4f7f Pod \u5355\u72ec\u63a5\u5165\u5b58\u50a8\u7b49\u4e13\u7528\u7f51\u7edc\uff0c\u4fdd\u969c\u72ec\u7acb\u5e26\u5bbd\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u53ef\u901a\u8fc7\u8def\u7531\u8c03\u8c10\uff0c\u57fa\u4e8e Cilium \u5b9e\u73b0 Pod \u7684 Underlay IP \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u786e\u4fdd Pod \u6570\u636e\u5305\u8bbf\u95ee\u65f6\u7684\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u8def\u5f84\u95ee\u9898\u800c\u5bfc\u81f4\u8def\u7531\u5668\u4e22\u5305\u3002 \u53ef\u57fa\u4e8e Pod \u7684 annotation: ipam.spidernet.io/default-route-nic \u7075\u6d3b\u6307\u5b9a Pod \u9ed8\u8ba4\u8def\u7531\u7684\u6240\u5728\u7f51\u5361\u3002 \u6ce8: \u672c\u6587\u5c06\u4f7f\u7528\u7b80\u5199 NAD \u6765\u4ee3\u6307 Multus CRD NetworkAttachmentDefinition \uff0cNAD \u4e3a\u5176\u9996\u5b57\u6bcd\u7b80\u5199 \u5b89\u88c5\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Cilium \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: Cilium \u5728\u8fd0\u884c\u65f6\u4f1a\u626b\u63cf\u53ef\u7528\u7684 VLAN \u8bbe\u5907\u548c\u6807\u7b7e\uff0c\u5e76\u5c06\u8fc7\u6ee4\u6240\u6709\u672a\u77e5\u6d41\u91cf\u3002\u5728\u4e00\u4e9b\u8de8 VLAN \u573a\u666f\u4e0b\u8bbf\u95ee\u65f6\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u901a\u8baf\u95ee\u9898\uff0c\u8bf7\u5728\u5b89\u88c5\u65f6\u4f7f\u7528 --set bpf.vlanBypass={0} \u5141\u8bb8\u6240\u6709\u7684 VLAN \u6807\u8bb0\u901a\u8fc7\uff0c\u66f4\u591a\u7ec6\u8282\u53c2\u8003 Cilium VLAN 802.1q \u652f\u6301 \u3002 ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system --set bpf.vlanBypass ={ 0 } ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5 Spiderpool \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Cilium \u914d\u7f6e\u7684 Pod \u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 \u76ee\u524d Spiderpool \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u89e3\u51b3\u5f02\u5e38\u60c5\u51b5: \u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap, \u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002 \u521b\u5efa SpiderIPPool \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF Note: subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u5982\u679c\u9700\u8981\u4e3a Pod \u6dfb\u52a0\u591a\u5f20 Underlay \u7f51\u5361\u7684\u53ef\u4ee5\u53c2\u8003 Multi-Underlay-NIC \u3002 \u521b\u5efa SpiderMultusConfig \u672c\u6587\u4f7f\u7528 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Note: spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.ippools.ipv4 \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .6.212.131 lookup 101 32762 \uff1afrom 10 .233.120.101 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 10 .233.65.96 dev eth0 \u4ee5\u4e0a\u4fe1\u606f\u89e3\u91ca: Pod \u5206\u914d\u4e86\u4e24\u5f20\u7f51\u5361: eth0(cilium)\u3001net1(macvlan),\u5bf9\u5e94\u7684 IPv4 \u5730\u5740\u5206\u522b\u4e3a: 10.233.120.101 \u548c 10.6.212.202 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod \u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.131 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 eth0\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 net1\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: net1\" \u5b9e\u73b0\u3002 \u5bf9\u4e8e\u9ed8\u8ba4\u8def\u7531\u5728 eth0 \u7684\u573a\u666f\uff0cpod \u4e2d\u4f1a\u5b58\u5728\u4e00\u6761 table \u4e3a 100 \u7684\u7b56\u7565\u8def\u7531\uff0c \u8be5\u8def\u7531\u786e\u4fdd\u4ece eth0 \u63a5\u6536\u7684\u6d41\u91cf\u4ece eth0 \u8f6c\u53d1\uff0c\u9632\u6b62\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\u4e22\u5305\u3002 \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u7684\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#cilium-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Cilium \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u5f20\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0\u4ee5\u4e0b\u6548\u679c: \u96c6\u7fa4\u5916\u90e8\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Pod \u7684 Underlay IP \u8bbf\u95ee Pod, \u800c\u65e0\u9700\u501f\u52a9 NodePort \u7684\u65b9\u5f0f\u66b4\u9732 Pod\u3002 \u53ef\u4e3a Pod \u5355\u72ec\u63a5\u5165\u4e00\u5f20 Underlay \u7f51\u5361\uff0c\u4f7f Pod \u5355\u72ec\u63a5\u5165\u5b58\u50a8\u7b49\u4e13\u7528\u7f51\u7edc\uff0c\u4fdd\u969c\u72ec\u7acb\u5e26\u5bbd\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u53ef\u901a\u8fc7\u8def\u7531\u8c03\u8c10\uff0c\u57fa\u4e8e Cilium \u5b9e\u73b0 Pod \u7684 Underlay IP \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u786e\u4fdd Pod \u6570\u636e\u5305\u8bbf\u95ee\u65f6\u7684\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u8def\u5f84\u95ee\u9898\u800c\u5bfc\u81f4\u8def\u7531\u5668\u4e22\u5305\u3002 \u53ef\u57fa\u4e8e Pod \u7684 annotation: ipam.spidernet.io/default-route-nic \u7075\u6d3b\u6307\u5b9a Pod \u9ed8\u8ba4\u8def\u7531\u7684\u6240\u5728\u7f51\u5361\u3002 \u6ce8: \u672c\u6587\u5c06\u4f7f\u7528\u7b80\u5199 NAD \u6765\u4ee3\u6307 Multus CRD NetworkAttachmentDefinition \uff0cNAD \u4e3a\u5176\u9996\u5b57\u6bcd\u7b80\u5199","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#_1","text":"\u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Cilium \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: Cilium \u5728\u8fd0\u884c\u65f6\u4f1a\u626b\u63cf\u53ef\u7528\u7684 VLAN \u8bbe\u5907\u548c\u6807\u7b7e\uff0c\u5e76\u5c06\u8fc7\u6ee4\u6240\u6709\u672a\u77e5\u6d41\u91cf\u3002\u5728\u4e00\u4e9b\u8de8 VLAN \u573a\u666f\u4e0b\u8bbf\u95ee\u65f6\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u901a\u8baf\u95ee\u9898\uff0c\u8bf7\u5728\u5b89\u88c5\u65f6\u4f7f\u7528 --set bpf.vlanBypass={0} \u5141\u8bb8\u6240\u6709\u7684 VLAN \u6807\u8bb0\u901a\u8fc7\uff0c\u66f4\u591a\u7ec6\u8282\u53c2\u8003 Cilium VLAN 802.1q \u652f\u6301 \u3002 ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system --set bpf.vlanBypass ={ 0 } ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm \u4e8c\u8fdb\u5236","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spiderpool","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Cilium \u914d\u7f6e\u7684 Pod \u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 \u76ee\u524d Spiderpool \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u89e3\u51b3\u5f02\u5e38\u60c5\u51b5: \u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap, \u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spiderippool","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF Note: subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u5982\u679c\u9700\u8981\u4e3a Pod \u6dfb\u52a0\u591a\u5f20 Underlay \u7f51\u5361\u7684\u53ef\u4ee5\u53c2\u8003 Multi-Underlay-NIC \u3002","title":"\u521b\u5efa SpiderIPPool"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spidermultusconfig","text":"\u672c\u6587\u4f7f\u7528 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Note: spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.ippools.ipv4 \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"\u521b\u5efa SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .6.212.131 lookup 101 32762 \uff1afrom 10 .233.120.101 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 10 .233.65.96 dev eth0 \u4ee5\u4e0a\u4fe1\u606f\u89e3\u91ca: Pod \u5206\u914d\u4e86\u4e24\u5f20\u7f51\u5361: eth0(cilium)\u3001net1(macvlan),\u5bf9\u5e94\u7684 IPv4 \u5730\u5740\u5206\u522b\u4e3a: 10.233.120.101 \u548c 10.6.212.202 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod \u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.131 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 eth0\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 net1\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: net1\" \u5b9e\u73b0\u3002 \u5bf9\u4e8e\u9ed8\u8ba4\u8def\u7531\u5728 eth0 \u7684\u573a\u666f\uff0cpod \u4e2d\u4f1a\u5b58\u5728\u4e00\u6761 table \u4e3a 100 \u7684\u7b56\u7565\u8def\u7531\uff0c \u8be5\u8def\u7531\u786e\u4fdd\u4ece eth0 \u63a5\u6536\u7684\u6d41\u91cf\u4ece eth0 \u8f6c\u53d1\uff0c\u9632\u6b62\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\u4e22\u5305\u3002 \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u7684\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/overlay/get-started-cilium/","text":"Cilium Quick Start English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Cilium serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . The advantages offered by Spiderpool's solution are: External clients outside the cluster can directly access Pods through their Underlay IP without the need for exposing Pods using the NodePort approach. Pods can individually connect to an Underlay network interface, allowing them to access dedicated networks such as storage with guaranteed independent bandwidth. When Pods have multiple network interfaces attached, such as Cilium and Macvlan, route tuning can be applied to address issues related to Underlay IP access to ClusterIP based on the Cilium setup. When Pods have multiple network interfaces attached, such as Cilium and Macvlan, subnet route tuning is performed to ensure consistent round-trip paths for Pod data packet access, avoiding routing issues that may lead to packet loss. It is possible to flexibly specify the network interface for the default route of Pods based on the Pod's annotation: ipam.spidernet.io/default-route-nic. This document will use the abbreviation NAD to refer to the Multus CRD NetworkAttachmentDefinition, with NAD being its acronym. Prerequisites System requirements A ready Kubernetes cluster. Cilium has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: Cilium will scan available VLAN devices and tags when running, and will filter all unknown traffic. In some cross-VLAN scenarios,there may be communication problems. Please use --set bpf.vlanBypass={0} during installation to allow all VLAN tags to pass. For more details, refer to Cilium VLAN 802.1q support . ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system --set bpf.vlanBypass ={ 0 } ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm binary Install Spiderpool Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present, Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can take either of the following approaches: Manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Once created, Spiderpool will automatically synchronize its status. Create SpiderIPPool The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. If you need to add multiple underlay NICs to a Pod, you can refer to Multi-Underlay-NIC . Create SpiderMultusConfig The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 . Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' Create an application Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller Use the command ip to view the Pod's information such as routes: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .6.212.131 lookup 101 32762 \uff1afrom 10 .233.120.101 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 10 .233.65.96 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (cilium) and net1 (macvlan), having IPv4 addresses of 10.233.120.101 and 10.6.212.202, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, traffic will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in eth0. To reserve it in net1, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: net1\". If the default route is eth0, a policy-based route with table 100 exists in the pod. This route ensures that traffic received from eth0 is forwarded from eth0 to prevent packet loss caused by inconsistent forward and return paths. To test the east-west connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Cilium"},{"location":"usage/install/overlay/get-started-cilium/#cilium-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Cilium serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . The advantages offered by Spiderpool's solution are: External clients outside the cluster can directly access Pods through their Underlay IP without the need for exposing Pods using the NodePort approach. Pods can individually connect to an Underlay network interface, allowing them to access dedicated networks such as storage with guaranteed independent bandwidth. When Pods have multiple network interfaces attached, such as Cilium and Macvlan, route tuning can be applied to address issues related to Underlay IP access to ClusterIP based on the Cilium setup. When Pods have multiple network interfaces attached, such as Cilium and Macvlan, subnet route tuning is performed to ensure consistent round-trip paths for Pod data packet access, avoiding routing issues that may lead to packet loss. It is possible to flexibly specify the network interface for the default route of Pods based on the Pod's annotation: ipam.spidernet.io/default-route-nic. This document will use the abbreviation NAD to refer to the Multus CRD NetworkAttachmentDefinition, with NAD being its acronym.","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium/#prerequisites","text":"System requirements A ready Kubernetes cluster. Cilium has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: Cilium will scan available VLAN devices and tags when running, and will filter all unknown traffic. In some cross-VLAN scenarios,there may be communication problems. Please use --set bpf.vlanBypass={0} during installation to allow all VLAN tags to pass. For more details, refer to Cilium VLAN 802.1q support . ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system --set bpf.vlanBypass ={ 0 } ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm binary","title":"Prerequisites"},{"location":"usage/install/overlay/get-started-cilium/#install-spiderpool","text":"Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present, Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can take either of the following approaches: Manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Once created, Spiderpool will automatically synchronize its status.","title":"Install Spiderpool"},{"location":"usage/install/overlay/get-started-cilium/#create-spiderippool","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. If you need to add multiple underlay NICs to a Pod, you can refer to Multi-Underlay-NIC .","title":"Create SpiderIPPool"},{"location":"usage/install/overlay/get-started-cilium/#create-spidermultusconfig","text":"The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 . Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"Create SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-cilium/#create-an-application","text":"Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller Use the command ip to view the Pod's information such as routes: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .6.212.131 lookup 101 32762 \uff1afrom 10 .233.120.101 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 101 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 / # ip route show table 100 default via 10 .233.65.96 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (cilium) and net1 (macvlan), having IPv4 addresses of 10.233.120.101 and 10.6.212.202, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, traffic will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in eth0. To reserve it in net1, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: net1\". If the default route is eth0, a policy-based route with table 100 exists in the pod. This route ensures that traffic received from eth0 is forwarded from eth0 to prevent packet loss caused by inconsistent forward and return paths. To test the east-west connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Create an application"},{"location":"usage/install/overlay/multi-underlay-nic-zh_CN/","text":"Calico with multi underlay NIC English | \u7b80\u4f53\u4e2d\u6587 \u57fa\u4e8e Webhook \u81ea\u52a8\u4e3a Pod \u9644\u52a0\u591a\u5f20 Underlay \u7f51\u5361 \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 \uff0c ens193 \u6240\u5728\u5b50\u7f51\u4e3a 10.7.0.0/16 \uff0c\u4ee5\u6b64\u521b\u5efa SpiderIPPool: $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens192 spec: disable: false gateway: 10.6.0.1 subnet: 10.6.0.0/16 ips: - 10.6.212.100-10.6.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens193 spec: disable: false gateway: 10.7.0.1 subnet: 10.7.0.0/16 ips: - 10.7.212.100-10.7.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - macvlan-ens192 vlanID: 0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens193 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens193 ippools: ipv4: - macvlan-ens193 vlanID: 0 EOF \u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 \u4e3a\u5e94\u7528\u4e5f\u6dfb\u52a0\u76f8\u540c\u6ce8\u89e3: ... spec : template : metadata : annotations : cni.spidernet.io/network-resource-inject : multi-network \u6ce8\u610f\uff1a\u4f7f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u65f6\uff0c\u4e0d\u80fd\u4e3a\u5e94\u7528\u6dfb\u52a0\u5176\u4ed6\u7f51\u7edc\u914d\u7f6e\u6ce8\u89e3(\u5982 k8s.v1.cni.cncf.io/networks \u548c ipam.spidernet.io ippools \u7b49)\uff0c\u5426\u5219\u4f1a\u5f71\u54cd\u8d44\u6e90\u81ea\u52a8\u6ce8\u5165\u529f\u80fd\u3002 \u5f53 Pod \u88ab\u521b\u5efa\u540e\uff0c\u53ef\u89c2\u6d4b\u5230 Pod \u88ab\u81ea\u52a8\u6ce8\u5165\u4e86\u7f51\u5361 annotation ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"macvlan-ens192\",\"namespace\":\"spiderpool\"}, {\"name\":\"macvlan-ens193\",\"namespace\":\"spiderpool\"}] ....","title":"Calico with multi underlay NIC"},{"location":"usage/install/overlay/multi-underlay-nic-zh_CN/#calico-with-multi-underlay-nic","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Calico with multi underlay NIC"},{"location":"usage/install/overlay/multi-underlay-nic-zh_CN/#webhook-pod-underlay","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 \uff0c ens193 \u6240\u5728\u5b50\u7f51\u4e3a 10.7.0.0/16 \uff0c\u4ee5\u6b64\u521b\u5efa SpiderIPPool: $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens192 spec: disable: false gateway: 10.6.0.1 subnet: 10.6.0.0/16 ips: - 10.6.212.100-10.6.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens193 spec: disable: false gateway: 10.7.0.1 subnet: 10.7.0.0/16 ips: - 10.7.212.100-10.7.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - macvlan-ens192 vlanID: 0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens193 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens193 ippools: ipv4: - macvlan-ens193 vlanID: 0 EOF","title":"\u57fa\u4e8e Webhook \u81ea\u52a8\u4e3a Pod \u9644\u52a0\u591a\u5f20 Underlay \u7f51\u5361"},{"location":"usage/install/overlay/multi-underlay-nic-zh_CN/#_1","text":"\u4e3a\u5e94\u7528\u4e5f\u6dfb\u52a0\u76f8\u540c\u6ce8\u89e3: ... spec : template : metadata : annotations : cni.spidernet.io/network-resource-inject : multi-network \u6ce8\u610f\uff1a\u4f7f\u7528 webhook \u81ea\u52a8\u6ce8\u5165\u7f51\u7edc\u8d44\u6e90\u529f\u80fd\u65f6\uff0c\u4e0d\u80fd\u4e3a\u5e94\u7528\u6dfb\u52a0\u5176\u4ed6\u7f51\u7edc\u914d\u7f6e\u6ce8\u89e3(\u5982 k8s.v1.cni.cncf.io/networks \u548c ipam.spidernet.io ippools \u7b49)\uff0c\u5426\u5219\u4f1a\u5f71\u54cd\u8d44\u6e90\u81ea\u52a8\u6ce8\u5165\u529f\u80fd\u3002 \u5f53 Pod \u88ab\u521b\u5efa\u540e\uff0c\u53ef\u89c2\u6d4b\u5230 Pod \u88ab\u81ea\u52a8\u6ce8\u5165\u4e86\u7f51\u5361 annotation ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"macvlan-ens192\",\"namespace\":\"spiderpool\"}, {\"name\":\"macvlan-ens193\",\"namespace\":\"spiderpool\"}] ....","title":"\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528"},{"location":"usage/install/overlay/multi-underlay-nic/","text":"Calico with multi underlay NIC English | \u7b80\u4f53\u4e2d\u6587 Auto attach multiple underlay NICs to Pod based on Webhook The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . The subnet for the interface ens193 on the cluster nodes here is 10.7.0.0/16 . Create SpiderIPPools using these subnets: $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens192 spec: disable: false gateway: 10.6.0.1 subnet: 10.6.0.0/16 ips: - 10.6.212.100-10.6.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens193 spec: disable: false gateway: 10.7.0.1 subnet: 10.7.0.0/16 ips: - 10.7.212.100-10.7.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - macvlan-ens192 vlanID: 0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens193 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens193 ippools: ipv4: - macvlan-ens193 vlanID: 0 EOF Create an application Add the same annotation to the application: ... spec : template : metadata : annotations : cni.spidernet.io/network-resource-inject : multi-network Note: When using the webhook automatic injection of network resources feature, do not add other network configuration annotations (such as k8s.v1.cni.cncf.io/networks and ipam.spidernet.io/ippools ) to the application, as it will affect the automatic injection of resources. Once the Pod is created, you can observe that the Pod has been automatically injected with network card annotations. ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"macvlan-ens192\",\"namespace\":\"spiderpool\"}, {\"name\":\"macvlan-ens193\",\"namespace\":\"spiderpool\"}] ....","title":"Calico with multi underlay NIC"},{"location":"usage/install/overlay/multi-underlay-nic/#calico-with-multi-underlay-nic","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Calico with multi underlay NIC"},{"location":"usage/install/overlay/multi-underlay-nic/#auto-attach-multiple-underlay-nics-to-pod-based-on-webhook","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . The subnet for the interface ens193 on the cluster nodes here is 10.7.0.0/16 . Create SpiderIPPools using these subnets: $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens192 spec: disable: false gateway: 10.6.0.1 subnet: 10.6.0.0/16 ips: - 10.6.212.100-10.6.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ens193 spec: disable: false gateway: 10.7.0.1 subnet: 10.7.0.0/16 ips: - 10.7.212.100-10.7.212.200 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - macvlan-ens192 vlanID: 0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens193 namespace: spiderpool annotations: cni.spidernet.io/network-resource-inject: multi-network spec: cniType: macvlan macvlan: master: - ens193 ippools: ipv4: - macvlan-ens193 vlanID: 0 EOF","title":"Auto attach multiple underlay NICs to Pod based on Webhook"},{"location":"usage/install/overlay/multi-underlay-nic/#create-an-application","text":"Add the same annotation to the application: ... spec : template : metadata : annotations : cni.spidernet.io/network-resource-inject : multi-network Note: When using the webhook automatic injection of network resources feature, do not add other network configuration annotations (such as k8s.v1.cni.cncf.io/networks and ipam.spidernet.io/ippools ) to the application, as it will affect the automatic injection of resources. Once the Pod is created, you can observe that the Pod has been automatically injected with network card annotations. ... spec : template : metadata : annotations : k8s.v1.cni.cncf.io/networks : |- [{\"name\":\"macvlan-ens192\",\"namespace\":\"spiderpool\"}, {\"name\":\"macvlan-ens193\",\"namespace\":\"spiderpool\"}] ....","title":"Create an application"},{"location":"usage/install/underlay/get-started-calico-zh_CN/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3: \u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740 IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9 \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528 \u5b89\u88c5\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u4e00\u4e2a Kubernetes \u96c6\u7fa4\uff08\u63a8\u8350 k8s version > 1.22\uff09\uff0c\u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002 \u786e\u8ba4 Calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002 \u786e\u8ba4 Calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002 Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177 \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u521b\u5efa Pod \u4f7f\u7528\u7684 SpiderIPPool \u5b9e\u4f8b\uff1a cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602 \u914d\u7f6e Calico BGP [\u53ef\u9009] \u672c\u4f8b\u5e0c\u671b Calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u7684 IP \u6c60\u6240\u5728\u7684\u5b50\u7f51\uff08 10.244.0.0/16 \uff09\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 Pod IP\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002 \u7f51\u7edc\u62d3\u6251\u5982\u4e0b: \u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router \u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR\uff1a root@router:~# apt install -y frr FRR \u5f00\u542f BGP \u529f\u80fd\uff1a root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr \u914d\u7f6e FRR\uff1a root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy \u914d\u7f6e\u89e3\u91ca: Router \u4fa7\u7684 AS \u4e3a 23000 \uff0c\u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a 64512 \u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a ebgp \uff0c\u8282\u70b9\u4e4b\u95f4\u4e3a ibgp \u9700\u8981\u5173\u95ed ebgp-requires-policy \uff0c\u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb 172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP \u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting \u3002 \u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45 Calico \u9700\u8981\u914d\u7f6e calico_backend: bird \uff0c\u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd\uff1a [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 \u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh \uff1a [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' \u521b\u5efa BGPPeer\uff1a [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740 asNumber \u4e3a BGP Router \u7684 AS \u53f7 \u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. \u66f4\u591a Calico BGP \u914d\u7f6e\uff0c\u8bf7\u53c2\u8003 Calico BGP \u3002 \u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60 \u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60\uff0c\u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531\uff1a cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-ippool spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51\uff1a 10.244.0.0/16 \u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a\uff1aNever \u5207\u6362 Calico \u7684 IPAM \u4e3a Spiderpool \u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a Calico \u7684 CNI \u914d\u7f6e\u6587\u4ef6\uff1a /etc/cni/net.d/10-calico.conflist \uff0c\u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a Spiderpool\uff1a \"ipam\" : { \"type\" : \"spiderpool\" }, \u521b\u5efa\u5e94\u7528 \u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' # (1) labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u4ece \"nginx-ippool-v4\" SpiderIPPool \u4e2d\u5206\u914d\u56fa\u5b9a IP \u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annotations \u6307\u5b9a\u7684 ippool: nginx-ippool-v4 \u4e2d\u7ed9 Pod \u5206\u914d IP\u3002 [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false \u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 nginx-ippool-v4 \u7684 IP \u6c60\u8303\u56f4\u5185\uff1a [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> \u6269\u5bb9\u526f\u672c\u6570\u5230 3 \uff0c\u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece IP \u6c60 nginx-ippool-v4 \u4e2d\u5206\u914d\uff1a [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> \u67e5\u770b IP \u6c60 nginx-ippool-v4 \u7684 ALLOCATED-IP-COUNT \u65b0\u589e 1\uff1a [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 25602 false false \u7ed3\u8bba \u7ecf\u8fc7\u6d4b\u8bd5\uff1a\u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38\uff08\u5305\u62ec\u8de8 Calico \u5b50\u7f51\uff09\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3: \u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740 IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9 \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_1","text":"\u5b89\u88c5\u8981\u6c42 \u4e00\u4e2a Kubernetes \u96c6\u7fa4\uff08\u63a8\u8350 k8s version > 1.22\uff09\uff0c\u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002 \u786e\u8ba4 Calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002 \u786e\u8ba4 Calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002 Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#spiderpool","text":"helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u521b\u5efa Pod \u4f7f\u7528\u7684 SpiderIPPool \u5b9e\u4f8b\uff1a cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-bgp","text":"\u672c\u4f8b\u5e0c\u671b Calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u7684 IP \u6c60\u6240\u5728\u7684\u5b50\u7f51\uff08 10.244.0.0/16 \uff09\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 Pod IP\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002 \u7f51\u7edc\u62d3\u6251\u5982\u4e0b: \u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router \u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR\uff1a root@router:~# apt install -y frr FRR \u5f00\u542f BGP \u529f\u80fd\uff1a root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr \u914d\u7f6e FRR\uff1a root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy \u914d\u7f6e\u89e3\u91ca: Router \u4fa7\u7684 AS \u4e3a 23000 \uff0c\u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a 64512 \u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a ebgp \uff0c\u8282\u70b9\u4e4b\u95f4\u4e3a ibgp \u9700\u8981\u5173\u95ed ebgp-requires-policy \uff0c\u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb 172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP \u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting \u3002 \u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45 Calico \u9700\u8981\u914d\u7f6e calico_backend: bird \uff0c\u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd\uff1a [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 \u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh \uff1a [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' \u521b\u5efa BGPPeer\uff1a [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740 asNumber \u4e3a BGP Router \u7684 AS \u53f7 \u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. \u66f4\u591a Calico BGP \u914d\u7f6e\uff0c\u8bf7\u53c2\u8003 Calico BGP \u3002","title":"\u914d\u7f6e Calico BGP [\u53ef\u9009]"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-ip","text":"\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60\uff0c\u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531\uff1a cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-ippool spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51\uff1a 10.244.0.0/16 \u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a\uff1aNever","title":"\u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-ipam-spiderpool","text":"\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a Calico \u7684 CNI \u914d\u7f6e\u6587\u4ef6\uff1a /etc/cni/net.d/10-calico.conflist \uff0c\u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a Spiderpool\uff1a \"ipam\" : { \"type\" : \"spiderpool\" },","title":"\u5207\u6362 Calico \u7684 IPAM \u4e3a Spiderpool"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_2","text":"\u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' # (1) labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u4ece \"nginx-ippool-v4\" SpiderIPPool \u4e2d\u5206\u914d\u56fa\u5b9a IP \u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annotations \u6307\u5b9a\u7684 ippool: nginx-ippool-v4 \u4e2d\u7ed9 Pod \u5206\u914d IP\u3002 [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false \u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 nginx-ippool-v4 \u7684 IP \u6c60\u8303\u56f4\u5185\uff1a [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> \u6269\u5bb9\u526f\u672c\u6570\u5230 3 \uff0c\u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece IP \u6c60 nginx-ippool-v4 \u4e2d\u5206\u914d\uff1a [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> \u67e5\u770b IP \u6c60 nginx-ippool-v4 \u7684 ALLOCATED-IP-COUNT \u65b0\u589e 1\uff1a [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 25602 false false","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_3","text":"\u7ecf\u8fc7\u6d4b\u8bd5\uff1a\u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38\uff08\u5305\u62ec\u8de8 Calico \u5b50\u7f51\uff09\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002","title":"\u7ed3\u8bba"},{"location":"usage/install/underlay/get-started-calico/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements: Assign static IP addresses to applications Scale IP pools dynamically based on replica counts Enable external clients outside the cluster to access applications without their IPs Prerequisites System requirements An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI. Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks. Confirm that Calico has enabled BGP configuration in full-mesh mode. Helm and Calicoctl Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Create the SpiderIPPool instance used by the Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF Verify the installation\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602 Configure Calico BGP [optional] In this example, we want Calico to work in underlay mode and announce the subnet where Spiderpool's IPPool resides ( 10.244.0.0/16 ) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router. If you don't need external clients to access pod IPs directly, skip this step. The network topology is as follows: Configure a host outside the cluster as BGP Router We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand: root@router:~# apt install -y frr FRR enable BGP: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr Configure FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy Configuration descriptions: The AS on the router side is 23000 , and the AS on the cluster node side is 64512 . The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp. ebgp-requires-policy needs to be disabled, otherwise the BGP session cannot be established. 172.16.13.11/21 is the IP address of the cluster node. For more information, refer to frrouting . Configure BGP neighbor for Calico calico_backend: bird needs to be configured to establish a BGP session: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled: [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' Create BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP is the IP address of BGP Router asNumber is the AS number of BGP Router Check if the BGP session is established: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. For more information on Calico BGP configuration, refer to Calico BGP . Create a Calico IP pool in the same subnet Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-subnet spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF The CIDR needs to correspond to the subnet of Spiderpool: 10.244.0.0/16 Set ipipMode and vxlanMode to: Never Switch Calico's IPAM to Spiderpool Change the Calico CNI configuration file /etc/cni/net.d/10-calico.conflist on each node to switch the ipam field to Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" }, Create applications Take the Nginx application as an example: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' # (1) labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF Assign static IPs from \"nginx-ippool-v4\" SpiderIPPool When the application Pod is created, Spiderpool assigns the IP to the Pod from the ippool: nginx-ippool-v4 specified in the annotations. [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false When replicas are restarted, their IPs are fixed within the range of the nginx-ippool-v4 IPPool: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> Expand the number of replicas to 3 , the IP address of the new replica is still allocated from the IPPool: nginx-ippool-v4 : [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> View IP pool: Added 1 to ALLOCATED-IP-COUNT of nginx-ippool-v4 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 5 false false Conclusion The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.","title":"Calico"},{"location":"usage/install/underlay/get-started-calico/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements: Assign static IP addresses to applications Scale IP pools dynamically based on replica counts Enable external clients outside the cluster to access applications without their IPs","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico/#prerequisites","text":"System requirements An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI. Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks. Confirm that Calico has enabled BGP configuration in full-mesh mode. Helm and Calicoctl","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-calico/#install-spiderpool","text":"helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Create the SpiderIPPool instance used by the Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF Verify the installation\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-calico/#configure-calico-bgp-optional","text":"In this example, we want Calico to work in underlay mode and announce the subnet where Spiderpool's IPPool resides ( 10.244.0.0/16 ) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router. If you don't need external clients to access pod IPs directly, skip this step. The network topology is as follows: Configure a host outside the cluster as BGP Router We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand: root@router:~# apt install -y frr FRR enable BGP: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr Configure FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy Configuration descriptions: The AS on the router side is 23000 , and the AS on the cluster node side is 64512 . The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp. ebgp-requires-policy needs to be disabled, otherwise the BGP session cannot be established. 172.16.13.11/21 is the IP address of the cluster node. For more information, refer to frrouting . Configure BGP neighbor for Calico calico_backend: bird needs to be configured to establish a BGP session: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled: [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' Create BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP is the IP address of BGP Router asNumber is the AS number of BGP Router Check if the BGP session is established: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. For more information on Calico BGP configuration, refer to Calico BGP .","title":"Configure Calico BGP [optional]"},{"location":"usage/install/underlay/get-started-calico/#create-a-calico-ip-pool-in-the-same-subnet","text":"Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-subnet spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF The CIDR needs to correspond to the subnet of Spiderpool: 10.244.0.0/16 Set ipipMode and vxlanMode to: Never","title":"Create a Calico IP pool in the same subnet"},{"location":"usage/install/underlay/get-started-calico/#switch-calicos-ipam-to-spiderpool","text":"Change the Calico CNI configuration file /etc/cni/net.d/10-calico.conflist on each node to switch the ipam field to Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" },","title":"Switch Calico's IPAM to Spiderpool"},{"location":"usage/install/underlay/get-started-calico/#create-applications","text":"Take the Nginx application as an example: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' # (1) labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF Assign static IPs from \"nginx-ippool-v4\" SpiderIPPool When the application Pod is created, Spiderpool assigns the IP to the Pod from the ippool: nginx-ippool-v4 specified in the annotations. [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false When replicas are restarted, their IPs are fixed within the range of the nginx-ippool-v4 IPPool: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> Expand the number of replicas to 3 , the IP address of the new replica is still allocated from the IPPool: nginx-ippool-v4 : [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> View IP pool: Added 1 to ALLOCATED-IP-COUNT of nginx-ippool-v4 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 5 false false","title":"Create applications"},{"location":"usage/install/underlay/get-started-calico/#conclusion","text":"The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.","title":"Conclusion"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/","text":"Macvlan Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Macvlan \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1 \u5b89\u88c5\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5df2\u5b89\u88c5 Helm \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u63d2\u4ef6\u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528 Ifacer \u63d2\u4ef6 \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u82e5\u76ee\u5f55\u4e0b\u4e0d\u5b58\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced: ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: [] phase: Synced serviceCIDR: - 10 .233.0.0/18 \u5982\u679c\u72b6\u6001\u4e3a NotReady ,\u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u76ee\u524d Spiderpool: * \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 * \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u7b49\u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap\uff0c\u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 \u521b\u5efa\u4e0e\u7f51\u7edc\u63a5\u53e3 eth0 \u5728\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u6c60\u4ee5\u4f9b Pod \u4f7f\u7528\uff0c\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF \u9a8c\u8bc1\u5b89\u88c5 ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false \u521b\u5efa CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528 \u82e5\u6709 vlan \u7684\u9700\u6c42\uff0c\u53ef\u5728 spec.vlanID \u5b57\u6bb5\u4e2d\u6307\u5b9a vlan \u53f7\uff0c\u6211\u4eec\u5c06\u4f1a\u4e3a\u7f51\u5361\u521b\u5efa\u5bf9\u5e94\u7684 vlan \u5b50\u63a5\u53e3 \u6211\u4eec\u8fd8\u63d0\u4f9b\u5bf9\u7f51\u5361 bond \u7684\u652f\u6301\uff0c\u53ea\u9700\u5728 spec.bond.name \u548c spec.bond.mode \u91cc\u6307\u5b9a bond \u7f51\u5361\u7684\u540d\u5b57\u548c\u6a21\u5f0f\u5373\u53ef\u3002\u968f\u540e\u6211\u4eec\u4f1a\u81ea\u52a8\u4e3a\u4f60\u5b9e\u73b0\u591a\u5f20\u7f51\u5361 bond \u6210\u4e00\u5f20\u7f51\u5361\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR \uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms \u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a \u67e5\u770b service \u7684 IP\uff1a ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#macvlan-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Macvlan \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#_1","text":"\u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5df2\u5b89\u88c5 Helm \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u63d2\u4ef6\u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528 Ifacer \u63d2\u4ef6 \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u82e5\u76ee\u5f55\u4e0b\u4e0d\u5b58\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced: ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: [] phase: Synced serviceCIDR: - 10 .233.0.0/18 \u5982\u679c\u72b6\u6001\u4e3a NotReady ,\u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u76ee\u524d Spiderpool: * \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 * \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u7b49\u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap\uff0c\u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 \u521b\u5efa\u4e0e\u7f51\u7edc\u63a5\u53e3 eth0 \u5728\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u6c60\u4ee5\u4f9b Pod \u4f7f\u7528\uff0c\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF \u9a8c\u8bc1\u5b89\u88c5 ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528 \u82e5\u6709 vlan \u7684\u9700\u6c42\uff0c\u53ef\u5728 spec.vlanID \u5b57\u6bb5\u4e2d\u6307\u5b9a vlan \u53f7\uff0c\u6211\u4eec\u5c06\u4f1a\u4e3a\u7f51\u5361\u521b\u5efa\u5bf9\u5e94\u7684 vlan \u5b50\u63a5\u53e3 \u6211\u4eec\u8fd8\u63d0\u4f9b\u5bf9\u7f51\u5361 bond \u7684\u652f\u6301\uff0c\u53ea\u9700\u5728 spec.bond.name \u548c spec.bond.mode \u91cc\u6307\u5b9a bond \u7f51\u5361\u7684\u540d\u5b57\u548c\u6a21\u5f0f\u5373\u53ef\u3002\u968f\u540e\u6211\u4eec\u4f1a\u81ea\u52a8\u4e3a\u4f60\u5b9e\u73b0\u591a\u5f20\u7f51\u5361 bond \u6210\u4e00\u5f20\u7f51\u5361\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR \uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"\u521b\u5efa CNI \u914d\u7f6e"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#_2","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms \u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a \u67e5\u770b service \u7684 IP\uff1a ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-macvlan/","text":"Macvlan Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Macvlan and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort. Prerequisites System requirements Make sure a Kubernetes cluster is ready. Helm has been already installed. If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the plugin coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you want to create VLAN and Bond interfaces through Ifacer plugin , NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If Macvlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install Macvlan in your cluster. If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Please check if Spidercoordinator.status.phase is Synced : ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present: * Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. * If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Create a SpiderIPPool instance. Create an IP Pool in the same subnet as the network interface eth0 for Pods to use, the following is an example of creating a related SpiderIPPool: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF Verify installation ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false Create CNI configuration To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR . Here is an example of creating a Macvlan SpiderMultusConfig configuration: Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0. If there is a VLAN requirement, you can specify the VLAN ID in the spec.vlanID field. We will create the corresponding VLAN sub-interface for the network card. We also provide support for network card bonding. Just specify the name of the bond network card and its mode in the spec.bond.name and spec.bond.mode respectively. We will automatically combine multiple network cards into one bonded network card for you. MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF In the example of this article, use the above configuration to create the following Macvlan SpiderMultusConfig, which will automatically generate Multus NetworkAttachmentDefinition CR based on it, which corresponds to the eth0 network card of the host. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m Create applications Create test Pods and service via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the status of Pods: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s Test the communication between Pods: ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms Test the communication between Pods and service IP: ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Macvlan"},{"location":"usage/install/underlay/get-started-macvlan/#macvlan-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Macvlan and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan/#prerequisites","text":"System requirements Make sure a Kubernetes cluster is ready. Helm has been already installed. If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the plugin coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you want to create VLAN and Bond interfaces through Ifacer plugin , NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-macvlan/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If Macvlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install Macvlan in your cluster. If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Please check if Spidercoordinator.status.phase is Synced : ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present: * Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. * If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Create a SpiderIPPool instance. Create an IP Pool in the same subnet as the network interface eth0 for Pods to use, the following is an example of creating a related SpiderIPPool: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF Verify installation ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-macvlan/#create-cni-configuration","text":"To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR . Here is an example of creating a Macvlan SpiderMultusConfig configuration: Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0. If there is a VLAN requirement, you can specify the VLAN ID in the spec.vlanID field. We will create the corresponding VLAN sub-interface for the network card. We also provide support for network card bonding. Just specify the name of the bond network card and its mode in the spec.bond.name and spec.bond.mode respectively. We will automatically combine multiple network cards into one bonded network card for you. MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF In the example of this article, use the above configuration to create the following Macvlan SpiderMultusConfig, which will automatically generate Multus NetworkAttachmentDefinition CR based on it, which corresponds to the eth0 network card of the host. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"Create CNI configuration"},{"location":"usage/install/underlay/get-started-macvlan/#create-applications","text":"Create test Pods and service via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the status of Pods: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s Test the communication between Pods: ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms Test the communication between Pods and service IP: ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Create applications"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/","text":"Ovs-cni Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Ovs-cni \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002 ovs-cni \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002 \u5b89\u88c5\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u5b89\u88c5\u8bf4\u660e \u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002 ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Ifacer \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager \u8282\u70b9\u4e0a\u914d\u7f6e Open vSwitch \u7f51\u6865 \u5982\u4e0b\u662f\u521b\u5efa\u5e76\u914d\u7f6e\u6301\u4e45 OVS Bridge \u7684\u793a\u4f8b\uff0c\u672c\u6587\u4e2d\u4ee5 eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\u3002 Ubuntu \u7cfb\u7edf\u4f7f\u7528 netplan \u6301\u4e45\u5316 OVS Bridge \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f Ubuntu \u7cfb\u7edf\uff0c\u53ef\u4ee5\u53c2\u8003\u672c\u7ae0\u8282\u901a\u8fc7 netplan \u914d\u7f6e OVS Bridge\u3002 \u521b\u5efa OVS Bridge ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip link set br1 up \u5728 /etc/netplan \u76ee\u5f55\u4e0b\u521b\u5efa 12-br1.yaml \u540e\uff0c\u901a\u8fc7 netplan apply \u751f\u6548\u3002\u4e3a\u786e\u4fdd\u5728\u91cd\u542f\u4e3b\u673a\u7b49\u573a\u666f\u4e0b br1 \u4ecd\u7136\u53ef\u7528\uff0c\u8bf7\u68c0\u67e5 eth0 \u7f51\u5361\u662f\u5426\u4e5f\u88ab netplan \u7eb3\u7ba1\u3002 12-br1.yaml network : version : 2 renderer : networkd ethernets : br1 : addresses : - \"<IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801>\" # 172.18.10.10/16 \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever Fedora\u3001Centos \u7b49\u4f7f\u7528 NetworkManager \u6301\u4e45\u5316 OVS Bridge \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c\u63a8\u8350\u4f7f\u7528 NetworkManager \u6301\u4e45\u5316 OVS Bridge\u3002\u901a\u8fc7 NetworkManager \u6301\u4e45\u5316 OVS Bridge \u662f\u4e00\u79cd\u4e0d\u5c40\u9650\u64cd\u4f5c\u7cfb\u7edf\uff0c\u66f4\u901a\u7528\u7684\u4e00\u79cd\u65b9\u5f0f\u3002 \u4f7f\u7528 NetworkManager \u6301\u4e45\u5316 OVS Bridge\uff0c\u4f60\u9700\u8981\u5b89\u88c5 OVS NetworkManager \u63d2\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a ~# sudo dnf install -y NetworkManager-ovs ~# sudo systemctl restart NetworkManager \u521b\u5efa ovs \u7f51\u6865\u3001\u7aef\u53e3\u548c\u63a5\u53e3\u3002 ~# sudo nmcli con add type ovs-bridge conn.interface br1 con-name br1 ~# sudo nmcli con add type ovs-port conn.interface br1-port master br1 con-name br1-port ~# sudo nmcli con add type ovs-interface slave-type ovs-port conn.interface br1 master br1-port con-name br1-int \u5728\u7f51\u6865\u4e0a\u521b\u5efa\u53e6\u4e00\u4e2a\u7aef\u53e3\uff0c\u5e76\u9009\u62e9\u6211\u4eec\u7684\u7269\u7406\u8bbe\u5907\u4e2d\u7684 eth0 \u7f51\u5361\u4f5c\u4e3a\u5176\u4ee5\u592a\u7f51\u63a5\u53e3\uff0c\u4ee5\u4fbf\u771f\u6b63\u7684\u6d41\u91cf\u53ef\u4ee5\u5728\u7f51\u7edc\u4e0a\u6d41\u8f6c\u3002 ~# sudo nmcli con add type ovs-port conn.interface ovs-port-eth0 master br1 con-name ovs-port-eth0 ~# sudo nmcli con add type ethernet conn.interface eth0 master ovs-port-eth0 con-name ovs-port-eth0-int \u914d\u7f6e\u4e0e\u6fc0\u6d3b ovs \u7f51\u6865\u3002 \u901a\u8fc7\u8bbe\u7f6e\u9759\u6001 IP \u7684\u65b9\u5f0f\u914d\u7f6e\u7f51\u6865 ~# sudo nmcli con modify br1-int ipv4.method static ipv4.address \"<IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801>\" # 172.18.10.10/16 \u5982\u679c\u53ea\u6709\u4e00\u5f20\u7f51\u5361\uff0c\u4e14\u7f51\u5361 IP \u5730\u5740\u662f\u9759\u6001 IP \uff0c\u5f53 IP \u88ab\u914d\u7f6e\u7ed9 ovs bridge \u540e\uff0cSSH \u767b\u5f55\u5c31\u4f1a\u5931\u6548\uff0c\u6b64\u65f6\u5982\u4ecd\u9700 SSH \u8fdc\u7a0b\u8bbf\u95ee\u8be5 IP \uff0c\u5219\u9700\u4e3a\u7f51\u6865\u8bbe\u7f6e\u7f51\u5173\u548c DNS\u3002\u5982\u679c\u6709\u591a\u5f20\u7f51\u5361\uff0c\u53ef\u5ffd\u7565\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 ~# sudo nmcli con modify br1-int ipv4.gateway <\u7f51\u5173\u5730\u5740> # 172.18.0.1 ~# sudo nmcli con modify br1-int ipv4.dns <DNS \u5730\u5740> # 223.5.5.5 \u6fc0\u6d3b\u7f51\u6865\u3002 ~# sudo nmcli con down \"eth0\" ~# sudo nmcli con up ovs-port-eth0-int ~# sudo nmcli con up br1-int \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u4fe1\u606f\u3002 ~# nmcli c br1-int dbb1c9be-e1ab-4659-8d4b-564e3f8858fa ovs-interface br1 br1 a85626c1-2392-443b-a767-f86a57a1cff5 ovs-bridge br1 br1-port fe30170f-32d2-489e-9ca3-62c1f5371c6c ovs-port br1-port ovs-port-eth0 a43771a9-d840-4d2d-b1c3-c501a6da80ed ovs-port ovs-port-eth0 ovs-port-eth0-int 1334f49b-dae4-4225-830b-4d101ab6fad6 ethernet eth0 ~# ovs-vsctl show 203dd6d0-45f4-4137-955e-c4c36b9709e6 Bridge br1 Port ovs-port-eth0 Interface eth0 type: system Port br1-port Interface br1 type: internal ovs_version: \"3.2.1\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true \u5982\u679c\u672a\u5b89\u88c5 ovs-cni, \u53ef\u4ee5\u901a\u8fc7 Helm \u53c2\u6570 '-set plugins.installOvsCNI=true' \u5b89\u88c5\u5b83\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u4ee5\u5e2e\u52a9\u60a8\u5feb\u901f\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced: ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: [] phase: Synced serviceCIDR: - 10 .233.0.0/18 \u5982\u679c\u72b6\u6001\u4e3a NotReady ,\u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u76ee\u524d Spiderpool: * \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 * \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u7b49\u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap\uff0c\u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5 IP \u6c60\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5 IP \u6c60\u7684\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Ovs SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u7f51\u6865\u540d\u79f0\uff0c\u672c\u4f8b\u5b50\u4ee5 br1 \u4e3a\u4f8b: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool \u4e3a\u5e94\u7528\u5206\u914d\u4e86 IP\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#ovs-cni-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Ovs-cni \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002 ovs-cni \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#_1","text":"\u5b89\u88c5\u8981\u6c42 \u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u5b89\u88c5\u8bf4\u660e \u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002 ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Ifacer \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#open-vswitch","text":"\u5982\u4e0b\u662f\u521b\u5efa\u5e76\u914d\u7f6e\u6301\u4e45 OVS Bridge \u7684\u793a\u4f8b\uff0c\u672c\u6587\u4e2d\u4ee5 eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\u3002","title":"\u8282\u70b9\u4e0a\u914d\u7f6e Open vSwitch \u7f51\u6865"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#ubuntu-netplan-ovs-bridge","text":"\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f Ubuntu \u7cfb\u7edf\uff0c\u53ef\u4ee5\u53c2\u8003\u672c\u7ae0\u8282\u901a\u8fc7 netplan \u914d\u7f6e OVS Bridge\u3002 \u521b\u5efa OVS Bridge ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip link set br1 up \u5728 /etc/netplan \u76ee\u5f55\u4e0b\u521b\u5efa 12-br1.yaml \u540e\uff0c\u901a\u8fc7 netplan apply \u751f\u6548\u3002\u4e3a\u786e\u4fdd\u5728\u91cd\u542f\u4e3b\u673a\u7b49\u573a\u666f\u4e0b br1 \u4ecd\u7136\u53ef\u7528\uff0c\u8bf7\u68c0\u67e5 eth0 \u7f51\u5361\u662f\u5426\u4e5f\u88ab netplan \u7eb3\u7ba1\u3002 12-br1.yaml network : version : 2 renderer : networkd ethernets : br1 : addresses : - \"<IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801>\" # 172.18.10.10/16 \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever","title":"Ubuntu \u7cfb\u7edf\u4f7f\u7528 netplan \u6301\u4e45\u5316 OVS Bridge"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#fedoracentos-networkmanager-ovs-bridge","text":"\u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c\u63a8\u8350\u4f7f\u7528 NetworkManager \u6301\u4e45\u5316 OVS Bridge\u3002\u901a\u8fc7 NetworkManager \u6301\u4e45\u5316 OVS Bridge \u662f\u4e00\u79cd\u4e0d\u5c40\u9650\u64cd\u4f5c\u7cfb\u7edf\uff0c\u66f4\u901a\u7528\u7684\u4e00\u79cd\u65b9\u5f0f\u3002 \u4f7f\u7528 NetworkManager \u6301\u4e45\u5316 OVS Bridge\uff0c\u4f60\u9700\u8981\u5b89\u88c5 OVS NetworkManager \u63d2\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a ~# sudo dnf install -y NetworkManager-ovs ~# sudo systemctl restart NetworkManager \u521b\u5efa ovs \u7f51\u6865\u3001\u7aef\u53e3\u548c\u63a5\u53e3\u3002 ~# sudo nmcli con add type ovs-bridge conn.interface br1 con-name br1 ~# sudo nmcli con add type ovs-port conn.interface br1-port master br1 con-name br1-port ~# sudo nmcli con add type ovs-interface slave-type ovs-port conn.interface br1 master br1-port con-name br1-int \u5728\u7f51\u6865\u4e0a\u521b\u5efa\u53e6\u4e00\u4e2a\u7aef\u53e3\uff0c\u5e76\u9009\u62e9\u6211\u4eec\u7684\u7269\u7406\u8bbe\u5907\u4e2d\u7684 eth0 \u7f51\u5361\u4f5c\u4e3a\u5176\u4ee5\u592a\u7f51\u63a5\u53e3\uff0c\u4ee5\u4fbf\u771f\u6b63\u7684\u6d41\u91cf\u53ef\u4ee5\u5728\u7f51\u7edc\u4e0a\u6d41\u8f6c\u3002 ~# sudo nmcli con add type ovs-port conn.interface ovs-port-eth0 master br1 con-name ovs-port-eth0 ~# sudo nmcli con add type ethernet conn.interface eth0 master ovs-port-eth0 con-name ovs-port-eth0-int \u914d\u7f6e\u4e0e\u6fc0\u6d3b ovs \u7f51\u6865\u3002 \u901a\u8fc7\u8bbe\u7f6e\u9759\u6001 IP \u7684\u65b9\u5f0f\u914d\u7f6e\u7f51\u6865 ~# sudo nmcli con modify br1-int ipv4.method static ipv4.address \"<IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801>\" # 172.18.10.10/16 \u5982\u679c\u53ea\u6709\u4e00\u5f20\u7f51\u5361\uff0c\u4e14\u7f51\u5361 IP \u5730\u5740\u662f\u9759\u6001 IP \uff0c\u5f53 IP \u88ab\u914d\u7f6e\u7ed9 ovs bridge \u540e\uff0cSSH \u767b\u5f55\u5c31\u4f1a\u5931\u6548\uff0c\u6b64\u65f6\u5982\u4ecd\u9700 SSH \u8fdc\u7a0b\u8bbf\u95ee\u8be5 IP \uff0c\u5219\u9700\u4e3a\u7f51\u6865\u8bbe\u7f6e\u7f51\u5173\u548c DNS\u3002\u5982\u679c\u6709\u591a\u5f20\u7f51\u5361\uff0c\u53ef\u5ffd\u7565\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 ~# sudo nmcli con modify br1-int ipv4.gateway <\u7f51\u5173\u5730\u5740> # 172.18.0.1 ~# sudo nmcli con modify br1-int ipv4.dns <DNS \u5730\u5740> # 223.5.5.5 \u6fc0\u6d3b\u7f51\u6865\u3002 ~# sudo nmcli con down \"eth0\" ~# sudo nmcli con up ovs-port-eth0-int ~# sudo nmcli con up br1-int \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u4fe1\u606f\u3002 ~# nmcli c br1-int dbb1c9be-e1ab-4659-8d4b-564e3f8858fa ovs-interface br1 br1 a85626c1-2392-443b-a767-f86a57a1cff5 ovs-bridge br1 br1-port fe30170f-32d2-489e-9ca3-62c1f5371c6c ovs-port br1-port ovs-port-eth0 a43771a9-d840-4d2d-b1c3-c501a6da80ed ovs-port ovs-port-eth0 ovs-port-eth0-int 1334f49b-dae4-4225-830b-4d101ab6fad6 ethernet eth0 ~# ovs-vsctl show 203dd6d0-45f4-4137-955e-c4c36b9709e6 Bridge br1 Port ovs-port-eth0 Interface eth0 type: system Port br1-port Interface br1 type: internal ovs_version: \"3.2.1\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever","title":"Fedora\u3001Centos \u7b49\u4f7f\u7528 NetworkManager \u6301\u4e45\u5316 OVS Bridge"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true \u5982\u679c\u672a\u5b89\u88c5 ovs-cni, \u53ef\u4ee5\u901a\u8fc7 Helm \u53c2\u6570 '-set plugins.installOvsCNI=true' \u5b89\u88c5\u5b83\u3002 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u4ee5\u5e2e\u52a9\u60a8\u5feb\u901f\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced: ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: [] phase: Synced serviceCIDR: - 10 .233.0.0/18 \u5982\u679c\u72b6\u6001\u4e3a NotReady ,\u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u76ee\u524d Spiderpool: * \u4f18\u5148\u901a\u8fc7\u67e5\u8be2 kube-system/kubeadm-config ConfigMap \u83b7\u53d6\u96c6\u7fa4\u7684 Pod \u548c Service \u5b50\u7f51\u3002 * \u5982\u679c kubeadm-config \u4e0d\u5b58\u5728\u5bfc\u81f4\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u5b50\u7f51\uff0c\u90a3\u4e48 Spiderpool \u4f1a\u4ece Kube-controller-manager Pod \u4e2d\u83b7\u53d6\u96c6\u7fa4 Pod \u548c Service \u7684\u5b50\u7f51\u3002 \u5982\u679c\u60a8\u96c6\u7fa4\u7684 Kube-controller-manager \u7ec4\u4ef6\u4ee5 systemd \u7b49\u65b9\u5f0f\u800c\u4e0d\u662f\u4ee5\u9759\u6001 Pod \u8fd0\u884c\u3002\u90a3\u4e48 Spiderpool \u4ecd\u7136\u65e0\u6cd5\u83b7\u53d6\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f\u3002 \u5982\u679c\u4e0a\u9762\u4e24\u79cd\u65b9\u5f0f\u90fd\u5931\u8d25\uff0cSpiderpool \u4f1a\u540c\u6b65 status.phase \u4e3a NotReady, \u8fd9\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u521b\u5efa kubeadm-config ConfigMap\uff0c\u5e76\u6b63\u786e\u914d\u7f6e\u96c6\u7fa4\u7684\u5b50\u7f51\u4fe1\u606f: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF \u4e00\u65e6\u521b\u5efa\u5b8c\u6210\uff0cSpiderpool \u5c06\u4f1a\u81ea\u52a8\u540c\u6b65\u5176\u72b6\u6001\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5 IP \u6c60\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5 IP \u6c60\u7684\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Ovs SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u7f51\u6865\u540d\u79f0\uff0c\u672c\u4f8b\u5b50\u4ee5 br1 \u4e3a\u4f8b: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#_2","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool \u4e3a\u5e94\u7528\u5206\u914d\u4e86 IP\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-ovs/","text":"Ovs-cni Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus , Ovs-cni , and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster. ovs-cni is a Kubernetes CNI plugin that utilizes Open vSwitch (OVS) to enable network virtualization within a Kubernetes cluster. Prerequisites System requirements Make sure a multi-node Kubernetes cluster is ready. Helm has been already installed. Open vSwitch must be installed and running on the host. It could refer to Installation . The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system. ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager Configure Open vSwitch bridge on the node The following is an example of creating and configuring a persistent OVS Bridge. This article takes the eth0 network card as an example and needs to be executed on each node. Ubuntu system uses netplan persistence OVS Bridge If you are using an Ubuntu system, you can refer to this chapter to configure OVS Bridge through netplan . Create OVS Bridge ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip link set br1 up After creating 12-br1.yaml in the /etc/netplan directory, run netplan apply to take effect. To ensure that br1 is still available in scenarios such as restarting the host, please check whether the eth0 network card is also managed by netplan. 12-br1.yaml network : version : 2 renderer : networkd ethernets : br1 : addresses : - \"<IP address>/<Subnet mask>\" # 172.18.10.10/16 After creation, you can view the following bridge information on each node: ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever Fedora, Centos, etc. use NetworkManager to persist OVS Bridge If you use OS such as Fedora, Centos, etc., it is recommended to use NetworkManager persistent OVS Bridge. Persisting OVS Bridge through NetworkManager is a more general method that is not limited to operating systems. To use NetworkManager to persist OVS Bridge, you need to install the OVS NetworkManager plug-in. The example is as follows: ~# sudo dnf install -y NetworkManager-ovs ~# sudo systemctl restart NetworkManager Create ovs bridges, ports and interfaces. ~# sudo nmcli con add type ovs-bridge conn.interface br1 con-name br1 ~# sudo nmcli con add type ovs-port conn.interface br1-port master br1 con-name br1-port ~# sudo nmcli con add type ovs-interface slave-type ovs-port conn.interface br1 master br1-port con-name br1-int Create another port on the bridge and select the eth0 NIC in the physical device as its Ethernet interface so that real traffic can flow on the network. ~# sudo nmcli con add type ovs-port conn.interface ovs-port-eth0 master br1 con-name ovs-port-eth0 ~# sudo nmcli con add type ethernet conn.interface eth0 master ovs-port-eth0 con-name ovs-port-eth0-int Configure and activate the ovs bridge. Configure the bridge by setting a static IP. ~# sudo nmcli con modify br1-int ipv4.method static ipv4.address \"<IP address>/<subnet mask>\" # 172.18.10.10/16 If there is only one network card with static IP, since this IP is configuired to ovs bridge, the original ssh will fail, you need to set gateway and DNS address for the bridge to make ssh work again. If there are multiple network cards, you can ignore this two steps. ~# sudo nmcli con modify br1-int ipv4.gateway <gateway> # 172.18.0.1 ~# sudo nmcli con modify br1-int ipv4.dns <DNS> # 223.5.5.5 Activate bridge ~# sudo nmcli con down \"eth0\" ~# sudo nmcli con up ovs-port-eth0-int ~# sudo nmcli con up br1-int After creation, you can view information similar to the following on each node. ~# nmcli c br1-int dbb1c9be-e1ab-4659-8d4b-564e3f8858fa ovs-interface br1 br1 a85626c1-2392-443b-a767-f86a57a1cff5 ovs-bridge br1 br1-port fe30170f-32d2-489e-9ca3-62c1f5371c6c ovs-port br1-port ovs-port-eth0 a43771a9-d840-4d2d-b1c3-c501a6da80ed ovs-port ovs-port-eth0 ovs-port-eth0-int 1334f49b-dae4-4225-830b-4d101ab6fad6 ethernet eth0 ~# ovs-vsctl show 203dd6d0-45f4-4137-955e-c4c36b9709e6 Bridge br1 Port ovs-port-eth0 Interface eth0 type: system Port br1-port Interface br1 type: internal ovs_version: \"3.2.1\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true If ovs-cni is not installed, you can install it by specifying the Helm parameter --set plugins.installOvsCNI=true . If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Please check if Spidercoordinator.status.phase is Synced : ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present: Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Create a SpiderIPPool instance. The Pod will obtain an IP address from the IP pool for underlying network communication, so the subnet of the IP pool needs to correspond to the underlying subnet being accessed. Here is an example of creating a SpiderSubnet instance: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF Verify the installation\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an ovs-cni SpiderMultusConfig configuration: Confirm the bridge name for ovs-cni. Take the host bridge: br1 as an example: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF Create Applications In the following example Yaml, 2 copies of the Deployment are created, of which: v1.multus-cni.io/default-network : used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool assigns an IP to the application, and the application's IP will be automatically fixed within this IP range: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane Testing Pod communication with cross-node Pods: ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Ovs"},{"location":"usage/install/underlay/get-started-ovs/#ovs-cni-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus , Ovs-cni , and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster. ovs-cni is a Kubernetes CNI plugin that utilizes Open vSwitch (OVS) to enable network virtualization within a Kubernetes cluster.","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs/#prerequisites","text":"System requirements Make sure a multi-node Kubernetes cluster is ready. Helm has been already installed. Open vSwitch must be installed and running on the host. It could refer to Installation . The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system. ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-ovs/#configure-open-vswitch-bridge-on-the-node","text":"The following is an example of creating and configuring a persistent OVS Bridge. This article takes the eth0 network card as an example and needs to be executed on each node.","title":"Configure Open vSwitch bridge on the node"},{"location":"usage/install/underlay/get-started-ovs/#ubuntu-system-uses-netplan-persistence-ovs-bridge","text":"If you are using an Ubuntu system, you can refer to this chapter to configure OVS Bridge through netplan . Create OVS Bridge ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip link set br1 up After creating 12-br1.yaml in the /etc/netplan directory, run netplan apply to take effect. To ensure that br1 is still available in scenarios such as restarting the host, please check whether the eth0 network card is also managed by netplan. 12-br1.yaml network : version : 2 renderer : networkd ethernets : br1 : addresses : - \"<IP address>/<Subnet mask>\" # 172.18.10.10/16 After creation, you can view the following bridge information on each node: ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever","title":"Ubuntu system uses netplan persistence OVS Bridge"},{"location":"usage/install/underlay/get-started-ovs/#fedora-centos-etc-use-networkmanager-to-persist-ovs-bridge","text":"If you use OS such as Fedora, Centos, etc., it is recommended to use NetworkManager persistent OVS Bridge. Persisting OVS Bridge through NetworkManager is a more general method that is not limited to operating systems. To use NetworkManager to persist OVS Bridge, you need to install the OVS NetworkManager plug-in. The example is as follows: ~# sudo dnf install -y NetworkManager-ovs ~# sudo systemctl restart NetworkManager Create ovs bridges, ports and interfaces. ~# sudo nmcli con add type ovs-bridge conn.interface br1 con-name br1 ~# sudo nmcli con add type ovs-port conn.interface br1-port master br1 con-name br1-port ~# sudo nmcli con add type ovs-interface slave-type ovs-port conn.interface br1 master br1-port con-name br1-int Create another port on the bridge and select the eth0 NIC in the physical device as its Ethernet interface so that real traffic can flow on the network. ~# sudo nmcli con add type ovs-port conn.interface ovs-port-eth0 master br1 con-name ovs-port-eth0 ~# sudo nmcli con add type ethernet conn.interface eth0 master ovs-port-eth0 con-name ovs-port-eth0-int Configure and activate the ovs bridge. Configure the bridge by setting a static IP. ~# sudo nmcli con modify br1-int ipv4.method static ipv4.address \"<IP address>/<subnet mask>\" # 172.18.10.10/16 If there is only one network card with static IP, since this IP is configuired to ovs bridge, the original ssh will fail, you need to set gateway and DNS address for the bridge to make ssh work again. If there are multiple network cards, you can ignore this two steps. ~# sudo nmcli con modify br1-int ipv4.gateway <gateway> # 172.18.0.1 ~# sudo nmcli con modify br1-int ipv4.dns <DNS> # 223.5.5.5 Activate bridge ~# sudo nmcli con down \"eth0\" ~# sudo nmcli con up ovs-port-eth0-int ~# sudo nmcli con up br1-int After creation, you can view information similar to the following on each node. ~# nmcli c br1-int dbb1c9be-e1ab-4659-8d4b-564e3f8858fa ovs-interface br1 br1 a85626c1-2392-443b-a767-f86a57a1cff5 ovs-bridge br1 br1-port fe30170f-32d2-489e-9ca3-62c1f5371c6c ovs-port br1-port ovs-port-eth0 a43771a9-d840-4d2d-b1c3-c501a6da80ed ovs-port ovs-port-eth0 ovs-port-eth0-int 1334f49b-dae4-4225-830b-4d101ab6fad6 ethernet eth0 ~# ovs-vsctl show 203dd6d0-45f4-4137-955e-c4c36b9709e6 Bridge br1 Port ovs-port-eth0 Interface eth0 type: system Port br1-port Interface br1 type: internal ovs_version: \"3.2.1\" ~# ip a show br1 208 : br1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 00 :50:56:b4:5f:fd brd ff:ff:ff:ff:ff:ff inet 172 .18.10.10/16 brd 172 .18.255.255 scope global noprefixroute br1 valid_lft forever preferred_lft forever inet6 fe80::4f28:8ef1:6b82:a9e4/64 scope link noprefixroute valid_lft forever preferred_lft forever","title":"Fedora, Centos, etc. use NetworkManager to persist OVS Bridge"},{"location":"usage/install/underlay/get-started-ovs/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true If ovs-cni is not installed, you can install it by specifying the Helm parameter --set plugins.installOvsCNI=true . If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Please check if Spidercoordinator.status.phase is Synced : ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 podRPFilter: 0 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 At present: Spiderpool prioritizes obtaining the cluster's Pod and Service subnets by querying the kube-system/kubeadm-config ConfigMap. If the kubeadm-config does not exist, causing the failure to obtain the cluster subnet, Spiderpool will attempt to retrieve the cluster Pod and Service subnets from the kube-controller-manager Pod. If the kube-controller-manager component in your cluster runs in systemd mode instead of as a static Pod, Spiderpool still cannot retrieve the cluster's subnet information. If both of the above methods fail, Spiderpool will synchronize the status.phase as NotReady, preventing Pod creation. To address such abnormal situations, we can manually create the kubeadm-config ConfigMap and correctly configure the cluster's subnet information: export POD_SUBNET = <YOUR_POD_SUBNET> export SERVICE_SUBNET = <YOUR_SERVICE_SUBNET> cat << EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: kubeadm-config namespace: kube-system data: ClusterConfiguration: | networking: podSubnet: ${POD_SUBNET} serviceSubnet: ${SERVICE_SUBNET} EOF Create a SpiderIPPool instance. The Pod will obtain an IP address from the IP pool for underlying network communication, so the subnet of the IP pool needs to correspond to the underlying subnet being accessed. Here is an example of creating a SpiderSubnet instance: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF Verify the installation\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an ovs-cni SpiderMultusConfig configuration: Confirm the bridge name for ovs-cni. Take the host bridge: br1 as an example: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-ovs/#create-applications","text":"In the following example Yaml, 2 copies of the Deployment are created, of which: v1.multus-cni.io/default-network : used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool assigns an IP to the application, and the application's IP will be automatically fixed within this IP range: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane Testing Pod communication with cross-node Pods: ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Create Applications"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/","text":"SR-IOV Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 SR-IOV \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u7684\u7f51\u5361\u5177\u6709 SR-IOV \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1 \u5b89\u88c5\u8981\u6c42 System requirements \u4e00\u4e2a Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361 \u67e5\u8be2\u7f51\u5361 bus-info\uff1a ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 \u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 Single Root I/O Virtualization (SR-IOV) \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Ifacer \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" \u5e26\u4e0a helm \u9009\u9879 --set sriov.install=true \uff0c \u4f1a\u5b89\u88c5 sriov-network-operator \uff0cresourcePrefix \u9ed8\u8ba4\u4e3a \"spidernet.io\"\uff0c\u53ef\u901a\u8fc7 helm \u9009\u9879 --set sriov.resourcePrefix \u4fee\u6539 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7ed9\u5e0c\u671b\u8fd0\u884c SR-IOV CNI \u7684\u8282\u70b9\uff0c\u6309\u7167\u5982\u4e0b\u547d\u4ee4\u6253\u4e0a label\uff0c\u8fd9\u6837\uff0csriov-network-operator \u624d\u4f1a\u5728\u6307\u5b9a\u7684\u8282\u70b9\u4e0a\u5b89\u88c5\u7ec4\u4ef6 kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" \u5728\u8282\u70b9\u4e0a\u521b\u5efa VF \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u8282\u70b9\u4e0a\u7684\u53ef\u7528\u7f51\u5361 $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded \u5982\u679c SriovNetworkNodeState CRs \u7684\u72b6\u6001\u4e3a InProgress , \u8bf4\u660e sriov-operator \u6b63\u5728\u540c\u6b65\u8282\u70b9\u72b6\u6001\uff0c\u7b49\u5f85\u72b6\u6001\u4e3a Succeeded \u8bf4\u660e\u540c\u6b65\u5b8c\u6210\u3002\u67e5\u770b CR, \u786e\u8ba4 sriov-network-operator \u5df2\u7ecf\u53d1\u73b0\u8282\u70b9\u4e0a\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361\u3002 \u4ece\u4e0a\u9762\u53ef\u77e5\uff0c\u8282\u70b9 node-1 \u4e0a\u7684\u7f51\u5361 enp4s0f0np0 \u5177\u6709 SR-IOV \u529f\u80fd\uff0c\u5e76\u4e14\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\u4e3a 8\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7\u521b\u5efa SriovNetworkNodePolicy CRs \u5e76\u901a\u8fc7 nicSelector.pfNames \u6307\u5b9a PF (Physical function, \u7269\u7406\u7f51\u5361)\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u8282\u70b9\u4e0a\u7684\u8fd9\u4e9b\u7f51\u5361\u521b\u5efa\u51fa VF(Virtual Function): $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # \u6e34\u671b\u7684 VFs \u6570\u91cf resourceName: sriov_netdevice EOF \u4e0b\u53d1\u5982\u4e0a\u547d\u4ee4\u540e\uff0c\u56e0\u4e3a\u9700\u8981\u914d\u7f6e\u8282\u70b9\u542f\u7528 SR-IOV \u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u91cd\u542f\u8282\u70b9\u3002\u5982\u6709\u9700\u8981\uff0c\u6307\u5b9a\u5de5\u4f5c\u8282\u70b9\u800c\u975e Master \u8282\u70b9\u3002 resourceName \u4e0d\u80fd\u4e3a\u7279\u6b8a\u5b57\u7b26\uff0c\u652f\u6301\u7684\u5b57\u7b26: [0-9],[a-zA-Z] \u548c \"_\". \u5728\u4e0b\u53d1 SriovNetworkNodePolicy CRs \u4e4b\u540e\uff0c\u518d\u6b21\u67e5\u770b SriovNetworkNodeState CRs \u7684\u72b6\u6001, \u53ef\u4ee5\u770b\u89c1 status \u4e2d VF \u5df2\u7ecf\u5f97\u5230\u914d\u7f6e: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... \u67e5\u770b Node \u53d1\u73b0\u540d\u4e3a spidernet.io/sriov_netdevice \u7684 SR-IOV \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d VF \u7684\u6570\u91cf\u4e3a 8: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } sriov-network-config-daemon Pod \u8d1f\u8d23\u5728\u8282\u70b9\u4e0a\u914d\u7f6e VF \uff0c\u5176\u4f1a\u987a\u5e8f\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u5b8c\u6210\u8be5\u5de5\u4f5c\u3002\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u914d\u7f6e VF \u65f6\uff0csriov-network-config-daemon \u4f1a\u5bf9\u8282\u70b9\u4e0a\u7684\u6240\u6709 Pod \u8fdb\u884c\u9a71\u9010\uff0c\u914d\u7f6e VF \uff0c\u5e76\u53ef\u80fd\u91cd\u542f\u8282\u70b9\u3002\u5f53 sriov-network-config-daemon \u9a71\u9010\u67d0\u4e2a Pod \u5931\u8d25\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6240\u6709\u6d41\u7a0b\u90fd\u505c\u6ede\uff0c\u4ece\u800c\u5bfc\u81f4 node \u7684 VF \u6570\u91cf\u4e00\u76f4\u4e3a 0\u3002 \u8fd9\u79cd\u60c5\u51b5\u65f6\uff0csriov-network-config-daemon Pod \u4f1a\u770b\u5230\u5982\u4e0b\u7c7b\u4f3c\u65e5\u5fd7\uff1a error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... \u8be5\u95ee\u9898\u53ef\u53c2\u8003 sriov-network-operator \u793e\u533a\u7684\u7c7b\u4f3c issue \u6b64\u65f6\uff0c\u53ef\u6392\u67e5\u6307\u5b9a Pod \u4e3a\u5565\u65e0\u6cd5\u9a71\u9010\u7684\u539f\u56e0\uff0c\u6709\u5982\u4e0b\u53ef\u80fd\uff1a \u8be5\u9a71\u9010\u5931\u8d25\u7684 Pod \u53ef\u80fd\u914d\u7f6e\u4e86 PodDisruptionBudget\uff0c\u5bfc\u81f4\u53ef\u7528\u526f\u672c\u6570\u4e0d\u8db3\u3002\u8bf7\u8c03\u6574 PodDisruptionBudget \u96c6\u7fa4\u4e2d\u7684\u53ef\u7528\u8282\u70b9\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6ca1\u6709\u8282\u70b9\u53ef\u4ee5\u8c03\u5ea6 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF \u521b\u5efa SpiderMultusConfig \u5b9e\u4f8b\u3002 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice EOF SpiderIPPool.Spec.multusName: kube-system/sriov-test \u8981\u548c\u521b\u5efa\u7684 SpiderMultusConfig \u5b9e\u4f8b\u7684 Name \u548c Namespace \u76f8\u5339\u914d resourceName: spidernet.io/sriov_netdevice \u7531\u5b89\u88c5 sriov-operator \u6307\u5b9a\u7684 resourcePrefix: spidernet.io \u548c\u521b\u5efa SriovNetworkNodePolicy CR \u65f6\u6307\u5b9a\u7684 resourceName: sriov_netdevice \u62fc\u63a5\u800c\u6210 \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet.io/sriov_netdevice: '1' limits: spidernet.io/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF \u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a spidernet/sriov_netdevice : \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 SR-IOV \u8d44\u6e90\u3002 v1.multus-cni.io/default-network \uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002 \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3 \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001 ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185: ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms \u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf \u67e5\u770b Service \u7684 IP\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a ```shell ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10.43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08:22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15:53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"SR-IOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#sr-iov-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 SR-IOV \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u7684\u7f51\u5361\u5177\u6709 SR-IOV \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1","title":"SR-IOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#_1","text":"System requirements \u4e00\u4e2a Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361 \u67e5\u8be2\u7f51\u5361 bus-info\uff1a ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 \u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 Single Root I/O Virtualization (SR-IOV) \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Ifacer \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" \u5e26\u4e0a helm \u9009\u9879 --set sriov.install=true \uff0c \u4f1a\u5b89\u88c5 sriov-network-operator \uff0cresourcePrefix \u9ed8\u8ba4\u4e3a \"spidernet.io\"\uff0c\u53ef\u901a\u8fc7 helm \u9009\u9879 --set sriov.resourcePrefix \u4fee\u6539 \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7ed9\u5e0c\u671b\u8fd0\u884c SR-IOV CNI \u7684\u8282\u70b9\uff0c\u6309\u7167\u5982\u4e0b\u547d\u4ee4\u6253\u4e0a label\uff0c\u8fd9\u6837\uff0csriov-network-operator \u624d\u4f1a\u5728\u6307\u5b9a\u7684\u8282\u70b9\u4e0a\u5b89\u88c5\u7ec4\u4ef6 kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" \u5728\u8282\u70b9\u4e0a\u521b\u5efa VF \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u8282\u70b9\u4e0a\u7684\u53ef\u7528\u7f51\u5361 $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded \u5982\u679c SriovNetworkNodeState CRs \u7684\u72b6\u6001\u4e3a InProgress , \u8bf4\u660e sriov-operator \u6b63\u5728\u540c\u6b65\u8282\u70b9\u72b6\u6001\uff0c\u7b49\u5f85\u72b6\u6001\u4e3a Succeeded \u8bf4\u660e\u540c\u6b65\u5b8c\u6210\u3002\u67e5\u770b CR, \u786e\u8ba4 sriov-network-operator \u5df2\u7ecf\u53d1\u73b0\u8282\u70b9\u4e0a\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361\u3002 \u4ece\u4e0a\u9762\u53ef\u77e5\uff0c\u8282\u70b9 node-1 \u4e0a\u7684\u7f51\u5361 enp4s0f0np0 \u5177\u6709 SR-IOV \u529f\u80fd\uff0c\u5e76\u4e14\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\u4e3a 8\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7\u521b\u5efa SriovNetworkNodePolicy CRs \u5e76\u901a\u8fc7 nicSelector.pfNames \u6307\u5b9a PF (Physical function, \u7269\u7406\u7f51\u5361)\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u8282\u70b9\u4e0a\u7684\u8fd9\u4e9b\u7f51\u5361\u521b\u5efa\u51fa VF(Virtual Function): $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # \u6e34\u671b\u7684 VFs \u6570\u91cf resourceName: sriov_netdevice EOF \u4e0b\u53d1\u5982\u4e0a\u547d\u4ee4\u540e\uff0c\u56e0\u4e3a\u9700\u8981\u914d\u7f6e\u8282\u70b9\u542f\u7528 SR-IOV \u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u91cd\u542f\u8282\u70b9\u3002\u5982\u6709\u9700\u8981\uff0c\u6307\u5b9a\u5de5\u4f5c\u8282\u70b9\u800c\u975e Master \u8282\u70b9\u3002 resourceName \u4e0d\u80fd\u4e3a\u7279\u6b8a\u5b57\u7b26\uff0c\u652f\u6301\u7684\u5b57\u7b26: [0-9],[a-zA-Z] \u548c \"_\". \u5728\u4e0b\u53d1 SriovNetworkNodePolicy CRs \u4e4b\u540e\uff0c\u518d\u6b21\u67e5\u770b SriovNetworkNodeState CRs \u7684\u72b6\u6001, \u53ef\u4ee5\u770b\u89c1 status \u4e2d VF \u5df2\u7ecf\u5f97\u5230\u914d\u7f6e: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... \u67e5\u770b Node \u53d1\u73b0\u540d\u4e3a spidernet.io/sriov_netdevice \u7684 SR-IOV \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d VF \u7684\u6570\u91cf\u4e3a 8: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } sriov-network-config-daemon Pod \u8d1f\u8d23\u5728\u8282\u70b9\u4e0a\u914d\u7f6e VF \uff0c\u5176\u4f1a\u987a\u5e8f\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u5b8c\u6210\u8be5\u5de5\u4f5c\u3002\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u914d\u7f6e VF \u65f6\uff0csriov-network-config-daemon \u4f1a\u5bf9\u8282\u70b9\u4e0a\u7684\u6240\u6709 Pod \u8fdb\u884c\u9a71\u9010\uff0c\u914d\u7f6e VF \uff0c\u5e76\u53ef\u80fd\u91cd\u542f\u8282\u70b9\u3002\u5f53 sriov-network-config-daemon \u9a71\u9010\u67d0\u4e2a Pod \u5931\u8d25\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6240\u6709\u6d41\u7a0b\u90fd\u505c\u6ede\uff0c\u4ece\u800c\u5bfc\u81f4 node \u7684 VF \u6570\u91cf\u4e00\u76f4\u4e3a 0\u3002 \u8fd9\u79cd\u60c5\u51b5\u65f6\uff0csriov-network-config-daemon Pod \u4f1a\u770b\u5230\u5982\u4e0b\u7c7b\u4f3c\u65e5\u5fd7\uff1a error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... \u8be5\u95ee\u9898\u53ef\u53c2\u8003 sriov-network-operator \u793e\u533a\u7684\u7c7b\u4f3c issue \u6b64\u65f6\uff0c\u53ef\u6392\u67e5\u6307\u5b9a Pod \u4e3a\u5565\u65e0\u6cd5\u9a71\u9010\u7684\u539f\u56e0\uff0c\u6709\u5982\u4e0b\u53ef\u80fd\uff1a \u8be5\u9a71\u9010\u5931\u8d25\u7684 Pod \u53ef\u80fd\u914d\u7f6e\u4e86 PodDisruptionBudget\uff0c\u5bfc\u81f4\u53ef\u7528\u526f\u672c\u6570\u4e0d\u8db3\u3002\u8bf7\u8c03\u6574 PodDisruptionBudget \u96c6\u7fa4\u4e2d\u7684\u53ef\u7528\u8282\u70b9\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6ca1\u6709\u8282\u70b9\u53ef\u4ee5\u8c03\u5ea6 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF \u521b\u5efa SpiderMultusConfig \u5b9e\u4f8b\u3002 $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice EOF SpiderIPPool.Spec.multusName: kube-system/sriov-test \u8981\u548c\u521b\u5efa\u7684 SpiderMultusConfig \u5b9e\u4f8b\u7684 Name \u548c Namespace \u76f8\u5339\u914d resourceName: spidernet.io/sriov_netdevice \u7531\u5b89\u88c5 sriov-operator \u6307\u5b9a\u7684 resourcePrefix: spidernet.io \u548c\u521b\u5efa SriovNetworkNodePolicy CR \u65f6\u6307\u5b9a\u7684 resourceName: sriov_netdevice \u62fc\u63a5\u800c\u6210","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#_2","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet.io/sriov_netdevice: '1' limits: spidernet.io/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF \u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a spidernet/sriov_netdevice : \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 SR-IOV \u8d44\u6e90\u3002 v1.multus-cni.io/default-network \uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002 \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3 \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001 ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185: ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms \u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf \u67e5\u770b Service \u7684 IP\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a ```shell ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10.43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08:22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15:53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-sriov/","text":"SR-IOV Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , SR-IOV , Veth , and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort. Prerequisites System requirements Make sure a Kubernetes cluster is ready Helm has already been installed A SR-IOV-enabled NIC Check the NIC's bus-info: ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 Check whether the NIC supports SR-IOV via bus-info. If the Single Root I/O Virtualization (SR-IOV) field appears, it means that SR-IOV is supported: ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" When using the helm option --set sriov.install=true , it will install the sriov-network-operator . The default value for resourcePrefix is \"spidernet.io\" which can be modified via the helm option --set sriov.resourcePrefix . For users in the Chinese mainland, it is recommended to specify the spec --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. To enable the SR-IOV CNI on specific nodes, you need to apply the following command to label those nodes. This will allow the sriov-network-operator to install the components on the designated nodes. kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" Create VFs on the node Use the following command to view the available network interfaces on the node: $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded If the status of SriovNetworkNodeState CRs is InProgress , it indicates that the sriov-operator is currently synchronizing the node state. Wait for the status to become Succeeded to confirm that the synchronization is complete. Check the CR to ensure that the sriov-network-operator has discovered the network interfaces on the node that support SR-IOV. Based on the given information, it is known that the network interface's enp4s0f0np0 on the node node-1 supports SR-IOV capability with a maximum of 8 VFs. Now, let's create SriovNetworkNodePolicy CRs and specify PF (Physical function, physical network interface) through nicSelector.pfNames to generate VFs(Virtual Function) on these network interfaces of the respective nodes: $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # desired number of VFs resourceName: sriov_netdevice EOF After executing the above command, please note that configuring nodes to enable SR-IOV functionality may require a node restart. If needed, specify worker nodes instead of master nodes for this configuration. The resourceName should not contain special characters and is limited to [0-9], [a-zA-Z], and \"_\". After applying the SriovNetworkNodePolicy CRs, you can check the status of the SriovNetworkNodeState CRs again to verify that the VFs have been successfully configured: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... To confirm that the SR-IOV resources named spidernet.io/sriov_netdevice have been successfully enabled on a specific node and that the number of VFs is set to 8, you can use the following command: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } The sriov-network-config-daemon Pod is responsible for configuring VF on nodes, and it will sequentially complete the work on each node. When configuring VF on each node, the SR-IOV network configuration daemon will evict all Pods on the node, configure VF, and possibly restart the node. When SR-IOV network configuration daemon fails to evict a Pod, it will cause all processes to stop, resulting in the vf number of nodes remaining at 0. In this case, the SR-IOV network configuration daemon Pod will see logs similar to the following: error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... This issue can be referred to similar topics in the sriov-network-operator community issue The reason why the designated Pod cannot be expelled can be investigated, which may include the following: The Pod that failed the eviction may have been configured with a PodDisruptionBudget, resulting in a shortage of available replicas. Please adjust the PodDisruptionBudget Insufficient available nodes in the cluster, resulting in no nodes available for scheduling Create a SpiderIPPool instance. The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF Create a SpiderMultusConfig instance. shell $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice SpiderIPPool.Spec.multusName: 'kube-system/sriov-test' must be to match the Name and Namespace of the SpiderMultusConfig instance created. Create applications Create test Pods and Services via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet/sriov_netdevice: '1' limits: spidernet/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF Spec descriptions: spidernet/sriov_netdevice : Sriov resources used. v1.multus-cni.io/default-network : specifies the CNI configuration for Multus. For more information on Multus annotations, refer to Multus Quickstart . Check the status of Pods: ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> Spiderpool ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 Test the communication between Pods: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms Test the communication between Pods and Services: Check Services' IPs\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Access its own service within the Pod: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"SR-IOV"},{"location":"usage/install/underlay/get-started-sriov/#sr-iov-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , SR-IOV , Veth , and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.","title":"SR-IOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov/#prerequisites","text":"System requirements Make sure a Kubernetes cluster is ready Helm has already been installed A SR-IOV-enabled NIC Check the NIC's bus-info: ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 Check whether the NIC supports SR-IOV via bus-info. If the Single Root I/O Virtualization (SR-IOV) field appears, it means that SR-IOV is supported: ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat > /etc/NetworkManager/conf.d/spidernet.conf <<EOF [keyfile] unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} EOF ~# systemctl restart NetworkManager","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-sriov/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" When using the helm option --set sriov.install=true , it will install the sriov-network-operator . The default value for resourcePrefix is \"spidernet.io\" which can be modified via the helm option --set sriov.resourcePrefix . For users in the Chinese mainland, it is recommended to specify the spec --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. To enable the SR-IOV CNI on specific nodes, you need to apply the following command to label those nodes. This will allow the sriov-network-operator to install the components on the designated nodes. kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" Create VFs on the node Use the following command to view the available network interfaces on the node: $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded If the status of SriovNetworkNodeState CRs is InProgress , it indicates that the sriov-operator is currently synchronizing the node state. Wait for the status to become Succeeded to confirm that the synchronization is complete. Check the CR to ensure that the sriov-network-operator has discovered the network interfaces on the node that support SR-IOV. Based on the given information, it is known that the network interface's enp4s0f0np0 on the node node-1 supports SR-IOV capability with a maximum of 8 VFs. Now, let's create SriovNetworkNodePolicy CRs and specify PF (Physical function, physical network interface) through nicSelector.pfNames to generate VFs(Virtual Function) on these network interfaces of the respective nodes: $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # desired number of VFs resourceName: sriov_netdevice EOF After executing the above command, please note that configuring nodes to enable SR-IOV functionality may require a node restart. If needed, specify worker nodes instead of master nodes for this configuration. The resourceName should not contain special characters and is limited to [0-9], [a-zA-Z], and \"_\". After applying the SriovNetworkNodePolicy CRs, you can check the status of the SriovNetworkNodeState CRs again to verify that the VFs have been successfully configured: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... To confirm that the SR-IOV resources named spidernet.io/sriov_netdevice have been successfully enabled on a specific node and that the number of VFs is set to 8, you can use the following command: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } The sriov-network-config-daemon Pod is responsible for configuring VF on nodes, and it will sequentially complete the work on each node. When configuring VF on each node, the SR-IOV network configuration daemon will evict all Pods on the node, configure VF, and possibly restart the node. When SR-IOV network configuration daemon fails to evict a Pod, it will cause all processes to stop, resulting in the vf number of nodes remaining at 0. In this case, the SR-IOV network configuration daemon Pod will see logs similar to the following: error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... This issue can be referred to similar topics in the sriov-network-operator community issue The reason why the designated Pod cannot be expelled can be investigated, which may include the following: The Pod that failed the eviction may have been configured with a PodDisruptionBudget, resulting in a shortage of available replicas. Please adjust the PodDisruptionBudget Insufficient available nodes in the cluster, resulting in no nodes available for scheduling Create a SpiderIPPool instance. The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF Create a SpiderMultusConfig instance. shell $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice SpiderIPPool.Spec.multusName: 'kube-system/sriov-test' must be to match the Name and Namespace of the SpiderMultusConfig instance created.","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-sriov/#create-applications","text":"Create test Pods and Services via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet/sriov_netdevice: '1' limits: spidernet/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF Spec descriptions: spidernet/sriov_netdevice : Sriov resources used. v1.multus-cni.io/default-network : specifies the CNI configuration for Multus. For more information on Multus annotations, refer to Multus Quickstart . Check the status of Pods: ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> Spiderpool ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 Test the communication between Pods: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms Test the communication between Pods and Services: Check Services' IPs\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Access its own service within the Pod: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"Create applications"},{"location":"usage/install/underlay/get-started-weave-zh_CN/","text":"Weave Quick Start English | \u7b80\u4f53\u4e2d\u6587 Weave \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c Weave \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 IPAM \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 IPAM \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d Weave , \u5728\u4fdd\u7559 Weave \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 Spiderpool \u6269\u5c55 Weave \u7684 IPAM \u80fd\u529b\u3002 \u5b89\u88c5\u8981\u6c42 \u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177 \u5b89\u88c5 \u5b89\u88c5 Weave\uff1a kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml \u7b49\u5f85 Pod Running\uff1a [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u6240\u4f7f\u7528\u7684 IP \u6c60: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave \u4f7f\u7528 10.32.0.0/12 \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u5185 SpiderIPPool\u3002 \u9a8c\u8bc1\u5b89\u88c5 [ root@node1 ~ ] # kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 0 12901 false \u5207\u6362 Weave \u7684 IPAM \u4e3a Spiderpool \u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: /etc/cni/net.d/10-weave.conflist \u7684 ipam \u5b57\u6bb5: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } \u4fee\u6539\u4e3a\uff1a { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } \u6216\u53ef\u901a\u8fc7 jq \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 jq \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: # \u4ee5 centos7 \u4e3a\u4f8b yum -y install jq \u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist \u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u6ce8\u89e3: ipam.spidernet.io/ippool \u6307\u5b9a Pod \u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d IP: [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4ece SpiderIPPool: weave-ippool-v4 \u4e2d\u5206\u914d IP Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool \u4e2d\u5206\u914d IP \u5730\u5740: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false \u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002\u901a\u8fc7 Spiderpool \uff0c\u786e\u5b9e\u6269\u5c55\u4e86 Weave \u7684 IPAM \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528 \uff0c\u4f53\u9a8c Spiderpool \u5176\u4ed6\u7684\u529f\u80fd\u3002","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#weave-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Weave \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c Weave \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 IPAM \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 IPAM \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d Weave , \u5728\u4fdd\u7559 Weave \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 Spiderpool \u6269\u5c55 Weave \u7684 IPAM \u80fd\u529b\u3002","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_1","text":"\u5b89\u88c5\u8981\u6c42 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177","title":"\u5b89\u88c5\u8981\u6c42"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_2","text":"\u5b89\u88c5 Weave\uff1a kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml \u7b49\u5f85 Pod Running\uff1a [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u4e2d\u56fd\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u6765\u4f7f\u7528\u56fd\u5185\u7684\u955c\u50cf\u6e90\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u6240\u4f7f\u7528\u7684 IP \u6c60: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave \u4f7f\u7528 10.32.0.0/12 \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u5185 SpiderIPPool\u3002 \u9a8c\u8bc1\u5b89\u88c5 [ root@node1 ~ ] # kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 0 12901 false","title":"\u5b89\u88c5"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#weave-ipam-spiderpool","text":"\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: /etc/cni/net.d/10-weave.conflist \u7684 ipam \u5b57\u6bb5: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } \u4fee\u6539\u4e3a\uff1a { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } \u6216\u53ef\u901a\u8fc7 jq \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 jq \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: # \u4ee5 centos7 \u4e3a\u4f8b yum -y install jq \u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist \u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c","title":"\u5207\u6362 Weave \u7684 IPAM \u4e3a Spiderpool"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_3","text":"\u4f7f\u7528\u6ce8\u89e3: ipam.spidernet.io/ippool \u6307\u5b9a Pod \u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d IP: [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4ece SpiderIPPool: weave-ippool-v4 \u4e2d\u5206\u914d IP Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool \u4e2d\u5206\u914d IP \u5730\u5740: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false \u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002\u901a\u8fc7 Spiderpool \uff0c\u786e\u5b9e\u6269\u5c55\u4e86 Weave \u7684 IPAM \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528 \uff0c\u4f53\u9a8c Spiderpool \u5176\u4ed6\u7684\u529f\u80fd\u3002","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-weave/","text":"Weave Quick Start English | \u7b80\u4f53\u4e2d\u6587 Weave , an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, Weave utilizes the built-in IPAM to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how Weave and Spiderpool can be integrated to extend Weave 's IPAM capabilities while preserving its original functions. Prerequisites System requirements A ready Kubernetes cluster without any CNI installed Helm, Kubectl and Jq (optional) Install Install Weave: kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml Wait for Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Wait for Pod Running and create the IPPool used by Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave uses 10.32.0.0/12 as the cluster's default subnet, and thus a SpiderIPPool with \uff0c/the same subnet needs to be created in this case. Verify installation shell [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1/1 Running 0 13m spiderpool-agent-kxf27 1/1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1/1 Running 0 13m spiderpool-init 0/1 Completed 0 13m [root@node1 ~]# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10.32.0.0/12 0 12901 false Switch Weave 's IPAM to Spiderpool Change the ipam field of /etc/cni/net.d/10-weave.conflist on each node: Change the following: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } To: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } Alternatively, it can be changed with jq in one step. If jq is not installed, you can use the following command to install it: # Take centos7 as an example yum -y install jq Change the CNI configuration file: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist Make sure to run this command at each node Create applications Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation ipam.spidernet.io/ippool : [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/subnet : specifies that the Pods will be assigned IPs from SpiderSubnet: weave-ippool-v4 . The Pods have been created and allocated IP addresses from Spiderpool Subnets: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false To test connectivity, let's use inter-node communication between Pods as an example: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms The test results indicate that IP allocation and network connectivity are normal. Spiderpool has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of Spiderpool .","title":"Weave"},{"location":"usage/install/underlay/get-started-weave/#weave-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Weave , an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, Weave utilizes the built-in IPAM to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how Weave and Spiderpool can be integrated to extend Weave 's IPAM capabilities while preserving its original functions.","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave/#prerequisites","text":"System requirements A ready Kubernetes cluster without any CNI installed Helm, Kubectl and Jq (optional)","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-weave/#install","text":"Install Weave: kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml Wait for Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are a mainland user who is not available to access ghcr.io, you can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Wait for Pod Running and create the IPPool used by Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave uses 10.32.0.0/12 as the cluster's default subnet, and thus a SpiderIPPool with \uff0c/the same subnet needs to be created in this case. Verify installation shell [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1/1 Running 0 13m spiderpool-agent-kxf27 1/1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1/1 Running 0 13m spiderpool-init 0/1 Completed 0 13m [root@node1 ~]# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10.32.0.0/12 0 12901 false","title":"Install"},{"location":"usage/install/underlay/get-started-weave/#switch-weaves-ipam-to-spiderpool","text":"Change the ipam field of /etc/cni/net.d/10-weave.conflist on each node: Change the following: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } To: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } Alternatively, it can be changed with jq in one step. If jq is not installed, you can use the following command to install it: # Take centos7 as an example yum -y install jq Change the CNI configuration file: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist Make sure to run this command at each node","title":"Switch Weave's IPAM to Spiderpool"},{"location":"usage/install/underlay/get-started-weave/#create-applications","text":"Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation ipam.spidernet.io/ippool : [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/subnet : specifies that the Pods will be assigned IPs from SpiderSubnet: weave-ippool-v4 . The Pods have been created and allocated IP addresses from Spiderpool Subnets: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false To test connectivity, let's use inter-node communication between Pods as an example: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms The test results indicate that IP allocation and network connectivity are normal. Spiderpool has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of Spiderpool .","title":"Create applications"}]}
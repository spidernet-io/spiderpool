{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About spiderpool","title":"About"},{"location":"#about","text":"spiderpool","title":"About"},{"location":"cmdref/spiderpool-agent/","text":"spiderpool-agent CLI spiderpool-agent daemon Synopsis run spiderpool agent daemon Options --config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin ENV SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_PPROF enable pprof (true|false) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5711) SPIDERPOOL_HEALTH_PORT http port (default to 5710) spiderpool-agent shutdown Synopsis notify to stop spiderpool-agent daemon spiderpool-agent metric get local metric Options --port string http server port of local metric (default to 5711)","title":"spiderpool-agent"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent","text":"CLI","title":"spiderpool-agent"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-daemon","text":"","title":"spiderpool-agent daemon"},{"location":"cmdref/spiderpool-agent/#synopsis","text":"run spiderpool agent daemon","title":"Synopsis"},{"location":"cmdref/spiderpool-agent/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin","title":"Options"},{"location":"cmdref/spiderpool-agent/#env","text":"SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_PPROF enable pprof (true|false) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5711) SPIDERPOOL_HEALTH_PORT http port (default to 5710)","title":"ENV"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-shutdown","text":"","title":"spiderpool-agent shutdown"},{"location":"cmdref/spiderpool-agent/#synopsis_1","text":"notify to stop spiderpool-agent daemon","title":"Synopsis"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-metric","text":"get local metric","title":"spiderpool-agent metric"},{"location":"cmdref/spiderpool-agent/#options_1","text":"--port string http server port of local metric (default to 5711)","title":"Options"},{"location":"cmdref/spiderpool-controller/","text":"spiderpool-controller CLI spiderpool-controller daemon Synopsis run spiderpool controller daemon Options --config-dir string config file path (default /tmp/spiderpool/config-map) ENV SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_PPROF enable pprof (true|false) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5721) SPIDERPOOL_GC_IPPOOL_ENABLED enable GC ip in ippool, prior to other GC environment (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED enable GC ip of terminating pod whose graceful-time times out (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY delay to GC ip after graceful-time times out (second, default to 0) SPIDERPOOL_GC_EVICTED_POD_IP_ENABLED enable GC ip of evicted pod (true|false, default to true) SPIDERPOOL_GC_EVICTED_POD_IP_DELAY delay to GC ip of evicted pod (second, default to 0) SPIDERPOOL_HEALTH_PORT http port (default to 5710) spiderpool-controller shutdown Synopsis notify to stop spiderpool-controller daemon spiderpool-controller metric get local metric Options --port string http server port of local metric (default to 5721) spiderpool-controller status show status: (1) whether local is controller leader (2)... Options --port string http server port of local metric (default to 5720)","title":"spiderpool-controller"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller","text":"CLI","title":"spiderpool-controller"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-daemon","text":"","title":"spiderpool-controller daemon"},{"location":"cmdref/spiderpool-controller/#synopsis","text":"run spiderpool controller daemon","title":"Synopsis"},{"location":"cmdref/spiderpool-controller/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map)","title":"Options"},{"location":"cmdref/spiderpool-controller/#env","text":"SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_PPROF enable pprof (true|false) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5721) SPIDERPOOL_GC_IPPOOL_ENABLED enable GC ip in ippool, prior to other GC environment (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED enable GC ip of terminating pod whose graceful-time times out (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY delay to GC ip after graceful-time times out (second, default to 0) SPIDERPOOL_GC_EVICTED_POD_IP_ENABLED enable GC ip of evicted pod (true|false, default to true) SPIDERPOOL_GC_EVICTED_POD_IP_DELAY delay to GC ip of evicted pod (second, default to 0) SPIDERPOOL_HEALTH_PORT http port (default to 5710)","title":"ENV"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-shutdown","text":"","title":"spiderpool-controller shutdown"},{"location":"cmdref/spiderpool-controller/#synopsis_1","text":"notify to stop spiderpool-controller daemon","title":"Synopsis"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-metric","text":"get local metric","title":"spiderpool-controller metric"},{"location":"cmdref/spiderpool-controller/#options_1","text":"--port string http server port of local metric (default to 5721)","title":"Options"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-status","text":"show status: (1) whether local is controller leader (2)...","title":"spiderpool-controller status"},{"location":"cmdref/spiderpool-controller/#options_2","text":"--port string http server port of local metric (default to 5720)","title":"Options"},{"location":"cmdref/spiderpoolctl/","text":"spiderpoolctl CLI for debug spiderpoolctl gc trigger GC request to spiderpool-controller --address string [optional] address for spider-controller (default to service address) spiderpoolctl ip show show pod who is taking this ip Options --ip string [required] ip spiderpoolctl ip release try to release ip Options --ip string [optional] ip --force [optional] force release ip spiderpoolctl ip set set ip to be taken by a pod , this will update ippool and workloadendpoint resource Options --ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"spiderpoolctl"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl","text":"CLI for debug","title":"spiderpoolctl"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-gc","text":"trigger GC request to spiderpool-controller --address string [optional] address for spider-controller (default to service address)","title":"spiderpoolctl gc"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-show","text":"show pod who is taking this ip","title":"spiderpoolctl ip show"},{"location":"cmdref/spiderpoolctl/#options","text":"--ip string [required] ip","title":"Options"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-release","text":"try to release ip","title":"spiderpoolctl ip release"},{"location":"cmdref/spiderpoolctl/#options_1","text":"--ip string [optional] ip --force [optional] force release ip","title":"Options"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-set","text":"set ip to be taken by a pod , this will update ippool and workloadendpoint resource","title":"spiderpoolctl ip set"},{"location":"cmdref/spiderpoolctl/#options_2","text":"--ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"Options"},{"location":"concepts/allocation/","text":"IP Allocation when a pod is creating, it observes following steps to assign ip: get all ippool candidates For which ippool is used by pod, the following rule is listed from high to low priority Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used for specify ippool. It could refer to pod annotation for detail. Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used for specify ippool. It could refer to namespace annotation for detail. Cluster default ippool. Cluster default ippool could be set to \"clusterDefaultIpv4Ippool\" and \"clusterDefaultIpv6Ippool\" in configmap \"spiderpool-conf\". It could refer to configuration for detail. filter valid ippool candidate after getting ipv4 ippool and ipv6 ippool candidates, it looks into each ippool and figure out whether it meets following rules, and get which candidate ippool is available the \"disable\" filed of the ippool is \"false\" the \"ipversion\" filed of the ippool must meet the claim the \"namespaceSelector\" filed of the ippool must meet the namespace of the pod the \"podSelector\" filed of the ippool must meet the pod the \"nodeSelector\" filed of the ippool must meet the scheduled node of the pod the available IP resource of the ippool is not exhausted assign ip from valid ippool candidate when trying to assign IP from the candidate ippool, it follows below rules the IP is not reserved by the \"exclude_ips\" filed of the ippool and all ReservedIP instance when the controller of the pod is statefulset, it allocates IP by order","title":"IP Allocation"},{"location":"concepts/allocation/#ip-allocation","text":"when a pod is creating, it observes following steps to assign ip: get all ippool candidates For which ippool is used by pod, the following rule is listed from high to low priority Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used for specify ippool. It could refer to pod annotation for detail. Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used for specify ippool. It could refer to namespace annotation for detail. Cluster default ippool. Cluster default ippool could be set to \"clusterDefaultIpv4Ippool\" and \"clusterDefaultIpv6Ippool\" in configmap \"spiderpool-conf\". It could refer to configuration for detail. filter valid ippool candidate after getting ipv4 ippool and ipv6 ippool candidates, it looks into each ippool and figure out whether it meets following rules, and get which candidate ippool is available the \"disable\" filed of the ippool is \"false\" the \"ipversion\" filed of the ippool must meet the claim the \"namespaceSelector\" filed of the ippool must meet the namespace of the pod the \"podSelector\" filed of the ippool must meet the pod the \"nodeSelector\" filed of the ippool must meet the scheduled node of the pod the available IP resource of the ippool is not exhausted assign ip from valid ippool candidate when trying to assign IP from the candidate ippool, it follows below rules the IP is not reserved by the \"exclude_ips\" filed of the ippool and all ReservedIP instance when the controller of the pod is statefulset, it allocates IP by order","title":"IP Allocation"},{"location":"concepts/arch/","text":"architect architecture the spiderpool consists of following components: spiderpool IPAM plugin, a binary installed on each host. It is called by CNI plugin to assign and release IP for a pod spiderpool agent, deployed as a daemonset. It receives IPAM request from IPAM plugin, assign and release IP from ippool resource spiderpool controller, deployed as a deployment. It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. It refers to Resource Reclaim for details. It uses webhook to watch ippool resource, help the administrator validate the creation, modification, deletion. spiderpoolctl, CLI tool for debugging CRD the spiderpool designs following CRD: ippool CRD. It is used to store the IP resource for a subnet. It refers to ippool for detail. workloadendpoint CRD. It is used to store the IP assigned to a pod. It refers to workloadendpoint for detail. reservedip CRD. Is ti used to set reserved IP, who will not assign to pod, even set in the ippool. It refers to reservedip for detail.","title":"architect"},{"location":"concepts/arch/#architect","text":"","title":"architect"},{"location":"concepts/arch/#architecture","text":"the spiderpool consists of following components: spiderpool IPAM plugin, a binary installed on each host. It is called by CNI plugin to assign and release IP for a pod spiderpool agent, deployed as a daemonset. It receives IPAM request from IPAM plugin, assign and release IP from ippool resource spiderpool controller, deployed as a deployment. It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. It refers to Resource Reclaim for details. It uses webhook to watch ippool resource, help the administrator validate the creation, modification, deletion. spiderpoolctl, CLI tool for debugging","title":"architecture"},{"location":"concepts/arch/#crd","text":"the spiderpool designs following CRD: ippool CRD. It is used to store the IP resource for a subnet. It refers to ippool for detail. workloadendpoint CRD. It is used to store the IP assigned to a pod. It refers to workloadendpoint for detail. reservedip CRD. Is ti used to set reserved IP, who will not assign to pod, even set in the ippool. It refers to reservedip for detail.","title":"CRD"},{"location":"concepts/gc/","text":"Resource Reclaim IP collection context When a pod is normally deleted , the CNI plugin will be called to clean IP on Pod interface and make IP free on IPAM database. This make sure all IP is managed correctly and no leaked IP issue. But on cases, it may go wrong and IP of IPAM database is still recorded to be used by a non-existed pod. when some error happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fail to call CNI plugin. A node is breakdown and then always not recovery, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing CNI binary on host when pod deletion. This issue will make bad result: the new pod maybe fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual pod number does not grow. Some CNI or IPAM plugins, they could not handle this issue. For some CNI, the administrator self needs to find issue IP and use CLI tool to reclaim them. For some CNI, it runs an interval job to find the issue IP and not reclaim them in time. For some CNI, there is not any mechanism at all, to fix these issue IP. solution For some CNI, its IP CIDR is big enough, so the leaked IP issue is not urgent. For spiderpool, all IP resource is managed by Administrator, and application will be bound to fixed IP, so the IP reclaim must be finished in time. The spiderpool controller takes charge of this responsibility. There is some environment to set its reclaim behaves. // TODO (Icarus9913), describe the environment ippool collection To prevent IP from leaking when ippool resource is deleted, the spiderpool has some rules: For an ippool, if there is still IP taken by pods, the spiderpool uses webhook to reject deleting request of ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool is set a finalizer by the spiderpool controller. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IP in the ippool is free.","title":"Resource Reclaim"},{"location":"concepts/gc/#resource-reclaim","text":"","title":"Resource Reclaim"},{"location":"concepts/gc/#ip-collection","text":"","title":"IP collection"},{"location":"concepts/gc/#context","text":"When a pod is normally deleted , the CNI plugin will be called to clean IP on Pod interface and make IP free on IPAM database. This make sure all IP is managed correctly and no leaked IP issue. But on cases, it may go wrong and IP of IPAM database is still recorded to be used by a non-existed pod. when some error happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fail to call CNI plugin. A node is breakdown and then always not recovery, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing CNI binary on host when pod deletion. This issue will make bad result: the new pod maybe fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual pod number does not grow. Some CNI or IPAM plugins, they could not handle this issue. For some CNI, the administrator self needs to find issue IP and use CLI tool to reclaim them. For some CNI, it runs an interval job to find the issue IP and not reclaim them in time. For some CNI, there is not any mechanism at all, to fix these issue IP.","title":"context"},{"location":"concepts/gc/#solution","text":"For some CNI, its IP CIDR is big enough, so the leaked IP issue is not urgent. For spiderpool, all IP resource is managed by Administrator, and application will be bound to fixed IP, so the IP reclaim must be finished in time. The spiderpool controller takes charge of this responsibility. There is some environment to set its reclaim behaves. // TODO (Icarus9913), describe the environment","title":"solution"},{"location":"concepts/gc/#ippool-collection","text":"To prevent IP from leaking when ippool resource is deleted, the spiderpool has some rules: For an ippool, if there is still IP taken by pods, the spiderpool uses webhook to reject deleting request of ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool is set a finalizer by the spiderpool controller. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IP in the ippool is free.","title":"ippool collection"},{"location":"concepts/ippool/","text":"ippool // TODO (iiiceoo), describe the CRD","title":"ippool"},{"location":"concepts/ippool/#ippool","text":"// TODO (iiiceoo), describe the CRD","title":"ippool"},{"location":"concepts/reservedip/","text":"reservedip // todo(Icarus9913), describe the CRD","title":"reservedip"},{"location":"concepts/reservedip/#reservedip","text":"// todo(Icarus9913), describe the CRD","title":"reservedip"},{"location":"concepts/roadmap/","text":"spiderpool road map feature [ ] (alpha) a pod use multiple ippools, in case that an ippool is out of use [ ] (alpha) cooperate with macvlan [ ] (alpha) support pod / namespace annotations to customize ip [ ] (alpha) dual stack support. For ipv4-only, ipv6-only, dual-stack cluster, [ ] assign ipv4/ipv6 ip to pod [ ] all component could service on ipv4/ipv6 ip [ ] (alpha) ippool selector [ ] select namespace. Each namespace could occupy non-shared ippool [ ] select pod. For deployment and statefulset case, pod could occupy some ip range [ ] select node. For different zone, pod have specified ip range [ ] (beta) cooperate with multus. multus may call multiple CNI to assign different interface and assign ip [ ] (beta) fixed ip for application, especially for statefulset [ ] (beta) health check [ ] (beta) CLI for debug [ ] (beta) retrieve leaked ip [ ] for graceful timeout of terminating state [ ] for finished job [ ] trigger by hand or automatically [ ] trigger by pod event and happen at interval [ ] (beta) DCE5 integration [ ] (GA) metrics [ ] (GA) reserved ip [ ] (GA) administrator edit ip safely, preventing from race with IPAM CNI, and avoid ip conflicting between ippools [ ] (GA) good performance [ ] take xxx second at most, to assign a ip [ ] take xxx second at most, to assign 1000 ip [ ] (GA) good reliability [ ] (GA) cooperate with spiderflat [ ] Unit-Test [ ] (alpha) 40% coverage at least [ ] (beta) 70% coverage at least [ ] (GA) 90% coverage at least [ ] e2e test [ ] (alpha) 30% coverage for test case of alpha feature [ ] (beta) 80% coverage for test case of beta/alpha feature [ ] (GA) 100% coverage for test case of all feature [ ] (GA) chaos test case, performance test case [ ] All CICD pipeline. nightly ci, auto release chart/image/release, code lint, doc lint, unitest, e2e test [X] (alpha) 80% CICD pipeline [ ] (GA) 100% CICD pipeline [ ] documentation [ ] (alpha) architecture, contributing [ ] (beta) concept, get started, configuration [ ] (GA) command reference goal of April [x] \u7b2c\u4e00\u4e2a Helm Release\uff0c\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\uff08\u53ef\u6301\u7eed CICD\uff09 [x] \u8f93\u51fa Road Map\uff0c\u5b8c\u6210 6\u4e2a\u6708\u4ee5\u4e0a\u7684\u89c4\u5212 [x] Unit-Test \u8986\u76d6\u7387 10 % [x] \u7b2c\u4e00\u4e2a \u81ea\u52a8\u5316\u7684 e2e \u6d4b\u8bd5\uff08\u6bcf\u5929\u665a\u4e0a\u90fd\u8981\u81ea\u52a8\u8dd1\uff09 [x] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 10% [x] GitHub (Star) 100 [x] webhook \u7cbe\u901a\u53ca\u5206\u4eab [ ] go builder \u7684 SDK \u751f\u6210\uff0c\u6240\u6709\u7684 client \u90fd\u80fd\u5de5\u4f5c\uff0c\u751f\u6210 CRD yaml\u3002 \u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 openapi \u63a5\u53e3\u548c SDK \uff0c\u9a8c\u8bc1\u90fd\u80fd\u5de5\u4f5c\u3002\u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 spiderpool ipam plugin \uff0cagent\u3001 controller\u8fdb\u7a0b\u80fd\u591f\u8dd1 \uff0c\u786e\u4fdd helm apply \u80fd\u591f\u90e8\u7f72\uff08\u4e0d\u8981\u6c42\u80fd\u8dd1\u4e1a\u52a1\uff09 [x] \u642d\u5efa e2e \u6846\u67b6 [x] \u7cbe\u8bfb golang\u3001ginkgo\u3001\u5f00\u6e90\u9879\u76ee e2e\uff0c \u4ee5macvlan + whereabout \u5b8c\u6210\u7b2c\u4e00\u4e2ae2e\u6d4b\u8bd5 goal of May [ ] \u5b8c\u6210 IPAM plug in [ ] \u5b8c\u6210 Agent \u4e3b\u8981\u4ee3\u7801\uff0c\u80fd\u5206\u914d\u51fa ipv4/ipv6 \u5730\u5740 [ ] \u4ee5 macvlan + whereabout \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 50% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u5b8c\u6210 100% alpha doc [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100% [ ] 5 \u4e2a\u5916\u90e8\u53cd\u9988\u7528\u6237\uff0c10\u4e2a ISSUE ????? [ ] 3\u4e2a\u5916\u90e8\u8d21\u732e\u8005 ???? [ ] GitHub (Star) 200 ??? [ ] Unit-Test \u8986\u76d6\u7387 30% [ ] \u5f00\u653e\u7684 \u72ec\u7acb\u7f51\u7ad9 \u548c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ???? [ ] \u5f00\u53d1\u7684 \u72ec\u7acb\u7f51\u7ad9 QuickStart\uff0c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ??? [ ] Spider Pool e2e \u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1 (\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545\u3002(\u8bc4\u5ba1\u4f1a\u4e4b\u524d\uff0c\u5f00\u4f1areview) [ ] \u5145\u5206\u7684 e2e \u6d4b\u8bd5 \u8bbe\u8ba1(\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545 ?? goal of June [ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 alpha feature \u7684\u4e1a\u52a1\u4ee3\u7801 [ ] \u4ee5 macvlan + spiderpool \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 100% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u6536\u96c6 10\u4e2a\u4ee5\u4e0a alpha \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] Unit-Test \u8986\u76d6\u7387 80% [ ] \u6027\u80fd\u6d4b\u8bd5\uff0c\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c??? [ ] \u5145\u5206\u7684\u707e\u96be\u573a\u666f\u6d4b\u8bd5\u3002\u548c \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 ?? [ ] \u517c\u5bb9\u6027\u6d4b\u8bd5\u9a8c\u8bc1\uff08\u6d89\u53ca\u7684\u5f00\u6e90\u8f6f\u4ef6\uff1a\u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u6e90\u9879\u76ee \u548c\u5404\u4e2a\u516c\u6709\u4e91\u73af\u5883\uff09 [ ] \u5ba1\u89c6\u201cL3\u4e8b\u6545\u201d\uff0c\u786e\u8ba4\u4e0d\u4f1a\u51fa\u73b0\u4e4b\u524d\u53d1\u751f\u8fc7\u7684\u4e8b\u6545 [ ] \u707e\u96be\u6062\u590d\u6f14\u7ec3 ?? beta [ ] \u5b8c\u6210 \u548c KubeGrid \u7684 \u4f1a\u5e08 ?? beta [ ] \u89e3\u51b3\u6240\u6709\u91cd\u5927bug [ ] \u5408\u4f5c\u4f19\u4f34 1 \u4e2a\uff0c\u53cd\u9988\u7528\u6237 1\u4e2a goal of July [ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 beta feature \u7684\u4ee3\u7801 [ ] \u5b8c\u6210 100% beta doc goal of August [ ] finish most of GA feature , wait for spiderflat ready to debug [ ] \u6536\u96c6 10 \u4e2a\u4ee5\u4e0a beta \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] \u5c06\u6240\u6709\u5df2\u77e5 BUG \u89e3\u51b3 [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100 % goal of September [ ] start spiderflat","title":"spiderpool road map"},{"location":"concepts/roadmap/#spiderpool-road-map","text":"","title":"spiderpool road map"},{"location":"concepts/roadmap/#feature","text":"[ ] (alpha) a pod use multiple ippools, in case that an ippool is out of use [ ] (alpha) cooperate with macvlan [ ] (alpha) support pod / namespace annotations to customize ip [ ] (alpha) dual stack support. For ipv4-only, ipv6-only, dual-stack cluster, [ ] assign ipv4/ipv6 ip to pod [ ] all component could service on ipv4/ipv6 ip [ ] (alpha) ippool selector [ ] select namespace. Each namespace could occupy non-shared ippool [ ] select pod. For deployment and statefulset case, pod could occupy some ip range [ ] select node. For different zone, pod have specified ip range [ ] (beta) cooperate with multus. multus may call multiple CNI to assign different interface and assign ip [ ] (beta) fixed ip for application, especially for statefulset [ ] (beta) health check [ ] (beta) CLI for debug [ ] (beta) retrieve leaked ip [ ] for graceful timeout of terminating state [ ] for finished job [ ] trigger by hand or automatically [ ] trigger by pod event and happen at interval [ ] (beta) DCE5 integration [ ] (GA) metrics [ ] (GA) reserved ip [ ] (GA) administrator edit ip safely, preventing from race with IPAM CNI, and avoid ip conflicting between ippools [ ] (GA) good performance [ ] take xxx second at most, to assign a ip [ ] take xxx second at most, to assign 1000 ip [ ] (GA) good reliability [ ] (GA) cooperate with spiderflat [ ] Unit-Test [ ] (alpha) 40% coverage at least [ ] (beta) 70% coverage at least [ ] (GA) 90% coverage at least [ ] e2e test [ ] (alpha) 30% coverage for test case of alpha feature [ ] (beta) 80% coverage for test case of beta/alpha feature [ ] (GA) 100% coverage for test case of all feature [ ] (GA) chaos test case, performance test case [ ] All CICD pipeline. nightly ci, auto release chart/image/release, code lint, doc lint, unitest, e2e test [X] (alpha) 80% CICD pipeline [ ] (GA) 100% CICD pipeline [ ] documentation [ ] (alpha) architecture, contributing [ ] (beta) concept, get started, configuration [ ] (GA) command reference","title":"feature"},{"location":"concepts/roadmap/#goal-of-april","text":"[x] \u7b2c\u4e00\u4e2a Helm Release\uff0c\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\uff08\u53ef\u6301\u7eed CICD\uff09 [x] \u8f93\u51fa Road Map\uff0c\u5b8c\u6210 6\u4e2a\u6708\u4ee5\u4e0a\u7684\u89c4\u5212 [x] Unit-Test \u8986\u76d6\u7387 10 % [x] \u7b2c\u4e00\u4e2a \u81ea\u52a8\u5316\u7684 e2e \u6d4b\u8bd5\uff08\u6bcf\u5929\u665a\u4e0a\u90fd\u8981\u81ea\u52a8\u8dd1\uff09 [x] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 10% [x] GitHub (Star) 100 [x] webhook \u7cbe\u901a\u53ca\u5206\u4eab [ ] go builder \u7684 SDK \u751f\u6210\uff0c\u6240\u6709\u7684 client \u90fd\u80fd\u5de5\u4f5c\uff0c\u751f\u6210 CRD yaml\u3002 \u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 openapi \u63a5\u53e3\u548c SDK \uff0c\u9a8c\u8bc1\u90fd\u80fd\u5de5\u4f5c\u3002\u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 spiderpool ipam plugin \uff0cagent\u3001 controller\u8fdb\u7a0b\u80fd\u591f\u8dd1 \uff0c\u786e\u4fdd helm apply \u80fd\u591f\u90e8\u7f72\uff08\u4e0d\u8981\u6c42\u80fd\u8dd1\u4e1a\u52a1\uff09 [x] \u642d\u5efa e2e \u6846\u67b6 [x] \u7cbe\u8bfb golang\u3001ginkgo\u3001\u5f00\u6e90\u9879\u76ee e2e\uff0c \u4ee5macvlan + whereabout \u5b8c\u6210\u7b2c\u4e00\u4e2ae2e\u6d4b\u8bd5","title":"goal of April"},{"location":"concepts/roadmap/#goal-of-may","text":"[ ] \u5b8c\u6210 IPAM plug in [ ] \u5b8c\u6210 Agent \u4e3b\u8981\u4ee3\u7801\uff0c\u80fd\u5206\u914d\u51fa ipv4/ipv6 \u5730\u5740 [ ] \u4ee5 macvlan + whereabout \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 50% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u5b8c\u6210 100% alpha doc [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100% [ ] 5 \u4e2a\u5916\u90e8\u53cd\u9988\u7528\u6237\uff0c10\u4e2a ISSUE ????? [ ] 3\u4e2a\u5916\u90e8\u8d21\u732e\u8005 ???? [ ] GitHub (Star) 200 ??? [ ] Unit-Test \u8986\u76d6\u7387 30% [ ] \u5f00\u653e\u7684 \u72ec\u7acb\u7f51\u7ad9 \u548c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ???? [ ] \u5f00\u53d1\u7684 \u72ec\u7acb\u7f51\u7ad9 QuickStart\uff0c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ??? [ ] Spider Pool e2e \u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1 (\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545\u3002(\u8bc4\u5ba1\u4f1a\u4e4b\u524d\uff0c\u5f00\u4f1areview) [ ] \u5145\u5206\u7684 e2e \u6d4b\u8bd5 \u8bbe\u8ba1(\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545 ??","title":"goal of May"},{"location":"concepts/roadmap/#goal-of-june","text":"[ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 alpha feature \u7684\u4e1a\u52a1\u4ee3\u7801 [ ] \u4ee5 macvlan + spiderpool \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 100% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u6536\u96c6 10\u4e2a\u4ee5\u4e0a alpha \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] Unit-Test \u8986\u76d6\u7387 80% [ ] \u6027\u80fd\u6d4b\u8bd5\uff0c\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c??? [ ] \u5145\u5206\u7684\u707e\u96be\u573a\u666f\u6d4b\u8bd5\u3002\u548c \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 ?? [ ] \u517c\u5bb9\u6027\u6d4b\u8bd5\u9a8c\u8bc1\uff08\u6d89\u53ca\u7684\u5f00\u6e90\u8f6f\u4ef6\uff1a\u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u6e90\u9879\u76ee \u548c\u5404\u4e2a\u516c\u6709\u4e91\u73af\u5883\uff09 [ ] \u5ba1\u89c6\u201cL3\u4e8b\u6545\u201d\uff0c\u786e\u8ba4\u4e0d\u4f1a\u51fa\u73b0\u4e4b\u524d\u53d1\u751f\u8fc7\u7684\u4e8b\u6545 [ ] \u707e\u96be\u6062\u590d\u6f14\u7ec3 ?? beta [ ] \u5b8c\u6210 \u548c KubeGrid \u7684 \u4f1a\u5e08 ?? beta [ ] \u89e3\u51b3\u6240\u6709\u91cd\u5927bug [ ] \u5408\u4f5c\u4f19\u4f34 1 \u4e2a\uff0c\u53cd\u9988\u7528\u6237 1\u4e2a","title":"goal of June"},{"location":"concepts/roadmap/#goal-of-july","text":"[ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 beta feature \u7684\u4ee3\u7801 [ ] \u5b8c\u6210 100% beta doc","title":"goal of July"},{"location":"concepts/roadmap/#goal-of-august","text":"[ ] finish most of GA feature , wait for spiderflat ready to debug [ ] \u6536\u96c6 10 \u4e2a\u4ee5\u4e0a beta \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] \u5c06\u6240\u6709\u5df2\u77e5 BUG \u89e3\u51b3 [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100 %","title":"goal of August"},{"location":"concepts/roadmap/#goal-of-september","text":"[ ] start spiderflat","title":"goal of September"},{"location":"concepts/workloadendpoint/","text":"workloadendpoint // TODO(iiiceoo), describe the CRD","title":"workloadendpoint"},{"location":"concepts/workloadendpoint/#workloadendpoint","text":"// TODO(iiiceoo), describe the CRD","title":"workloadendpoint"},{"location":"develop/changelog/","text":"changelog how automatically generate the changelog all PR should be labeled with \"pr/release/***\" and could be merged when pushing a tag, automatically create the changelog the changelog content include: New Features: it includes all PR labeled with \"pr/release/feature-new\" Changed Features: it include all PR labeled with \"pr/release/feature-changed\" Fixes: it include all PR labeled with \"pr/release/bug\" all historical commit within this version the changelog will be attached to github RELEASE and submit to /changelogs of branch 'github_pages'","title":"changelog"},{"location":"develop/changelog/#changelog","text":"how automatically generate the changelog all PR should be labeled with \"pr/release/***\" and could be merged when pushing a tag, automatically create the changelog the changelog content include: New Features: it includes all PR labeled with \"pr/release/feature-new\" Changed Features: it include all PR labeled with \"pr/release/feature-changed\" Fixes: it include all PR labeled with \"pr/release/bug\" all historical commit within this version the changelog will be attached to github RELEASE and submit to /changelogs of branch 'github_pages'","title":"changelog"},{"location":"develop/pullrequest/","text":"Submit Pull request A pull request will be checked by following workflow, which is the required condition for merging action: PR should be signed off action: check yaml files If this check fails, it could refer to the yaml rule Once the issue is fixed , it could be verified on local host by command make lint-yaml notice: for ignoring yaml rule, it could be add to .github/yamllint-conf.yml action: check golang source code It checks the following against any updated golang file mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg comment // TODO should follow format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job unitest and upload coverage to codecov each golang test file should mark ginkgo label action: check license Any golang or shell file should be licensed correctly. action: check markdown file check markdown format, if fails, it could refer to the Markdown Rule It could be tested on local machine with command make lint-markdown-format It could try to fix it on local machine with command make fix-markdown-format For ignoring case, it could be added to .github/markdownlint.yaml check markdown spell error. It could be tested on local machine with command make lint-markdown-spell-colour . For ignoring case, it could be added it to .github/.spelling action: lint yaml file if it fails, the reason could refer to https://yamllint.readthedocs.io/en/stable/rules.html It could be tested on local machine with command make lint-yaml action: lint chart action: lint openapi.yaml action: check code spell Any code spell error of golang files will be checked. It could be checked on local machine with command make lint-code-spell . It could be auto fixed on local machine with command make fix-code-spell . For ignored error case, please edit .github/codespell-ignorewords and make sure all letters should be lower-case","title":"Submit Pull request"},{"location":"develop/pullrequest/#submit-pull-request","text":"A pull request will be checked by following workflow, which is the required condition for merging","title":"Submit Pull request"},{"location":"develop/pullrequest/#action-pr-should-be-signed-off","text":"","title":"action: PR should be signed off"},{"location":"develop/pullrequest/#action-check-yaml-files","text":"If this check fails, it could refer to the yaml rule Once the issue is fixed , it could be verified on local host by command make lint-yaml notice: for ignoring yaml rule, it could be add to .github/yamllint-conf.yml","title":"action: check yaml files"},{"location":"develop/pullrequest/#action-check-golang-source-code","text":"It checks the following against any updated golang file mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg comment // TODO should follow format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job unitest and upload coverage to codecov each golang test file should mark ginkgo label","title":"action: check golang source code"},{"location":"develop/pullrequest/#action-check-license","text":"Any golang or shell file should be licensed correctly.","title":"action: check license"},{"location":"develop/pullrequest/#action-check-markdown-file","text":"check markdown format, if fails, it could refer to the Markdown Rule It could be tested on local machine with command make lint-markdown-format It could try to fix it on local machine with command make fix-markdown-format For ignoring case, it could be added to .github/markdownlint.yaml check markdown spell error. It could be tested on local machine with command make lint-markdown-spell-colour . For ignoring case, it could be added it to .github/.spelling","title":"action: check markdown file"},{"location":"develop/pullrequest/#action-lint-yaml-file","text":"if it fails, the reason could refer to https://yamllint.readthedocs.io/en/stable/rules.html It could be tested on local machine with command make lint-yaml","title":"action: lint yaml file"},{"location":"develop/pullrequest/#action-lint-chart","text":"","title":"action: lint chart"},{"location":"develop/pullrequest/#action-lint-openapiyaml","text":"","title":"action: lint openapi.yaml"},{"location":"develop/pullrequest/#action-check-code-spell","text":"Any code spell error of golang files will be checked. It could be checked on local machine with command make lint-code-spell . It could be auto fixed on local machine with command make fix-code-spell . For ignored error case, please edit .github/codespell-ignorewords and make sure all letters should be lower-case","title":"action: check code spell"},{"location":"develop/release/","text":"workflow for release If a tag is pushed , the following steps will run: build the images with the pushed tag, and push to ghcr registry generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". build the chart package with the pushed tag, and submit a PR to branch 'github_pages' submit '/docs' of branch 'main' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\"","title":"workflow for release"},{"location":"develop/release/#workflow-for-release","text":"If a tag is pushed , the following steps will run: build the images with the pushed tag, and push to ghcr registry generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". build the chart package with the pushed tag, and submit a PR to branch 'github_pages' submit '/docs' of branch 'main' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\"","title":"workflow for release"},{"location":"develop/swagger_openapi/","text":"SWAGGER OPENAPI Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes. Features Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs. Usages There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR . validate spec Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec generate source codes with the given spec To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen verify the spec with current source codes to make sure whether the current source codes is out of date To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify clean the generated source codes To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code Use swagger-ui To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web. Steps For Developers Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#swagger-openapi","text":"Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#features","text":"Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs.","title":"Features"},{"location":"develop/swagger_openapi/#usages","text":"There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR .","title":"Usages"},{"location":"develop/swagger_openapi/#validate-spec","text":"Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec","title":"validate spec"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","text":"To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen","title":"generate source codes with the given spec"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","text":"To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify","title":"verify the spec with current source codes to make sure whether the current source codes is out of date"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","text":"To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code","title":"clean the generated source codes"},{"location":"develop/swagger_openapi/#use-swagger-ui","text":"To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.","title":"Use swagger-ui"},{"location":"develop/swagger_openapi/#steps-for-developers","text":"Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Steps For Developers"},{"location":"develop/test/","text":"test you could follow the below steps to test: check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # setup the kind cluster # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # run all e2e test $ make e2e_test # run smoke test $ make e2e_test -e GINKGO_OPTION=\"--label-filter=smoke\" # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --repeat=10 \" # example: run a case until fails $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} # run all e2e test $ make e2e_test","title":"test"},{"location":"develop/test/#test","text":"you could follow the below steps to test: check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # setup the kind cluster # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # run all e2e test $ make e2e_test # run smoke test $ make e2e_test -e GINKGO_OPTION=\"--label-filter=smoke\" # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --repeat=10 \" # example: run a case until fails $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} # run all e2e test $ make e2e_test","title":"test"},{"location":"usage/advance/","text":"Advance Usage Fixed IP Usage // TODO(anyone), an example to use a ippool bound to an deployment Order IP Usage // TODO(anyone), an example to use a ippool bound to an statefulset Multiple Ippool Usage // TODO(anyone), an example to use multiple ippool for an deployment, for backup usage , or cross-zone usage","title":"Advance Usage"},{"location":"usage/advance/#advance-usage","text":"","title":"Advance Usage"},{"location":"usage/advance/#fixed-ip-usage","text":"// TODO(anyone), an example to use a ippool bound to an deployment","title":"Fixed IP Usage"},{"location":"usage/advance/#order-ip-usage","text":"// TODO(anyone), an example to use a ippool bound to an statefulset","title":"Order IP Usage"},{"location":"usage/advance/#multiple-ippool-usage","text":"// TODO(anyone), an example to use multiple ippool for an deployment, for backup usage , or cross-zone usage","title":"Multiple Ippool Usage"},{"location":"usage/annotation/","text":"annotation Pod Annotation Pod could specify spiderpool annotation for special request \"ipam.spidernet.io/ippool\", specify which ippool is used for the assigning IP. \"ipam.spidernet.io/ippool\": { \"interface\": \"eth0\", \"ipv4pool\": \"v4pool1\", \"ipv6pool\": \"v6pool1,v6pool2\" } interface: optional, when integrate with multus CNI , it could specify which ippool is used to the interface. ipv4pool: specify which ippool is used to assign ipv4 ip. It could set with multiple ippool by comma-seperated string. When enableIpv4 in configmap \"spiderpool-conf\" is set to true, this filed is required. ipv6pool: specify which ippool is used to assign ipv6 ip. It could set with multiple ippool by comma-seperated string. When enableIpv6 in configmap \"spiderpool-conf\" is set to true, this filed is required. \"ipam.spidernet.io/ippools\", this one is similar to \"ipam.spidernet.io/ippool\", but could be used to multiple interface case. BTW, the \"ipam.spidernet.io/ippools\" has precedence over \"ipam.spidernet.io/ippool\". \"ipam.spidernet.io/ippools\": [{ \"interface\": \"eth0\", \"ipv4pool\": \"v4pool1\", \"ipv6pool\": \"v6pool1\", \"defaultRoute\": true }, { \"interface\": \"eth1\", \"ipv4pool\": \"v4pool2\", \"ipv6pool\": \"v6pool2\", \"defaultRoute\": false}] defaultRoute: required, if set to be true, the IPAM plugin will return the default gateway route recorded in the ippool. limit: For different interface, it is forbid to use ippools in a same subnet. \"ipam.spidernet.io/routes\", administrator could use this to take effect additional route. \"ipam.spidernet.io/routes\":[{\"interface\": \"eth0\", \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\"}] \"ipam.spidernet.io/assigned-INTERFACE\", this is the IP assigned result for an interface. \"ipam.spidernet.io/assigned-eth0\": { \"interface\": \"eth0\", \"ipv4pool\": \"v4pool1\", \"ipv6pool\": \"v6pool1\", \"ipv4\": \"172.16.0.100/16\", \"ipv6\": \"fd00::100/64\", \"vlan\": 100 } Namespace Annotation the namespace resource could set following annotation to specify default ippool, to override the default ippool of the cluster recorded in configmap \"spiderpool-conf\". \"ipam.spidernet.io/defaultv4ippool\" \"ipam.spidernet.io/defaultv4ippool\": [ \"ipv4pool1\",\"ipv4pool2\"] notice: if multiple ippool are listed, it will try to assign IP from the later ippool when the former one is not allocatable. \"ipam.spidernet.io/defaultv6ippool\" \"ipam.spidernet.io/defaultv6ippool\": [ \"ipv6pool1\",\"ipv6pool2\"] notice: if multiple ippool are listed, it will try to assign IP from the later ippool when the former one is not allocatable.","title":"annotation"},{"location":"usage/annotation/#annotation","text":"","title":"annotation"},{"location":"usage/annotation/#pod-annotation","text":"Pod could specify spiderpool annotation for special request \"ipam.spidernet.io/ippool\", specify which ippool is used for the assigning IP. \"ipam.spidernet.io/ippool\": { \"interface\": \"eth0\", \"ipv4pool\": \"v4pool1\", \"ipv6pool\": \"v6pool1,v6pool2\" } interface: optional, when integrate with multus CNI , it could specify which ippool is used to the interface. ipv4pool: specify which ippool is used to assign ipv4 ip. It could set with multiple ippool by comma-seperated string. When enableIpv4 in configmap \"spiderpool-conf\" is set to true, this filed is required. ipv6pool: specify which ippool is used to assign ipv6 ip. It could set with multiple ippool by comma-seperated string. When enableIpv6 in configmap \"spiderpool-conf\" is set to true, this filed is required. \"ipam.spidernet.io/ippools\", this one is similar to \"ipam.spidernet.io/ippool\", but could be used to multiple interface case. BTW, the \"ipam.spidernet.io/ippools\" has precedence over \"ipam.spidernet.io/ippool\". \"ipam.spidernet.io/ippools\": [{ \"interface\": \"eth0\", \"ipv4pool\": \"v4pool1\", \"ipv6pool\": \"v6pool1\", \"defaultRoute\": true }, { \"interface\": \"eth1\", \"ipv4pool\": \"v4pool2\", \"ipv6pool\": \"v6pool2\", \"defaultRoute\": false}] defaultRoute: required, if set to be true, the IPAM plugin will return the default gateway route recorded in the ippool. limit: For different interface, it is forbid to use ippools in a same subnet. \"ipam.spidernet.io/routes\", administrator could use this to take effect additional route. \"ipam.spidernet.io/routes\":[{\"interface\": \"eth0\", \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\"}] \"ipam.spidernet.io/assigned-INTERFACE\", this is the IP assigned result for an interface. \"ipam.spidernet.io/assigned-eth0\": { \"interface\": \"eth0\", \"ipv4pool\": \"v4pool1\", \"ipv6pool\": \"v6pool1\", \"ipv4\": \"172.16.0.100/16\", \"ipv6\": \"fd00::100/64\", \"vlan\": 100 }","title":"Pod Annotation"},{"location":"usage/annotation/#namespace-annotation","text":"the namespace resource could set following annotation to specify default ippool, to override the default ippool of the cluster recorded in configmap \"spiderpool-conf\". \"ipam.spidernet.io/defaultv4ippool\" \"ipam.spidernet.io/defaultv4ippool\": [ \"ipv4pool1\",\"ipv4pool2\"] notice: if multiple ippool are listed, it will try to assign IP from the later ippool when the former one is not allocatable. \"ipam.spidernet.io/defaultv6ippool\" \"ipam.spidernet.io/defaultv6ippool\": [ \"ipv6pool1\",\"ipv6pool2\"] notice: if multiple ippool are listed, it will try to assign IP from the later ippool when the former one is not allocatable.","title":"Namespace Annotation"},{"location":"usage/cli/","text":"spiderpoolctl the spiderpoolctl is CLI tool, help to debug the spiderpool","title":"spiderpoolctl"},{"location":"usage/cli/#spiderpoolctl","text":"the spiderpoolctl is CLI tool, help to debug the spiderpool","title":"spiderpoolctl"},{"location":"usage/config/","text":"Configuration IPAM Plugin Configuration the following is an example for IPAM configuration { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-pod-network\", \"plugins\": [ { \"name\": \"macvlan-pod-network\", \"type\": \"macvlan\", \"master\": \"ens256\", \"mode\": \"bridge\", \"mtu\": 1500, \"ipam\": { \"type\": \"spiderpool\", \"log_file_path\": \"/var/run/spidernet/spiderpool.log\", \"log_file_max_size\": \"100M\", \"log_file_max_age\": \"30d\", \"log_file_max_count\": 7, \"log_level\": \"INFO\" } } ] } log_file_path optional, log file path of the IPAM plugin, default to \"/var/run/spidernet/spiderpool.log\" log_file_max_size optional, max file size for each rotated file, default to \"100M\" log_file_max_age optional, max file age for each rotated file, default to \"30d\" log_file_max_count optional, max number of rotated file, default to \"7\" log_level optional, log level, default to \"INFO\". It could be \"INFO\", \"DEBUG\", \"WARN\", \"ERROR\" Configmap Configuration The configmap \"spiderpool-conf\" is the global configuration of spiderpool. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: ipamUnixSocketPath: \"/var/run/spidernet/spiderpool.sock\" enableIpv4: true enableIpv6: true clusterDefaultIpv4Ippool: [] clusterDefaultIpv6Ippool: [] networkMode: \"legacy\" ipamUnixSocketPath the spiderpool agent pod will listen on this unix socket file, and handle IPAM request from the IPAM plugin enableIpv4 true: the spiderpool will assign ipv4 IP, if fail to assign an ipv4 IP, the IPAM plugin will fail for pod creating false: the spiderpool will ignore assigning ipv4 IP enableIpv6 true: the spiderpool will assign ipv6 IP, if fail to assign an ipv6 IP, the IPAM plugin will fail for pod creating false: the spiderpool will ignore assigning ipv6 IP clusterDefaultIpv4Ippool the global default ippool of ipv4, it could set to multiple ippool for backup case. Notice, the IP version of these ippool must be IPv4. clusterDefaultIpv6Ippool the global default ippool of ipv6, it could set to multiple ippool for backup case. Notice, the IP version of these ippool must be IPv6. networkMode network mode of spiderpool, currently, it only support: \"legacy\" spiderpool controller environment environment description value SPIDERPOOL_LOG_LEVEL log level \"INFO\", \"DEBUG\", \"ERROR\", default to \"INFO\" SPIDERPOOL_ENABLED_PPROF enable pprof for debug 5721 SPIDERPOOL_ENABLED_METRIC enable metrics \"true\" or \"false\". default to \"false\" SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 SPIDERPOOL_HEALTH_PORT SPIDERPOOL_GC_IPPOOL_ENABLED SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY SPIDERPOOL_GC_EVICTED_POD_IP_ENABLED SPIDERPOOL_GC_EVICTED_POD_IP_DELAY spiderpool agent environment environment description value SPIDERPOOL_LOG_LEVEL log level \"INFO\", \"DEBUG\", \"ERROR\", default to \"INFO\" SPIDERPOOL_ENABLED_PPROF enable pprof for debug 5721 SPIDERPOOL_ENABLED_METRIC enable metrics \"true\" or \"false\". default to \"false\" SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 SPIDERPOOL_HEALTH_PORT","title":"Configuration"},{"location":"usage/config/#configuration","text":"","title":"Configuration"},{"location":"usage/config/#ipam-plugin-configuration","text":"the following is an example for IPAM configuration { \"cniVersion\": \"0.3.1\", \"name\": \"macvlan-pod-network\", \"plugins\": [ { \"name\": \"macvlan-pod-network\", \"type\": \"macvlan\", \"master\": \"ens256\", \"mode\": \"bridge\", \"mtu\": 1500, \"ipam\": { \"type\": \"spiderpool\", \"log_file_path\": \"/var/run/spidernet/spiderpool.log\", \"log_file_max_size\": \"100M\", \"log_file_max_age\": \"30d\", \"log_file_max_count\": 7, \"log_level\": \"INFO\" } } ] } log_file_path optional, log file path of the IPAM plugin, default to \"/var/run/spidernet/spiderpool.log\" log_file_max_size optional, max file size for each rotated file, default to \"100M\" log_file_max_age optional, max file age for each rotated file, default to \"30d\" log_file_max_count optional, max number of rotated file, default to \"7\" log_level optional, log level, default to \"INFO\". It could be \"INFO\", \"DEBUG\", \"WARN\", \"ERROR\"","title":"IPAM Plugin Configuration"},{"location":"usage/config/#configmap-configuration","text":"The configmap \"spiderpool-conf\" is the global configuration of spiderpool. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: ipamUnixSocketPath: \"/var/run/spidernet/spiderpool.sock\" enableIpv4: true enableIpv6: true clusterDefaultIpv4Ippool: [] clusterDefaultIpv6Ippool: [] networkMode: \"legacy\" ipamUnixSocketPath the spiderpool agent pod will listen on this unix socket file, and handle IPAM request from the IPAM plugin enableIpv4 true: the spiderpool will assign ipv4 IP, if fail to assign an ipv4 IP, the IPAM plugin will fail for pod creating false: the spiderpool will ignore assigning ipv4 IP enableIpv6 true: the spiderpool will assign ipv6 IP, if fail to assign an ipv6 IP, the IPAM plugin will fail for pod creating false: the spiderpool will ignore assigning ipv6 IP clusterDefaultIpv4Ippool the global default ippool of ipv4, it could set to multiple ippool for backup case. Notice, the IP version of these ippool must be IPv4. clusterDefaultIpv6Ippool the global default ippool of ipv6, it could set to multiple ippool for backup case. Notice, the IP version of these ippool must be IPv6. networkMode network mode of spiderpool, currently, it only support: \"legacy\"","title":"Configmap Configuration"},{"location":"usage/config/#spiderpool-controller-environment","text":"environment description value SPIDERPOOL_LOG_LEVEL log level \"INFO\", \"DEBUG\", \"ERROR\", default to \"INFO\" SPIDERPOOL_ENABLED_PPROF enable pprof for debug 5721 SPIDERPOOL_ENABLED_METRIC enable metrics \"true\" or \"false\". default to \"false\" SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 SPIDERPOOL_HEALTH_PORT SPIDERPOOL_GC_IPPOOL_ENABLED SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY SPIDERPOOL_GC_EVICTED_POD_IP_ENABLED SPIDERPOOL_GC_EVICTED_POD_IP_DELAY","title":"spiderpool controller environment"},{"location":"usage/config/#spiderpool-agent-environment","text":"environment description value SPIDERPOOL_LOG_LEVEL log level \"INFO\", \"DEBUG\", \"ERROR\", default to \"INFO\" SPIDERPOOL_ENABLED_PPROF enable pprof for debug 5721 SPIDERPOOL_ENABLED_METRIC enable metrics \"true\" or \"false\". default to \"false\" SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 SPIDERPOOL_HEALTH_PORT","title":"spiderpool agent environment"},{"location":"usage/demo/","text":"Quick Start install spiderpool tools/cert/generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT=` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY=` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` helm install spiderpool spidernet/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.server.cert=\"${SERVER_CERT}\" \\ --set spiderpoolController.tls.server.key=\"${SERVER_KEY}\" \\ --set spiderpoolController.tls.server.ca=\"${CA}\" create ippool create application get metrics","title":"Quick Start"},{"location":"usage/demo/#quick-start","text":"","title":"Quick Start"},{"location":"usage/demo/#install-spiderpool","text":"tools/cert/generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT=` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY=` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` helm install spiderpool spidernet/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.server.cert=\"${SERVER_CERT}\" \\ --set spiderpoolController.tls.server.key=\"${SERVER_KEY}\" \\ --set spiderpoolController.tls.server.ca=\"${CA}\"","title":"install spiderpool"},{"location":"usage/demo/#create-ippool","text":"","title":"create ippool"},{"location":"usage/demo/#create-application","text":"","title":"create application"},{"location":"usage/demo/#get-metrics","text":"","title":"get metrics"},{"location":"usage/metrics/","text":"Metric the spiderpool provides reach metrics spiderpool controller the metric of spiderpool controller is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool controller provides following metrics Metric Name Description spiderpool agent the metric of spiderpool agent is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool agent provides following metrics Metric Name Description","title":"Metric"},{"location":"usage/metrics/#metric","text":"the spiderpool provides reach metrics","title":"Metric"},{"location":"usage/metrics/#spiderpool-controller","text":"the metric of spiderpool controller is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool controller provides following metrics Metric Name Description","title":"spiderpool controller"},{"location":"usage/metrics/#spiderpool-agent","text":"the metric of spiderpool agent is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool agent provides following metrics Metric Name Description","title":"spiderpool agent"},{"location":"usage/qa/","text":"Q&A","title":"Q&A"},{"location":"usage/qa/#qa","text":"","title":"Q&amp;A"},{"location":"usage/reservedip/","text":"Reserved IP If any IP is not expected to be assigned to Pod, there is some solution ReservedIP CRD. ReservedIP takes effect global scope, it could prevent assigning IP from all ippool instances. It makes much sense to use ReservedIP like cases: no matter how many ippool there is, or no matter what CIDR each ippool belong to, just set the reserved IP to ReservedIP CRD excludeIPs field in ippool CRD. excludeIPs field take effect just in its ippool, it just prevent assigning IP from local ippool. It makes much sense to use ReservedIP like following cases: ips: [\"192.168.0.0/24\"] excludeIPs: [\"192.168.0.1\",\"192.168.0.255\"]","title":"Reserved IP"},{"location":"usage/reservedip/#reserved-ip","text":"If any IP is not expected to be assigned to Pod, there is some solution ReservedIP CRD. ReservedIP takes effect global scope, it could prevent assigning IP from all ippool instances. It makes much sense to use ReservedIP like cases: no matter how many ippool there is, or no matter what CIDR each ippool belong to, just set the reserved IP to ReservedIP CRD excludeIPs field in ippool CRD. excludeIPs field take effect just in its ippool, it just prevent assigning IP from local ippool. It makes much sense to use ReservedIP like following cases: ips: [\"192.168.0.0/24\"] excludeIPs: [\"192.168.0.1\",\"192.168.0.255\"]","title":"Reserved IP"}]}
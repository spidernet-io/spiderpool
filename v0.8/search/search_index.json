{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spiderpool English | \u7b80\u4f53\u4e2d\u6587 As a CNCF Landscape Level Project , Spiderpool is the underlay and RDMA network solution of the Kubernetes, for bare metal, VM and any public cloud Introduction Spiderpool is a Kubernetes underlay network solution that enhances the capabilities of Macvlan CNI , ipvlan CNI , SR-IOV CNI .It fulfills various networking needs and enables the utilization of underlay network solutions in bare metal, virtual machine, and public cloud environments . Spiderpool delivers exceptional network performance, particularly benefiting network I/O-intensive and low-latency applications like storage, middleware, and AI . It could refer to website for more details. Why does Spiderpool select macvlan, ipvlan, and SR-IOV as datapaths? macvlan, ipvlan, and SR-IOV is crucial for supporting RDMA network acceleration. RDMA significantly enhances performance for AI applicaitons, latency-sensitive and network I/O-intensive applications, surpassing overlay network solutions in terms of network performance. Unlike CNI solutions based on veth virtual interfaces, underlay networks eliminate layer 3 network forwarding on the host, avoiding tunnel encapsulation overhead. This translates to excellent network performance with high throughput, low latency, and reduced CPU utilization for network forwarding. Connecting seamlessly with underlay layer 2 VLAN networks enables both layer 2 and layer 3 communication for applications. It supports multicast and broadcast communication, while allowing packets to be controlled by firewalls. Data packages carry the actual IP addresses of Pods, enabling direct north-south communication based on Pod IPs. This connectivity across multi-cloud networks enhances flexibility and ease of use. Underlay CNI can create virtual interfaces using different parent network interfaces on the host, providing isolated subnets for applications with high network overhead, such as storage and observability. What enhancements does Spiderpool provide for macvlan, ipvlan, and SR-IOV CNI? Simplified installation and usage Spiderpool simplifies the installation process by eliminating the need for manually installing multiple components like Multus CNI . It provides streamlined installation procedures, encapsulates relevant CRDs, and offers comprehensive documentation for easy setup and management. CRD-based dual-stack IPAM capability Spiderpool provides exclusive and shared IP address pools, supporting various affinity settings. It allows configuring specific IP addresses for stateful applications like middleware and kubevirt, while enabling fixed IP address ranges for stateless ones. Spiderpool automates the management of exclusive IP pools, ensuring excellent IP reclamation to avoid IP leakage. In additions, it owns wonderful IPAM performance . Multiple network interface access for Pods Spiderpool enables scenarios where Pods can have multiple underlay CNI interfaces or a combination of overlay and underlay CNI interfaces. It ensures proper IP addressing for each CNI interface and effectively manages policy routing to maintain consistent data paths, eliminating packet loss concerns. It could strengthen cilium , calico , kubevirt . Enhanced network connectivity Spiderpool establishes seamless connectivity between Pods and host machines, ensuring smooth functioning of Pod health checks. It enables Pods to access services through kube-proxy or eBPF-based kube-proxy replacement. Additionally, it supports advanced features like IP conflict detection and gateway reachability checks. eBPF enhancements The eBPF-based kube-proxy replacement significantly accelerates service access, while socket short-circuiting technology improves local Pod communication efficiency within the same node. Compared with kube-proxy manner, the improvement of the performance is Up to 25% on network delay, up to 50% on network throughput . RDMA support Spiderpool provides RDMA solutions based on RoCE and InfiniBand technologies. Dual-stack network support Spiderpool supports IPv4-only, IPv6-only, and dual-stack environments. Good network performance of latency and throughput Spiderpool performs better than overlay CNI on network latency and throughput, referring to performance report Metrics Which scenarios can Spiderpool be applied in? Spiderpool, powered by underlay CNI, offers unparalleled network performance compared to overlay CNI solutions, as evidenced in I/O Performance . It can be effectively applied in various scenarios, including: Provide a unified underlay CNI solution for bare metal, virtual machine, and public cloud environments. Traditional host applications. Network I/O-intensive applications such as middleware, data storage, log observability, and AI training. Latency-sensitive application. Quick start Refer to Quick start to explore Spiderpool quickly. Spiderpool Architecture Spiderpool features a well-designed and comprehensive architecture that caters to various application scenarios, including: Pods have multiple underlay CNI network interfaces to establish connections with underlay networks. Pods have an underlay CNI and multiple underlay CNI network interfaces, enabling dual-network connectivity. Underlay CNIs run in public cloud environments or virtual machines. Leverage RDMA for efficient network transmission. For more detailed information, please refer to Spiderpool Architecture . Major Features Features macvlan ipvlan SR-IOV Service by kubeproxy Beta Beta Beta Service by kubeproxy replacement Alpha Alpha Alpha Network policy In-plan Alpha In-plan Bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta Egress policy Alpha Alpha Alpha Multiple NIC and routing coordination Beta Beta Beta Applied scenarios Bare metal Bare metal and VM Bare metal For detailed information about all the planned features, please refer to the roadmap . Blogs Refer to Blogs Governance The project is governed by a group of Maintainers and Committers . How they are selected and govern is outlined in our governance document . Adopters A list of adopters who are deploying Spiderpool in production, and of their use cases, can be found in file . Contribution Refer to Contribution to join us for developing Spiderppol. Contact Us If you have any questions, please feel free to reach out to us through the following channels: Slack: join the #Spiderpool channel on CNCF Slack by requesting an invitation from CNCF Slack. Once you have access to CNCF Slack, you can join the Spiderpool channel. Email: refer to the MAINTAINERS.md to find the email addresses of all maintainers. Feel free to contact them via email to report any issues or ask questions. WeChat group: scan the QR code below to join the Spiderpool technical discussion group and engage in further conversations with us. License Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Spiderpool enriches the CNCF Cloud Native Landscape .","title":"Spiderpool"},{"location":"#spiderpool","text":"English | \u7b80\u4f53\u4e2d\u6587 As a CNCF Landscape Level Project , Spiderpool is the underlay and RDMA network solution of the Kubernetes, for bare metal, VM and any public cloud","title":"Spiderpool"},{"location":"#introduction","text":"Spiderpool is a Kubernetes underlay network solution that enhances the capabilities of Macvlan CNI , ipvlan CNI , SR-IOV CNI .It fulfills various networking needs and enables the utilization of underlay network solutions in bare metal, virtual machine, and public cloud environments . Spiderpool delivers exceptional network performance, particularly benefiting network I/O-intensive and low-latency applications like storage, middleware, and AI . It could refer to website for more details. Why does Spiderpool select macvlan, ipvlan, and SR-IOV as datapaths? macvlan, ipvlan, and SR-IOV is crucial for supporting RDMA network acceleration. RDMA significantly enhances performance for AI applicaitons, latency-sensitive and network I/O-intensive applications, surpassing overlay network solutions in terms of network performance. Unlike CNI solutions based on veth virtual interfaces, underlay networks eliminate layer 3 network forwarding on the host, avoiding tunnel encapsulation overhead. This translates to excellent network performance with high throughput, low latency, and reduced CPU utilization for network forwarding. Connecting seamlessly with underlay layer 2 VLAN networks enables both layer 2 and layer 3 communication for applications. It supports multicast and broadcast communication, while allowing packets to be controlled by firewalls. Data packages carry the actual IP addresses of Pods, enabling direct north-south communication based on Pod IPs. This connectivity across multi-cloud networks enhances flexibility and ease of use. Underlay CNI can create virtual interfaces using different parent network interfaces on the host, providing isolated subnets for applications with high network overhead, such as storage and observability. What enhancements does Spiderpool provide for macvlan, ipvlan, and SR-IOV CNI? Simplified installation and usage Spiderpool simplifies the installation process by eliminating the need for manually installing multiple components like Multus CNI . It provides streamlined installation procedures, encapsulates relevant CRDs, and offers comprehensive documentation for easy setup and management. CRD-based dual-stack IPAM capability Spiderpool provides exclusive and shared IP address pools, supporting various affinity settings. It allows configuring specific IP addresses for stateful applications like middleware and kubevirt, while enabling fixed IP address ranges for stateless ones. Spiderpool automates the management of exclusive IP pools, ensuring excellent IP reclamation to avoid IP leakage. In additions, it owns wonderful IPAM performance . Multiple network interface access for Pods Spiderpool enables scenarios where Pods can have multiple underlay CNI interfaces or a combination of overlay and underlay CNI interfaces. It ensures proper IP addressing for each CNI interface and effectively manages policy routing to maintain consistent data paths, eliminating packet loss concerns. It could strengthen cilium , calico , kubevirt . Enhanced network connectivity Spiderpool establishes seamless connectivity between Pods and host machines, ensuring smooth functioning of Pod health checks. It enables Pods to access services through kube-proxy or eBPF-based kube-proxy replacement. Additionally, it supports advanced features like IP conflict detection and gateway reachability checks. eBPF enhancements The eBPF-based kube-proxy replacement significantly accelerates service access, while socket short-circuiting technology improves local Pod communication efficiency within the same node. Compared with kube-proxy manner, the improvement of the performance is Up to 25% on network delay, up to 50% on network throughput . RDMA support Spiderpool provides RDMA solutions based on RoCE and InfiniBand technologies. Dual-stack network support Spiderpool supports IPv4-only, IPv6-only, and dual-stack environments. Good network performance of latency and throughput Spiderpool performs better than overlay CNI on network latency and throughput, referring to performance report Metrics Which scenarios can Spiderpool be applied in? Spiderpool, powered by underlay CNI, offers unparalleled network performance compared to overlay CNI solutions, as evidenced in I/O Performance . It can be effectively applied in various scenarios, including: Provide a unified underlay CNI solution for bare metal, virtual machine, and public cloud environments. Traditional host applications. Network I/O-intensive applications such as middleware, data storage, log observability, and AI training. Latency-sensitive application.","title":"Introduction"},{"location":"#quick-start","text":"Refer to Quick start to explore Spiderpool quickly.","title":"Quick start"},{"location":"#spiderpool-architecture","text":"Spiderpool features a well-designed and comprehensive architecture that caters to various application scenarios, including: Pods have multiple underlay CNI network interfaces to establish connections with underlay networks. Pods have an underlay CNI and multiple underlay CNI network interfaces, enabling dual-network connectivity. Underlay CNIs run in public cloud environments or virtual machines. Leverage RDMA for efficient network transmission. For more detailed information, please refer to Spiderpool Architecture .","title":"Spiderpool Architecture"},{"location":"#major-features","text":"Features macvlan ipvlan SR-IOV Service by kubeproxy Beta Beta Beta Service by kubeproxy replacement Alpha Alpha Alpha Network policy In-plan Alpha In-plan Bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta Egress policy Alpha Alpha Alpha Multiple NIC and routing coordination Beta Beta Beta Applied scenarios Bare metal Bare metal and VM Bare metal For detailed information about all the planned features, please refer to the roadmap .","title":"Major Features"},{"location":"#blogs","text":"Refer to Blogs","title":"Blogs"},{"location":"#governance","text":"The project is governed by a group of Maintainers and Committers . How they are selected and govern is outlined in our governance document .","title":"Governance"},{"location":"#adopters","text":"A list of adopters who are deploying Spiderpool in production, and of their use cases, can be found in file .","title":"Adopters"},{"location":"#contribution","text":"Refer to Contribution to join us for developing Spiderppol.","title":"Contribution"},{"location":"#contact-us","text":"If you have any questions, please feel free to reach out to us through the following channels: Slack: join the #Spiderpool channel on CNCF Slack by requesting an invitation from CNCF Slack. Once you have access to CNCF Slack, you can join the Spiderpool channel. Email: refer to the MAINTAINERS.md to find the email addresses of all maintainers. Feel free to contact them via email to report any issues or ask questions. WeChat group: scan the QR code below to join the Spiderpool technical discussion group and engage in further conversations with us.","title":"Contact Us"},{"location":"#license","text":"Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Spiderpool enriches the CNCF Cloud Native Landscape .","title":"License"},{"location":"README-zh_CN/","text":"Spiderpool English | \u7b80\u4f53\u4e2d\u6587 \u4f5c\u4e3a\u4e00\u4e2a CNCF Landscape \u9879\u76ee \uff0cSpiderpool \u63d0\u4f9b\u4e86\u4e00\u4e2a Kubernetes \u7684 underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u80fd\u8fd0\u884c\u5728\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u4e0a Spiderpool \u4ecb\u7ecd Spiderpool \u662f\u4e00\u4e2a kubernetes \u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u589e\u5f3a\u4e86 Macvlan CNI , ipvlan CNI , SR-IOV CNI \u7684\u529f\u80fd\uff0c\u6ee1\u8db3\u4e86\u5404\u79cd\u7f51\u7edc\u9700\u6c42\uff0c\u4f7f\u5f97 underlay \u7f51\u7edc\u65b9\u6848\u53ef\u5e94\u7528\u5728**\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883**\u4e2d\uff0c\u53ef\u4e3a\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u3001\u4f4e\u5ef6\u65f6\u5e94\u7528\u5e26\u6765\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec**\u5b58\u50a8\u3001\u4e2d\u95f4\u4ef6\u3001AI \u7b49\u5e94\u7528**\u3002\u8be6\u7ec6\u7684\u6587\u6863\u53ef\u53c2\u8003 \u6587\u6863\u7ad9 \u4e3a\u4ec0\u4e48 Spiderpool \u9009\u62e9 macvlan\u3001ipvlan\u3001SR-IOV \u4e3a datapath \uff1f macvlan\u3001ipvlan\u3001SR-IOV \u662f\u627f\u8f7d RDMA \u7f51\u7edc\u52a0\u901f\u7684\u91cd\u8981\u6280\u672f\uff0cRDMA \u80fd\u4e3a AI \u5e94\u7528\u3001\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528\u3001\u7f51\u7edc I/O \u5bc6\u96c6\u578b\u5e94\u7528\u5e26\u6765\u6781\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u7f51\u7edc\u6027\u80fd\u5927\u5e45\u8d85\u8fc7 overlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u533a\u522b\u4e8e\u57fa\u4e8e veth \u865a\u62df\u7f51\u5361\u7684 CNI \u89e3\u51b3\u65b9\u6848\uff0cunderlay \u7f51\u7edc\u6570\u636e\u5305\u907f\u514d\u4e86\u5bbf\u4e3b\u673a\u7684\u4e09\u5c42\u7f51\u7edc\u8f6c\u53d1\uff0c\u6ca1\u6709\u96a7\u9053\u5c01\u88c5\u5f00\u9500\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u80fd\u4e3a\u5e94\u7528\u63d0\u4f9b\u4e86\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec\u4f18\u79c0\u7684\u7f51\u7edc\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u65f6\uff0c\u8282\u7701\u4e86 CPU \u7684\u7f51\u7edc\u8f6c\u53d1\u5f00\u9500\u3002 \u53ef\u76f4\u63a5\u5bf9\u63a5 underlay \u4e8c\u5c42 VLAN \u7f51\u7edc\uff0c\u5e94\u7528\u53ef\u8fdb\u884c\u4e8c\u5c42\u3001\u4e09\u5c42\u7f51\u7edc\u901a\u4fe1\uff0c\u53ef\u8fdb\u884c\u7ec4\u64ad\u3001\u591a\u64ad\u901a\u4fe1\uff0c\u6570\u636e\u5305\u53ef\u53d7\u9632\u706b\u5899\u7ba1\u63a7\u3002 \u6570\u636e\u5305\u643a\u5e26 Pod \u7684\u771f\u6b63 IP \u5730\u5740\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u57fa\u4e8e Pod IP \u8fdb\u884c\u5357\u5317\u5411\u901a\u4fe1\uff0c\u591a\u4e91\u7f51\u7edc\u5929\u7136\u8054\u901a\u3002 underlay CNI \u53ef\u57fa\u4e8e\u5bbf\u4e3b\u673a\u4e0d\u540c\u7684\u7236\u7f51\u5361\u6765\u521b\u5efa\u865a\u62df\u673a\u63a5\u53e3\uff0c\u56e0\u6b64\u53ef\u4e3a\u5b58\u50a8\u3001\u89c2\u6d4b\u6027\u7b49\u7f51\u7edc\u5f00\u9500\u5927\u7684\u5e94\u7528\u63d0\u4f9b\u9694\u79bb\u7684\u5b50\u7f51\u3002 Spiderpool \u4e3a macvlan\u3001ipvlan\u3001SR-IOV CNI \u589e\u5f3a\u4e86\u4ec0\u4e48\uff1f \u7b80\u5316\u5b89\u88c5\u548c\u4f7f\u7528 \u5f53\u524d\u5f00\u6e90\u793e\u533a\u5bf9\u4e8e underlay CNI \u7684\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u5b89\u88c5 Multus CNI \u7b49\u8bf8\u591a\u7ec4\u4ef6\uff0cSpiderpool \u7b80\u5316\u4e86\u5b89\u88c5\u6d41\u7a0b\uff0c\u5bf9\u76f8\u5173\u7684 CRD \u8fdb\u884c\u4e86\u5c01\u88c5\uff0c\u63d0\u4f9b\u4e86\u5404\u79cd\u573a\u666f\u7684\u5b8c\u5907\u6587\u6863\uff0c\u4f7f\u5f97\u4f7f\u7528\u3001\u7ba1\u7406\u66f4\u52a0\u4fbf\u6377\u3002 \u57fa\u4e8e CRD \u7684\u53cc\u6808 IPAM \u80fd\u529b \u63d0\u4f9b\u4e86\u72ec\u4eab\u3001\u5171\u4eab\u7684 IP \u5730\u5740\u6c60\uff0c\u652f\u6301\u8bbe\u7f6e\u5404\u79cd\u4eb2\u548c\u6027\uff0c\u4e3a\u4e2d\u95f4\u4ef6\u7b49\u6709\u72b6\u6001\u5e94\u7528\u548c kubevirt \u7b49\u56fa\u5b9a IP \u5730\u5740\u503c\uff0c\u4e3a\u65e0\u72b6\u6001\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\u8303\u56f4\uff0c\u81ea\u52a8\u5316\u7ba1\u7406\u72ec\u4eab\u7684 IP \u6c60\uff0c\u4f18\u79c0\u7684 IP \u56de\u6536\u907f\u514d IP \u6cc4\u9732\u7b49\u3002\u5e76\u4e14\uff0c\u5177\u5907\u4f18\u79c0\u7684 IPAM \u5206\u914d\u6027\u80fd \u3002 Pod \u63a5\u5165\u591a\u7f51\u5361 \u5b83\u5305\u62ec\u4e86 \u201cPod \u63d2\u5165\u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u3001\u201cPod \u63d2\u5165\u4e00\u4e2a overlay CNI \u548c \u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u4e24\u79cd\u573a\u666f\uff0cPod \u5177\u5907\u591a\u79cd CNI \u7f51\u5361\uff0cSpiderpool \u80fd\u591f\u4e3a\u591a\u4e2a underlay CNI \u7f51\u5361\u5b9a\u5236\u4e0d\u540c\u7684 IP \u5730\u5740\uff0c\u8c03\u534f\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\u800c\u907f\u514d\u4e22\u5305\u3002\u5b83\u80fd\u591f\u4e3a cilium , calico , kubevirt \u7b49\u9879\u76ee\u8fdb\u884c\u589e\u5f3a\u3002 \u589e\u5f3a\u7f51\u7edc\u8fde\u901a\u6027 \u6253\u901a Pod \u548c\u5bbf\u4e3b\u673a\u7684\u8fde\u901a\u6027\uff0c\u786e\u4fdd Pod \u5065\u5eb7\u68c0\u6d4b\u5de5\u4f5c\u6b63\u5e38\uff0c\u5e76\u53ef\u901a\u8fc7 kube-proxy \u6216 eBPF kube-proxy replacement \u4f7f\u5f97 Pod \u8bbf\u95ee service\uff0c\u652f\u6301 Pod \u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u7b49\u3002 eBPF \u589e\u5f3a kube-proxy replacement \u6280\u672f\u6781\u5927\u52a0\u901f\u4e86\u8bbf\u95ee service \u573a\u666f\uff0c\u540c\u8282\u70b9\u4e0a\u7684 socket \u77ed\u8def\u6280\u672f\u52a0\u901f\u4e86\u672c\u5730 Pod \u7684\u901a\u4fe1\u6548\u7387\u3002\u76f8\u6bd4 kube proxy \u89e3\u6790\u65b9\u5f0f\uff0c \u7f51\u7edc\u5ef6\u65f6\u6709\u6700\u5927 25% \u7684\u6539\u5584\uff0c\u7f51\u7edc\u541e\u5410\u6709 50% \u7684\u63d0\u9ad8 \u3002 RDMA \u63d0\u4f9b\u4e86\u57fa\u4e8e RoCE\u3001infiniband \u6280\u672f\u4e0b\u7684 RDMA \u89e3\u51b3\u65b9\u6848\u3002 \u7f51\u7edc\u53cc\u6808\u652f\u6301 Spiderpool \u7ec4\u4ef6\u548c\u5176\u63d0\u4f9b\u7684\u6240\u6709\u529f\u80fd\uff0c\u652f\u6301 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u3002 \u4f18\u79c0\u7684\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u6027\u80fd Spiderpool \u5728\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86 overlay CNI\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u6307\u6807 Spiderpool \u53ef\u5e94\u7528\u5728\u54ea\u4e9b\u573a\u666f\uff1f Spiderpool \u57fa\u4e8e underlay CNI \u63d0\u4f9b\u4e86\u6bd4 overlay CNI \u8fd8\u4f18\u8d8a\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u3002\u5177\u4f53\u53ef\u5e94\u7528\u5728\u5982\u4e0b\uff1a \u4e3a\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u3001\u5404\u5927\u516c\u6709\u4e91\u5382\u5546\u7684\u73af\u5883\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684 underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u4f20\u7edf\u7684\u4e3b\u673a\u5e94\u7528 \u4e2d\u95f4\u4ef6\u3001\u6570\u636e\u5b58\u50a8\u3001\u65e5\u5fd7\u89c2\u6d4b\u3001AI \u8bad\u7ec3\u7b49\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u5e94\u7528 \u7f51\u7edc\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528 \u5feb\u901f\u5f00\u59cb \u53ef\u53c2\u8003 \u5feb\u901f\u642d\u5efa \u6765\u4f7f\u7528 Spiderpool Spiderpool \u67b6\u6784 Spiderpool \u62e5\u6709\u6e05\u6670\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5305\u62ec\u4e86\u5982\u4e0b\u5e94\u7528\u573a\u666f\uff1a Pod \u63a5\u5165\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361\uff0c\u63a5\u5165 underlay \u7f51\u7edc Pod \u63a5\u5165\u4e00\u4e2a underlay CNI \u548c\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361\uff0c\u540c\u65f6\u63a5\u5165\u53cc\u7f51\u7edc underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a \u57fa\u4e8e RDMA \u8fdb\u884c\u7f51\u7edc\u4f20\u8f93 \u5177\u4f53\u53ef\u53c2\u8003 \u67b6\u6784 \u6838\u5fc3\u529f\u80fd \u529f\u80fd macvlan ipvlan SR-IOV service by kubeproxy Beta Beta Beta service by kubeproxy replacement Alpha Alpha Alpha network policy In-plan Alpha In-plan bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta egress policy Alpha Alpha Alpha \u591a\u7f51\u5361\u548c\u8def\u7531\u8c03\u8c10 beta beta beta \u9002\u7528\u573a\u666f \u88f8\u91d1\u5c5e \u88f8\u91d1\u5c5e\u548c\u865a\u62df\u673a \u88f8\u91d1\u5c5e \u5173\u4e8e\u6240\u6709\u7684\u529f\u80fd\u89c4\u5212\uff0c\u5177\u4f53\u53ef\u53c2\u8003 roadmap Blogs \u53ef\u53c2\u8003 Blogs Governance Maintainers and Committers \uff0c \u9075\u5faa governance document . \u4f7f\u7528\u8005 \u4f7f\u7528\u4e86 Spiderpool \u9879\u76ee\u7684 \u7528\u6237 . \u53c2\u4e0e\u5f00\u53d1 \u53ef\u53c2\u8003 \u5f00\u53d1\u642d\u5efa\u6587\u6863 . \u8054\u7cfb\u6211\u4eec \u5982\u679c\u6709\u4efb\u4f55\u5173\u4e8e Spiderpool \u7684\u95ee\u9898\uff0c\u6b22\u8fce\u60a8\u968f\u65f6\u901a\u8fc7\u4ee5\u4e0b\u7684\u65b9\u5f0f\u8054\u7cfb\u6211\u4eec\ud83d\udc4f: Slack: \u5982\u679c\u4f60\u60f3\u5728 CNCF Slack \u52a0\u5165 Spiderpool \u7684\u9891\u9053, \u8bf7\u5148\u5f97\u5230 CNCF Slack \u7684 \u9080\u8bf7 \u7136\u540e\u52a0\u5165 #Spiderpool \u7684\u9891\u9053\u3002 \u90ae\u4ef6: \u60a8\u53ef\u4ee5\u67e5\u770b MAINTAINERS.md \u83b7\u53d6\u6240\u6709\u7ef4\u62a4\u8005\u7684\u90ae\u7bb1\u5730\u5740\uff0c \u8054\u7cfb\u90ae\u7bb1\u5730\u5740\u4ee5\u62a5\u544a\u4efb\u4f55\u95ee\u9898\u3002 \u5fae\u4fe1\u7fa4: \u60a8\u53ef\u4ee5\u626b\u63cf\u5fae\u4fe1\u4e8c\u7ef4\u7801\uff0c\u52a0\u5165\u5230 Spiderpool \u6280\u672f\u4ea4\u6d41\u7fa4\u4e0e\u6211\u4eec\u8fdb\u4e00\u6b65\u4ea4\u6d41\u3002 License Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Spiderpool \u4e30\u5bcc\u4e86 CNCF \u4e91\u539f\u751f\u5168\u666f\u56fe \u3002","title":"Spiderpool"},{"location":"README-zh_CN/#spiderpool","text":"English | \u7b80\u4f53\u4e2d\u6587 \u4f5c\u4e3a\u4e00\u4e2a CNCF Landscape \u9879\u76ee \uff0cSpiderpool \u63d0\u4f9b\u4e86\u4e00\u4e2a Kubernetes \u7684 underlay \u548c RDMA \u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u80fd\u8fd0\u884c\u5728\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u4e0a","title":"Spiderpool"},{"location":"README-zh_CN/#spiderpool_1","text":"Spiderpool \u662f\u4e00\u4e2a kubernetes \u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u589e\u5f3a\u4e86 Macvlan CNI , ipvlan CNI , SR-IOV CNI \u7684\u529f\u80fd\uff0c\u6ee1\u8db3\u4e86\u5404\u79cd\u7f51\u7edc\u9700\u6c42\uff0c\u4f7f\u5f97 underlay \u7f51\u7edc\u65b9\u6848\u53ef\u5e94\u7528\u5728**\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883**\u4e2d\uff0c\u53ef\u4e3a\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u3001\u4f4e\u5ef6\u65f6\u5e94\u7528\u5e26\u6765\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec**\u5b58\u50a8\u3001\u4e2d\u95f4\u4ef6\u3001AI \u7b49\u5e94\u7528**\u3002\u8be6\u7ec6\u7684\u6587\u6863\u53ef\u53c2\u8003 \u6587\u6863\u7ad9 \u4e3a\u4ec0\u4e48 Spiderpool \u9009\u62e9 macvlan\u3001ipvlan\u3001SR-IOV \u4e3a datapath \uff1f macvlan\u3001ipvlan\u3001SR-IOV \u662f\u627f\u8f7d RDMA \u7f51\u7edc\u52a0\u901f\u7684\u91cd\u8981\u6280\u672f\uff0cRDMA \u80fd\u4e3a AI \u5e94\u7528\u3001\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528\u3001\u7f51\u7edc I/O \u5bc6\u96c6\u578b\u5e94\u7528\u5e26\u6765\u6781\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u7f51\u7edc\u6027\u80fd\u5927\u5e45\u8d85\u8fc7 overlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u533a\u522b\u4e8e\u57fa\u4e8e veth \u865a\u62df\u7f51\u5361\u7684 CNI \u89e3\u51b3\u65b9\u6848\uff0cunderlay \u7f51\u7edc\u6570\u636e\u5305\u907f\u514d\u4e86\u5bbf\u4e3b\u673a\u7684\u4e09\u5c42\u7f51\u7edc\u8f6c\u53d1\uff0c\u6ca1\u6709\u96a7\u9053\u5c01\u88c5\u5f00\u9500\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u80fd\u4e3a\u5e94\u7528\u63d0\u4f9b\u4e86\u4f18\u79c0\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u5305\u62ec\u4f18\u79c0\u7684\u7f51\u7edc\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u65f6\uff0c\u8282\u7701\u4e86 CPU \u7684\u7f51\u7edc\u8f6c\u53d1\u5f00\u9500\u3002 \u53ef\u76f4\u63a5\u5bf9\u63a5 underlay \u4e8c\u5c42 VLAN \u7f51\u7edc\uff0c\u5e94\u7528\u53ef\u8fdb\u884c\u4e8c\u5c42\u3001\u4e09\u5c42\u7f51\u7edc\u901a\u4fe1\uff0c\u53ef\u8fdb\u884c\u7ec4\u64ad\u3001\u591a\u64ad\u901a\u4fe1\uff0c\u6570\u636e\u5305\u53ef\u53d7\u9632\u706b\u5899\u7ba1\u63a7\u3002 \u6570\u636e\u5305\u643a\u5e26 Pod \u7684\u771f\u6b63 IP \u5730\u5740\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u57fa\u4e8e Pod IP \u8fdb\u884c\u5357\u5317\u5411\u901a\u4fe1\uff0c\u591a\u4e91\u7f51\u7edc\u5929\u7136\u8054\u901a\u3002 underlay CNI \u53ef\u57fa\u4e8e\u5bbf\u4e3b\u673a\u4e0d\u540c\u7684\u7236\u7f51\u5361\u6765\u521b\u5efa\u865a\u62df\u673a\u63a5\u53e3\uff0c\u56e0\u6b64\u53ef\u4e3a\u5b58\u50a8\u3001\u89c2\u6d4b\u6027\u7b49\u7f51\u7edc\u5f00\u9500\u5927\u7684\u5e94\u7528\u63d0\u4f9b\u9694\u79bb\u7684\u5b50\u7f51\u3002 Spiderpool \u4e3a macvlan\u3001ipvlan\u3001SR-IOV CNI \u589e\u5f3a\u4e86\u4ec0\u4e48\uff1f \u7b80\u5316\u5b89\u88c5\u548c\u4f7f\u7528 \u5f53\u524d\u5f00\u6e90\u793e\u533a\u5bf9\u4e8e underlay CNI \u7684\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u5b89\u88c5 Multus CNI \u7b49\u8bf8\u591a\u7ec4\u4ef6\uff0cSpiderpool \u7b80\u5316\u4e86\u5b89\u88c5\u6d41\u7a0b\uff0c\u5bf9\u76f8\u5173\u7684 CRD \u8fdb\u884c\u4e86\u5c01\u88c5\uff0c\u63d0\u4f9b\u4e86\u5404\u79cd\u573a\u666f\u7684\u5b8c\u5907\u6587\u6863\uff0c\u4f7f\u5f97\u4f7f\u7528\u3001\u7ba1\u7406\u66f4\u52a0\u4fbf\u6377\u3002 \u57fa\u4e8e CRD \u7684\u53cc\u6808 IPAM \u80fd\u529b \u63d0\u4f9b\u4e86\u72ec\u4eab\u3001\u5171\u4eab\u7684 IP \u5730\u5740\u6c60\uff0c\u652f\u6301\u8bbe\u7f6e\u5404\u79cd\u4eb2\u548c\u6027\uff0c\u4e3a\u4e2d\u95f4\u4ef6\u7b49\u6709\u72b6\u6001\u5e94\u7528\u548c kubevirt \u7b49\u56fa\u5b9a IP \u5730\u5740\u503c\uff0c\u4e3a\u65e0\u72b6\u6001\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\u8303\u56f4\uff0c\u81ea\u52a8\u5316\u7ba1\u7406\u72ec\u4eab\u7684 IP \u6c60\uff0c\u4f18\u79c0\u7684 IP \u56de\u6536\u907f\u514d IP \u6cc4\u9732\u7b49\u3002\u5e76\u4e14\uff0c\u5177\u5907\u4f18\u79c0\u7684 IPAM \u5206\u914d\u6027\u80fd \u3002 Pod \u63a5\u5165\u591a\u7f51\u5361 \u5b83\u5305\u62ec\u4e86 \u201cPod \u63d2\u5165\u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u3001\u201cPod \u63d2\u5165\u4e00\u4e2a overlay CNI \u548c \u591a\u4e2a underlay CNI \u7f51\u5361\u201d\u4e24\u79cd\u573a\u666f\uff0cPod \u5177\u5907\u591a\u79cd CNI \u7f51\u5361\uff0cSpiderpool \u80fd\u591f\u4e3a\u591a\u4e2a underlay CNI \u7f51\u5361\u5b9a\u5236\u4e0d\u540c\u7684 IP \u5730\u5740\uff0c\u8c03\u534f\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u7684\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\u800c\u907f\u514d\u4e22\u5305\u3002\u5b83\u80fd\u591f\u4e3a cilium , calico , kubevirt \u7b49\u9879\u76ee\u8fdb\u884c\u589e\u5f3a\u3002 \u589e\u5f3a\u7f51\u7edc\u8fde\u901a\u6027 \u6253\u901a Pod \u548c\u5bbf\u4e3b\u673a\u7684\u8fde\u901a\u6027\uff0c\u786e\u4fdd Pod \u5065\u5eb7\u68c0\u6d4b\u5de5\u4f5c\u6b63\u5e38\uff0c\u5e76\u53ef\u901a\u8fc7 kube-proxy \u6216 eBPF kube-proxy replacement \u4f7f\u5f97 Pod \u8bbf\u95ee service\uff0c\u652f\u6301 Pod \u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\u7b49\u3002 eBPF \u589e\u5f3a kube-proxy replacement \u6280\u672f\u6781\u5927\u52a0\u901f\u4e86\u8bbf\u95ee service \u573a\u666f\uff0c\u540c\u8282\u70b9\u4e0a\u7684 socket \u77ed\u8def\u6280\u672f\u52a0\u901f\u4e86\u672c\u5730 Pod \u7684\u901a\u4fe1\u6548\u7387\u3002\u76f8\u6bd4 kube proxy \u89e3\u6790\u65b9\u5f0f\uff0c \u7f51\u7edc\u5ef6\u65f6\u6709\u6700\u5927 25% \u7684\u6539\u5584\uff0c\u7f51\u7edc\u541e\u5410\u6709 50% \u7684\u63d0\u9ad8 \u3002 RDMA \u63d0\u4f9b\u4e86\u57fa\u4e8e RoCE\u3001infiniband \u6280\u672f\u4e0b\u7684 RDMA \u89e3\u51b3\u65b9\u6848\u3002 \u7f51\u7edc\u53cc\u6808\u652f\u6301 Spiderpool \u7ec4\u4ef6\u548c\u5176\u63d0\u4f9b\u7684\u6240\u6709\u529f\u80fd\uff0c\u652f\u6301 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u3002 \u4f18\u79c0\u7684\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u6027\u80fd Spiderpool \u5728\u7f51\u7edc\u5ef6\u65f6\u548c\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86 overlay CNI\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u6307\u6807 Spiderpool \u53ef\u5e94\u7528\u5728\u54ea\u4e9b\u573a\u666f\uff1f Spiderpool \u57fa\u4e8e underlay CNI \u63d0\u4f9b\u4e86\u6bd4 overlay CNI \u8fd8\u4f18\u8d8a\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u6027\u80fd\u62a5\u544a \u3002\u5177\u4f53\u53ef\u5e94\u7528\u5728\u5982\u4e0b\uff1a \u4e3a\u88f8\u91d1\u5c5e\u3001\u865a\u62df\u673a\u3001\u5404\u5927\u516c\u6709\u4e91\u5382\u5546\u7684\u73af\u5883\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684 underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u4f20\u7edf\u7684\u4e3b\u673a\u5e94\u7528 \u4e2d\u95f4\u4ef6\u3001\u6570\u636e\u5b58\u50a8\u3001\u65e5\u5fd7\u89c2\u6d4b\u3001AI \u8bad\u7ec3\u7b49\u7f51\u7edc I/O \u5bc6\u96c6\u6027\u5e94\u7528 \u7f51\u7edc\u5ef6\u65f6\u654f\u611f\u578b\u5e94\u7528","title":"Spiderpool \u4ecb\u7ecd"},{"location":"README-zh_CN/#_1","text":"\u53ef\u53c2\u8003 \u5feb\u901f\u642d\u5efa \u6765\u4f7f\u7528 Spiderpool","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"README-zh_CN/#spiderpool_2","text":"Spiderpool \u62e5\u6709\u6e05\u6670\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5305\u62ec\u4e86\u5982\u4e0b\u5e94\u7528\u573a\u666f\uff1a Pod \u63a5\u5165\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361\uff0c\u63a5\u5165 underlay \u7f51\u7edc Pod \u63a5\u5165\u4e00\u4e2a underlay CNI \u548c\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361\uff0c\u540c\u65f6\u63a5\u5165\u53cc\u7f51\u7edc underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a \u57fa\u4e8e RDMA \u8fdb\u884c\u7f51\u7edc\u4f20\u8f93 \u5177\u4f53\u53ef\u53c2\u8003 \u67b6\u6784","title":"Spiderpool \u67b6\u6784"},{"location":"README-zh_CN/#_2","text":"\u529f\u80fd macvlan ipvlan SR-IOV service by kubeproxy Beta Beta Beta service by kubeproxy replacement Alpha Alpha Alpha network policy In-plan Alpha In-plan bandwidth In-plan Alpha In-plan RDMA Alpha Alpha Alpha IPAM Beta Beta Beta egress policy Alpha Alpha Alpha \u591a\u7f51\u5361\u548c\u8def\u7531\u8c03\u8c10 beta beta beta \u9002\u7528\u573a\u666f \u88f8\u91d1\u5c5e \u88f8\u91d1\u5c5e\u548c\u865a\u62df\u673a \u88f8\u91d1\u5c5e \u5173\u4e8e\u6240\u6709\u7684\u529f\u80fd\u89c4\u5212\uff0c\u5177\u4f53\u53ef\u53c2\u8003 roadmap","title":"\u6838\u5fc3\u529f\u80fd"},{"location":"README-zh_CN/#blogs","text":"\u53ef\u53c2\u8003 Blogs","title":"Blogs"},{"location":"README-zh_CN/#governance","text":"Maintainers and Committers \uff0c \u9075\u5faa governance document .","title":"Governance"},{"location":"README-zh_CN/#_3","text":"\u4f7f\u7528\u4e86 Spiderpool \u9879\u76ee\u7684 \u7528\u6237 .","title":"\u4f7f\u7528\u8005"},{"location":"README-zh_CN/#_4","text":"\u53ef\u53c2\u8003 \u5f00\u53d1\u642d\u5efa\u6587\u6863 .","title":"\u53c2\u4e0e\u5f00\u53d1"},{"location":"README-zh_CN/#_5","text":"\u5982\u679c\u6709\u4efb\u4f55\u5173\u4e8e Spiderpool \u7684\u95ee\u9898\uff0c\u6b22\u8fce\u60a8\u968f\u65f6\u901a\u8fc7\u4ee5\u4e0b\u7684\u65b9\u5f0f\u8054\u7cfb\u6211\u4eec\ud83d\udc4f: Slack: \u5982\u679c\u4f60\u60f3\u5728 CNCF Slack \u52a0\u5165 Spiderpool \u7684\u9891\u9053, \u8bf7\u5148\u5f97\u5230 CNCF Slack \u7684 \u9080\u8bf7 \u7136\u540e\u52a0\u5165 #Spiderpool \u7684\u9891\u9053\u3002 \u90ae\u4ef6: \u60a8\u53ef\u4ee5\u67e5\u770b MAINTAINERS.md \u83b7\u53d6\u6240\u6709\u7ef4\u62a4\u8005\u7684\u90ae\u7bb1\u5730\u5740\uff0c \u8054\u7cfb\u90ae\u7bb1\u5730\u5740\u4ee5\u62a5\u544a\u4efb\u4f55\u95ee\u9898\u3002 \u5fae\u4fe1\u7fa4: \u60a8\u53ef\u4ee5\u626b\u63cf\u5fae\u4fe1\u4e8c\u7ef4\u7801\uff0c\u52a0\u5165\u5230 Spiderpool \u6280\u672f\u4ea4\u6d41\u7fa4\u4e0e\u6211\u4eec\u8fdb\u4e00\u6b65\u4ea4\u6d41\u3002","title":"\u8054\u7cfb\u6211\u4eec"},{"location":"README-zh_CN/#license","text":"Spiderpool is licensed under the Apache License, Version 2.0. See LICENSE for the full license text. Spiderpool \u4e30\u5bcc\u4e86 CNCF \u4e91\u539f\u751f\u5168\u666f\u56fe \u3002","title":"License"},{"location":"USERS/","text":"Adopter orgnization description a bank in ShangHai, China a cluster with a size of 2000+ pods, production environment a bank in SiChuan, China about 80 clusters , not more thant 100 nodes for each cluster a broker in ShangHai, China a cluster with a size of about 50 nodes a smart phone vendor, china a smart phone vendor, china","title":"Adopter"},{"location":"USERS/#adopter","text":"orgnization description a bank in ShangHai, China a cluster with a size of 2000+ pods, production environment a bank in SiChuan, China about 80 clusters , not more thant 100 nodes for each cluster a broker in ShangHai, China a cluster with a size of about 50 nodes a smart phone vendor, china a smart phone vendor, china","title":"Adopter"},{"location":"concepts/arch-zh_CN/","text":"\u67b6\u6784 \u7b80\u4f53\u4e2d\u6587 | English \u67b6\u6784 Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool controller: \u662f\u4e00\u7ec4 deployment\uff0c\u5b9e\u65bd\u4e86\u5bf9\u5404\u79cd CRD \u6821\u9a8c\u3001\u72b6\u6001\u66f4\u65b0\u3001IP \u56de\u6536\u3001\u81ea\u52a8 IP \u6c60\u7684\u7ba1\u7406\u7b49 Spiderpool agent\uff1a\u662f\u4e00\u7ec4 daemonset\uff0c\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u4fe1\u606f\u540c\u6b65 CNI plugins\uff0c\u5b83\u4eec\u5305\u62ec\u5982\u4e0b\uff1a Spiderpool IPAM plugin\uff1a\u4f9b main CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d\u3002 coordinator plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u8c10\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001MAC \u5730\u5740\u56fa\u5b9a\u7b49 ifacer plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u53ef\u81ea\u52a8\u521b\u5efa bond\u3001vlan \u865a\u62df\u63a5\u53e3\uff0c\u4f5c\u4e3a macvlan\u3001ipvlan \u7b49\u63d2\u4ef6\u7684\u7236\u63a5\u53e3\u4f7f\u7528\u3002 Multus CNI : CNI plugin \u7684\u8c03\u5ea6\u5668 CNI plugins: \u5305\u62ec Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI \u7b49\u3002 SR-IOV \u76f8\u5173\u7ec4\u4ef6\uff1a RDMA shared device plugin RDMA CNI SR-IOV network operator \u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361 \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI \uff08\u4f8b\u5982 macvlan CNI , SR-IOV CNI \uff09\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u8c10\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c\u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 Pod \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c\u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c\u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002 \u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u4e00\u4e2a overlay CNI \u548c\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361 \u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a\u4e3a Pod \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361\uff08\u4f8b\u5982 calico , cilium \uff09\u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\uff0c\u53ef\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u8c10\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u8c10\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 Pod \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c\u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 Pod \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002 \u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c\u5728\u4fdd\u969c\u96c6\u7fa4\u5185 Pod \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90 \u5e94\u7528\u573a\u666f \uff1aunderlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a \u5728\u516c\u6709\u4e91\u3001OpenStack\u3001vmvare \u7b49\u73af\u5883\u4e0b\u5b9e\u65bd underlay CNI\uff0c\u901a\u5e38\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u7684\u5382\u5546 CNI \u63d2\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u73af\u5883\u901a\u5e38\u6709\u5982\u4e0b\u9650\u5236\uff1a IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e8c\u5c42\u62a5\u5934\u4e2d\u7684 MAC \u9650\u5236\uff0c\u4e00\u65b9\u9762\uff0c\u5bf9\u6e90 MAC \u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c \u4ee5\u786e\u4fdd\u6e90 MAC \u5730\u5740\u4e0e\u865a\u62df\u673a\u7f51\u5361 MAC \u76f8\u540c\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u76ee\u7684 MAC \u505a\u4e86\u9650\u5236\uff0c\u53ea\u652f\u6301\u8f6c\u53d1 IAAS \u4e2d\u6240\u6709\u865a\u62df\u673a\u7f51\u5361\u7684 MAC\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0cPod \u5206\u914d\u7684\u7f51\u5361\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e09\u5c42\u62a5\u5934\u7684 IP \u9650\u5236\uff0c\u53ea\u6709\u6570\u636e\u5305\u7684\u76ee\u7684\u548c\u6e90 IP \u662f\u5728 IAAS \u4e2d\u5206\u914d\u7ed9\u4e86\u865a\u62df\u673a\u7f51\u5361\u65f6\uff0c\u6570\u636e\u5305\u624d\u80fd\u5f97\u5230\u8f6c\u53d1\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0c\u7ed9 Pod \u5206\u914d\u7684 IP \u5730\u5740\u4e0d\u7b26\u5408 IAAS \u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u4e0e\u865a\u62df\u673a\u7684\u76f8\u540c IP \u5206\u914d\u8bbe\u7f6e\u5bf9\u9f50\uff0c\u518d\u914d\u5408 ipvlan CNI\uff0c \u4ece\u800c\u80fd\u591f\u4e3a\u5404\u79cd\u516c\u6709\u4e91\u73af\u5883\u63d0\u4f9b underlay CNI \u89e3\u51b3\u65b9\u6848\u3002 \u5e94\u7528\u573a\u666f \uff1a\u4f7f\u7528 RDMA \u8fdb\u884c\u7f51\u7edc\u4f20\u8f93\u7684\u5e94\u7528 RDMA \u529f\u80fd\u4f7f\u5f97\u7f51\u5361\u80fd\u591f\u76f4\u63a5\u8bfb\u5199\u5185\u5b58\uff0c\u964d\u4f4e\u4e86 CPU \u7684\u8d1f\u62c5\u548c\u5185\u6838\u534f\u8bae\u6808\u7684\u5904\u7406\uff0c\u662f\u4e00\u79cd\u7f51\u7edc\u534f\u8bae\u6808 offload \u5230\u7f51\u5361\u7684\u6280\u672f\uff0c\u5b83\u80fd\u6709\u6548\u964d\u4f4e\u7f51\u7edc\u4f20\u8f93\u5ef6\u65f6\u3001\u63d0\u9ad8\u541e\u5410\u91cf\u3002 \u5f53\u524d\uff0cRDMA \u6280\u672f\u5728 AI \u8ba1\u7b97\u3001\u5b58\u50a8\u7b49\u5e94\u7528\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u3002Macvlan\u3001IPvlan \u548c SR-IOV CNI\uff0c\u5b83\u4eec\u80fd\u591f\u5728 kubernetes \u5e73\u53f0\u4e0b\u628a RDMA \u7f51\u5361\u900f\u4f20\u7ed9 Pod \u4f7f\u7528\uff0cSpiderpool \u589e\u5f3a\u4e86\u8fd9\u4e9b CNI \u80fd\u529b\uff0c\u5305\u62ec IPAM\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001ClusterIP \u8bbf\u95ee\u7b49\uff0c\u5e76\u4e14\u7b80\u5316\u4e86\u793e\u533a\u4e2d\u7684\u4f9d\u8d56\u7ec4\u4ef6\u5b89\u88c5\u6d41\u7a0b\u548c\u4f7f\u7528\u6b65\u9aa4\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u6613\u7528\u6027\u3002","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#_2","text":"Spiderpool \u67b6\u6784\u5982\u4e0a\u6240\u793a\uff0c\u5305\u542b\u4e86\u4ee5\u4e0b\u7ec4\u4ef6\uff1a Spiderpool controller: \u662f\u4e00\u7ec4 deployment\uff0c\u5b9e\u65bd\u4e86\u5bf9\u5404\u79cd CRD \u6821\u9a8c\u3001\u72b6\u6001\u66f4\u65b0\u3001IP \u56de\u6536\u3001\u81ea\u52a8 IP \u6c60\u7684\u7ba1\u7406\u7b49 Spiderpool agent\uff1a\u662f\u4e00\u7ec4 daemonset\uff0c\u5176\u5e2e\u52a9 Spiderpool plugin \u5b9e\u65bd IP \u5206\u914d\uff0c\u5e2e\u52a9 coordinator plugin \u5b9e\u65bd\u4fe1\u606f\u540c\u6b65 CNI plugins\uff0c\u5b83\u4eec\u5305\u62ec\u5982\u4e0b\uff1a Spiderpool IPAM plugin\uff1a\u4f9b main CNI \u8c03\u7528\uff0c\u5b9e\u65bd IP \u5206\u914d\u3002 coordinator plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u5b9e\u65bd\u591a\u7f51\u5361\u8def\u7531\u8c03\u8c10\u3001IP \u51b2\u7a81\u68c0\u67e5\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001MAC \u5730\u5740\u56fa\u5b9a\u7b49 ifacer plugin\uff1a\u4f5c\u4e3a chain plugin\uff0c\u53ef\u81ea\u52a8\u521b\u5efa bond\u3001vlan \u865a\u62df\u63a5\u53e3\uff0c\u4f5c\u4e3a macvlan\u3001ipvlan \u7b49\u63d2\u4ef6\u7684\u7236\u63a5\u53e3\u4f7f\u7528\u3002 Multus CNI : CNI plugin \u7684\u8c03\u5ea6\u5668 CNI plugins: \u5305\u62ec Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI \u7b49\u3002 SR-IOV \u76f8\u5173\u7ec4\u4ef6\uff1a RDMA shared device plugin RDMA CNI SR-IOV network operator","title":"\u67b6\u6784"},{"location":"concepts/arch-zh_CN/#pod-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 underlay \u6a21\u5f0f\u4e0b\uff0c\u53ef\u914d\u5408 underlay CNI \uff08\u4f8b\u5982 macvlan CNI , SR-IOV CNI \uff09\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u8005\u591a\u4e2a underlay \u7f51\u5361\uff0c\u5e76\u80fd\u8c03\u8c10\u591a\u4e2a underlay CNI \u7f51\u5361\u95f4\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u901a\u8fc7\u989d\u5916\u63a5\u5165 veth \u7f51\u5361\u548c\u8def\u7531\u63a7\u5236\uff0c\u5e2e\u52a9\u5f00\u6e90 underlay CNI \u8054\u901a\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u7b49 \u5f53\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5b58\u5728\u591a\u79cd\u57fa\u7840\u8bbe\u7f6e\u65f6\uff0c\u5982\u4f55\u4f7f\u7528\u5355\u4e00\u7684 underlay CNI \u6765\u90e8\u7f72\u5bb9\u5668\u5462\uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u8282\u70b9\u662f\u865a\u62df\u673a\uff0c\u4f8b\u5982\u672a\u6253\u5f00\u6df7\u6742\u8f6c\u53d1\u6a21\u5f0f\u7684 vmware \u865a\u62df\u673a\uff0c\u800c\u90e8\u5206\u8282\u70b9\u662f\u88f8\u91d1\u5c5e\uff0c\u63a5\u5165\u4e86\u4f20\u7edf\u4ea4\u6362\u673a\u7f51\u7edc\u3002\u56e0\u6b64\u5728\u4e24\u7c7b\u8282\u70b9\u4e0a\u90e8\u7f72\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u53ea\u5177\u5907\u4e00\u5f20 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b 64 \u4e2a VF\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u66f4\u591a\u7684 Pod \uff1f \u5728\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u90e8\u5206\u88f8\u91d1\u5c5e\u8282\u70b9\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u4f4e\u5ef6\u65f6\u5e94\u7528\uff0c\u90e8\u5206\u8282\u70b9\u4e0d\u5177\u5907 SRIOV \u9ad8\u901f\u7f51\u5361\uff0c\u53ef\u4ee5\u8fd0\u884c\u666e\u901a\u5e94\u7528\u3002\u4f46\u5728\u4e24\u7c7b\u8282\u70b9\u90e8\u7f72\u4e0a\u4ec0\u4e48 CNI \u65b9\u6848\u5462 \uff1f \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u591a\u79cd underlay CNI\uff0c\u5145\u5206\u6574\u5408\u96c6\u7fa4\u4e2d\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u8282\u70b9\u7684\u8d44\u6e90\uff0c\u6765\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 \u4f8b\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u6709\u7684\u8282\u70b9\u5177\u5907 SR-IOV \u7f51\u5361\uff0c\u53ef\u8fd0\u884c SR-IOV CNI\uff0c\u6709\u7684\u8282\u70b9\u5177\u5907\u666e\u901a\u7684\u7f51\u5361\uff0c\u53ef\u8fd0\u884c macvlan CNI \uff0c\u6709\u7684\u8282\u70b9\u7f51\u7edc\u8bbf\u95ee\u53d7\u9650\uff08\u4f8b\u5982\u4e8c\u5c42\u7f51\u7edc\u8f6c\u53d1\u53d7\u9650\u7684 vmware \u865a\u62df\u673a\uff09\uff0c\u53ef\u8fd0\u884c ipvlan CNI\u3002","title":"\u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361"},{"location":"concepts/arch-zh_CN/#pod-overlay-cni-underlay-cni","text":"\u5982\u4e0a\u6240\u793a\uff0cSpiderpool \u5de5\u4f5c\u5728 overlay \u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528 multus \u540c\u65f6\u4e3a\u4e3a Pod \u63d2\u5165\u4e00\u5f20 overlay \u7f51\u5361\uff08\u4f8b\u5982 calico , cilium \uff09\u548c\u82e5\u5e72\u5f20 underlay \u7f51\u5361\uff08\u4f8b\u5982 macvlan CNI , sriov CNI \uff09\uff0c\u53ef\u5b9e\u73b0: \u4e3a underlay CNI \u63d0\u4f9b\u4e30\u5bcc\u7684 IPAM \u80fd\u529b,\u5305\u62ec\u5171\u4eab/\u56fa\u5b9a IP\u3001\u591a\u7f51\u5361 IP \u5206\u914d\u3001\u53cc\u6808\u652f\u6301\u7b49 \u4e3a Pod \u7684\u591a\u4e2a underlay CNI \u7f51\u5361\u548c overlay \u7f51\u5361\u8c03\u8c10\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u7545\u901a \u4ee5 overlay \u7f51\u5361\u4f5c\u4e3a\u7f3a\u7701\u7f51\u5361\uff0c\u5e76\u8c03\u8c10\u8def\u7531\uff0c\u901a\u8fc7 overlay \u7f51\u5361\u8054\u901a\u672c\u5730\u5bbf\u4e3b\u673a\uff0c\u5b9e\u73b0 clusterIP \u8bbf\u95ee\u3001\u5e94\u7528\u7684\u672c\u5730\u5065\u5eb7\u68c0\u6d4b\u3001overlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 overlay \u7f51\u7edc\u8f6c\u53d1\uff0c\u800c underlay \u7f51\u7edc\u6d41\u91cf\u901a\u8fc7 underlay \u7f51\u5361\u8f6c\u53d1\u3002 \u7ed3\u5408 multus \u7684 CNI \u914d\u7f6e\u7ba1\u7406\u548c Spiderpool IPAM \u7684\u901a\u7528\u6027\uff0c\u53ef\u540c\u65f6\u8fd0\u884c\u4e00\u79cd overlay CNI \u548c \u591a\u79cd underlay CNI\u3002\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u4e2a\u96c6\u7fa4\u4e0b\u5177\u5907\u4e0d\u540c\u7f51\u7edc\u80fd\u529b\u7684\u8282\u70b9\uff0c \u88f8\u91d1\u5c5e\u8282\u70b9\u4e0a\u7684 Pod \u540c\u65f6\u63a5\u5165 overlay CNI \u548c underlay CNI \u7f51\u5361\uff0c\u865a\u62df\u673a\u8282\u70b9\u4e0a\u7684 Pod \u53ea\u63d0\u4f9b\u96c6\u7fa4\u4e1c\u897f\u5411\u670d\u52a1\uff0c\u53ea\u63a5\u5165 overlay CNI \u7f51\u5361\u3002 \u5e26\u6765\u4e86\u5982\u4e0b\u597d\u5904\uff1a \u628a\u63d0\u4f9b\u4e1c\u897f\u5411\u670d\u52a1\u7684\u5e94\u7528\u53ea\u63a5\u5165 overlay \u7f51\u5361\uff0c\u63d0\u4f9b\u5357\u5317\u5411\u670d\u52a1\u7684\u5e94\u7528\u540c\u65f6\u63a5\u5165 overlay \u548c underlay \u7f51\u5361\uff0c\u5728\u4fdd\u969c\u96c6\u7fa4\u5185 Pod \u8fde\u901a\u6027\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u964d\u4f4e underlay IP \u8d44\u6e90\u7684\u7528\u91cf\uff0c\u51cf\u5c11\u76f8\u5e94\u7684\u4eba\u5de5\u8fd0\u7ef4\u6210\u672c \u5145\u5206\u6574\u5408\u865a\u62df\u673a\u548c\u88f8\u91d1\u5c5e\u8282\u70b9\u8d44\u6e90","title":"\u5e94\u7528\u573a\u666f\uff1aPod \u63a5\u5165\u4e00\u4e2a overlay CNI \u548c\u82e5\u5e72\u4e2a underlay CNI \u7f51\u5361"},{"location":"concepts/arch-zh_CN/#underlay-cni","text":"\u5728\u516c\u6709\u4e91\u3001OpenStack\u3001vmvare \u7b49\u73af\u5883\u4e0b\u5b9e\u65bd underlay CNI\uff0c\u901a\u5e38\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u7684\u5382\u5546 CNI \u63d2\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u73af\u5883\u901a\u5e38\u6709\u5982\u4e0b\u9650\u5236\uff1a IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e8c\u5c42\u62a5\u5934\u4e2d\u7684 MAC \u9650\u5236\uff0c\u4e00\u65b9\u9762\uff0c\u5bf9\u6e90 MAC \u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c \u4ee5\u786e\u4fdd\u6e90 MAC \u5730\u5740\u4e0e\u865a\u62df\u673a\u7f51\u5361 MAC \u76f8\u540c\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u76ee\u7684 MAC \u505a\u4e86\u9650\u5236\uff0c\u53ea\u652f\u6301\u8f6c\u53d1 IAAS \u4e2d\u6240\u6709\u865a\u62df\u673a\u7f51\u5361\u7684 MAC\uff0c\u4e0d\u652f\u6301\u672a\u77e5\u76ee\u7684 MAC\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0cPod \u5206\u914d\u7684\u7f51\u5361\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 IAAS \u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u5bf9\u865a\u62df\u673a\u7f51\u5361\u53d1\u51fa\u7684\u6570\u636e\u5305\uff0c\u5b9e\u65bd\u4e86\u4e09\u5c42\u62a5\u5934\u7684 IP \u9650\u5236\uff0c\u53ea\u6709\u6570\u636e\u5305\u7684\u76ee\u7684\u548c\u6e90 IP \u662f\u5728 IAAS \u4e2d\u5206\u914d\u7ed9\u4e86\u865a\u62df\u673a\u7f51\u5361\u65f6\uff0c\u6570\u636e\u5305\u624d\u80fd\u5f97\u5230\u8f6c\u53d1\u3002\u901a\u5e38\u7684 CNI \u63d2\u4ef6\uff0c\u7ed9 Pod \u5206\u914d\u7684 IP \u5730\u5740\u4e0d\u7b26\u5408 IAAS \u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97 Pod \u901a\u4fe1\u5931\u8d25\u3002 Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u4e0e\u865a\u62df\u673a\u7684\u76f8\u540c IP \u5206\u914d\u8bbe\u7f6e\u5bf9\u9f50\uff0c\u518d\u914d\u5408 ipvlan CNI\uff0c \u4ece\u800c\u80fd\u591f\u4e3a\u5404\u79cd\u516c\u6709\u4e91\u73af\u5883\u63d0\u4f9b underlay CNI \u89e3\u51b3\u65b9\u6848\u3002","title":"\u5e94\u7528\u573a\u666f \uff1aunderlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u548c\u865a\u62df\u673a"},{"location":"concepts/arch-zh_CN/#rdma","text":"RDMA \u529f\u80fd\u4f7f\u5f97\u7f51\u5361\u80fd\u591f\u76f4\u63a5\u8bfb\u5199\u5185\u5b58\uff0c\u964d\u4f4e\u4e86 CPU \u7684\u8d1f\u62c5\u548c\u5185\u6838\u534f\u8bae\u6808\u7684\u5904\u7406\uff0c\u662f\u4e00\u79cd\u7f51\u7edc\u534f\u8bae\u6808 offload \u5230\u7f51\u5361\u7684\u6280\u672f\uff0c\u5b83\u80fd\u6709\u6548\u964d\u4f4e\u7f51\u7edc\u4f20\u8f93\u5ef6\u65f6\u3001\u63d0\u9ad8\u541e\u5410\u91cf\u3002 \u5f53\u524d\uff0cRDMA \u6280\u672f\u5728 AI \u8ba1\u7b97\u3001\u5b58\u50a8\u7b49\u5e94\u7528\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u3002Macvlan\u3001IPvlan \u548c SR-IOV CNI\uff0c\u5b83\u4eec\u80fd\u591f\u5728 kubernetes \u5e73\u53f0\u4e0b\u628a RDMA \u7f51\u5361\u900f\u4f20\u7ed9 Pod \u4f7f\u7528\uff0cSpiderpool \u589e\u5f3a\u4e86\u8fd9\u4e9b CNI \u80fd\u529b\uff0c\u5305\u62ec IPAM\u3001\u5bbf\u4e3b\u673a\u8054\u901a\u3001ClusterIP \u8bbf\u95ee\u7b49\uff0c\u5e76\u4e14\u7b80\u5316\u4e86\u793e\u533a\u4e2d\u7684\u4f9d\u8d56\u7ec4\u4ef6\u5b89\u88c5\u6d41\u7a0b\u548c\u4f7f\u7528\u6b65\u9aa4\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u6613\u7528\u6027\u3002","title":"\u5e94\u7528\u573a\u666f \uff1a\u4f7f\u7528 RDMA \u8fdb\u884c\u7f51\u7edc\u4f20\u8f93\u7684\u5e94\u7528"},{"location":"concepts/arch/","text":"Spiderpool Architecture English | \u7b80\u4f53\u4e2d\u6587 Architecture Spiderpool consists of the following components: Spiderpool controller: a set of deployments that manage CRD validation, status updates, IP recovery, and automated IP pools Spiderpool agent: a set of daemonsets that help Spiderpool plugin by performing IP allocation and coordinator plugin for information synchronization. Spiderpool plugin: a binary plugin on each host that CNI can utilize to implement IP allocation. CNI plugins include: Spiderpool IPAM plugin: a main CNI used to handle IP allocation. coordinator plugin: as a chain plugin, it performs various functions such as routing coordination for multiple network interfaces, checking for IP conflicts, ensuring host connectivity, and fixing MAC addresses. ifacer plugin: as a chain plugin, it automates the creation of bond and VLAN virtual interfaces that serve as parent interfaces for plugins like macvlan and ipvlan. Multus CNI : a scheduler for other CNI plugins. CNI plugins: include Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI , etc. SR-IOV related components: RDMA shared device plugin RDMA CNI SR-IOV network operator Use case: Pod with multiple underlay CNI interfaces In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI and SR-IOV CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding). Use case: Pod with one overlay interface and multiple underlay interfaces In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes. Use case: underlay CNI on public cloud and VM It is hard to implement underlay CNI in public cloud, OpenStack, VMware. It requires the vendor underlay CNI on specific environments, as these environments typically have the following limitations: The IAAS network infrastructure implements MAC restrictions for packets. On the one hand, security checks are conducted on the source MAC to ensure that the source MAC address is the same as the MAC address of VM network interface. On the other hand, restrictions have been placed on the destination MAC, which only supports packet forwarding by the MAC address of VM network interfaces. The MAC address of the Pod in the common CNI plugin is newly generated, which leads to Pod communication failure. The IAAS network infrastructure implements IP restrictions on packets. Only when the destination and source IP of the packet are assigned to VM, packet could be forwarded rightly. The common CNI plugin assigns IP addresses to Pods that do not comply with IAAS settings, which leads to Pod communication failure. Spiderpool provides IP pool based on node topology, aligning with IP allocation settings of VMs. In conjunction with ipvlan CNI, it provides underlay CNI solutions for various public cloud environments. Use case: utilize RDMA for network transmission RDMA (Remote Direct Memory Access) allows network cards to directly interact with memory, reducing CPU overhead and alleviating the burden on the kernel protocol stack. This technology offloads the network protocol stack to the network card, resulting in effective reduction of network transmission latency and increased throughput. Currently, RDMA finds extensive applications in fields such as AI computing and storage. Macvlan, IPvlan, and SR-IOV CNIs enable transparent RDMA network card passthrough to Pods within the Kubernetes platform. Spiderpool enhances these CNIs by providing additional capabilities including IPAM, host connectivity, clusterIP access, as well as simplifying the installation process and usage steps of dependent components in the community.","title":"Architecture"},{"location":"concepts/arch/#spiderpool-architecture","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Spiderpool Architecture"},{"location":"concepts/arch/#architecture","text":"Spiderpool consists of the following components: Spiderpool controller: a set of deployments that manage CRD validation, status updates, IP recovery, and automated IP pools Spiderpool agent: a set of daemonsets that help Spiderpool plugin by performing IP allocation and coordinator plugin for information synchronization. Spiderpool plugin: a binary plugin on each host that CNI can utilize to implement IP allocation. CNI plugins include: Spiderpool IPAM plugin: a main CNI used to handle IP allocation. coordinator plugin: as a chain plugin, it performs various functions such as routing coordination for multiple network interfaces, checking for IP conflicts, ensuring host connectivity, and fixing MAC addresses. ifacer plugin: as a chain plugin, it automates the creation of bond and VLAN virtual interfaces that serve as parent interfaces for plugins like macvlan and ipvlan. Multus CNI : a scheduler for other CNI plugins. CNI plugins: include Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Calico CNI , Weave CNI , Cilium CNI , etc. SR-IOV related components: RDMA shared device plugin RDMA CNI SR-IOV network operator","title":"Architecture"},{"location":"concepts/arch/#use-case-pod-with-multiple-underlay-cni-interfaces","text":"In underlay networks, Spiderpool can work with underlay CNIs such as Macvlan CNI and SR-IOV CNI to provide the following benefits: Rich IPAM capabilities for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support One or more underlay NICs for Pods with coordinating routes between multiple NICs to ensure smooth communication with consistent request and reply data paths Enhanced connectivity between open-source underlay CNIs and hosts using additional veth network interfaces and route control. This enables clusterIP access, local health checks of applications, and much more How can you deploy containers using a single underlay CNI, when a cluster has multiple underlying setups? Some nodes in the cluster are virtual machines like VMware that don't enable promiscuous mode, while others are bare metal and connected to traditional switch networks. What CNI solution should be deployed on each type of node? Some bare metal nodes only have one SR-IOV high-speed NIC that provides 64 VFs. How can more pods run on such a node? Some bare metal nodes have an SR-IOV high-speed NIC capable of running low-latency applications, while others have only ordinary network cards for running regular applications. What CNI solution should be deployed on each type of node? By simultaneously deploying multiple underlay CNIs through Multus CNI configuration and Spiderpool's IPAM abilities, resources from various infrastructure nodes across the cluster can be integrated to solve these problems. For example, as shown in the above diagram, different nodes with varying networking capabilities in a cluster can use various underlay CNIs, such as SR-IOV CNI for nodes with SR-IOV network cards, Macvlan CNI for nodes with ordinary network cards, and ipvlan CNI for nodes with restricted network access (e.g., VMware virtual machines with limited layer 2 network forwarding).","title":"Use case: Pod with multiple underlay CNI interfaces"},{"location":"concepts/arch/#use-case-pod-with-one-overlay-interface-and-multiple-underlay-interfaces","text":"In overlay networks, Spiderpool uses Multus to add an overlay NIC (such as Calico or Cilium ) and multiple underlay NICs (such as Macvlan CNI or SR-IOV CNI) for each Pod. This offers several benefits: Rich IPAM features for underlay CNIs, including shared/fixed IPs, multi-NIC IP allocation, and dual-stack support. Route coordination for multiple underlay CNI NICs and an overlay NIC for Pods, ensuring the consistent request and reply data paths for smooth communication. Use the overlay NIC as the default one with route coordination and enable local host connectivity to enable clusterIP access, local health checks of applications, and forwarding overlay network traffic through overlay networks while forwarding underlay network traffic through underlay networks. The integration of Multus CNI and Spiderpool IPAM enables the collaboration of an overlay CNI and multiple underlay CNIs. For example, in clusters with nodes of varying network capabilities, Pods on bare-metal nodes can access both overlay and underlay NICs. Meanwhile, Pods on virtual machine nodes only serving east-west services are connected to the Overlay NIC. This approach provides several benefits: Applications providing east-west services can be restricted to being allocated only the overlay NIC while those providing north-south services can simultaneously access overlay and underlay NICs. This results in reduced Underlay IP resource usage, lower manual maintenance costs, and preserved pod connectivity within the cluster. Fully integrate resources from virtual machines and bare-metal nodes.","title":"Use case: Pod with one overlay interface and multiple underlay interfaces"},{"location":"concepts/arch/#use-case-underlay-cni-on-public-cloud-and-vm","text":"It is hard to implement underlay CNI in public cloud, OpenStack, VMware. It requires the vendor underlay CNI on specific environments, as these environments typically have the following limitations: The IAAS network infrastructure implements MAC restrictions for packets. On the one hand, security checks are conducted on the source MAC to ensure that the source MAC address is the same as the MAC address of VM network interface. On the other hand, restrictions have been placed on the destination MAC, which only supports packet forwarding by the MAC address of VM network interfaces. The MAC address of the Pod in the common CNI plugin is newly generated, which leads to Pod communication failure. The IAAS network infrastructure implements IP restrictions on packets. Only when the destination and source IP of the packet are assigned to VM, packet could be forwarded rightly. The common CNI plugin assigns IP addresses to Pods that do not comply with IAAS settings, which leads to Pod communication failure. Spiderpool provides IP pool based on node topology, aligning with IP allocation settings of VMs. In conjunction with ipvlan CNI, it provides underlay CNI solutions for various public cloud environments.","title":"Use case: underlay CNI on public cloud and VM"},{"location":"concepts/arch/#use-case-utilize-rdma-for-network-transmission","text":"RDMA (Remote Direct Memory Access) allows network cards to directly interact with memory, reducing CPU overhead and alleviating the burden on the kernel protocol stack. This technology offloads the network protocol stack to the network card, resulting in effective reduction of network transmission latency and increased throughput. Currently, RDMA finds extensive applications in fields such as AI computing and storage. Macvlan, IPvlan, and SR-IOV CNIs enable transparent RDMA network card passthrough to Pods within the Kubernetes platform. Spiderpool enhances these CNIs by providing additional capabilities including IPAM, host connectivity, clusterIP access, as well as simplifying the installation process and usage steps of dependent components in the community.","title":"Use case: utilize RDMA for network transmission"},{"location":"concepts/blog-zh_CN/","text":"Blogs \u7b80\u4f53\u4e2d\u6587 | English Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 \u4e91\u539f\u751f Spiderpool\uff1a\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d SpiderPool\uff1aCalico \u56fa\u5b9a\u5e94\u7528 IP \u7684\u4e00\u79cd\u65b0\u9009\u62e9 \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6","title":"Blogs"},{"location":"concepts/blog-zh_CN/#blogs","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 \u4e91\u539f\u751f Spiderpool\uff1a\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d SpiderPool\uff1aCalico \u56fa\u5b9a\u5e94\u7528 IP \u7684\u4e00\u79cd\u65b0\u9009\u62e9 \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6","title":"Blogs"},{"location":"concepts/blog/","text":"Blogs English | \u7b80\u4f53\u4e2d\u6587 Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 Cloud-Native Spiderpool: IP Allocation Across Network Zones Spiderpool: a new solution to fixed application IPs for Calico \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6","title":"Blogs"},{"location":"concepts/blog/#blogs","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool v0.6.0\uff1a\u516c\u6709\u4e91\u573a\u666f\u4e0b\u7edf\u4e00\u7684\u4e91\u539f\u751f Underlay \u7f51\u7edc\u65b9\u6848 Spiderpool\uff1a\u5982\u4f55\u89e3\u51b3\u50f5\u5c38 IP \u56de\u6536\u7684\u95ee\u9898 Cloud-Native Spiderpool: IP Allocation Across Network Zones Spiderpool: a new solution to fixed application IPs for Calico \u4e91\u539f\u751f\u7f51\u7edc\u65b0\u73a9\u6cd5\uff1a\u4e00\u79cd\u652f\u6301\u56fa\u5b9a\u591a\u7f51\u5361IP\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848 SpiderPool - \u4e91\u539f\u751f\u5bb9\u5668\u7f51\u7edc IPAM \u63d2\u4ef6","title":"Blogs"},{"location":"concepts/coordinator-zh_CN/","text":"Coordinator English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u5185\u7f6e\u4e00\u4e2a\u53eb coordinator \u7684 CNI meta-plugin, \u5b83\u5728 Main CNI \u88ab\u8c03\u7528\u4e4b\u540e\u518d\u5de5\u4f5c\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u529f\u80fd: \u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u5728 Pod \u591a\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u8def\u7531\uff0c\u786e\u4fdd\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e00\u81f4 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u6ce8\u610f: \u5982\u679c\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u662f\u4f7f\u7528 NetworkManager \u7684 OS\uff0c\u6bd4\u5982 Fedora\u3001Centos\u7b49\uff0c\u5f3a\u70c8\u5efa\u8bae\u914d\u7f6e NetworkManager \u7684\u914d\u7f6e\u6587\u4ef6(/etc/NetworkManager/conf.d/spidernet.conf)\uff0c\u907f\u514d NetworkManager \u5e72\u6270 coordinator \u521b\u5efa\u7684 Veth \u865a\u62df\u63a5\u53e3\uff0c\u5f71\u54cd\u901a\u4fe1: ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecd coordinator \u5982\u4f55\u89e3\u51b3\u6216\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u3002 CNI \u914d\u7f6e\u5b57\u6bb5\u8bf4\u660e Field Description Schema Validation Default type CNI \u7684\u7c7b\u578b \u5b57\u7b26\u4e32 required coordinator mode coordinator \u8fd0\u884c\u7684\u6a21\u5f0f. \"auto\": coordinator \u81ea\u52a8\u5224\u65ad\u8fd0\u884c\u5728 Underlay \u6216\u8005 Overlay; \"underlay\": \u4e3a Pod \u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002\u7531 Pod \u7684 Underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"overlay\": \u4e0d\u989d\u5916\u521b\u5efa veth \u8bbe\u5907\uff0c\u8fd0\u884c\u5728\u591a\u7f51\u5361\u6a21\u5f0f\u3002\u7531 overlay \u7c7b\u578b\u7684 CNI(calico\uff0ccilium) \u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\uff0c\u7531 underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"disable\": \u7981\u7528 coordinator \u5b57\u7b26\u4e32 optional auto tunePodRoutes Pod \u591a\u7f51\u5361\u6a21\u5f0f\u4e0b\uff0c\u662f\u5426\u8c03\u534f Pod \u7684\u8def\u7531\uff0c\u89e3\u51b3\u8bbf\u95ee\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898 \u5e03\u5c14\u578b optional true podDefaultRouteNic Pod \u591a\u7f51\u5361\u65f6\uff0c\u914d\u7f6e Pod \u7684\u9ed8\u8ba4\u8def\u7531\u7f51\u5361\u3002\u9ed8\u8ba4\u4e3a \"\", \u5176 value \u5b9e\u9645\u4e3a Pod \u7b2c\u4e00\u5f20\u62e5\u6709\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361 \u5b57\u7b26\u4e32 optional \"\" podDefaultCniNic K8s \u4e2d Pod \u9ed8\u8ba4\u7684\u7b2c\u4e00\u5f20\u7f51\u5361 \u5e03\u5c14\u578b optional eth0 detectGateway \u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u5e03\u5c14\u578b optional false detectIPConflict \u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5 Pod \u7684 IP \u662f\u5426\u53ef\u8fbe \u5e03\u5c14\u578b optional false podMACPrefix \u662f\u5426\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u5b57\u7b26\u4e32 optional \"\" overlayPodCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Pod \u7684\u5b50\u7f51\uff0c\u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 serviceCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Service \u5b50\u7f51\uff0c \u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 hijackCIDR \u989d\u5916\u7684\u9700\u8981\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\u3002\u6bd4\u5982nodelocaldns \u7684\u5730\u5740: 169.254.20.10/32 []stirng optional \u7a7a hostRuleTable \u7b56\u7565\u8def\u7531\u8868\u53f7\uff0c\u540c\u4e3b\u673a\u4e0e Pod \u901a\u4fe1\u7684\u8def\u7531\u5c06\u4f1a\u5b58\u653e\u4e8e\u8fd9\u4e2a\u8868\u53f7 \u6574\u6570\u578b optional 500 hostRPFilter \u8bbe\u7f6e\u4e3b\u673a\u4e0a\u7684 sysctl \u53c2\u6570 rp_filter \u6574\u6570\u578b optional 0 detectOptions \u68c0\u6d4b\u5730\u5740\u51b2\u7a81\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u7684\u9ad8\u7ea7\u914d\u7f6e\u9879: \u5305\u62ec\u91cd\u8bd5\u6b21\u6570(\u9ed8\u8ba4\u4e3a 3 \u6b21), \u63a2\u6d4b\u95f4\u9694(\u9ed8\u8ba4\u4e3a 1s) \u548c \u8d85\u65f6\u65f6\u95f4(\u9ed8\u8ba4\u4e3a 1s) \u5bf9\u8c61\u7c7b\u578b optional \u7a7a logOptions \u65e5\u5fd7\u914d\u7f6e\uff0c\u5305\u62ec logLevel(\u9ed8\u8ba4\u4e3a debug) \u548c logFile(\u9ed8\u8ba4\u4e3a /var/log/spidernet/coordinator.log) \u5bf9\u8c61\u7c7b\u578b optional - \u5982\u679c\u60a8\u901a\u8fc7 SpinderMultusConfig CR \u5e2e\u52a9\u521b\u5efa NetworkAttachmentDefinition CR\uff0c\u60a8\u53ef\u4ee5\u5728 SpinderMultusConfig \u4e2d\u914d\u7f6e coordinator (\u6240\u6709\u5b57\u6bb5)\u3002\u53c2\u8003: SpinderMultusConfig \u3002 Spidercoordinators CR \u4f5c\u4e3a coordinator \u63d2\u4ef6\u7684\u5168\u5c40\u7f3a\u7701\u914d\u7f6e(\u6240\u6709\u5b57\u6bb5)\uff0c\u5176\u4f18\u5148\u7ea7\u4f4e\u4e8e NetworkAttachmentDefinition CR \u4e2d\u7684\u914d\u7f6e\u3002 \u5982\u679c\u5728 NetworkAttachmentDefinition CR \u672a\u914d\u7f6e, \u5c06\u4f7f\u7528 Spidercoordinator CR \u4f5c\u4e3a\u7f3a\u7701\u503c\u3002\u66f4\u591a\u8be6\u60c5\u53c2\u8003: Spidercoordinator \u3002 \u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u6211\u4eec\u5728\u4f7f\u7528\u4e00\u4e9b\u5982 Macvlan\u3001IPvlan\u3001SR-IOV \u7b49 Underlay CNI\u65f6\uff0c\u4f1a\u9047\u5230\u5176 Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8fd9\u5e38\u5e38\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee CLusterIP \u9700\u8981\u7ecf\u8fc7\u5728\u4ea4\u6362\u673a\u7684\u7f51\u5173\uff0c\u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 ClusterIP \u7684\u8def\u7531\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bbf\u95ee\u3002 \u5173\u4e8e Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8bf7\u53c2\u8003 Underlay-CNI\u8bbf\u95ee Service \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81( alpha \u9636\u6bb5) \u5bf9\u4e8e Underlay \u7f51\u7edc\uff0cIP \u51b2\u7a81\u662f\u65e0\u6cd5\u63a5\u53d7\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u95ee\u9898\u3002\u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff0c\u652f\u6301\u540c\u65f6\u68c0\u6d4b IPv4 \u548c IPv6 \u5730\u5740\u3002\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\uff0c \u5982\u679c\u53d1\u73b0\u56de\u590d\u62a5\u6587\u7684 Mac \u5730\u5740\u4e0d\u662f Pod \u672c\u8eab\uff0c\u90a3\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e2a IP \u662f\u51b2\u7a81\u7684\uff0c\u5e76\u62d2\u7edd IP \u51b2\u7a81\u7684 Pod \u88ab\u521b\u5efa: \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-ip namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] coordinator : detectIPConflict : true # Enable detectIPConflict \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe(alpha) \u5728 Underlay \u7f51\u7edc\u4e0b\uff0cPod \u8bbf\u95ee\u5916\u90e8\u9700\u8981\u901a\u8fc7\u7f51\u5173\u8f6c\u53d1\u3002\u5982\u679c\u7f51\u5173\u4e0d\u53ef\u8fbe\uff0c\u90a3\u4e48\u5728\u5916\u754c\u770b\u6765\uff0c\u8fd9\u4e2a Pod \u5b9e\u9645\u662f\u5931\u8054\u7684\u3002\u6709\u65f6\u5019\u6211\u4eec\u5e0c\u671b\u521b\u5efa Pod \u65f6\uff0c\u5176\u7f51\u5173\u662f\u53ef\u8fbe\u7684\u3002 \u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe\uff0c \u652f\u6301\u68c0\u6d4b IPv4 \u548c IPv6 \u7684\u7f51\u5173\u5730\u5740\u3002\u6211\u4eec\u901a\u8fc7\u53d1\u9001 ICMP \u62a5\u6587\uff0c\u63a2\u6d4b\u7f51\u5173\u5730\u5740\u662f\u5426\u53ef\u8fbe\u3002\u5982\u679c\u7f51\u5173\u4e0d\u53ef\u8fbe\uff0c\u5c06\u4f1a\u963b\u6b62 Pod \u521b\u5efa: \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-gateway namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : detectGateway : true # Enable detectGateway \u6ce8\u610f: \u6709\u4e00\u4e9b\u4ea4\u6362\u673a\u4e0d\u5141\u8bb8\u88ab arp \u63a2\u6d4b\uff0c\u5426\u5219\u4f1a\u53d1\u51fa\u544a\u8b66\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e detectGateway \u4e3a false \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00(alpha) \u6709\u4e00\u4e9b\u4f20\u7edf\u5e94\u7528\u53ef\u80fd\u9700\u8981\u901a\u8fc7\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216\u8005 IP \u5730\u5740\u6765\u8026\u5408\u5e94\u7528\u7684\u884c\u4e3a\u3002\u6bd4\u5982 License Server \u53ef\u80fd\u9700\u8981\u5e94\u7528\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216 IP \u5730\u5740\u4e3a\u5e94\u7528\u9881\u53d1 License\u3002\u5982\u679c Pod \u7684 Mac \u5730\u5740\u53d1\u751f\u6539\u53d8\uff0c\u5df2\u9881\u53d1\u7684 License \u53ef\u80fd\u65e0\u6548\u3002 \u6240\u4ee5\u9700\u8981\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u3002 Spiderpool \u53ef\u901a\u8fc7 coordinator \u56fa\u5b9a\u5e94\u7528\u7684 Mac \u5730\u5740\uff0c\u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002 \u6ce8\u610f: \u76ee\u524d\u652f\u6301\u4fee\u6539 Macvlan \u548c SR-IOV \u4f5c\u4e3a CNI \u7684 Pod\u3002 IPVlan L2 \u6a21\u5f0f\u4e0b\u4e3b\u63a5\u53e3\u4e0e\u5b50\u63a5\u53e3 Mac \u5730\u5740\u4e00\u81f4\uff0c\u4e0d\u652f\u6301\u4fee\u6539 \u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002\u4e00\u4e2a IPv4 \u5730\u5740\u957f\u5ea6 4 \u5b57\u8282\uff0c\u53ef\u4ee5\u5b8c\u5168\u8f6c\u6362\u4e3a2 \u4e2a 16 \u8fdb\u5236\u6570\u3002\u5bf9\u4e8e IPv6 \u5730\u5740\uff0c\u53ea\u53d6\u6700\u540e 4 \u4e2a\u5b57\u8282\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u6211\u4eec\u53ef\u4ee5\u68c0\u6d4b Pod \u7684 Mac \u5730\u5740\u7684\u524d\u7f00\u662f\u5426\u662f \"0a:1b\" \u5df2\u77e5\u95ee\u9898 underlay \u6a21\u5f0f\u4e0b\uff0cunderlay Pod \u4e0e Overlay Pod(calico or cilium) \u8fdb\u884c TCP \u901a\u4fe1\u5931\u8d25 \u6b64\u95ee\u9898\u662f\u56e0\u4e3a\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\uff0c\u53d1\u51fa\u7684\u8bf7\u6c42\u62a5\u6587\u5339\u914d\u6e90Pod \u4fa7\u7684\u8def\u7531\uff0c\u4f1a\u901a\u8fc7 veth0 \u8f6c\u53d1\u5230\u4e3b\u673a\u4fa7\uff0c\u518d\u7531\u4e3b\u673a\u4fa7\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u76ee\u6807 Pod \u770b\u89c1\u6570\u636e\u5305\u7684\u6e90 IP \u4e3a \u6e90 Pod \u7684 Underlay IP\uff0c\u76f4\u63a5\u8d70 Underlay \u7f51\u7edc\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u6e90 Pod \u6240\u5728\u4e3b\u673a\u3002 \u5728\u8be5\u4e3b\u673a\u770b\u6765\u8fd9\u662f\u4e00\u4e2a\u975e\u6cd5\u7684\u6570\u636e\u5305(\u610f\u5916\u7684\u6536\u5230 TCP \u7684\u7b2c\u4e8c\u6b21\u63e1\u624b\u62a5\u6587\uff0c\u8ba4\u4e3a\u662f conntrack table invalid), \u6240\u4ee5\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u663e\u5f0f\u7684 drop \u3002 \u76ee\u524d\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 kube-proxy \u7684\u6a21\u5f0f\u4e3a ipvs \u89c4\u907f\u3002\u8fd9\u4e2a\u95ee\u9898\u9884\u8ba1\u5728 k8s 1.29 \u4fee\u590d\u3002 \u5f53 sysctl nf_conntrack_tcp_be_liberal \u8bbe\u7f6e\u4e3a 1 \u65f6\uff0ckube-proxy \u5c06\u4e0d\u4f1a\u4e0b\u53d1\u8fd9\u6761 DROP \u89c4\u5219\u3002 overlay \u6a21\u5f0f\u4e0b, \u5f53 Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\u3002\u5982\u679c\u96c6\u7fa4\u7684\u7f3a\u7701CNI \u4e3a Cilium, Pod \u7684 underlay \u7f51\u5361 \u65e0\u6cd5\u4e0e\u8282\u70b9\u901a\u4fe1\u3002 \u6211\u4eec\u501f\u52a9\u7f3a\u7701CNI\u521b\u5efa Veth \u8bbe\u5907\uff0c\u5b9e\u73b0 Pod \u7684 underlay IP \u4e0e\u8282\u70b9\u901a\u4fe1(\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0cmacvlan \u5728 bridge \u6a21\u5f0f\u4e0b\uff0c \u5176\u7236\u5b50\u63a5\u53e3\u65e0\u6cd5\u76f4\u63a5)\uff0c\u4f46 Cilium \u4e0d\u5141\u8bb8\u975e Cilium \u5b50\u7f51\u7684 IP \u4ece Veth \u8bbe\u5907\u8f6c\u53d1\u3002","title":"Coordinator"},{"location":"concepts/coordinator-zh_CN/#coordinator","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u5185\u7f6e\u4e00\u4e2a\u53eb coordinator \u7684 CNI meta-plugin, \u5b83\u5728 Main CNI \u88ab\u8c03\u7528\u4e4b\u540e\u518d\u5de5\u4f5c\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4ee5\u4e0b\u51e0\u4e2a\u4e3b\u8981\u529f\u80fd: \u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u5728 Pod \u591a\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u8def\u7531\uff0c\u786e\u4fdd\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e00\u81f4 \u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81 \u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u6ce8\u610f: \u5982\u679c\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u662f\u4f7f\u7528 NetworkManager \u7684 OS\uff0c\u6bd4\u5982 Fedora\u3001Centos\u7b49\uff0c\u5f3a\u70c8\u5efa\u8bae\u914d\u7f6e NetworkManager \u7684\u914d\u7f6e\u6587\u4ef6(/etc/NetworkManager/conf.d/spidernet.conf)\uff0c\u907f\u514d NetworkManager \u5e72\u6270 coordinator \u521b\u5efa\u7684 Veth \u865a\u62df\u63a5\u53e3\uff0c\u5f71\u54cd\u901a\u4fe1: ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager \u4e0b\u9762\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecd coordinator \u5982\u4f55\u89e3\u51b3\u6216\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u3002","title":"Coordinator"},{"location":"concepts/coordinator-zh_CN/#cni","text":"Field Description Schema Validation Default type CNI \u7684\u7c7b\u578b \u5b57\u7b26\u4e32 required coordinator mode coordinator \u8fd0\u884c\u7684\u6a21\u5f0f. \"auto\": coordinator \u81ea\u52a8\u5224\u65ad\u8fd0\u884c\u5728 Underlay \u6216\u8005 Overlay; \"underlay\": \u4e3a Pod \u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002\u7531 Pod \u7684 Underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"overlay\": \u4e0d\u989d\u5916\u521b\u5efa veth \u8bbe\u5907\uff0c\u8fd0\u884c\u5728\u591a\u7f51\u5361\u6a21\u5f0f\u3002\u7531 overlay \u7c7b\u578b\u7684 CNI(calico\uff0ccilium) \u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\uff0c\u7531 underlay \u7f51\u5361\u8f6c\u53d1\u5357\u5317\u5411\u6d41\u91cf; \"disable\": \u7981\u7528 coordinator \u5b57\u7b26\u4e32 optional auto tunePodRoutes Pod \u591a\u7f51\u5361\u6a21\u5f0f\u4e0b\uff0c\u662f\u5426\u8c03\u534f Pod \u7684\u8def\u7531\uff0c\u89e3\u51b3\u8bbf\u95ee\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u7684\u95ee\u9898 \u5e03\u5c14\u578b optional true podDefaultRouteNic Pod \u591a\u7f51\u5361\u65f6\uff0c\u914d\u7f6e Pod \u7684\u9ed8\u8ba4\u8def\u7531\u7f51\u5361\u3002\u9ed8\u8ba4\u4e3a \"\", \u5176 value \u5b9e\u9645\u4e3a Pod \u7b2c\u4e00\u5f20\u62e5\u6709\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361 \u5b57\u7b26\u4e32 optional \"\" podDefaultCniNic K8s \u4e2d Pod \u9ed8\u8ba4\u7684\u7b2c\u4e00\u5f20\u7f51\u5361 \u5e03\u5c14\u578b optional eth0 detectGateway \u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5\u7f51\u5173\u662f\u5426\u53ef\u8fbe \u5e03\u5c14\u578b optional false detectIPConflict \u521b\u5efa Pod \u65f6\u662f\u5426\u68c0\u67e5 Pod \u7684 IP \u662f\u5426\u53ef\u8fbe \u5e03\u5c14\u578b optional false podMACPrefix \u662f\u5426\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00 \u5b57\u7b26\u4e32 optional \"\" overlayPodCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Pod \u7684\u5b50\u7f51\uff0c\u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 serviceCIDR \u9ed8\u8ba4\u7684\u96c6\u7fa4 Service \u5b50\u7f51\uff0c \u4f1a\u6ce8\u5165\u5230 Pod \u4e2d\u3002\u4e0d\u9700\u8981\u914d\u7f6e\uff0c\u81ea\u52a8\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 []stirng optional \u9ed8\u8ba4\u4ece Spidercoordinator default \u4e2d\u83b7\u53d6 hijackCIDR \u989d\u5916\u7684\u9700\u8981\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\u3002\u6bd4\u5982nodelocaldns \u7684\u5730\u5740: 169.254.20.10/32 []stirng optional \u7a7a hostRuleTable \u7b56\u7565\u8def\u7531\u8868\u53f7\uff0c\u540c\u4e3b\u673a\u4e0e Pod \u901a\u4fe1\u7684\u8def\u7531\u5c06\u4f1a\u5b58\u653e\u4e8e\u8fd9\u4e2a\u8868\u53f7 \u6574\u6570\u578b optional 500 hostRPFilter \u8bbe\u7f6e\u4e3b\u673a\u4e0a\u7684 sysctl \u53c2\u6570 rp_filter \u6574\u6570\u578b optional 0 detectOptions \u68c0\u6d4b\u5730\u5740\u51b2\u7a81\u548c\u7f51\u5173\u53ef\u8fbe\u6027\u7684\u9ad8\u7ea7\u914d\u7f6e\u9879: \u5305\u62ec\u91cd\u8bd5\u6b21\u6570(\u9ed8\u8ba4\u4e3a 3 \u6b21), \u63a2\u6d4b\u95f4\u9694(\u9ed8\u8ba4\u4e3a 1s) \u548c \u8d85\u65f6\u65f6\u95f4(\u9ed8\u8ba4\u4e3a 1s) \u5bf9\u8c61\u7c7b\u578b optional \u7a7a logOptions \u65e5\u5fd7\u914d\u7f6e\uff0c\u5305\u62ec logLevel(\u9ed8\u8ba4\u4e3a debug) \u548c logFile(\u9ed8\u8ba4\u4e3a /var/log/spidernet/coordinator.log) \u5bf9\u8c61\u7c7b\u578b optional - \u5982\u679c\u60a8\u901a\u8fc7 SpinderMultusConfig CR \u5e2e\u52a9\u521b\u5efa NetworkAttachmentDefinition CR\uff0c\u60a8\u53ef\u4ee5\u5728 SpinderMultusConfig \u4e2d\u914d\u7f6e coordinator (\u6240\u6709\u5b57\u6bb5)\u3002\u53c2\u8003: SpinderMultusConfig \u3002 Spidercoordinators CR \u4f5c\u4e3a coordinator \u63d2\u4ef6\u7684\u5168\u5c40\u7f3a\u7701\u914d\u7f6e(\u6240\u6709\u5b57\u6bb5)\uff0c\u5176\u4f18\u5148\u7ea7\u4f4e\u4e8e NetworkAttachmentDefinition CR \u4e2d\u7684\u914d\u7f6e\u3002 \u5982\u679c\u5728 NetworkAttachmentDefinition CR \u672a\u914d\u7f6e, \u5c06\u4f7f\u7528 Spidercoordinator CR \u4f5c\u4e3a\u7f3a\u7701\u503c\u3002\u66f4\u591a\u8be6\u60c5\u53c2\u8003: Spidercoordinator \u3002","title":"CNI \u914d\u7f6e\u5b57\u6bb5\u8bf4\u660e"},{"location":"concepts/coordinator-zh_CN/#underlay-pod-clusterip","text":"\u6211\u4eec\u5728\u4f7f\u7528\u4e00\u4e9b\u5982 Macvlan\u3001IPvlan\u3001SR-IOV \u7b49 Underlay CNI\u65f6\uff0c\u4f1a\u9047\u5230\u5176 Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8fd9\u5e38\u5e38\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee CLusterIP \u9700\u8981\u7ecf\u8fc7\u5728\u4ea4\u6362\u673a\u7684\u7f51\u5173\uff0c\u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 ClusterIP \u7684\u8def\u7531\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bbf\u95ee\u3002 \u5173\u4e8e Underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898\uff0c\u8bf7\u53c2\u8003 Underlay-CNI\u8bbf\u95ee Service","title":"\u89e3\u51b3 underlay Pod \u65e0\u6cd5\u8bbf\u95ee ClusterIP \u7684\u95ee\u9898"},{"location":"concepts/coordinator-zh_CN/#pod-ip-alpha","text":"\u5bf9\u4e8e Underlay \u7f51\u7edc\uff0cIP \u51b2\u7a81\u662f\u65e0\u6cd5\u63a5\u53d7\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u95ee\u9898\u3002\u5728\u521b\u5efa Pod \u65f6\uff0c\u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81\uff0c\u652f\u6301\u540c\u65f6\u68c0\u6d4b IPv4 \u548c IPv6 \u5730\u5740\u3002\u901a\u8fc7\u53d1\u9001 ARP \u6216 NDP \u63a2\u6d4b\u62a5\u6587\uff0c \u5982\u679c\u53d1\u73b0\u56de\u590d\u62a5\u6587\u7684 Mac \u5730\u5740\u4e0d\u662f Pod \u672c\u8eab\uff0c\u90a3\u6211\u4eec\u8ba4\u4e3a\u8fd9\u4e2a IP \u662f\u51b2\u7a81\u7684\uff0c\u5e76\u62d2\u7edd IP \u51b2\u7a81\u7684 Pod \u88ab\u521b\u5efa: \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-ip namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] coordinator : detectIPConflict : true # Enable detectIPConflict","title":"\u652f\u6301\u68c0\u6d4b Pod \u7684 IP \u662f\u5426\u51b2\u7a81( alpha \u9636\u6bb5)"},{"location":"concepts/coordinator-zh_CN/#pod-alpha","text":"\u5728 Underlay \u7f51\u7edc\u4e0b\uff0cPod \u8bbf\u95ee\u5916\u90e8\u9700\u8981\u901a\u8fc7\u7f51\u5173\u8f6c\u53d1\u3002\u5982\u679c\u7f51\u5173\u4e0d\u53ef\u8fbe\uff0c\u90a3\u4e48\u5728\u5916\u754c\u770b\u6765\uff0c\u8fd9\u4e2a Pod \u5b9e\u9645\u662f\u5931\u8054\u7684\u3002\u6709\u65f6\u5019\u6211\u4eec\u5e0c\u671b\u521b\u5efa Pod \u65f6\uff0c\u5176\u7f51\u5173\u662f\u53ef\u8fbe\u7684\u3002 \u6211\u4eec\u53ef\u501f\u52a9 coordinator \u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe\uff0c \u652f\u6301\u68c0\u6d4b IPv4 \u548c IPv6 \u7684\u7f51\u5173\u5730\u5740\u3002\u6211\u4eec\u901a\u8fc7\u53d1\u9001 ICMP \u62a5\u6587\uff0c\u63a2\u6d4b\u7f51\u5173\u5730\u5740\u662f\u5426\u53ef\u8fbe\u3002\u5982\u679c\u7f51\u5173\u4e0d\u53ef\u8fbe\uff0c\u5c06\u4f1a\u963b\u6b62 Pod \u521b\u5efa: \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-gateway namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : detectGateway : true # Enable detectGateway \u6ce8\u610f: \u6709\u4e00\u4e9b\u4ea4\u6362\u673a\u4e0d\u5141\u8bb8\u88ab arp \u63a2\u6d4b\uff0c\u5426\u5219\u4f1a\u53d1\u51fa\u544a\u8b66\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e detectGateway \u4e3a false","title":"\u652f\u6301\u68c0\u6d4b Pod \u7684\u7f51\u5173\u662f\u5426\u53ef\u8fbe(alpha)"},{"location":"concepts/coordinator-zh_CN/#pod-mac-alpha","text":"\u6709\u4e00\u4e9b\u4f20\u7edf\u5e94\u7528\u53ef\u80fd\u9700\u8981\u901a\u8fc7\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216\u8005 IP \u5730\u5740\u6765\u8026\u5408\u5e94\u7528\u7684\u884c\u4e3a\u3002\u6bd4\u5982 License Server \u53ef\u80fd\u9700\u8981\u5e94\u7528\u56fa\u5b9a\u7684 Mac \u5730\u5740\u6216 IP \u5730\u5740\u4e3a\u5e94\u7528\u9881\u53d1 License\u3002\u5982\u679c Pod \u7684 Mac \u5730\u5740\u53d1\u751f\u6539\u53d8\uff0c\u5df2\u9881\u53d1\u7684 License \u53ef\u80fd\u65e0\u6548\u3002 \u6240\u4ee5\u9700\u8981\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u3002 Spiderpool \u53ef\u901a\u8fc7 coordinator \u56fa\u5b9a\u5e94\u7528\u7684 Mac \u5730\u5740\uff0c\u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002 \u6ce8\u610f: \u76ee\u524d\u652f\u6301\u4fee\u6539 Macvlan \u548c SR-IOV \u4f5c\u4e3a CNI \u7684 Pod\u3002 IPVlan L2 \u6a21\u5f0f\u4e0b\u4e3b\u63a5\u53e3\u4e0e\u5b50\u63a5\u53e3 Mac \u5730\u5740\u4e00\u81f4\uff0c\u4e0d\u652f\u6301\u4fee\u6539 \u56fa\u5b9a\u7684\u89c4\u5219\u662f\u914d\u7f6e Mac \u5730\u5740\u524d\u7f00(2\u5b57\u8282) + \u8f6c\u5316 Pod \u7684 IP(4\u5b57\u8282) \u7ec4\u6210\u3002\u4e00\u4e2a IPv4 \u5730\u5740\u957f\u5ea6 4 \u5b57\u8282\uff0c\u53ef\u4ee5\u5b8c\u5168\u8f6c\u6362\u4e3a2 \u4e2a 16 \u8fdb\u5236\u6570\u3002\u5bf9\u4e8e IPv6 \u5730\u5740\uff0c\u53ea\u53d6\u6700\u540e 4 \u4e2a\u5b57\u8282\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 Spidermultusconfig \u914d\u7f6e\u5b83: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway \u5f53 Pod \u521b\u5efa\u5b8c\u6210\uff0c\u6211\u4eec\u53ef\u4ee5\u68c0\u6d4b Pod \u7684 Mac \u5730\u5740\u7684\u524d\u7f00\u662f\u5426\u662f \"0a:1b\"","title":"\u652f\u6301\u56fa\u5b9a Pod \u7684 Mac \u5730\u5740\u524d\u7f00(alpha)"},{"location":"concepts/coordinator-zh_CN/#_1","text":"underlay \u6a21\u5f0f\u4e0b\uff0cunderlay Pod \u4e0e Overlay Pod(calico or cilium) \u8fdb\u884c TCP \u901a\u4fe1\u5931\u8d25 \u6b64\u95ee\u9898\u662f\u56e0\u4e3a\u6570\u636e\u5305\u6765\u56de\u8def\u5f84\u4e0d\u4e00\u81f4\u5bfc\u81f4\uff0c\u53d1\u51fa\u7684\u8bf7\u6c42\u62a5\u6587\u5339\u914d\u6e90Pod \u4fa7\u7684\u8def\u7531\uff0c\u4f1a\u901a\u8fc7 veth0 \u8f6c\u53d1\u5230\u4e3b\u673a\u4fa7\uff0c\u518d\u7531\u4e3b\u673a\u4fa7\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u76ee\u6807 Pod \u770b\u89c1\u6570\u636e\u5305\u7684\u6e90 IP \u4e3a \u6e90 Pod \u7684 Underlay IP\uff0c\u76f4\u63a5\u8d70 Underlay \u7f51\u7edc\u800c\u4e0d\u4f1a\u7ecf\u8fc7\u6e90 Pod \u6240\u5728\u4e3b\u673a\u3002 \u5728\u8be5\u4e3b\u673a\u770b\u6765\u8fd9\u662f\u4e00\u4e2a\u975e\u6cd5\u7684\u6570\u636e\u5305(\u610f\u5916\u7684\u6536\u5230 TCP \u7684\u7b2c\u4e8c\u6b21\u63e1\u624b\u62a5\u6587\uff0c\u8ba4\u4e3a\u662f conntrack table invalid), \u6240\u4ee5\u88ab kube-proxy \u7684\u4e00\u6761 iptables \u89c4\u5219\u663e\u5f0f\u7684 drop \u3002 \u76ee\u524d\u53ef\u4ee5\u901a\u8fc7\u5207\u6362 kube-proxy \u7684\u6a21\u5f0f\u4e3a ipvs \u89c4\u907f\u3002\u8fd9\u4e2a\u95ee\u9898\u9884\u8ba1\u5728 k8s 1.29 \u4fee\u590d\u3002 \u5f53 sysctl nf_conntrack_tcp_be_liberal \u8bbe\u7f6e\u4e3a 1 \u65f6\uff0ckube-proxy \u5c06\u4e0d\u4f1a\u4e0b\u53d1\u8fd9\u6761 DROP \u89c4\u5219\u3002 overlay \u6a21\u5f0f\u4e0b, \u5f53 Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\u65f6\u3002\u5982\u679c\u96c6\u7fa4\u7684\u7f3a\u7701CNI \u4e3a Cilium, Pod \u7684 underlay \u7f51\u5361 \u65e0\u6cd5\u4e0e\u8282\u70b9\u901a\u4fe1\u3002 \u6211\u4eec\u501f\u52a9\u7f3a\u7701CNI\u521b\u5efa Veth \u8bbe\u5907\uff0c\u5b9e\u73b0 Pod \u7684 underlay IP \u4e0e\u8282\u70b9\u901a\u4fe1(\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0cmacvlan \u5728 bridge \u6a21\u5f0f\u4e0b\uff0c \u5176\u7236\u5b50\u63a5\u53e3\u65e0\u6cd5\u76f4\u63a5)\uff0c\u4f46 Cilium \u4e0d\u5141\u8bb8\u975e Cilium \u5b50\u7f51\u7684 IP \u4ece Veth \u8bbe\u5907\u8f6c\u53d1\u3002","title":"\u5df2\u77e5\u95ee\u9898"},{"location":"concepts/coordinator/","text":"Coordinator English | \u7b80\u4f53\u4e2d\u6587 Spiderpool incorporates a CNI meta-plugin called coordinator that works after the Main CNI is invoked. It mainly offers the following features: Resolve the problem of underlay Pods unable to access ClusterIP Coordinate the routing for Pods with multiple NICs, ensuring consistent packet paths Detect IP conflicts within Pods Check the reachability of Pod gateways Support fixed Mac address prefixes for Pods Note: If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with veth interfaces created through coordinator : ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager Let's delve into how coordinator implements these features. CNI fields description Field Description Schema Validation Default type The name of this Spidercoordinators resource string required coordinator mode the mode in which the coordinator run. \"auto\": Automatically determine if it's overlay or underlay; \"underlay\": All NICs for pods are underlay NICs, and in this case the coordinator will create veth-pairs device to solve the problem of underlay pods accessing services; \"overlay\": The coordinator does not create veth-pair devices, but the first NIC of the pod cannot be an underlay NIC, which is created by overlay CNI (e.g. calico, cilium). Solve the problem of pod access to service through the first NIC; \"disable\": The coordinator does nothing and exits directly string optional auto tunePodRoutes Tune the pod's routing tables while a pod is in multi-NIC mode bool optional true podDefaultRouteNic Configure the default routed NIC for the pod while a pod is in multi-NIC mode, The default value is 0, indicate that the first network interface of the pod has the default route. string optional \"\" podDefaultCniNic The name of the pod's first NIC defaults to eth0 in kubernetes bool optional eth0 detectGateway Enable gateway detection while creating pods, which prevent pod creation if the gateway is unreachable bool optional false detectIPConflict Enable IP conflicting checking for pods, which prevent pod creation if the pod's ip is conflicting bool optional false podMACPrefix Enable fixing MAC address prefixes for pods. empty value is mean to disable string optional \"\" overlayPodCIDR The default cluster CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} serviceCIDR The default service CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} hijackCIDR The CIDR that need to be forwarded via the host network, For example, the address of nodelocaldns(169.254.20.10/32 by default) []stirng optional []string{} hostRuleTable The routes on the host that communicates with the pod's underlay IPs will belong to this routing table number int optional 500 hostRPFilter Set the rp_filter sysctl parameter on the host, which is recommended to be set to 0 int optional 0 detectOptions The advanced configuration of detectGateway and detectIPConflict, including retry numbers(default is 3), interval(default is 1s) and timeout(default is 1s) obejct optional nil logOptions The configuration of logging, including logLevel(default is debug) and logFile(default is /var/log/spidernet/coordinator.log) obejct optional nil You can configure coordinator by specifying all the relevant fields in SpinderMultusConfig if a NetworkAttachmentDefinition CR is created via SpinderMultusConfig CR . For more information, please refer to SpinderMultusConfig . Spidercoordinators CR serves as the global default configuration (all fields) for coordinator . However, this configuration has a lower priority compared to the settings in the NetworkAttachmentDefinition CR. In cases where no configuration is provided in the NetworkAttachmentDefinition CR, the values from Spidercoordinators CR serve as the defaults. For detailed information, please refer to Spidercoordinator . Resolve the problem of underlay Pods unable to access ClusterIP(beta) When using underlay CNIs like Macvlan, IPvlan, SR-IOV, and others, a common challenge arises where underlay pods are unable to access ClusterIP. This occurs because accessing ClusterIP from underlay pods requires routing through the gateway on the switch. However, in many instances, the gateway is not configured with the proper routes to reach the ClusterIP, leading to restricted access. For more information about the Underlay Pod not being able to access the ClusterIP, please refer to Underlay CNI Access Service Detect Pod IP conflicts(alpha) IP conflicts are unacceptable for underlay networks, which can cause serious problems. When creating a pod, we can use the coordinator to detect whether the IP of the pod conflicts, and support both IPv4 and IPv6 addresses. By sending an ARP or NDP probe message, If the MAC address of the reply packet is not the pod itself, we consider the IP to be conflicting and reject the creation of the pod with conflicting IP addresses: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-ip namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] coordinator : detectIPConflict : true # Enable detectIPConflict Detect Pod gateway reachability(alpha) Under the underlay network, pod access to the outside needs to be forwarded through the gateway. If the gateway is unreachable, then the pod is actually lost. Sometimes we want to create a pod with a gateway reachable. We can use the 'coordinator' to check if the pod's gateway is reachable. Gateway addresses for IPv4 and IPv6 can be detected. We send an ICMP packet to check whether the gateway address is reachable. If the gateway is unreachable, pods will be prevented from creating: We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-gateway namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : detectGateway : true # Enable detectGateway Note: There are some switches that are not allowed to be probed by arp, otherwise an alarm will be issued, in this case, we need to set detectGateway to false Fix MAC address prefix for Pods(alpha) Some traditional applications may require a fixed MAC address or IP address to couple the behavior of the application. For example, the License Server may need to apply a fixed Mac address or IP address to issue a license for the app. If the MAC address of a pod changes, the issued license may be invalid. Therefore, you need to fix the MAC address of the pod. Spiderpool can fix the MAC address of the application through coordinator , and the fixed rule is to configure the MAC address prefix (2 bytes) + convert the IP of the pod (4 bytes). Note: currently supports updating Macvlan and SR-IOV as pods for CNI. In IPVlan L2 mode, the MAC addresses of the primary interface and the sub-interface are the same and cannot be modified. The fixed rule is to configure the MAC address prefix (2 bytes) + the IP of the converted pod (4 bytes). An IPv4 address is 4 bytes long and can be fully converted to 2 hexadecimal numbers. For IPv6 addresses, only the last 4 bytes are taken. We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway You can check if the MAC address prefix of the Pod starts with \"0a:1b\" after a Pod is created. Known issues Underlay mode: TCP communication between underlay Pods and overlay Pods (Calico or Cilium) fails This issue arises from inconsistent packet routing paths. Request packets are matched with the routing on the source Pod side and forwarded through veth0 to the host side. And then the packets are further forwarded to the target Pod. The target Pod perceives the source IP of the packet as the underlay IP of the source Pod, allowing it to bypass the source Pod's host and directly route through the underlay network. However, on the host, this is considered an invalid packet (as it receives unexpected TCP SYN-ACK packets that are conntrack table invalid), explicitly dropping it using an iptables rule in kube-proxy. Switching the kube-proxy mode to ipvs can address this issue. This issue is expected to be fixed in K8s 1.29. if the sysctl nf_conntrack_tcp_be_liberal is set to 1, kube-proxy will not deliver the DROP rule. Overlay mode: with Cilium as the default CNI and multiple NICs for the Pod, the underlay interface of the Pod cannot communicate with the node. Macvlan interfaces do not allow direct communication between parent and child interfaces in bridge mode in most cases. To facilitate communication between the underlay IP of the Pod and the node, we rely on the default CNI to create Veth devices. However, Cilium restricts the forwarding of IPs from non-Cilium subnets through these Veth devices.","title":"Plugin coordinator"},{"location":"concepts/coordinator/#coordinator","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool incorporates a CNI meta-plugin called coordinator that works after the Main CNI is invoked. It mainly offers the following features: Resolve the problem of underlay Pods unable to access ClusterIP Coordinate the routing for Pods with multiple NICs, ensuring consistent packet paths Detect IP conflicts within Pods Check the reachability of Pod gateways Support fixed Mac address prefixes for Pods Note: If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with veth interfaces created through coordinator : ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager Let's delve into how coordinator implements these features.","title":"Coordinator"},{"location":"concepts/coordinator/#cni-fields-description","text":"Field Description Schema Validation Default type The name of this Spidercoordinators resource string required coordinator mode the mode in which the coordinator run. \"auto\": Automatically determine if it's overlay or underlay; \"underlay\": All NICs for pods are underlay NICs, and in this case the coordinator will create veth-pairs device to solve the problem of underlay pods accessing services; \"overlay\": The coordinator does not create veth-pair devices, but the first NIC of the pod cannot be an underlay NIC, which is created by overlay CNI (e.g. calico, cilium). Solve the problem of pod access to service through the first NIC; \"disable\": The coordinator does nothing and exits directly string optional auto tunePodRoutes Tune the pod's routing tables while a pod is in multi-NIC mode bool optional true podDefaultRouteNic Configure the default routed NIC for the pod while a pod is in multi-NIC mode, The default value is 0, indicate that the first network interface of the pod has the default route. string optional \"\" podDefaultCniNic The name of the pod's first NIC defaults to eth0 in kubernetes bool optional eth0 detectGateway Enable gateway detection while creating pods, which prevent pod creation if the gateway is unreachable bool optional false detectIPConflict Enable IP conflicting checking for pods, which prevent pod creation if the pod's ip is conflicting bool optional false podMACPrefix Enable fixing MAC address prefixes for pods. empty value is mean to disable string optional \"\" overlayPodCIDR The default cluster CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} serviceCIDR The default service CIDR for the cluster. It doesn't need to be configured, and it collected automatically by SpiderCoordinator []stirng optional []string{} hijackCIDR The CIDR that need to be forwarded via the host network, For example, the address of nodelocaldns(169.254.20.10/32 by default) []stirng optional []string{} hostRuleTable The routes on the host that communicates with the pod's underlay IPs will belong to this routing table number int optional 500 hostRPFilter Set the rp_filter sysctl parameter on the host, which is recommended to be set to 0 int optional 0 detectOptions The advanced configuration of detectGateway and detectIPConflict, including retry numbers(default is 3), interval(default is 1s) and timeout(default is 1s) obejct optional nil logOptions The configuration of logging, including logLevel(default is debug) and logFile(default is /var/log/spidernet/coordinator.log) obejct optional nil You can configure coordinator by specifying all the relevant fields in SpinderMultusConfig if a NetworkAttachmentDefinition CR is created via SpinderMultusConfig CR . For more information, please refer to SpinderMultusConfig . Spidercoordinators CR serves as the global default configuration (all fields) for coordinator . However, this configuration has a lower priority compared to the settings in the NetworkAttachmentDefinition CR. In cases where no configuration is provided in the NetworkAttachmentDefinition CR, the values from Spidercoordinators CR serve as the defaults. For detailed information, please refer to Spidercoordinator .","title":"CNI fields description"},{"location":"concepts/coordinator/#resolve-the-problem-of-underlay-pods-unable-to-access-clusteripbeta","text":"When using underlay CNIs like Macvlan, IPvlan, SR-IOV, and others, a common challenge arises where underlay pods are unable to access ClusterIP. This occurs because accessing ClusterIP from underlay pods requires routing through the gateway on the switch. However, in many instances, the gateway is not configured with the proper routes to reach the ClusterIP, leading to restricted access. For more information about the Underlay Pod not being able to access the ClusterIP, please refer to Underlay CNI Access Service","title":"Resolve the problem of underlay Pods unable to access ClusterIP(beta)"},{"location":"concepts/coordinator/#detect-pod-ip-conflictsalpha","text":"IP conflicts are unacceptable for underlay networks, which can cause serious problems. When creating a pod, we can use the coordinator to detect whether the IP of the pod conflicts, and support both IPv4 and IPv6 addresses. By sending an ARP or NDP probe message, If the MAC address of the reply packet is not the pod itself, we consider the IP to be conflicting and reject the creation of the pod with conflicting IP addresses: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-ip namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] coordinator : detectIPConflict : true # Enable detectIPConflict","title":"Detect Pod IP conflicts(alpha)"},{"location":"concepts/coordinator/#detect-pod-gateway-reachabilityalpha","text":"Under the underlay network, pod access to the outside needs to be forwarded through the gateway. If the gateway is unreachable, then the pod is actually lost. Sometimes we want to create a pod with a gateway reachable. We can use the 'coordinator' to check if the pod's gateway is reachable. Gateway addresses for IPv4 and IPv6 can be detected. We send an ICMP packet to check whether the gateway address is reachable. If the gateway is unreachable, pods will be prevented from creating: We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : detect-gateway namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : detectGateway : true # Enable detectGateway Note: There are some switches that are not allowed to be probed by arp, otherwise an alarm will be issued, in this case, we need to set detectGateway to false","title":"Detect Pod gateway reachability(alpha)"},{"location":"concepts/coordinator/#fix-mac-address-prefix-for-podsalpha","text":"Some traditional applications may require a fixed MAC address or IP address to couple the behavior of the application. For example, the License Server may need to apply a fixed Mac address or IP address to issue a license for the app. If the MAC address of a pod changes, the issued license may be invalid. Therefore, you need to fix the MAC address of the pod. Spiderpool can fix the MAC address of the application through coordinator , and the fixed rule is to configure the MAC address prefix (2 bytes) + convert the IP of the pod (4 bytes). Note: currently supports updating Macvlan and SR-IOV as pods for CNI. In IPVlan L2 mode, the MAC addresses of the primary interface and the sub-interface are the same and cannot be modified. The fixed rule is to configure the MAC address prefix (2 bytes) + the IP of the converted pod (4 bytes). An IPv4 address is 4 bytes long and can be fully converted to 2 hexadecimal numbers. For IPv6 addresses, only the last 4 bytes are taken. We can configure it via Spidermultusconfig: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : overwrite-mac namespace : default spec : cniType : macvlan macvlan : master : [ \"eth0\" ] enableCoordinator : true coordinator : podMACPrefix : \"0a:1b\" # Enable detectGateway You can check if the MAC address prefix of the Pod starts with \"0a:1b\" after a Pod is created.","title":"Fix MAC address prefix for Pods(alpha)"},{"location":"concepts/coordinator/#known-issues","text":"Underlay mode: TCP communication between underlay Pods and overlay Pods (Calico or Cilium) fails This issue arises from inconsistent packet routing paths. Request packets are matched with the routing on the source Pod side and forwarded through veth0 to the host side. And then the packets are further forwarded to the target Pod. The target Pod perceives the source IP of the packet as the underlay IP of the source Pod, allowing it to bypass the source Pod's host and directly route through the underlay network. However, on the host, this is considered an invalid packet (as it receives unexpected TCP SYN-ACK packets that are conntrack table invalid), explicitly dropping it using an iptables rule in kube-proxy. Switching the kube-proxy mode to ipvs can address this issue. This issue is expected to be fixed in K8s 1.29. if the sysctl nf_conntrack_tcp_be_liberal is set to 1, kube-proxy will not deliver the DROP rule. Overlay mode: with Cilium as the default CNI and multiple NICs for the Pod, the underlay interface of the Pod cannot communicate with the node. Macvlan interfaces do not allow direct communication between parent and child interfaces in bridge mode in most cases. To facilitate communication between the underlay IP of the Pod and the node, we rely on the default CNI to create Veth devices. However, Cilium restricts the forwarding of IPs from non-Cilium subnets through these Veth devices.","title":"Known issues"},{"location":"concepts/io-performance-zh_CN/","text":"\u7f51\u7edc IO \u6027\u80fd \u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u642d\u914d Macvlan\u3001SR-IOV\u3001IPvlan \u53ef\u4ee5\u5b9e\u73b0\u4e00\u5957\u5b8c\u6574\u7684\u7f51\u7edc\u65b9\u6848\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684\u7f51\u7edc CNI \u63d2\u4ef6\uff08\u5982 cilium \uff0c calico \uff09 \u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u7f51\u7edc \u5ef6\u65f6 \u548c \u541e\u5410\u91cf \u73af\u5883 \u672c\u6b21\u6d4b\u8bd5\u5305\u542b\u5404\u79cd\u573a\u666f\u7684\u6027\u80fd\u57fa\u51c6\u6570\u636e\u3002\u6240\u6709\u6d4b\u8bd5\u5747\u901a\u8fc7\u5728 10 Gbit/s \u7f51\u7edc\u63a5\u53e3\u7684\u4e24\u4e2a\u4e0d\u540c\u88f8\u673a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u4e4b\u95f4\u6267\u884c\u3002 Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi \u6d4b\u8bd5\u5bf9\u8c61 \u672c\u6b21\u6d4b\u8bd5\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684 Calico \u3001 Cilium \u4e24\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u5982\u4e0b\u662f\u76f8\u5173\u7684\u7248\u672c\u7b49\u4fe1\u606f\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u8bf4\u660e Spiderpool based macvlan datapath Spiderpool \u7248\u672c v0.8.0 Calico Calico \u7248\u672c v3.26.1\uff0c\u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053 Cilium Cilium \u7248\u672c v1.14.3\uff0c\u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053 sockperf \u7f51\u7edc\u5ef6\u65f6\u6d4b\u8bd5 Sockperf \u662f\u4e00\u4e2a\u7f51\u7edc\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cf\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5b83\u5141\u8bb8\u60a8\u901a\u8fc7\u6d4b\u8bd5\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u5ef6\u8fdf\u6765\u8bc4\u4f30\u7f51\u7edc\u7684\u6027\u80fd\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b83\u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.3 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 29.1 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 24.3 usec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 26.2 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.9 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 30.2 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 36.8 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 27.7 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec netperf \u6027\u80fd\u6d4b\u8bd5 netperf \u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u7f51\u7edc\u6027\u80fd\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4f8b\u5982\u541e\u5410\u91cf\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 netperf \u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service \u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9985.7 \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17571.3 \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 19793.9 \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 19215.2 \u8282\u70b9\u5230\u8282\u70b9 47560.5 \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9782.2 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17236.5 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 16002.3 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 18992.9 rps \u8282\u70b9\u5230\u8282\u70b9 47560.5 rps iperf \u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5 iperf \u662f\u4e00\u79cd\u6d41\u884c\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u7f51\u7edc\u5e26\u5bbd\u3002\u5b83\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u8fde\u63a5\u7684\u5e26\u5bbd\u548c\u6027\u80fd\u3002\u5728\u672c\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5b83\u5206\u522b\u6765\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <Pod IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <cluster IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec redis-benchmark \u6027\u80fd\u6d4b\u8bd5 redis-benchmark \u65e8\u5728\u901a\u8fc7\u6a21\u62df\u591a\u4e2a\u5ba2\u6237\u7aef\u5e76\u6267\u884c\u5404\u79cd Redis \u547d\u4ee4\u6765\u6d4b\u91cf Redis \u670d\u52a1\u5668\u7684\u6027\u80fd\u548c\u541e\u5410\u91cf\u3002\u6211\u4eec\u901a\u8fc7 redis-benchmark \u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee\u90e8\u7f72\u4e86 Redis \u670d\u52a1\u7684 Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 45682.96 rps 46992.48 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 59737.16 rps 59988.00 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 66357.00 rps 66800.27 rps \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 67444.45 rps 67783.67 rps \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 46082.95 rps 46728.97 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 60496.07 rps 58927.52 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 45578.85 rps 46274.87 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 63211.12 rps 64061.50 rps \u603b\u7ed3 Spiderpool \u505a\u4e3a Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u5176 IO \u6027\u80fd\u5728\u5927\u90e8\u5206\u573a\u666f\u4e0b\u90fd\u9886\u5148\u4e8e Calico\u3001Cilium\u3002","title":"\u7f51\u7edc IO \u6027\u80fd"},{"location":"concepts/io-performance-zh_CN/#io","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u642d\u914d Macvlan\u3001SR-IOV\u3001IPvlan \u53ef\u4ee5\u5b9e\u73b0\u4e00\u5957\u5b8c\u6574\u7684\u7f51\u7edc\u65b9\u6848\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684\u7f51\u7edc CNI \u63d2\u4ef6\uff08\u5982 cilium \uff0c calico \uff09 \u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u7f51\u7edc \u5ef6\u65f6 \u548c \u541e\u5410\u91cf","title":"\u7f51\u7edc IO \u6027\u80fd"},{"location":"concepts/io-performance-zh_CN/#_1","text":"\u672c\u6b21\u6d4b\u8bd5\u5305\u542b\u5404\u79cd\u573a\u666f\u7684\u6027\u80fd\u57fa\u51c6\u6570\u636e\u3002\u6240\u6709\u6d4b\u8bd5\u5747\u901a\u8fc7\u5728 10 Gbit/s \u7f51\u7edc\u63a5\u53e3\u7684\u4e24\u4e2a\u4e0d\u540c\u88f8\u673a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u4e4b\u95f4\u6267\u884c\u3002 Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi","title":"\u73af\u5883"},{"location":"concepts/io-performance-zh_CN/#_2","text":"\u672c\u6b21\u6d4b\u8bd5\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u7684 Calico \u3001 Cilium \u4e24\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u5982\u4e0b\u662f\u76f8\u5173\u7684\u7248\u672c\u7b49\u4fe1\u606f\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u8bf4\u660e Spiderpool based macvlan datapath Spiderpool \u7248\u672c v0.8.0 Calico Calico \u7248\u672c v3.26.1\uff0c\u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053 Cilium Cilium \u7248\u672c v1.14.3\uff0c\u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053","title":"\u6d4b\u8bd5\u5bf9\u8c61"},{"location":"concepts/io-performance-zh_CN/#sockperf","text":"Sockperf \u662f\u4e00\u4e2a\u7f51\u7edc\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cf\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5b83\u5141\u8bb8\u60a8\u901a\u8fc7\u6d4b\u8bd5\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u5ef6\u8fdf\u6765\u8bc4\u4f30\u7f51\u7edc\u7684\u6027\u80fd\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5b83\u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.3 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 29.1 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 24.3 usec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 26.2 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u5ef6\u65f6\u6d4b\u8bd5\u3002 \u901a\u8fc7 sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u5ef6\u65f6\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u5ef6\u65f6 \u57fa\u4e8e iptables datapath \u548c\u65e0\u96a7\u9053\u7684 Calico 51.9 usec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f\u548c\u65e0\u96a7\u9053\u7684 cilium 30.2 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 36.8 usec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 27.7 usec \u8282\u70b9\u5230\u8282\u70b9 32.2 usec","title":"sockperf \u7f51\u7edc\u5ef6\u65f6\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#netperf","text":"netperf \u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u7f51\u7edc\u6027\u80fd\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4f8b\u5982\u541e\u5410\u91cf\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 netperf \u6765\u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service \u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9985.7 \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17571.3 \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 19793.9 \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 19215.2 \u8282\u70b9\u5230\u8282\u70b9 47560.5 \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 netperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u541e\u5410\u91cf\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 Throughput (rps) \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 9782.2 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 17236.5 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 16002.3 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 18992.9 rps \u8282\u70b9\u5230\u8282\u70b9 47560.5 rps","title":"netperf \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#iperf","text":"iperf \u662f\u4e00\u79cd\u6d41\u884c\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\uff0c\u53ef\u8ba9\u60a8\u6d4b\u91cf\u4e24\u4e2a\u7aef\u70b9\u4e4b\u95f4\u7684\u7f51\u7edc\u5e26\u5bbd\u3002\u5b83\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u8fde\u63a5\u7684\u5e26\u5bbd\u548c\u6027\u80fd\u3002\u5728\u672c\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5b83\u5206\u522b\u6765\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <Pod IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 iperf \u6d4b\u8bd5\u3002 \u901a\u8fc7 iperf3 -c <cluster IP> -d -P 1 \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u901a\u8fc7 \u2014P \u53c2\u6570\u5206\u522b\u6307\u5b9a\u7ebf\u7a0b\u4e3a 1\uff0c2\uff0c4\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u7ebf\u7a0b\u6570 1 \u7ebf\u7a0b\u6570 2 \u7ebf\u7a0b\u6570 4 \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec \u8282\u70b9\u5230\u8282\u70b9 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec","title":"iperf \u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#redis-benchmark","text":"redis-benchmark \u65e8\u5728\u901a\u8fc7\u6a21\u62df\u591a\u4e2a\u5ba2\u6237\u7aef\u5e76\u6267\u884c\u5404\u79cd Redis \u547d\u4ee4\u6765\u6d4b\u91cf Redis \u670d\u52a1\u5668\u7684\u6027\u80fd\u548c\u541e\u5410\u91cf\u3002\u6211\u4eec\u901a\u8fc7 redis-benchmark \u5206\u522b\u6d4b\u8bd5 Pod \u8de8\u8282\u70b9\u8bbf\u95ee\u90e8\u7f72\u4e86 Redis \u670d\u52a1\u7684 Pod \u548c Service\u3002\u5176\u4e2d\u6d4b\u8bd5\u8bbf\u95ee Service \u7684 cluster IP \u65f6\uff0c\u5206\u4e3a kube-proxy \u6216\u8005 cilium + kube-proxy replacement \u4e24\u79cd\u573a\u666f\u3002 \u4ee5 Pod IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee Pod IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 45682.96 rps 46992.48 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 59737.16 rps 59988.00 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u7684 Spiderpool Pod 66357.00 rps 66800.27 rps \u57fa\u4e8e macvlan \u7684\u8de8\u5b50\u7f51\u7684 Spiderpool Pod 67444.45 rps 67783.67 rps \u4ee5 cluster IP \u4e3a\u76ee\u7684\u7684\u8de8\u8282\u70b9 Pod \u7684 redis-benchmark \u6d4b\u8bd5\u3002 \u901a\u8fc7 redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set \u6d4b\u8bd5\u8de8\u8282\u70b9 Pod \u8bbf\u95ee cluster IP \u7684\u6027\u80fd\uff0c\u6570\u636e\u5982\u4e0b\u3002 \u6d4b\u8bd5\u5bf9\u8c61 get set \u57fa\u4e8e iptables datapath \u548c \u65e0\u96a7\u9053 \u7684 Calico 46082.95 rps 46728.97 rps \u57fa\u4e8e\u5168\u91cf eBPF \u52a0\u901f \u548c \u65e0\u96a7\u9053 \u7684 cilium 60496.07 rps 58927.52 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u548c kube-proxy \u7684 Spiderpool Pod 45578.85 rps 46274.87 rps \u57fa\u4e8e macvlan \u7684\u540c\u5b50\u7f51\u4e14\u5168\u91cf eBPF \u52a0\u901f\u7684 Spiderpool Pod 63211.12 rps 64061.50 rps","title":"redis-benchmark \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/io-performance-zh_CN/#_3","text":"Spiderpool \u505a\u4e3a Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u5176 IO \u6027\u80fd\u5728\u5927\u90e8\u5206\u573a\u666f\u4e0b\u90fd\u9886\u5148\u4e8e Calico\u3001Cilium\u3002","title":"\u603b\u7ed3"},{"location":"concepts/io-performance/","text":"Network I/O Performance English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used with Macvlan, SR-IOV, and IPvlan to implement a complete network solution. This article will compare it with the mainstream network CNI plug-ins on the market ( Such as cilium , calico ) Network Latency and Throughput in various scenarios ENV This test contains performance benchmark data for various scenarios. All tests were performed between containers running on two different bare metal nodes with 10 Gbit/s network interfaces. Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi Test object This test uses macvlan with Spiderpool as the test solution, and selected Calico , Cilium For comparison, two common network solutions are as follows. The following is the relevant version and other information: Test object illustrate Spiderpool based macvlan datapath Spiderpool version v0.8.0 Calico Calico version v3.26.1, based on iptables datapath and no tunnels Cilium Cilium version v1.14.3, based on full eBPF acceleration and no tunneling sockperf network latency test Sockperf is a network benchmarking tool that can be used to measure network latency. It allows you to evaluate the performance of your network by testing the latency between two endpoints. We can use it to separately test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod latency testing for Pod IP purposes. Use sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the Pod IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.3 usec Cilium based on full eBPF acceleration and no tunneling 29.1 usec Spiderpool Pod on the same subnet based on macvlan 24.3 usec Spiderpool Pod across subnets based on macvlan 26.2 usec node to node 32.2 usec Cross-node Pod latency test for cluster IP purpose. Use sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the cluster IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.9 usec Cilium based on full eBPF acceleration and no tunneling 30.2 usec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 36.8 usec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 27.7 usec node to node 32.2 usec netperf performance test netperf is a widely used network performance testing tool that allows you to measure various aspects of network performance, such as throughput. We can use netperf to test Pod's cross-node access to Pod and Service respectively. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Netperf testing of cross-node Pods for Pod IP purposes. Use netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pod access to Pod IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9985.7 Cilium based on full eBPF acceleration and no tunneling 17571.3 Spiderpool Pod on the same subnet based on macvlan 19793.9 Spiderpool Pod across subnets based on macvlan 19215.2 node to node 47560.5 Netperf testing across node Pods for cluster IP purposes. Use netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pods accessing the cluster IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9782.2 Cilium based on full eBPF acceleration and no tunneling 17236.5 Spiderpool Pod based on macvlan on the same subnet and kube-proxy 16002.3 Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 18992.9 node to node 47560.5 iperf network performance test iperf is a popular network performance testing tool that allows you to measure network bandwidth between two endpoints. It is widely used to evaluate the bandwidth and performance of network connections. In this chapter, we use it to test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . iperf testing of cross-node Pods for Pod IP purposes. Use iperf3 -c <Pod IP> -d -P 1 to test the performance of cross-node Pod access to Pod IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec Spiderpool Pod on the same subnet based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec Spiderpool Pod across subnets based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec iperf testing of cross-node Pods for cluster IP purposes. Use iperf3 -c <cluster IP> -d -P 1 to test the performance of cross-node Pod access to cluster IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec redis-benchmark performance test redis-benchmark is designed to measure the performance and throughput of a Redis server by simulating multiple clients and executing various Redis commands. We used redis-benchmark to test Pod's cross-node access to the Pod and Service where the Redis service is deployed. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod redis-benchmark testing based on Pod IP. Use redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to Pod IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 45682.96 rps 46992.48 rps Cilium based on full eBPF acceleration and no tunneling 59737.16 rps 59988.00 rps Spiderpool Pod on the same subnet based on macvlan 66357.00 rps 66800.27 rps Spiderpool Pod across subnets based on macvlan 67444.45 rps 67783.67 rps Cross-node Pod redis-benchmark testing for cluster IP purposes. Use redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to cluster IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 46082.95 rps 46728.97 rps Cilium based on full eBPF acceleration and no tunneling 60496.07 rps 58927.52 rps Spiderpool Pod based on macvlan on the same subnet and kube-proxy 45578.85 rps 46274.87 rps Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 63211.12 rps 64061.50 rps Summary When Spiderpool is used as an underlay network solution, its IO performance is ahead of Calico and Cilium in most scenarios.","title":"I/O Performance"},{"location":"concepts/io-performance/#network-io-performance","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used with Macvlan, SR-IOV, and IPvlan to implement a complete network solution. This article will compare it with the mainstream network CNI plug-ins on the market ( Such as cilium , calico ) Network Latency and Throughput in various scenarios","title":"Network I/O Performance"},{"location":"concepts/io-performance/#env","text":"This test contains performance benchmark data for various scenarios. All tests were performed between containers running on two different bare metal nodes with 10 Gbit/s network interfaces. Kubernetes: v1.28.2 container runtime: containerd 1.6.24 OS: ubuntu 23.04 kernel: 6.2.0-35-generic NIC: Mellanox Technologies MT27800 Family [ConnectX-5] Node Role CPU Memory master1 control-plane, worker 56C 125Gi worker1 worker 56C 125Gi","title":"ENV"},{"location":"concepts/io-performance/#test-object","text":"This test uses macvlan with Spiderpool as the test solution, and selected Calico , Cilium For comparison, two common network solutions are as follows. The following is the relevant version and other information: Test object illustrate Spiderpool based macvlan datapath Spiderpool version v0.8.0 Calico Calico version v3.26.1, based on iptables datapath and no tunnels Cilium Cilium version v1.14.3, based on full eBPF acceleration and no tunneling","title":"Test object"},{"location":"concepts/io-performance/#sockperf-network-latency-test","text":"Sockperf is a network benchmarking tool that can be used to measure network latency. It allows you to evaluate the performance of your network by testing the latency between two endpoints. We can use it to separately test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod latency testing for Pod IP purposes. Use sockperf pp --tcp -i <Pod IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the Pod IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.3 usec Cilium based on full eBPF acceleration and no tunneling 29.1 usec Spiderpool Pod on the same subnet based on macvlan 24.3 usec Spiderpool Pod across subnets based on macvlan 26.2 usec node to node 32.2 usec Cross-node Pod latency test for cluster IP purpose. Use sockperf pp --tcp -i <Cluster IP> -p 12345 -t 30 to test the latency of cross-node Pod access to the cluster IP. The data is as follows. Test object latency Calico based on iptables datapath and tunnelless 51.9 usec Cilium based on full eBPF acceleration and no tunneling 30.2 usec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 36.8 usec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 27.7 usec node to node 32.2 usec","title":"sockperf network latency test"},{"location":"concepts/io-performance/#netperf-performance-test","text":"netperf is a widely used network performance testing tool that allows you to measure various aspects of network performance, such as throughput. We can use netperf to test Pod's cross-node access to Pod and Service respectively. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Netperf testing of cross-node Pods for Pod IP purposes. Use netperf -H <Pod IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pod access to Pod IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9985.7 Cilium based on full eBPF acceleration and no tunneling 17571.3 Spiderpool Pod on the same subnet based on macvlan 19793.9 Spiderpool Pod across subnets based on macvlan 19215.2 node to node 47560.5 Netperf testing across node Pods for cluster IP purposes. Use netperf -H <cluster IP> -l 10 -c -t TCP_RR -- -r100,100 to test the throughput of cross-node Pods accessing the cluster IP. The data is as follows. Test object Throughput (rps) Calico based on iptables datapath and tunnelless 9782.2 Cilium based on full eBPF acceleration and no tunneling 17236.5 Spiderpool Pod based on macvlan on the same subnet and kube-proxy 16002.3 Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 18992.9 node to node 47560.5","title":"netperf performance test"},{"location":"concepts/io-performance/#iperf-network-performance-test","text":"iperf is a popular network performance testing tool that allows you to measure network bandwidth between two endpoints. It is widely used to evaluate the bandwidth and performance of network connections. In this chapter, we use it to test Pod's cross-node access to Pod and Service. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . iperf testing of cross-node Pods for Pod IP purposes. Use iperf3 -c <Pod IP> -d -P 1 to test the performance of cross-node Pod access to Pod IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.26 Gbits/sec 4.56 Gbits/sec 8.05 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.36 Gbits/sec 9.39 Gbits/sec Spiderpool Pod on the same subnet based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec Spiderpool Pod across subnets based on macvlan 9.36 Gbits/sec 9.37 Gbits/sec 9.38 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec iperf testing of cross-node Pods for cluster IP purposes. Use iperf3 -c <cluster IP> -d -P 1 to test the performance of cross-node Pod access to cluster IP. Use the -P parameter to specify threads 1, 2, and 4 respectively. The data is as follows. Test object Number of threads 1 Number of threads 2 Number of threads 4 Calico based on iptables datapath and tunnelless 3.06 Gbits/sec 4.63 Gbits/sec 8.02 Gbits/sec Cilium based on full eBPF acceleration and no tunneling 9.35 Gbits/sec 9.35 Gbits/sec 9.38 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and kube-proxy 3.42 Gbits/sec 6.75 Gbits/sec 9.24 Gbits/sec Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 9.36 Gbits/sec 9.38 Gbits/sec 9.39 Gbits/sec node to node 9.41 Gbits/sec 9.40 Gbits/sec 9.42 Gbits/sec","title":"iperf network performance test"},{"location":"concepts/io-performance/#redis-benchmark-performance-test","text":"redis-benchmark is designed to measure the performance and throughput of a Redis server by simulating multiple clients and executing various Redis commands. We used redis-benchmark to test Pod's cross-node access to the Pod and Service where the Redis service is deployed. When testing access to Service's cluster IP, there are two scenarios: kube-proxy or cilium + kube-proxy replacement . Cross-node Pod redis-benchmark testing based on Pod IP. Use redis-benchmark -h <Pod IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to Pod IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 45682.96 rps 46992.48 rps Cilium based on full eBPF acceleration and no tunneling 59737.16 rps 59988.00 rps Spiderpool Pod on the same subnet based on macvlan 66357.00 rps 66800.27 rps Spiderpool Pod across subnets based on macvlan 67444.45 rps 67783.67 rps Cross-node Pod redis-benchmark testing for cluster IP purposes. Use redis-benchmark -h <cluster IP> -p 6379 -d 1000 -t get,set to test the performance of cross-node Pod access to cluster IP. The data is as follows. Test object get set Calico based on iptables datapath and tunnelless 46082.95 rps 46728.97 rps Cilium based on full eBPF acceleration and no tunneling 60496.07 rps 58927.52 rps Spiderpool Pod based on macvlan on the same subnet and kube-proxy 45578.85 rps 46274.87 rps Spiderpool Pod based on macvlan on the same subnet and fully eBPF accelerated 63211.12 rps 64061.50 rps","title":"redis-benchmark performance test"},{"location":"concepts/io-performance/#summary","text":"When Spiderpool is used as an underlay network solution, its IO performance is ahead of Calico and Cilium in most scenarios.","title":"Summary"},{"location":"concepts/ipam-des-zh_CN/","text":"IPAM \u7b80\u4f53\u4e2d\u6587 | English Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u7684 IPAM \u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff1a\"Overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"Underlay \u7f51\u7edc\u65b9\u6848\"\u3002 \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 Spiderpool \u662f\u4e3a Underlay \u7f51\u7edc\u7279\u70b9\u800c\u8bbe\u8ba1\uff0c\u4ee5\u4e0b\u5bf9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002 Overlay \u7f51\u7edc\u65b9\u6848 IPAM \u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u7f51\u7edc\u540c\u5bbf\u4e3b\u673a\u7f51\u7edc\u7684\u89e3\u8026\uff0c\u4f8b\u5982 Calico \u3001 Cilium \u7b49 CNI \u63d2\u4ef6\uff0c \u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u8fd9\u7c7b\u6280\u672f\u6d41\u6d3e\u7684 IPAM \u5206\u914d\u7279\u70b9\u662f\uff1a Pod \u5b50\u7f51\u4e2d\u7684 IP \u5730\u5740\u6309\u7167\u8282\u70b9\u8fdb\u884c\u4e86\u5206\u5272 \u4ee5\u4e00\u4e2a\u66f4\u5c0f\u5b50\u7f51\u63a9\u7801\u957f\u5ea6\u4e3a\u5355\u4f4d\uff0c\u628a Pod subnet \u5206\u5272\u51fa\u66f4\u5c0f\u7684 IP block \u96c6\u5408\uff0c\u4f9d\u636e IP \u4f7f\u7528\u7684\u7528\u91cf\u60c5\u51b5\uff0c\u6bcf\u4e2a node \u90fd\u4f1a\u83b7\u53d6\u5230\u4e00\u4e2a\u6216\u8005\u591a\u4e2a IP block\u3002 \u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u7279\u70b9\uff1a\u7b2c\u4e00\uff0c\u6bcf\u4e2a node \u4e0a\u7684 IPAM \u63d2\u4ef6\u53ea\u9700\u8981\u5728\u672c\u5730\u7684 IP block \u4e2d\u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u65f6\uff0c\u4e0e\u5176\u5b83 node \u4e0a\u7684 IPAM \u65e0 IP \u5206\u914d\u51b2\u7a81\uff0cIPAM \u5206\u914d\u6548\u7387\u66f4\u9ad8\u3002 \u7b2c\u4e8c\uff0c\u67d0\u4e2a\u5177\u4f53\u7684 IP \u5730\u5740\u8ddf\u968f IP block \u96c6\u5408\uff0c\u4f1a\u76f8\u5bf9\u56fa\u5b9a\u7684\u4e00\u76f4\u5728\u67d0\u4e2a node \u4e0a\u88ab\u5206\u914d\uff0c\u6ca1\u6cd5\u968f\u540c Pod \u4e00\u8d77\u88ab\u8c03\u5ea6\u6f02\u79fb\u3002 IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b \u53ea\u8981 Pod \u5b50\u7f51\u4e0d\u4e0e\u76f8\u5173\u7f51\u7edc\u91cd\u53e0\uff0c\u518d\u80fd\u591f\u5408\u7406\u5229\u7528 NAT \u6280\u672f\uff0cKubernetes \u5355\u4e2a\u96c6\u7fa4\u53ef\u4ee5\u62e5\u6709\u5145\u6c9b\u7684 IP \u5730\u5740\u8d44\u6e90\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0d\u4f1a\u56e0\u4e3a IP \u4e0d\u591f\u800c\u542f\u52a8\u5931\u8d25\uff0cIPAM \u7ec4\u4ef6\u9762\u4e34\u7684\u5f02\u5e38 IP \u56de\u6536\u538b\u529b\u8f83\u5c0f\u3002 \u6ca1\u6709\u5e94\u7528 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42 \u5bf9\u4e8e\u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6709\u65e0\u72b6\u6001\u5e94\u7528\u548c\u6709\u72b6\u6001\u5e94\u7528\u7684\u533a\u522b\uff1a\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c \u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\" \u7684\u9700\u6c42\uff0c\u53ea\u80fd\u8ba9\u6240\u6709 Pod \u526f\u672c\u56fa\u5b9a\u5728\u4e00\u4e2a IP \u5730\u5740\u7684\u96c6\u5408\u5185\uff1b\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod name \u7b49\u4fe1\u606f\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u6709\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\uff0c\u8981\u5b9e\u73b0\u5355\u4e2a Pod \u548c\u5177\u4f53 IP \u5730\u5740\u7684\u5f3a\u7ed1\u5b9a\u3002 \u5728 \"Overlay \u7f51\u7edc\u65b9\u6848\"\u65b9\u6848\u4e0b\uff0c\u591a\u662f\u501f\u52a9\u4e86 NAT \u6280\u672f\u5411\u96c6\u7fa4\u5916\u90e8\u66b4\u9732\u670d\u52a1\u7684\u5165\u53e3\u548c\u6e90\u5730\u5740\uff0c\u501f\u52a9 DNS\u3001clusterIP \u7b49\u6280\u672f\u6765\u5b9e\u73b0\u96c6\u7fa4\u4e1c\u897f\u5411\u901a\u4fe1\u3002 \u5176\u6b21\uff0cIPAM \u7684 IP block \u65b9\u5f0f\u628a IP \u76f8\u5bf9\u56fa\u5b9a\u5230\u67d0\u4e2a\u8282\u70b9\u4e0a\uff0c\u800c\u4e0d\u80fd\u4fdd\u8bc1\u5e94\u7528\u526f\u672c\u7684\u8ddf\u968f\u8c03\u5ea6\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u7684 \"IP \u5730\u5740\u56fa\u5b9a\"\u80fd\u529b\u65e0\u7528\u6b66\u4e4b\u5730\uff0c\u5f53\u524d\u793e\u533a\u7684\u4e3b\u6d41 CNI \u591a\u6570\u4e0d\u652f\u6301 \"IP \u5730\u5740\u56fa\u5b9a\"\uff0c\u6216\u8005\u652f\u6301\u65b9\u6cd5\u8f83\u4e3a\u7b80\u964b\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u70b9\u662f\uff0c\u65e0\u8bba\u96c6\u7fa4\u90e8\u7f72\u5728\u4ec0\u4e48\u6837\u7684\u5e95\u5c42\u7f51\u7edc\u73af\u5883\u4e0a\uff0cCNI \u63d2\u4ef6\u7684\u517c\u5bb9\u6027\u90fd\u975e\u5e38\u597d\uff0c\u4e14\u90fd\u80fd\u591f\u4e3a Pod \u63d0\u4f9b\u5b50\u7f51\u72ec\u7acb\u3001IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b\u7684\u7f51\u7edc\u3002 Underlay \u7f51\u7edc\u65b9\u6848 IPAM \u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u5171\u4eab\u5bbf\u4e3b\u673a\u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5373 Pod \u76f4\u63a5\u83b7\u53d6\u5bbf\u4e3b\u673a\u7f51\u7edc\u4e2d\u7684 IP \u5730\u5740\u3002\u8fd9\u6837\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 Underlay \u7f51\u7edc\u65b9\u6848\u7684\u5b9e\u65bd\uff0c\u6709\u4e24\u79cd\u5178\u578b\u7684\u573a\u666f\uff1a\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5b9e\u65bd\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u4e0a\uff1b\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5728 IAAS \u73af\u5883\u4e0a\uff0c\u4f8b\u5982\u516c\u6709\u4e91\u3002\u4ee5\u4e0b\u603b\u7ed3\u4e86\"\u4f20\u7edf\u7f51\u7edc\u573a\u666f\"\u7684 IPAM \u7279\u70b9\uff1a \u5355\u4e2a IP \u5730\u5740\u5e94\u8be5\u80fd\u591f\u5728\u4efb\u4e00\u8282\u70b9\u4e0a\u88ab\u5206\u914d \u8fd9\u4e2a\u9700\u6c42\u6709\u591a\u65b9\u9762\u7684\u539f\u56e0\uff1a\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u8bbe\u5907\u589e\u52a0\u3001\u591a\u96c6\u7fa4\u6280\u672f\u7684\u53d1\u5c55\uff0cIPv4 \u5730\u5740\u8d44\u6e90\u7a00\u7f3a\uff0c\u8981\u6c42 IPAM \u63d0\u9ad8 IP \u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\uff1b \u5bf9\u4e8e\u6709 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5176 Pod \u526f\u672c\u53ef\u80fd\u4f1a\u8c03\u5ea6\u5230\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u8fd8\u4f1a\u53d1\u751f\u8282\u70b9\u95f4\u7684\u6f02\u79fb\uff0c\u8981\u6c42 IP \u5730\u5740\u4e00\u8d77\u6f02\u79fb\u3002 \u56e0\u6b64\uff0c\u5728\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4e00\u4e2a IP \u5730\u5740\u5e94\u8be5\u5177\u5907\u80fd\u591f\u88ab\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684\u53ef\u80fd\u3002 \u540c\u4e00\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\uff0c\u80fd\u5b9e\u73b0\u8de8\u5b50\u7f51\u83b7\u53d6 IP \u5730\u5740 \u4f8b\u5982\uff0c\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u5bbf\u4e3b\u673a1\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.1.0/24\uff0c\u800c\u5bbf\u4e3b\u673a2\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.2.0/24\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0c \u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\u3002 \u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a \u5f88\u591a\u4f20\u7edf\u5e94\u7528\u5728\u4e91\u5316\u6539\u9020\u524d\uff0c\u662f\u90e8\u7f72\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u7684\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u7f51\u7edc\u672a\u5f15\u5165 NAT \u5730\u5740\u8f6c\u6362\uff0c\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u9700\u8981\u611f\u77e5\u5bf9\u65b9\u7684\u6e90 IP \u6216\u76ee\u7684 IP\uff0c \u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u4e5f\u4e60\u60ef\u4e86\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u6765\u7cbe\u7ec6\u7ba1\u63a7\u7f51\u7edc\u5b89\u5168\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0a\u4e91\u540e\uff0c\u65e0\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u8303\u56f4\u7684\u56fa\u5b9a\uff0c\u6709\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u5730\u5740\u7684\u552f\u4e00\u5bf9\u5e94\uff0c\u8fd9\u6837\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u6539\u9020\u5de5\u4f5c\u3002 \u4e00\u4e2a Pod \u7684\u591a\u7f51\u5361\u83b7\u53d6\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740 \u65e2\u7136\u662f\u5bf9\u63a5 Underlay \u7f51\u7edc\uff0cPod \u5c31\u4f1a\u6709\u591a\u7f51\u5361\u9700\u6c42\uff0c\u4ee5\u4f7f\u5176\u901a\u8fbe\u4e0d\u540c\u7684 Underlay \u5b50\u7f51\uff0c\u8fd9\u8981\u6c42 IPAM \u80fd\u591f\u7ed9\u5e94\u7528\u7684\u4e0d\u540c\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u4e0b\u7684 IP \u5730\u5740\u3002 IP \u5730\u5740\u51b2\u7a81 \u5728 Underlay \u7f51\u7edc\u4e2d\uff0c\u66f4\u52a0\u5bb9\u6613\u51fa\u73b0 IP \u51b2\u7a81\uff0c\u4f8b\u5982\uff0cPod \u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u4e3b\u673a IP \u53d1\u751f\u4e86\u51b2\u7a81\uff0c\u4e0e\u5176\u5b83\u5bf9\u63a5\u4e86\u76f8\u540c\u5b50\u7f51\u7684\u96c6\u7fa4\u51b2\u7a81\uff0c \u800c IPAM \u7ec4\u4ef6\u5f88\u96be\u611f\u77e5\u5916\u90e8\u8fd9\u4e9b\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u591a\u9700\u8981\u501f\u52a9 CNI \u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3002 \u5df2\u7528 IP \u5730\u5740\u7684\u91ca\u653e\u56de\u6536 \u56e0\u4e3a Underlay \u7f51\u7edc IP \u5730\u5740\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5e94\u7528\u6709 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6240\u4ee5\uff0c\"\u5e94\u5f53\"\u88ab\u91ca\u653e\u7684 IP \u5730\u5740\u82e5\u672a\u88ab IPAM \u7ec4\u4ef6\u56de\u6536\uff0c\u65b0\u542f\u52a8\u7684 Pod \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7f3a\u5c11 IP \u5730\u5740\u800c\u5931\u8d25\u3002 \u8fd9\u5c31\u8981\u6c42 IPAM \u7ec4\u4ef6\u62e5\u6709\u66f4\u52a0\u7cbe\u51c6\u3001\u9ad8\u6548\u3001\u53ca\u65f6\u7684 IP \u56de\u6536\u673a\u5236\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u52bf\u6709\uff1a\u65e0\u9700\u7f51\u7edc NAT \u6620\u5c04\u7684\u5f15\u5165\uff0c\u5bf9\u5e94\u7528\u7684\u4e91\u5316\u7f51\u7edc\u6539\u9020\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u7684\u4fbf\u5229\uff1b\u5e95\u5c42\u7f51\u7edc\u7684\u706b\u5899\u7b49\u8bbe\u5907\uff0c\u53ef\u5bf9 Pod \u901a\u4fe1\u5b9e\u73b0\u76f8\u5bf9\u8f83\u4e3a\u7cbe\u7ec6\u7684\u7ba1\u63a7\uff1b\u65e0\u9700\u96a7\u9053\u6280\u672f\uff0c \u7f51\u7edc\u901a\u4fe1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u65f6\u6027\u80fd\u4e5f\u76f8\u5bf9\u7684\u63d0\u9ad8\u4e86\u3002 Spiderpool IPAM \u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool IPAM \u63d2\u4ef6\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI calico CNI , weave CNI Spiderpool IP \u5206\u914d\u7b97\u6cd5 \u5f53 Pod \u521b\u5efa\u65f6\uff0c\u5b83\u5c06\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u83b7\u53d6 IP \u5206\u914d\uff1bIP \u5206\u914d\u751f\u547d\u5468\u671f\u5c06\u7ecf\u5386 \u83b7\u53d6\u5019\u9009\u6c60 \u3001 \u8fc7\u6ee4\u5019\u9009\u6c60 \u3001 \u5019\u9009\u6c60\u6392\u5e8f \u4e09\u4e2a\u5927\u9636\u6bb5\u3002 \u83b7\u53d6\u5019\u9009\u6c60\uff1aSpiderpool \u6709\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u53d6**\u9ad8\u4f18\u5148\u7ea7\u89c4\u5219**\u547d\u4e2d\u7684\u6240\u6709\u6c60\uff0c\u5c06\u5b83\u4eec\u6807\u8bb0\u4e3a\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4ee5\u6709\u8d44\u683c\u88ab\u8fdb\u4e00\u6b65\u8003\u8651\u3002 \u8fc7\u6ee4\u5019\u9009\u6c60\uff1aSpiderpool \u901a\u8fc7\u4eb2\u548c\u6027\u7b49\u8fc7\u6ee4\u673a\u5236\uff0c\u66f4\u7cbe\u786e\u5730\u4ece\u6240\u6709\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5019\u9009\u6c60\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u9700\u6c42\u6216\u590d\u6742\u7684\u4f7f\u7528\u573a\u666f\u3002 \u5019\u9009\u6c60\u6392\u5e8f\uff1a\u5bf9\u4e8e\u591a\u5019\u9009\u6c60\uff0cSpiderpool \u6839\u636e SpiderIPPool \u5bf9\u8c61\u4e2d\u7684\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002 \u83b7\u53d6\u5019\u9009\u6c60 Spiderpool \u63d0\u4f9b\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u5728\u4e3a Pod \u5206\u914d IP \u5730\u5740\u65f6\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\u3002\u4ee5\u4e0b\u89c4\u5219\u6309\u7167\u4ece \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u987a\u5e8f\u5217\u51fa\uff0c\u5982\u679c\u540c\u65f6\u5b58\u5728\u4e0b\u9762\u7684\u591a\u4e2a\u89c4\u5219\uff0c\u524d\u4e00\u4e2a\u89c4\u5219\u5c06 \u8986\u76d6 \u540e\u4e00\u4e2a\u89c4\u5219\u3002 \u4f18\u5148\u7ea7 1 \uff1aSpiderSubnet \u6ce8\u89e3\u3002 SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 IPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002\u521b\u5efa\u5e94\u7528\u65f6\u53ef\u4ee5\u4f7f\u7528 ipam.spidernet.io/subnets \u6216 ipam.spidernet.io/subnet \u6ce8\u89e3\u6307\u5b9a Subnet\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u53d6 IP \u5730\u5740\u81ea\u52a8\u521b\u5efa IP \u6c60\uff0c\u5e76\u4ece\u6c60\u4e2d\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\u7ed9\u5e94\u7528\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderSubnet \u3002 \u4f18\u5148\u7ea7 2 \uff1aSpiderIPPool \u6ce8\u89e3\u3002 \u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u7ed3\u5408 SpiderIPPool \u6ce8\u89e3 ipam.spidernet.io/ippools \u6216 ipam.spidernet.io/ippool \u80fd\u7ed1\u5b9a\u4e0d\u540c\u7684 IPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 IPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderIPPool \u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 3 \uff1a\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 IP \u6c60\u3002 \u901a\u8fc7\u5728\u547d\u540d\u7a7a\u95f4\u4e2d\u8bbe\u7f6e\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u6216 ipam.spidernet.io/default-ipv6-ippool \u6307\u5b9a\u9ed8\u8ba4\u7684 IP \u6c60\u3002\u5728\u8be5\u79df\u6237\u4e2d\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u5982\u679c\u6ca1\u6709\u5176\u4ed6\u9ad8\u4f18\u5148\u7ea7\u7684\u6c60\u89c4\u5219\uff0c\u90a3\u4e48\u5c06\u4ece\u8be5\u79df\u6237\u53ef\u7528\u7684\u5019\u9009\u6c60\u4e2d\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 \u547d\u540d\u7a7a\u95f4\u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 4 \uff1aCNI \u914d\u7f6e\u6587\u4ef6\u3002 \u901a\u8fc7\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 default_ipv4_ippool \u548c default_ipv6_ippool \u5b57\u6bb5\u8bbe\u7f6e\u5168\u5c40\u7684 CNI \u9ed8\u8ba4\u6c60\uff0c\u5176\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0c\u5f53\u5e94\u7528\u4f7f\u7528\u8be5 CNI \u914d\u7f6e\u7f51\u7edc\u65f6\u5e76\u8c03\u7528 Spiderpool \uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5e94\u7528\u526f\u672c\uff0cSpiderpool \u90fd\u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u6bcf\u4e2a\u8282\u70b9\u5206\u5c5e\u4e0d\u540c\u7684\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\u7684\u573a\u666f\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684\u8282\u70b9\u4eb2\u548c\u89c4\u5219\uff0cPod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 CNI \u914d\u7f6e \u3002 \u4f18\u5148\u7ea7 5 \uff1a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\u3002 \u5728 SpiderIPPool CR \u5bf9\u8c61\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c06 spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true \uff0c\u5c06\u6c60\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\uff0c\u9ed8\u8ba4\u4e3a false \u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 \u96c6\u7fa4\u9ed8\u8ba4 IPPool \u8fc7\u6ee4\u5019\u9009\u6c60 \u901a\u8fc7\u4e0a\u8ff0\u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u5f97 IPv4 \u548c IPv6 \u7684 IPPool \u5019\u9009\u540e\uff0cSpiderpool \u4f1a\u6839\u636e\u4ee5\u4e0b\u89c4\u5219\u8fdb\u884c\u8fc7\u6ee4\uff0c\u4e86\u89e3\u54ea\u4e2a\u5019\u9009 IPPool \u53ef\u7528\u3002 IP \u6c60\u5904\u4e8e\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4f46\u5176\u5904\u4e8e terminating \u72b6\u6001\u7684\uff0cSpiderpool \u5c06\u4f1a\u8fc7\u6ee4\u8be5\u6c60\u3002 IP \u6c60\u7684 spec.disable \u5b57\u6bb5\u7528\u4e8e\u8bbe\u7f6e IPPool \u662f\u5426\u53ef\u7528\uff0c\u5f53\u8be5\u503c\u4e3a false \u65f6\uff0c\u610f\u5473\u7740 IPPool \u4e0d\u53ef\u4f7f\u7528\u3002 \u68c0\u67e5 IPPool.Spec.NodeName \u548c IPPool.Spec.NodeAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u8c03\u5ea6\u8282\u70b9\u5339\u914d\u3002 \u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.NamespaceName \u548c IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u547d\u540d\u7a7a\u95f4\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.PodAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684 matchLabels \u6240\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.MultusName \u5c5e\u6027\u662f\u5426\u4e0e Pod \u5f53\u524d NIC Multus \u914d\u7f6e\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab IPPool \u5b9e\u4f8b\u7684 exclude_ips \u5b57\u6bb5\u6240\u5305\u542b\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab ReservedIP \u5b9e\u4f8b\u6240\u4fdd\u7559\u4e86\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 IPPool \u7684\u53ef\u7528 IP \u8d44\u6e90\u88ab\u8017\u5c3d\uff0c\u5219\u8be5 IPPool \u4e5f\u5c06\u88ab\u8fc7\u6ee4\u3002 \u5019\u9009\u6c60\u6392\u5e8f \u8fc7\u6ee4\u5019\u9009\u6c60\u540e\uff0c\u53ef\u80fd\u4ecd\u5b58\u5728\u591a\u4e2a\u5019\u9009\u6c60\uff0cSpiderpool \u4f1a\u8fdb\u4e00\u6b65\u4f7f\u7528\u81ea\u5b9a\u4e49\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002 \u5177\u6709 IPPool.Spec.PodAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u9ad8\u4f18\u5148\u7ea7\u3002 \u5177\u6709 IPPool.Spec.NodeName \u6216 IPPool.Spec.NodeAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e8c\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NodeName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NodeAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.NamespaceName \u6216 IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e09\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NamespaceName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NamespaceAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.MultusName \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u4f4e\u4f18\u5148\u7ea7\u3002 \u6ce8\u610f\uff1a\u8fd9\u91cc\u6709\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u63cf\u8ff0\u8fd9\u4e2a\u89c4\u5219\u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5355\u4e00\u5173\u8054\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolB \u3002 \u5177\u6709\u5355\u4e2a\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.NodeName \u548c IPPool.Spec.NamespaceName \u7684 IPPoolB \u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u3001 IPPool.Spec.NamespaceName \u548c IPPool.Spec.MultusName \u7684 IPPoolB \u3002 NOTE\uff1a \u5982\u679c Pod \u5c5e\u4e8e StatefulSet\uff0c\u5219\u4f1a\u4f18\u5148\u5206\u914d\u7b26\u5408\u4e0a\u9762\u89c4\u5219\u7684 IP \u5730\u5740\u3002 \u4e00\u65e6 Pod \u91cd\u65b0\u542f\u52a8\uff0c\u5b83\u5c06\u5c1d\u8bd5\u91cd\u7528\u6700\u540e\u5206\u914d\u7684 IP \u5730\u5740\u3002 IP \u56de\u6536\u673a\u5236 \u5728 Kubernetes \u4e2d\uff0c\u5783\u573e\u56de\u6536\uff08Garbage Collection\uff0c\u7b80\u79f0GC\uff09\u5bf9\u4e8e IP \u5730\u5740\u7684\u56de\u6536\u975e\u5e38\u91cd\u8981\u3002IP \u5730\u5740\u7684\u53ef\u7528\u6027\u5173\u7cfb\u5230 Pod \u662f\u5426\u80fd\u591f\u542f\u52a8\u6210\u529f\u3002GC \u673a\u5236\u53ef\u4ee5\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u4e0d\u518d\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u548c IP \u5730\u5740\u7684\u8017\u5c3d\u3002 \u5728 IPAM \u4e2d\u8bb0\u5f55\u4e86\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u4f46\u662f\u8fd9\u4e9b Pod \u5728 Kubernetes \u96c6\u7fa4\u4e2d\u5df2\u7ecf\u4e0d\u590d\u5b58\u5728\uff0c\u8fd9\u4e9b IP \u53ef\u79f0\u4e3a \u50f5\u5c38 IP \uff0cSpiderpool \u53ef\u9488\u5bf9 \u50f5\u5c38 IP \u8fdb\u884c\u56de\u6536\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u5728\u96c6\u7fa4\u4e2d delete Pod \u65f6\uff0c\u4f46\u7531\u4e8e \u7f51\u7edc\u5f02\u5e38 \u6216 cni \u4e8c\u8fdb\u5236 crash \u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8c03\u7528 cni delete \u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4 IP \u5730\u5740\u65e0\u6cd5\u88ab cni \u56de\u6536\u3002 \u5728 cni delete \u5931\u8d25 \u7b49\u6545\u969c\u573a\u666f\uff0c\u5982\u679c\u4e00\u4e2a\u66fe\u7ecf\u5206\u914d\u4e86 IP \u7684 Pod \u88ab\u9500\u6bc1\u540e\uff0c\u4f46\u5728 IPAM \u4e2d\u8fd8\u8bb0\u5f55\u5206\u914d\u7740IP \u5730\u5740\uff0c\u5f62\u6210\u4e86\u50f5\u5c38 IP \u7684\u73b0\u8c61\u3002Spiderpool \u9488\u5bf9\u8fd9\u79cd\u95ee\u9898\uff0c\u4f1a\u57fa\u4e8e\u5468\u671f\u548c\u4e8b\u4ef6\u626b\u63cf\u673a\u5236\uff0c\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u50f5\u5c38 IP \u5730\u5740\u3002 \u8282\u70b9\u610f\u5916\u5b95\u673a\u540e\uff0c\u96c6\u7fa4\u4e2d\u7684 Pod \u6c38\u4e45\u5904\u4e8e deleting \u72b6\u6001\uff0cPod \u5360\u7528\u7684 IP \u5730\u5740\u65e0\u6cd5\u88ab\u91ca\u653e\u3002 \u5bf9\u5904\u4e8e Terminating \u72b6\u6001\u7684 Pod\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond \u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED \u6765\u63a7\u5236\u3002\u8be5\u80fd\u529b\u80fd\u591f\u7528\u4ee5\u89e3\u51b3 \u8282\u70b9\u610f\u5916\u5b95\u673a \u7684\u6545\u969c\u573a\u666f\u3002","title":"IPAM"},{"location":"concepts/ipam-des-zh_CN/#ipam","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"IPAM"},{"location":"concepts/ipam-des-zh_CN/#underlay-overlay-ipam","text":"\u4e91\u539f\u751f\u7f51\u7edc\u4e2d\u51fa\u73b0\u4e86\u4e24\u79cd\u6280\u672f\u7c7b\u522b\uff1a\"Overlay \u7f51\u7edc\u65b9\u6848\" \u548c \"Underlay \u7f51\u7edc\u65b9\u6848\"\u3002 \u4e91\u539f\u751f\u7f51\u7edc\u5bf9\u4e8e\u5b83\u4eec\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u5f88\u591a CNI \u9879\u76ee\u7684\u5b9e\u73b0\u539f\u7406\u4e2d\uff0c\u7b80\u5355\u62bd\u8c61\u51fa\u8fd9\u4e24\u79cd\u6280\u672f\u6d41\u6d3e\u7684\u7279\u70b9\uff0c\u5b83\u4eec\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\u3002 Spiderpool \u662f\u4e3a Underlay \u7f51\u7edc\u7279\u70b9\u800c\u8bbe\u8ba1\uff0c\u4ee5\u4e0b\u5bf9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u80fd\u591f\u66f4\u597d\u8bf4\u660e Spiderpool \u7684\u7279\u70b9\u548c\u4f7f\u7528\u573a\u666f\u3002","title":"Underlay \u7f51\u7edc\u548c Overlay \u7f51\u7edc\u7684 IPAM"},{"location":"concepts/ipam-des-zh_CN/#overlay-ipam","text":"\u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u7f51\u7edc\u540c\u5bbf\u4e3b\u673a\u7f51\u7edc\u7684\u89e3\u8026\uff0c\u4f8b\u5982 Calico \u3001 Cilium \u7b49 CNI \u63d2\u4ef6\uff0c \u8fd9\u4e9b\u63d2\u4ef6\u591a\u6570\u4f7f\u7528\u4e86 vxlan \u7b49\u96a7\u9053\u6280\u672f\uff0c\u642d\u5efa\u8d77\u4e00\u4e2a Overlay \u7f51\u7edc\u5e73\u9762\uff0c\u518d\u501f\u7528 NAT \u6280\u672f\u5b9e\u73b0\u5357\u5317\u5411\u7684\u901a\u4fe1\u3002 \u8fd9\u7c7b\u6280\u672f\u6d41\u6d3e\u7684 IPAM \u5206\u914d\u7279\u70b9\u662f\uff1a Pod \u5b50\u7f51\u4e2d\u7684 IP \u5730\u5740\u6309\u7167\u8282\u70b9\u8fdb\u884c\u4e86\u5206\u5272 \u4ee5\u4e00\u4e2a\u66f4\u5c0f\u5b50\u7f51\u63a9\u7801\u957f\u5ea6\u4e3a\u5355\u4f4d\uff0c\u628a Pod subnet \u5206\u5272\u51fa\u66f4\u5c0f\u7684 IP block \u96c6\u5408\uff0c\u4f9d\u636e IP \u4f7f\u7528\u7684\u7528\u91cf\u60c5\u51b5\uff0c\u6bcf\u4e2a node \u90fd\u4f1a\u83b7\u53d6\u5230\u4e00\u4e2a\u6216\u8005\u591a\u4e2a IP block\u3002 \u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u7279\u70b9\uff1a\u7b2c\u4e00\uff0c\u6bcf\u4e2a node \u4e0a\u7684 IPAM \u63d2\u4ef6\u53ea\u9700\u8981\u5728\u672c\u5730\u7684 IP block \u4e2d\u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u65f6\uff0c\u4e0e\u5176\u5b83 node \u4e0a\u7684 IPAM \u65e0 IP \u5206\u914d\u51b2\u7a81\uff0cIPAM \u5206\u914d\u6548\u7387\u66f4\u9ad8\u3002 \u7b2c\u4e8c\uff0c\u67d0\u4e2a\u5177\u4f53\u7684 IP \u5730\u5740\u8ddf\u968f IP block \u96c6\u5408\uff0c\u4f1a\u76f8\u5bf9\u56fa\u5b9a\u7684\u4e00\u76f4\u5728\u67d0\u4e2a node \u4e0a\u88ab\u5206\u914d\uff0c\u6ca1\u6cd5\u968f\u540c Pod \u4e00\u8d77\u88ab\u8c03\u5ea6\u6f02\u79fb\u3002 IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b \u53ea\u8981 Pod \u5b50\u7f51\u4e0d\u4e0e\u76f8\u5173\u7f51\u7edc\u91cd\u53e0\uff0c\u518d\u80fd\u591f\u5408\u7406\u5229\u7528 NAT \u6280\u672f\uff0cKubernetes \u5355\u4e2a\u96c6\u7fa4\u53ef\u4ee5\u62e5\u6709\u5145\u6c9b\u7684 IP \u5730\u5740\u8d44\u6e90\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0d\u4f1a\u56e0\u4e3a IP \u4e0d\u591f\u800c\u542f\u52a8\u5931\u8d25\uff0cIPAM \u7ec4\u4ef6\u9762\u4e34\u7684\u5f02\u5e38 IP \u56de\u6536\u538b\u529b\u8f83\u5c0f\u3002 \u6ca1\u6709\u5e94\u7528 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42 \u5bf9\u4e8e\u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6709\u65e0\u72b6\u6001\u5e94\u7528\u548c\u6709\u72b6\u6001\u5e94\u7528\u7684\u533a\u522b\uff1a\u5bf9\u4e8e Deployment \u8fd9\u7c7b\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod \u540d\u79f0\u4f1a\u968f\u7740 Pod \u91cd\u542f\u800c\u53d8\u5316\uff0c \u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\" \u7684\u9700\u6c42\uff0c\u53ea\u80fd\u8ba9\u6240\u6709 Pod \u526f\u672c\u56fa\u5b9a\u5728\u4e00\u4e2a IP \u5730\u5740\u7684\u96c6\u5408\u5185\uff1b\u5bf9\u4e8e StatefulSet \u8fd9\u7c7b\u6709\u72b6\u6001\u5e94\u7528\uff0c\u56e0\u4e3a Pod name \u7b49\u4fe1\u606f\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u5e94\u7528\u672c\u8eab\u7684\u4e1a\u52a1\u903b\u8f91\u4e5f\u662f\u6709\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8e \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\uff0c\u8981\u5b9e\u73b0\u5355\u4e2a Pod \u548c\u5177\u4f53 IP \u5730\u5740\u7684\u5f3a\u7ed1\u5b9a\u3002 \u5728 \"Overlay \u7f51\u7edc\u65b9\u6848\"\u65b9\u6848\u4e0b\uff0c\u591a\u662f\u501f\u52a9\u4e86 NAT \u6280\u672f\u5411\u96c6\u7fa4\u5916\u90e8\u66b4\u9732\u670d\u52a1\u7684\u5165\u53e3\u548c\u6e90\u5730\u5740\uff0c\u501f\u52a9 DNS\u3001clusterIP \u7b49\u6280\u672f\u6765\u5b9e\u73b0\u96c6\u7fa4\u4e1c\u897f\u5411\u901a\u4fe1\u3002 \u5176\u6b21\uff0cIPAM \u7684 IP block \u65b9\u5f0f\u628a IP \u76f8\u5bf9\u56fa\u5b9a\u5230\u67d0\u4e2a\u8282\u70b9\u4e0a\uff0c\u800c\u4e0d\u80fd\u4fdd\u8bc1\u5e94\u7528\u526f\u672c\u7684\u8ddf\u968f\u8c03\u5ea6\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u7684 \"IP \u5730\u5740\u56fa\u5b9a\"\u80fd\u529b\u65e0\u7528\u6b66\u4e4b\u5730\uff0c\u5f53\u524d\u793e\u533a\u7684\u4e3b\u6d41 CNI \u591a\u6570\u4e0d\u652f\u6301 \"IP \u5730\u5740\u56fa\u5b9a\"\uff0c\u6216\u8005\u652f\u6301\u65b9\u6cd5\u8f83\u4e3a\u7b80\u964b\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u70b9\u662f\uff0c\u65e0\u8bba\u96c6\u7fa4\u90e8\u7f72\u5728\u4ec0\u4e48\u6837\u7684\u5e95\u5c42\u7f51\u7edc\u73af\u5883\u4e0a\uff0cCNI \u63d2\u4ef6\u7684\u517c\u5bb9\u6027\u90fd\u975e\u5e38\u597d\uff0c\u4e14\u90fd\u80fd\u591f\u4e3a Pod \u63d0\u4f9b\u5b50\u7f51\u72ec\u7acb\u3001IP \u5730\u5740\u8d44\u6e90\u5145\u6c9b\u7684\u7f51\u7edc\u3002","title":"Overlay \u7f51\u7edc\u65b9\u6848 IPAM"},{"location":"concepts/ipam-des-zh_CN/#underlay-ipam","text":"\u672c\u65b9\u6848\u5b9e\u73b0\u4e86 Pod \u5171\u4eab\u5bbf\u4e3b\u673a\u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5373 Pod \u76f4\u63a5\u83b7\u53d6\u5bbf\u4e3b\u673a\u7f51\u7edc\u4e2d\u7684 IP \u5730\u5740\u3002\u8fd9\u6837\uff0c\u5e94\u7528\u53ef\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 IP \u5730\u5740\u8fdb\u884c\u4e1c\u897f\u5411\u548c\u5357\u5317\u5411\u901a\u4fe1\u3002 Underlay \u7f51\u7edc\u65b9\u6848\u7684\u5b9e\u65bd\uff0c\u6709\u4e24\u79cd\u5178\u578b\u7684\u573a\u666f\uff1a\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5b9e\u65bd\u5728\"\u4f20\u7edf\u7f51\u7edc\"\u4e0a\uff1b\u4e00\u79cd\u662f\u96c6\u7fa4\u90e8\u7f72\u5728 IAAS \u73af\u5883\u4e0a\uff0c\u4f8b\u5982\u516c\u6709\u4e91\u3002\u4ee5\u4e0b\u603b\u7ed3\u4e86\"\u4f20\u7edf\u7f51\u7edc\u573a\u666f\"\u7684 IPAM \u7279\u70b9\uff1a \u5355\u4e2a IP \u5730\u5740\u5e94\u8be5\u80fd\u591f\u5728\u4efb\u4e00\u8282\u70b9\u4e0a\u88ab\u5206\u914d \u8fd9\u4e2a\u9700\u6c42\u6709\u591a\u65b9\u9762\u7684\u539f\u56e0\uff1a\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u8bbe\u5907\u589e\u52a0\u3001\u591a\u96c6\u7fa4\u6280\u672f\u7684\u53d1\u5c55\uff0cIPv4 \u5730\u5740\u8d44\u6e90\u7a00\u7f3a\uff0c\u8981\u6c42 IPAM \u63d0\u9ad8 IP \u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\uff1b \u5bf9\u4e8e\u6709 \"IP \u5730\u5740\u56fa\u5b9a\"\u9700\u6c42\u7684\u5e94\u7528\uff0c\u5176 Pod \u526f\u672c\u53ef\u80fd\u4f1a\u8c03\u5ea6\u5230\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u8fd8\u4f1a\u53d1\u751f\u8282\u70b9\u95f4\u7684\u6f02\u79fb\uff0c\u8981\u6c42 IP \u5730\u5740\u4e00\u8d77\u6f02\u79fb\u3002 \u56e0\u6b64\uff0c\u5728\u96c6\u7fa4\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4e00\u4e2a IP \u5730\u5740\u5e94\u8be5\u5177\u5907\u80fd\u591f\u88ab\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684\u53ef\u80fd\u3002 \u540c\u4e00\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\uff0c\u80fd\u5b9e\u73b0\u8de8\u5b50\u7f51\u83b7\u53d6 IP \u5730\u5740 \u4f8b\u5982\uff0c\u4e00\u4e2a\u96c6\u7fa4\u4e2d\uff0c\u5bbf\u4e3b\u673a1\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.1.0/24\uff0c\u800c\u5bbf\u4e3b\u673a2\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.20.2.0/24\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0c \u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\u3002 \u5e94\u7528 IP \u5730\u5740\u56fa\u5b9a \u5f88\u591a\u4f20\u7edf\u5e94\u7528\u5728\u4e91\u5316\u6539\u9020\u524d\uff0c\u662f\u90e8\u7f72\u5728\u88f8\u91d1\u5c5e\u73af\u5883\u4e0a\u7684\uff0c\u670d\u52a1\u4e4b\u95f4\u7684\u7f51\u7edc\u672a\u5f15\u5165 NAT \u5730\u5740\u8f6c\u6362\uff0c\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u9700\u8981\u611f\u77e5\u5bf9\u65b9\u7684\u6e90 IP \u6216\u76ee\u7684 IP\uff0c \u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u4e5f\u4e60\u60ef\u4e86\u4f7f\u7528\u9632\u706b\u5899\u7b49\u624b\u6bb5\u6765\u7cbe\u7ec6\u7ba1\u63a7\u7f51\u7edc\u5b89\u5168\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u4e0a\u4e91\u540e\uff0c\u65e0\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u8303\u56f4\u7684\u56fa\u5b9a\uff0c\u6709\u72b6\u6001\u5e94\u7528\u5e0c\u671b\u80fd\u591f\u5b9e\u73b0 IP \u5730\u5740\u7684\u552f\u4e00\u5bf9\u5e94\uff0c\u8fd9\u6837\uff0c\u80fd\u591f\u51cf\u5c11\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u7684\u6539\u9020\u5de5\u4f5c\u3002 \u4e00\u4e2a Pod \u7684\u591a\u7f51\u5361\u83b7\u53d6\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740 \u65e2\u7136\u662f\u5bf9\u63a5 Underlay \u7f51\u7edc\uff0cPod \u5c31\u4f1a\u6709\u591a\u7f51\u5361\u9700\u6c42\uff0c\u4ee5\u4f7f\u5176\u901a\u8fbe\u4e0d\u540c\u7684 Underlay \u5b50\u7f51\uff0c\u8fd9\u8981\u6c42 IPAM \u80fd\u591f\u7ed9\u5e94\u7528\u7684\u4e0d\u540c\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u4e0b\u7684 IP \u5730\u5740\u3002 IP \u5730\u5740\u51b2\u7a81 \u5728 Underlay \u7f51\u7edc\u4e2d\uff0c\u66f4\u52a0\u5bb9\u6613\u51fa\u73b0 IP \u51b2\u7a81\uff0c\u4f8b\u5982\uff0cPod \u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u4e3b\u673a IP \u53d1\u751f\u4e86\u51b2\u7a81\uff0c\u4e0e\u5176\u5b83\u5bf9\u63a5\u4e86\u76f8\u540c\u5b50\u7f51\u7684\u96c6\u7fa4\u51b2\u7a81\uff0c \u800c IPAM \u7ec4\u4ef6\u5f88\u96be\u611f\u77e5\u5916\u90e8\u8fd9\u4e9b\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u591a\u9700\u8981\u501f\u52a9 CNI \u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u7684 IP \u51b2\u7a81\u68c0\u6d4b\u3002 \u5df2\u7528 IP \u5730\u5740\u7684\u91ca\u653e\u56de\u6536 \u56e0\u4e3a Underlay \u7f51\u7edc IP \u5730\u5740\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5e94\u7528\u6709 IP \u5730\u5740\u56fa\u5b9a\u9700\u6c42\uff0c\u6240\u4ee5\uff0c\"\u5e94\u5f53\"\u88ab\u91ca\u653e\u7684 IP \u5730\u5740\u82e5\u672a\u88ab IPAM \u7ec4\u4ef6\u56de\u6536\uff0c\u65b0\u542f\u52a8\u7684 Pod \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7f3a\u5c11 IP \u5730\u5740\u800c\u5931\u8d25\u3002 \u8fd9\u5c31\u8981\u6c42 IPAM \u7ec4\u4ef6\u62e5\u6709\u66f4\u52a0\u7cbe\u51c6\u3001\u9ad8\u6548\u3001\u53ca\u65f6\u7684 IP \u56de\u6536\u673a\u5236\u3002 \u8fd9\u4e2a\u65b9\u6848\u7684\u4f18\u52bf\u6709\uff1a\u65e0\u9700\u7f51\u7edc NAT \u6620\u5c04\u7684\u5f15\u5165\uff0c\u5bf9\u5e94\u7528\u7684\u4e91\u5316\u7f51\u7edc\u6539\u9020\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u7684\u4fbf\u5229\uff1b\u5e95\u5c42\u7f51\u7edc\u7684\u706b\u5899\u7b49\u8bbe\u5907\uff0c\u53ef\u5bf9 Pod \u901a\u4fe1\u5b9e\u73b0\u76f8\u5bf9\u8f83\u4e3a\u7cbe\u7ec6\u7684\u7ba1\u63a7\uff1b\u65e0\u9700\u96a7\u9053\u6280\u672f\uff0c \u7f51\u7edc\u901a\u4fe1\u7684\u541e\u5410\u91cf\u548c\u5ef6\u65f6\u6027\u80fd\u4e5f\u76f8\u5bf9\u7684\u63d0\u9ad8\u4e86\u3002","title":"Underlay \u7f51\u7edc\u65b9\u6848 IPAM"},{"location":"concepts/ipam-des-zh_CN/#spiderpool-ipam","text":"\u4efb\u4f55\u652f\u6301\u7b2c\u4e09\u65b9 IPAM \u63d2\u4ef6\u7684 CNI \u9879\u76ee\uff0c\u90fd\u53ef\u4ee5\u914d\u5408 Spiderpool IPAM \u63d2\u4ef6\uff0c\u4f8b\u5982\uff1a macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI , Multus CNI calico CNI , weave CNI","title":"Spiderpool IPAM"},{"location":"concepts/ipam-des-zh_CN/#spiderpool-ip","text":"\u5f53 Pod \u521b\u5efa\u65f6\uff0c\u5b83\u5c06\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u83b7\u53d6 IP \u5206\u914d\uff1bIP \u5206\u914d\u751f\u547d\u5468\u671f\u5c06\u7ecf\u5386 \u83b7\u53d6\u5019\u9009\u6c60 \u3001 \u8fc7\u6ee4\u5019\u9009\u6c60 \u3001 \u5019\u9009\u6c60\u6392\u5e8f \u4e09\u4e2a\u5927\u9636\u6bb5\u3002 \u83b7\u53d6\u5019\u9009\u6c60\uff1aSpiderpool \u6709\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u53d6**\u9ad8\u4f18\u5148\u7ea7\u89c4\u5219**\u547d\u4e2d\u7684\u6240\u6709\u6c60\uff0c\u5c06\u5b83\u4eec\u6807\u8bb0\u4e3a\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4ee5\u6709\u8d44\u683c\u88ab\u8fdb\u4e00\u6b65\u8003\u8651\u3002 \u8fc7\u6ee4\u5019\u9009\u6c60\uff1aSpiderpool \u901a\u8fc7\u4eb2\u548c\u6027\u7b49\u8fc7\u6ee4\u673a\u5236\uff0c\u66f4\u7cbe\u786e\u5730\u4ece\u6240\u6709\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5019\u9009\u6c60\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u9700\u6c42\u6216\u590d\u6742\u7684\u4f7f\u7528\u573a\u666f\u3002 \u5019\u9009\u6c60\u6392\u5e8f\uff1a\u5bf9\u4e8e\u591a\u5019\u9009\u6c60\uff0cSpiderpool \u6839\u636e SpiderIPPool \u5bf9\u8c61\u4e2d\u7684\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002","title":"Spiderpool IP \u5206\u914d\u7b97\u6cd5"},{"location":"concepts/ipam-des-zh_CN/#_1","text":"Spiderpool \u63d0\u4f9b\u591a\u79cd\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u5728\u4e3a Pod \u5206\u914d IP \u5730\u5740\u65f6\uff0c\u4f1a\u4e25\u683c\u9075\u5b88 \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u6c60\u9009\u62e9\u89c4\u5219\u3002\u4ee5\u4e0b\u89c4\u5219\u6309\u7167\u4ece \u9ad8\u4f18\u5148\u7ea7\u5230\u4f4e\u4f18\u5148\u7ea7 \u7684\u987a\u5e8f\u5217\u51fa\uff0c\u5982\u679c\u540c\u65f6\u5b58\u5728\u4e0b\u9762\u7684\u591a\u4e2a\u89c4\u5219\uff0c\u524d\u4e00\u4e2a\u89c4\u5219\u5c06 \u8986\u76d6 \u540e\u4e00\u4e2a\u89c4\u5219\u3002 \u4f18\u5148\u7ea7 1 \uff1aSpiderSubnet \u6ce8\u89e3\u3002 SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 IPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002\u521b\u5efa\u5e94\u7528\u65f6\u53ef\u4ee5\u4f7f\u7528 ipam.spidernet.io/subnets \u6216 ipam.spidernet.io/subnet \u6ce8\u89e3\u6307\u5b9a Subnet\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u53d6 IP \u5730\u5740\u81ea\u52a8\u521b\u5efa IP \u6c60\uff0c\u5e76\u4ece\u6c60\u4e2d\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\u7ed9\u5e94\u7528\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderSubnet \u3002 \u4f18\u5148\u7ea7 2 \uff1aSpiderIPPool \u6ce8\u89e3\u3002 \u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u7ed3\u5408 SpiderIPPool \u6ce8\u89e3 ipam.spidernet.io/ippools \u6216 ipam.spidernet.io/ippool \u80fd\u7ed1\u5b9a\u4e0d\u540c\u7684 IPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 IPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 SpiderIPPool \u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 3 \uff1a\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 IP \u6c60\u3002 \u901a\u8fc7\u5728\u547d\u540d\u7a7a\u95f4\u4e2d\u8bbe\u7f6e\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u6216 ipam.spidernet.io/default-ipv6-ippool \u6307\u5b9a\u9ed8\u8ba4\u7684 IP \u6c60\u3002\u5728\u8be5\u79df\u6237\u4e2d\u521b\u5efa\u5e94\u7528\u65f6\uff0c\u5982\u679c\u6ca1\u6709\u5176\u4ed6\u9ad8\u4f18\u5148\u7ea7\u7684\u6c60\u89c4\u5219\uff0c\u90a3\u4e48\u5c06\u4ece\u8be5\u79df\u6237\u53ef\u7528\u7684\u5019\u9009\u6c60\u4e2d\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\u3002\u6709\u5173\u8be6\u60c5\uff0c\u8bf7\u53c2\u9605 \u547d\u540d\u7a7a\u95f4\u6ce8\u89e3 \u3002 \u4f18\u5148\u7ea7 4 \uff1aCNI \u914d\u7f6e\u6587\u4ef6\u3002 \u901a\u8fc7\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 default_ipv4_ippool \u548c default_ipv6_ippool \u5b57\u6bb5\u8bbe\u7f6e\u5168\u5c40\u7684 CNI \u9ed8\u8ba4\u6c60\uff0c\u5176\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0c\u5f53\u5e94\u7528\u4f7f\u7528\u8be5 CNI \u914d\u7f6e\u7f51\u7edc\u65f6\u5e76\u8c03\u7528 Spiderpool \uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u5e94\u7528\u526f\u672c\uff0cSpiderpool \u90fd\u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u6bcf\u4e2a\u8282\u70b9\u5206\u5c5e\u4e0d\u540c\u7684\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\u7684\u573a\u666f\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684\u8282\u70b9\u4eb2\u548c\u89c4\u5219\uff0cPod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 CNI \u914d\u7f6e \u3002 \u4f18\u5148\u7ea7 5 \uff1a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\u3002 \u5728 SpiderIPPool CR \u5bf9\u8c61\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c06 spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true \uff0c\u5c06\u6c60\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4 IPPool\uff0c\u9ed8\u8ba4\u4e3a false \u3002\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003 \u96c6\u7fa4\u9ed8\u8ba4 IPPool","title":"\u83b7\u53d6\u5019\u9009\u6c60"},{"location":"concepts/ipam-des-zh_CN/#_2","text":"\u901a\u8fc7\u4e0a\u8ff0\u7684\u6c60\u9009\u62e9\u89c4\u5219\uff0c\u83b7\u5f97 IPv4 \u548c IPv6 \u7684 IPPool \u5019\u9009\u540e\uff0cSpiderpool \u4f1a\u6839\u636e\u4ee5\u4e0b\u89c4\u5219\u8fdb\u884c\u8fc7\u6ee4\uff0c\u4e86\u89e3\u54ea\u4e2a\u5019\u9009 IPPool \u53ef\u7528\u3002 IP \u6c60\u5904\u4e8e\u5019\u9009\u8005\u8eab\u4efd\uff0c\u4f46\u5176\u5904\u4e8e terminating \u72b6\u6001\u7684\uff0cSpiderpool \u5c06\u4f1a\u8fc7\u6ee4\u8be5\u6c60\u3002 IP \u6c60\u7684 spec.disable \u5b57\u6bb5\u7528\u4e8e\u8bbe\u7f6e IPPool \u662f\u5426\u53ef\u7528\uff0c\u5f53\u8be5\u503c\u4e3a false \u65f6\uff0c\u610f\u5473\u7740 IPPool \u4e0d\u53ef\u4f7f\u7528\u3002 \u68c0\u67e5 IPPool.Spec.NodeName \u548c IPPool.Spec.NodeAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u8c03\u5ea6\u8282\u70b9\u5339\u914d\u3002 \u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.NamespaceName \u548c IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684\u547d\u540d\u7a7a\u95f4\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.PodAffinity \u5c5e\u6027\u662f\u5426\u4e0e Pod \u7684 matchLabels \u6240\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool.Spec.MultusName \u5c5e\u6027\u662f\u5426\u4e0e Pod \u5f53\u524d NIC Multus \u914d\u7f6e\u5339\u914d\u3002\u5982\u679c\u4e0d\u5339\u914d\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab IPPool \u5b9e\u4f8b\u7684 exclude_ips \u5b57\u6bb5\u6240\u5305\u542b\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 \u68c0\u67e5 IPPool \u6240\u6709 IP \u662f\u4e0d\u662f\u90fd\u88ab ReservedIP \u5b9e\u4f8b\u6240\u4fdd\u7559\u4e86\uff0c\u5982\u679c\u662f\uff0c\u5219\u8be5 IPPool \u5c06\u88ab\u8fc7\u6ee4\u3002 IPPool \u7684\u53ef\u7528 IP \u8d44\u6e90\u88ab\u8017\u5c3d\uff0c\u5219\u8be5 IPPool \u4e5f\u5c06\u88ab\u8fc7\u6ee4\u3002","title":"\u8fc7\u6ee4\u5019\u9009\u6c60"},{"location":"concepts/ipam-des-zh_CN/#_3","text":"\u8fc7\u6ee4\u5019\u9009\u6c60\u540e\uff0c\u53ef\u80fd\u4ecd\u5b58\u5728\u591a\u4e2a\u5019\u9009\u6c60\uff0cSpiderpool \u4f1a\u8fdb\u4e00\u6b65\u4f7f\u7528\u81ea\u5b9a\u4e49\u4f18\u5148\u7ea7\u89c4\u5219\u5bf9\u8fd9\u4e9b\u5019\u9009\u8005\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4ece\u6709\u7a7a\u95f2 IP \u7684 IP \u6c60\u4e2d\u5f00\u59cb\u9009\u62e9 IP \u5730\u5740\u8fdb\u884c\u5206\u914d\u3002 \u5177\u6709 IPPool.Spec.PodAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u9ad8\u4f18\u5148\u7ea7\u3002 \u5177\u6709 IPPool.Spec.NodeName \u6216 IPPool.Spec.NodeAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e8c\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NodeName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NodeAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.NamespaceName \u6216 IPPool.Spec.NamespaceAffinity \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u7b2c\u4e09\u9ad8\u4f18\u5148\u7ea7\u3002\uff08 NamespaceName \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e NamespaceAffinity \uff09\u3002 \u5177\u6709 IPPool.Spec.MultusName \u5c5e\u6027\u7684 IPPool \u8d44\u6e90\u5177\u6709\u6700\u4f4e\u4f18\u5148\u7ea7\u3002 \u6ce8\u610f\uff1a\u8fd9\u91cc\u6709\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u63cf\u8ff0\u8fd9\u4e2a\u89c4\u5219\u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5355\u4e00\u5173\u8054\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolB \u3002 \u5177\u6709\u5355\u4e2a\u5c5e\u6027 IPPool.Spec.PodAffinity \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.NodeName \u548c IPPool.Spec.NamespaceName \u7684 IPPoolB \u3002 \u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u548c IPPool.Spec.NodeName \u7684 IPPoolA \u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u5177\u6709\u5c5e\u6027 IPPool.Spec.PodAffinity \u3001 IPPool.Spec.NamespaceName \u548c IPPool.Spec.MultusName \u7684 IPPoolB \u3002 NOTE\uff1a \u5982\u679c Pod \u5c5e\u4e8e StatefulSet\uff0c\u5219\u4f1a\u4f18\u5148\u5206\u914d\u7b26\u5408\u4e0a\u9762\u89c4\u5219\u7684 IP \u5730\u5740\u3002 \u4e00\u65e6 Pod \u91cd\u65b0\u542f\u52a8\uff0c\u5b83\u5c06\u5c1d\u8bd5\u91cd\u7528\u6700\u540e\u5206\u914d\u7684 IP \u5730\u5740\u3002","title":"\u5019\u9009\u6c60\u6392\u5e8f"},{"location":"concepts/ipam-des-zh_CN/#ip","text":"\u5728 Kubernetes \u4e2d\uff0c\u5783\u573e\u56de\u6536\uff08Garbage Collection\uff0c\u7b80\u79f0GC\uff09\u5bf9\u4e8e IP \u5730\u5740\u7684\u56de\u6536\u975e\u5e38\u91cd\u8981\u3002IP \u5730\u5740\u7684\u53ef\u7528\u6027\u5173\u7cfb\u5230 Pod \u662f\u5426\u80fd\u591f\u542f\u52a8\u6210\u529f\u3002GC \u673a\u5236\u53ef\u4ee5\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u4e0d\u518d\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u548c IP \u5730\u5740\u7684\u8017\u5c3d\u3002 \u5728 IPAM \u4e2d\u8bb0\u5f55\u4e86\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u4f46\u662f\u8fd9\u4e9b Pod \u5728 Kubernetes \u96c6\u7fa4\u4e2d\u5df2\u7ecf\u4e0d\u590d\u5b58\u5728\uff0c\u8fd9\u4e9b IP \u53ef\u79f0\u4e3a \u50f5\u5c38 IP \uff0cSpiderpool \u53ef\u9488\u5bf9 \u50f5\u5c38 IP \u8fdb\u884c\u56de\u6536\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u5728\u96c6\u7fa4\u4e2d delete Pod \u65f6\uff0c\u4f46\u7531\u4e8e \u7f51\u7edc\u5f02\u5e38 \u6216 cni \u4e8c\u8fdb\u5236 crash \u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8c03\u7528 cni delete \u5931\u8d25\uff0c\u4ece\u800c\u5bfc\u81f4 IP \u5730\u5740\u65e0\u6cd5\u88ab cni \u56de\u6536\u3002 \u5728 cni delete \u5931\u8d25 \u7b49\u6545\u969c\u573a\u666f\uff0c\u5982\u679c\u4e00\u4e2a\u66fe\u7ecf\u5206\u914d\u4e86 IP \u7684 Pod \u88ab\u9500\u6bc1\u540e\uff0c\u4f46\u5728 IPAM \u4e2d\u8fd8\u8bb0\u5f55\u5206\u914d\u7740IP \u5730\u5740\uff0c\u5f62\u6210\u4e86\u50f5\u5c38 IP \u7684\u73b0\u8c61\u3002Spiderpool \u9488\u5bf9\u8fd9\u79cd\u95ee\u9898\uff0c\u4f1a\u57fa\u4e8e\u5468\u671f\u548c\u4e8b\u4ef6\u626b\u63cf\u673a\u5236\uff0c\u81ea\u52a8\u56de\u6536\u8fd9\u4e9b\u50f5\u5c38 IP \u5730\u5740\u3002 \u8282\u70b9\u610f\u5916\u5b95\u673a\u540e\uff0c\u96c6\u7fa4\u4e2d\u7684 Pod \u6c38\u4e45\u5904\u4e8e deleting \u72b6\u6001\uff0cPod \u5360\u7528\u7684 IP \u5730\u5740\u65e0\u6cd5\u88ab\u91ca\u653e\u3002 \u5bf9\u5904\u4e8e Terminating \u72b6\u6001\u7684 Pod\uff0cSpiderpool \u5c06\u5728 Pod \u7684 spec.terminationGracePeriodSecond \u540e\uff0c\u81ea\u52a8\u91ca\u653e\u5176 IP \u5730\u5740\u3002\u8be5\u529f\u80fd\u53ef\u901a\u8fc7\u73af\u5883\u53d8\u91cf SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED \u6765\u63a7\u5236\u3002\u8be5\u80fd\u529b\u80fd\u591f\u7528\u4ee5\u89e3\u51b3 \u8282\u70b9\u610f\u5916\u5b95\u673a \u7684\u6545\u969c\u573a\u666f\u3002","title":"IP \u56de\u6536\u673a\u5236"},{"location":"concepts/ipam-des/","text":"IPAM English | \u7b80\u4f53\u4e2d\u6587 IPAM for Underlay and overlay network solutions There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool. IPAM for overlay networks These solutions implement the decoupling of the Pod network and host network, such as Calico , Cilium and other CNI plugins. Typically, they use tunnel technology such as vxlan to build an overlay network plane, and use NAT technology for north-south traffic. These IPAM solutions have the following characteristics: Divide Pod subnets into node-based IP blocks In terms of a smaller subnet mask, the Pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account. First, since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, achieving more efficient allocation. Second, a specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound Pod. Sufficient IP address resources Subnets not overlapping with any CIDR, could be used by the cluster, so the cluster has enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP addresses. No requirement for static IP addresses For the static IP address requirement, there is a difference between a stateless application and a stateful application. Regarding stateless application like deployment, the Pod's name will change when the Pod restarts, and the business logic of the application itself is stateless. Thus static IP addresses means that all the Pod replicas are fixed in a set of IP addresses; for stateful applications such as statefulset, considering both the fixed information including Pod's names and stateful business logic, the strong binding of one Pod and one specific IP address needs to be implemented for static IP addresses. The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology, and realizes the east-west communication through DNS, clusterIP and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas follow the scheduling. Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addressed\", or support it in a rough way. The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment, and can provide independent subnets with sufficient IP addresses for Pods. IPAM for underlay networks These solutions share the node's network for Pods, which means Pods can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications. There are two typical scenarios for underlay network solutions: clusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\": An IP address able to be assigned to any node As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the Pod replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together. Therefore, an IP address should be able to be allocated to a Pod on any node. Different replicas within one application could obtain IP addresses across subnets Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need to be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes. Static IP addresses For some traditional applications, the source IPs or destination IPs need to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means. Therefore, in order to reduce the transformation chores after the applications move to the Kubernetes, applications need static IP addresses. Pods with Multiple NICs need IP addresses of different underlay subnets Since the Pod is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets. IP conflict Underlay networks are more prone to IP conflicts. For instance, Pods conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection. Release and recover IP addresses Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched Pod may fail due to the lack of IP addresses owing to some IP addresses not being released by abnormal Pods. This requires IPAMs to have a more accurate, efficient and timely IP recovery mechanism. The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of Pod communication; no tunneling technology contributes to improved throughput and latency performance of network communications. Spiderpool IPAM Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool IPAM, such as: Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI IP Allocation Algorithm When creating a Pod, it will follow the steps below to get IP allocations.The lifecycle of IP allocation involves three major stages: candidate pool acquisition , candidate pool filtering , and candidate pool sorting . Candidate pool acquisition: Spiderpool follows a strict rule of selecting pools from high to low priority . It identifies all pools that match the high priority rules and marks them as candidates for further consideration. Candidate pool filtering: Spiderpool applies filtering mechanisms such as affinity to carefully select the appropriate candidate pools from the available options. This ensures that specific requirements or complex usage scenarios are satisfied. Candidate pool sorting: in cases where multiple candidate pools exist, Spiderpool sorts them based on the priority rules defined in the SpiderIPPool object. IP addresses are then allocated sequentially, starting from the pool with available addresses. Candidate Pool Acquisition Spiderpool offers a variety of pool selection rules when assigning IP addresses to Pods. The selection process strictly adheres to a high to low priority order. The following rules are listed in descending order of priority , and if multiple rules apply at the same time, the preceding rule will overwrite the subsequent one. The 1st priority: SpiderSubnet annotation The SpiderSubnet resource represents a collection of IP addresses. When an application requires a fixed IP address, the application administrator needs to inform their platform counterparts about the available IP addresses and routing attributes. However, as they belong to different operational departments, this process becomes cumbersome, resulting in complex workflows for creating each application. To simplify this, Spiderpool's SpiderSubnet feature automatically allocates IP addresses from subnets to IPPool and assigns fixed IP addresses to applications. This greatly reduces operational costs. When creating an application, you can use the ipam.spidernet.io/subnets or ipam.spidernet.io/subnet annotation to specify the Subnet. This allows for the automatic creation of an IP pool by randomly selecting IP addresses from the subnet, which can then be allocated as fixed IPs for the application. For more details, please refer to SpiderSubnet . The 2nd priority: SpiderIPPool annotation Different IP addresses within a Subnet can be stored in separate instances of IPPool (Spiderpool ensures that there is no overlap between the address sets of IPPools). The size of the IP collection in SpiderIPPool can vary based on requirements. This design feature is particularly beneficial when dealing with limited IP address resources in the Underlay network. When creating an application, the SpiderIPPool annotation ipam.spidernet.io/ippools or ipam.spidernet.io/ippool can be used to bind different IPPools or share the same IPPool. This allows all applications to share the same Subnet while maintaining \"micro-isolation\". For more details, please refer to SpiderIPPool annotation . The 3th priority: namespace default IP pool By setting the annotation ipam.spidernet.io/default-ipv4-ippool or ipam.spidernet.io/default-ipv6-ippool in the namespace, you can specify the default IP pool. When creating an application within that tenant, if there are no other higher-priority pool rules, it will attempt to allocate an IP address from the available candidate pools for that tenant. For more details, please refer to Namespace Annotation . The fourth priority: CNI configuration file The global CNI default pool can be set by configuring the default_ipv4_ippool and default_ipv6_ippool fields in the CNI configuration file. Multiple IP pools can be defined as alternative pools. When an application uses this CNI configuration network and invokes Spiderpool, each application replica is sequentially assigned an IP address according to the order of elements in the \"IP pool array\". In scenarios where nodes belong to different regions or data centers, if the node where an application replica scheduled matches the node affinity rule of the first IP pool, the Pod obtains an IP from that pool. If it doesn't meet the criteria, Spiderpool attempts to assign an IP from the alternative pools until all options have been exhausted. For more information, please refer to CNI Configuration . The fifth priority: cluster's default IP pool Within the SpiderIPPool CR object, setting the spec.default field to true designates the pool as the cluster's default IP pool (default value is false ). For more information, please refer to Cluster's Default IP Pool . Candidate Pool Filtering To determine the availability of candidate IP pools for IPv4 and IPv6, Spiderpool filters them using the following rules: IP pools in the terminating state are filtered out. The spec.disable field of an IP pool indicates its availability. A value of false means the IP pool is not usable. Check if the IPPool.Spec.NodeName and IPPool.Spec.NodeAffinity match the Pod's scheduling node. Mismatching values result in filtering out the IP pool. Check if the IPPool.Spec.NamespaceName and IPPool.Spec.NamespaceAffinity match the Pod's namespace. Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.NamespaceName matches the Pod's matchLabels . Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.MultusName matches the current NIC Multus configuration of the Pod. If there is no match, the IP pool is filtered out. Check if all IPs within the IP pool are included in the IPPool instance's exclude_ips field. If it is, the IP pool is filtered out. Check if all IPs in the pool are reserved in the ReservedIP instance. If it is, the IP pool is filtered out. An IP pool will be filtered out if its available IP resources are exhausted. Candidate Pool Sorting After filtering the candidate pools, Spiderpool may have multiple pools remaining. To determine the order of IP address allocation, Spiderpool applies custom priority rules to sort these candidates. IP addresses are then selected from the pools with available IPs in the following manner: IP pool resources with the IPPool.Spec.PodAffinity property are given the highest priority. IPPool resources with either the IPPool.Spec.NodeName or IPPool.Spec.NodeAffinity property are given the secondary priority. The NodeName takes precedence over NodeAffinity . Following that, IP pool resources with either the IPPool.Spec.NamespaceName or IPPool.Spec.NamespaceAffinity property maintain the third-highest priority. The NamespaceName takes precedence over NamespaceAffinity . IP pool resources with the IPPool.Spec.MultusName property receive the lowest priority. Here are some simple instances to describe this rule. IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with single affinity property IPPool.Spec.PodAffinity . IPPoolA with single property IPPool.Spec.PodAffinity has higher priority than IPPoolB with properties IPPool.Spec.NodeName and IPPool.Spec.NamespaceName . IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with properties IPPool.Spec.PodAffinity , IPPool.Spec.NamespaceName and IPPool.Spec.MultusName . If a Pod belongs to StatefulSet, IP addresses that meet the aforementioned rules will be allocated with priority. When a Pod is restarted, it will attempt to reuse the previously assigned IP address. IP Garbage Collection Context When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod. when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call CNI plugin. A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: the new pod may fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue. Solution For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time. SpiderIPPool garbage collection To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted. SpiderEndpoint garbage collection Once a pod is created and gets IPs from SpiderIPPool , Spiderpool will create a corresponding SpiderEndpoint object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to OwnerReference with the pod. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then spiderpool controller will remove the Current data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up) In Kubernetes, garbage collection (Garbage Collection, GC for short) is very important for the recycling of IP addresses. The availability of IP addresses is critical to whether a Pod can start successfully. The GC mechanism can automatically reclaim these unused IP addresses, avoiding waste of resources and exhaustion of IP addresses. This article will introduce Spiderpool's excellent GC capabilities. Project Functions The IP addresses assigned to Pods are recorded in IPAM, but these Pods no longer exist in the Kubernetes cluster. These IPs can be called zombie IPs . Spiderpool can recycle zombie IPs . Its implementation principle is as follows : When deleting Pod in the cluster, but due to problems such as network exception or cni binary crash , the call to cni delete fails, resulting in the IP address not being reclaimed by cni. In failure scenarios such as cni delete failure , if a Pod that has been assigned an IP is destroyed, but the IP address is still recorded in the IPAM, a phenomenon of zombie IP is formed. For this kind of problem, Spiderpool will automatically recycle these zombie IP addresses based on the cycle and event scanning mechanism. After a node goes down unexpectedly, the Pod in the cluster is permanently in the deleting state, and the IP address occupied by the Pod cannot be released. For a Pod in Terminating state, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond . This feature can be controlled by the environment variable SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED . This capability can be used to solve the failure scenario of unexpected node downtime .","title":"IPAM"},{"location":"concepts/ipam-des/#ipam","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"IPAM"},{"location":"concepts/ipam-des/#ipam-for-underlay-and-overlay-network-solutions","text":"There are two technologies in cloud-native networking: \"overlay network\" and \"underlay network\". Despite no strict definition for underlay and overlay networks in cloud-native networking, we can simply abstract their characteristics from many CNI projects. The two technologies meet the needs of different scenarios. Spiderpool is designed for underlay networks, and the following comparison of the two solutions can better illustrate the features and usage scenarios of Spiderpool.","title":"IPAM for Underlay and overlay network solutions"},{"location":"concepts/ipam-des/#ipam-for-overlay-networks","text":"These solutions implement the decoupling of the Pod network and host network, such as Calico , Cilium and other CNI plugins. Typically, they use tunnel technology such as vxlan to build an overlay network plane, and use NAT technology for north-south traffic. These IPAM solutions have the following characteristics: Divide Pod subnets into node-based IP blocks In terms of a smaller subnet mask, the Pod subnet is divided into smaller IP blocks, and each node is assigned one or more IP blocks depending on the actual IP allocation account. First, since the IPAM plugin on each node only needs to allocate and release IP addresses in the local IP block, there is no IP allocation conflict with IPAM on other nodes, achieving more efficient allocation. Second, a specific IP address follows an IP block and is allocated within one node all the time, so it cannot be assigned on other nodes together with a bound Pod. Sufficient IP address resources Subnets not overlapping with any CIDR, could be used by the cluster, so the cluster has enough IP address resources as long as NAT technology is used in an appropriate manner. As a result, IPAM components face less pressure to reclaim abnormal IP addresses. No requirement for static IP addresses For the static IP address requirement, there is a difference between a stateless application and a stateful application. Regarding stateless application like deployment, the Pod's name will change when the Pod restarts, and the business logic of the application itself is stateless. Thus static IP addresses means that all the Pod replicas are fixed in a set of IP addresses; for stateful applications such as statefulset, considering both the fixed information including Pod's names and stateful business logic, the strong binding of one Pod and one specific IP address needs to be implemented for static IP addresses. The \"overlay network solution\" mostly exposes the ingress and source addresses of services to the outside of the cluster with the help of NAT technology, and realizes the east-west communication through DNS, clusterIP and other technologies. In addition, although the IP block of IPAM fixes the IP to one node, it does not guarantee the application replicas follow the scheduling. Therefore, there is no scope for the static IP address capability. Most of the mainstream CNIs in the community have not yet supported \"static IP addressed\", or support it in a rough way. The advantage of the \"overlay network solution\" is that the CNI plugins are highly compatible with any underlying network environment, and can provide independent subnets with sufficient IP addresses for Pods.","title":"IPAM for overlay networks"},{"location":"concepts/ipam-des/#ipam-for-underlay-networks","text":"These solutions share the node's network for Pods, which means Pods can directly obtain IP addresses in the node network. Thus, applications can directly use their own IP addresses for east-west and north-south communications. There are two typical scenarios for underlay network solutions: clusters deployed on a \"legacy network\" and clusters deployed on an IAAS environment, such as a public cloud. The following summarizes the IPAM characteristics of the \"legacy network scenario\": An IP address able to be assigned to any node As the number of network devices in the data center increases and multi-cluster technology evolves, IPv4 address resources become scarce, thus requiring IPAM to improve the efficiency of IP usage. As the Pod replicas of the applications requiring \"static IP addresses\" could be scheduled to any node in the cluster and drift between nodes, IP addresses might drift together. Therefore, an IP address should be able to be allocated to a Pod on any node. Different replicas within one application could obtain IP addresses across subnets Take as an example one node could access subnet 172.20.1.0/24 while another node just only access subnet 172.20.2.0/24. In this case, when the replicas within one application need to be deployed across subnets, IPAM is required to be able to assign subnet-matched IP addresses to the application on different nodes. Static IP addresses For some traditional applications, the source IPs or destination IPs need to be sensed in the microservice. And network admins are used to enabling fine-grained network security control via firewalls and other means. Therefore, in order to reduce the transformation chores after the applications move to the Kubernetes, applications need static IP addresses. Pods with Multiple NICs need IP addresses of different underlay subnets Since the Pod is connected to an underlay network, it has the need for multiple NICs to reach different underlay subnets. IP conflict Underlay networks are more prone to IP conflicts. For instance, Pods conflict with host IPs outside the cluster, or conflict with other clusters under the same subnet. But it is difficult for IPAM to discover these conflicting IP addresses externally unless CNI plugins are involved for real-time IP conflict detection. Release and recover IP addresses Because of the scarcity of IP addresses in underlay networks and the static IP address requirements of applications, a newly launched Pod may fail due to the lack of IP addresses owing to some IP addresses not being released by abnormal Pods. This requires IPAMs to have a more accurate, efficient and timely IP recovery mechanism. The advantages of the underlay network solution include: no need for network NAT mapping, which makes cloud-based network transformation for applications way more convenient; the underlying network firewall and other devices can achieve relatively fine control of Pod communication; no tunneling technology contributes to improved throughput and latency performance of network communications.","title":"IPAM for underlay networks"},{"location":"concepts/ipam-des/#spiderpool-ipam","text":"Any CNI project compatible with third-party IPAM plugins can work well with Spiderpool IPAM, such as: Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI , Multus CNI , Calico CNI , Weave CNI","title":"Spiderpool IPAM"},{"location":"concepts/ipam-des/#ip-allocation-algorithm","text":"When creating a Pod, it will follow the steps below to get IP allocations.The lifecycle of IP allocation involves three major stages: candidate pool acquisition , candidate pool filtering , and candidate pool sorting . Candidate pool acquisition: Spiderpool follows a strict rule of selecting pools from high to low priority . It identifies all pools that match the high priority rules and marks them as candidates for further consideration. Candidate pool filtering: Spiderpool applies filtering mechanisms such as affinity to carefully select the appropriate candidate pools from the available options. This ensures that specific requirements or complex usage scenarios are satisfied. Candidate pool sorting: in cases where multiple candidate pools exist, Spiderpool sorts them based on the priority rules defined in the SpiderIPPool object. IP addresses are then allocated sequentially, starting from the pool with available addresses.","title":"IP Allocation Algorithm"},{"location":"concepts/ipam-des/#candidate-pool-acquisition","text":"Spiderpool offers a variety of pool selection rules when assigning IP addresses to Pods. The selection process strictly adheres to a high to low priority order. The following rules are listed in descending order of priority , and if multiple rules apply at the same time, the preceding rule will overwrite the subsequent one. The 1st priority: SpiderSubnet annotation The SpiderSubnet resource represents a collection of IP addresses. When an application requires a fixed IP address, the application administrator needs to inform their platform counterparts about the available IP addresses and routing attributes. However, as they belong to different operational departments, this process becomes cumbersome, resulting in complex workflows for creating each application. To simplify this, Spiderpool's SpiderSubnet feature automatically allocates IP addresses from subnets to IPPool and assigns fixed IP addresses to applications. This greatly reduces operational costs. When creating an application, you can use the ipam.spidernet.io/subnets or ipam.spidernet.io/subnet annotation to specify the Subnet. This allows for the automatic creation of an IP pool by randomly selecting IP addresses from the subnet, which can then be allocated as fixed IPs for the application. For more details, please refer to SpiderSubnet . The 2nd priority: SpiderIPPool annotation Different IP addresses within a Subnet can be stored in separate instances of IPPool (Spiderpool ensures that there is no overlap between the address sets of IPPools). The size of the IP collection in SpiderIPPool can vary based on requirements. This design feature is particularly beneficial when dealing with limited IP address resources in the Underlay network. When creating an application, the SpiderIPPool annotation ipam.spidernet.io/ippools or ipam.spidernet.io/ippool can be used to bind different IPPools or share the same IPPool. This allows all applications to share the same Subnet while maintaining \"micro-isolation\". For more details, please refer to SpiderIPPool annotation . The 3th priority: namespace default IP pool By setting the annotation ipam.spidernet.io/default-ipv4-ippool or ipam.spidernet.io/default-ipv6-ippool in the namespace, you can specify the default IP pool. When creating an application within that tenant, if there are no other higher-priority pool rules, it will attempt to allocate an IP address from the available candidate pools for that tenant. For more details, please refer to Namespace Annotation . The fourth priority: CNI configuration file The global CNI default pool can be set by configuring the default_ipv4_ippool and default_ipv6_ippool fields in the CNI configuration file. Multiple IP pools can be defined as alternative pools. When an application uses this CNI configuration network and invokes Spiderpool, each application replica is sequentially assigned an IP address according to the order of elements in the \"IP pool array\". In scenarios where nodes belong to different regions or data centers, if the node where an application replica scheduled matches the node affinity rule of the first IP pool, the Pod obtains an IP from that pool. If it doesn't meet the criteria, Spiderpool attempts to assign an IP from the alternative pools until all options have been exhausted. For more information, please refer to CNI Configuration . The fifth priority: cluster's default IP pool Within the SpiderIPPool CR object, setting the spec.default field to true designates the pool as the cluster's default IP pool (default value is false ). For more information, please refer to Cluster's Default IP Pool .","title":"Candidate Pool Acquisition"},{"location":"concepts/ipam-des/#candidate-pool-filtering","text":"To determine the availability of candidate IP pools for IPv4 and IPv6, Spiderpool filters them using the following rules: IP pools in the terminating state are filtered out. The spec.disable field of an IP pool indicates its availability. A value of false means the IP pool is not usable. Check if the IPPool.Spec.NodeName and IPPool.Spec.NodeAffinity match the Pod's scheduling node. Mismatching values result in filtering out the IP pool. Check if the IPPool.Spec.NamespaceName and IPPool.Spec.NamespaceAffinity match the Pod's namespace. Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.NamespaceName matches the Pod's matchLabels . Mismatching values lead to filtering out the IP pool. Check if the IPPool.Spec.MultusName matches the current NIC Multus configuration of the Pod. If there is no match, the IP pool is filtered out. Check if all IPs within the IP pool are included in the IPPool instance's exclude_ips field. If it is, the IP pool is filtered out. Check if all IPs in the pool are reserved in the ReservedIP instance. If it is, the IP pool is filtered out. An IP pool will be filtered out if its available IP resources are exhausted.","title":"Candidate Pool Filtering"},{"location":"concepts/ipam-des/#candidate-pool-sorting","text":"After filtering the candidate pools, Spiderpool may have multiple pools remaining. To determine the order of IP address allocation, Spiderpool applies custom priority rules to sort these candidates. IP addresses are then selected from the pools with available IPs in the following manner: IP pool resources with the IPPool.Spec.PodAffinity property are given the highest priority. IPPool resources with either the IPPool.Spec.NodeName or IPPool.Spec.NodeAffinity property are given the secondary priority. The NodeName takes precedence over NodeAffinity . Following that, IP pool resources with either the IPPool.Spec.NamespaceName or IPPool.Spec.NamespaceAffinity property maintain the third-highest priority. The NamespaceName takes precedence over NamespaceAffinity . IP pool resources with the IPPool.Spec.MultusName property receive the lowest priority. Here are some simple instances to describe this rule. IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with single affinity property IPPool.Spec.PodAffinity . IPPoolA with single property IPPool.Spec.PodAffinity has higher priority than IPPoolB with properties IPPool.Spec.NodeName and IPPool.Spec.NamespaceName . IPPoolA with properties IPPool.Spec.PodAffinity and IPPool.Spec.NodeName has higher priority than IPPoolB with properties IPPool.Spec.PodAffinity , IPPool.Spec.NamespaceName and IPPool.Spec.MultusName . If a Pod belongs to StatefulSet, IP addresses that meet the aforementioned rules will be allocated with priority. When a Pod is restarted, it will attempt to reuse the previously assigned IP address.","title":"Candidate Pool Sorting"},{"location":"concepts/ipam-des/#ip-garbage-collection","text":"","title":"IP Garbage Collection"},{"location":"concepts/ipam-des/#context","text":"When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still marked as used by a nonexistent pod. when some errors happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When a CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fails to call CNI plugin. A node breaks down and then always can not recover, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing the CNI binary on a host when pod deletion. This issue will make a bad result: the new pod may fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual number of pods does not grow. Some CNI or IPAM plugins could not handle this issue. For some CNIs, the administrator self needs to find the IP with this issue and use a CLI tool to reclaim them. For some CNIs, it runs an interval job to find the IP with this issue and not reclaim them in time. For some CNIs, there is not any mechanism at all to fix the IP issue.","title":"Context"},{"location":"concepts/ipam-des/#solution","text":"For some CNIs, its IP CIDR is big enough, so the leaked IP issue is not urgent. For Spiderpool, all IP resources are managed by administrator, and an application will be bound to a fixed IP, so the IP reclaim can be finished in time.","title":"Solution"},{"location":"concepts/ipam-des/#spiderippool-garbage-collection","text":"To prevent IP from leaking when the ippool resource is deleted, Spiderpool has some rules: For an ippool, if IP still taken by pods, Spiderpool uses webhook to reject deleting request of the ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool sets a finalizer by the spiderpool controller once it is created. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IPs in the ippool are free, then the ippool object will be deleted.","title":"SpiderIPPool garbage collection"},{"location":"concepts/ipam-des/#spiderendpoint-garbage-collection","text":"Once a pod is created and gets IPs from SpiderIPPool , Spiderpool will create a corresponding SpiderEndpoint object at the same time. It will take a finalizer (except the StatefulSet pod) and will be set to OwnerReference with the pod. When a pod is deleted, Spiderpool will release its IPs with the recorded data by a corresponding SpiderEndpoint object, then spiderpool controller will remove the Current data of SpiderEndpoint object and remove its finalizer. (For the StatefulSet SpiderEndpoint , Spiderpool will delete it directly if its Current data was cleaned up) In Kubernetes, garbage collection (Garbage Collection, GC for short) is very important for the recycling of IP addresses. The availability of IP addresses is critical to whether a Pod can start successfully. The GC mechanism can automatically reclaim these unused IP addresses, avoiding waste of resources and exhaustion of IP addresses. This article will introduce Spiderpool's excellent GC capabilities.","title":"SpiderEndpoint garbage collection"},{"location":"concepts/ipam-des/#project-functions","text":"The IP addresses assigned to Pods are recorded in IPAM, but these Pods no longer exist in the Kubernetes cluster. These IPs can be called zombie IPs . Spiderpool can recycle zombie IPs . Its implementation principle is as follows : When deleting Pod in the cluster, but due to problems such as network exception or cni binary crash , the call to cni delete fails, resulting in the IP address not being reclaimed by cni. In failure scenarios such as cni delete failure , if a Pod that has been assigned an IP is destroyed, but the IP address is still recorded in the IPAM, a phenomenon of zombie IP is formed. For this kind of problem, Spiderpool will automatically recycle these zombie IP addresses based on the cycle and event scanning mechanism. After a node goes down unexpectedly, the Pod in the cluster is permanently in the deleting state, and the IP address occupied by the Pod cannot be released. For a Pod in Terminating state, Spiderpool will automatically release its IP address after the Pod's spec.terminationGracePeriodSecond . This feature can be controlled by the environment variable SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED . This capability can be used to solve the failure scenario of unexpected node downtime .","title":"Project Functions"},{"location":"concepts/ipam-performance-zh_CN/","text":"IPAM \u6027\u80fd\u6d4b\u8bd5 \u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u662f\u4e00\u4e2a underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684 IPAM \u548c CNI \u6574\u5408\u80fd\u529b\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u8fd0\u884c\u5728 underlay \u573a\u666f\u4e0b\u7684 IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts \uff0c Kube-OVN \uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u3001 cilium \u5728 1000 Pod \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002 \u80cc\u666f \u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7684\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002 \u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002 underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u7684\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u7684\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002 \u73af\u5883 Kubernetes: v1.26.7 container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi \u6d4b\u8bd5\u5bf9\u8c61 \u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e 0.3.1 \u7248\u672c\u7684 CNI Specification \uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u7248\u672c Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3 \u65b9\u6848 \u6d4b\u8bd5\u601d\u8def\u4e3b\u8981\u662f\uff1a Underlay IP \u8d44\u6e90\u6709\u9650\uff0cIP \u7684\u6cc4\u9732\u548c\u5206\u914d\u91cd\u590d\uff0c\u5bb9\u6613\u9020\u6210\u5e72\u6270 \uff0c\u56e0\u6b64 IP \u5206\u914d\u7684\u51c6\u786e\u6027\u975e\u5e38\u91cd\u8981\u3002 \u5728\u5927\u91cf Pod \u542f\u52a8\u65f6\u7ade\u4e89\u5206\u914d IP\uff0cIPAM \u7684\u5206\u914d\u7b97\u6cd5\u8981\u9ad8\u6548\uff0c\u624d\u80fd\u4fdd\u969c Pod \u5feb\u901f\u53d1\u5e03\u6210\u529f\u3002 \u56e0\u6b64\uff0c\u8bbe\u8ba1\u4e86 IP \u8d44\u6e90\u548c Pod \u8d44\u6e90\u6570\u91cf\u76f8\u540c\u7684\u6781\u9650\u6d4b\u8bd5\uff0c\u8ba1\u65f6 Pod \u4ece\u521b\u5efa\u5230 Running \u7684\u65f6\u95f4\uff0c\u6765\u53d8\u76f8\u6d4b\u8bd5 IPAM \u7684\u7cbe\u786e\u6027\u548c\u5065\u58ee\u6027\u3002\u6d4b\u8bd5\u7684\u6761\u4ef6\u5982\u4e0b\uff1a IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002 \u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002 \u6d4b\u8bd5\u7ed3\u679c \u5982\u4e0b\u5c55\u793a\u4e86 IPAM \u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5176\u4e2d\uff0c\u5305\u542b\u4e86 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u548c \u4e0d\u9650\u5236 IP \u6570\u91cf \u4e24\u79cd\u573a\u666f\uff0c\u6765\u5206\u522b\u6d4b\u8bd5\u6bcf\u4e2a CNI \uff0c\u800c Calico \u548c Cilium \u7b49\u662f\u57fa\u4e8e IP block \u9884\u5206\u914d\u673a\u5236\u5206\u914d IP\uff0c\u56e0\u6b64\u6ca1\u6cd5\u76f8\u5bf9 \"\u516c\u5e73\" \u7684\u8fdb\u884c \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u6d4b\u8bd5\uff0c\u53ea\u8fdb\u884c \u4e0d\u9650\u5236 IP \u6570\u91cf \u573a\u666f\u6d4b\u8bd5\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u4e0d\u9650\u5236 IP \u6570\u91cf Spiderpool based on macvlan 207s 182 Whereabouts based on macvlan \u5931\u8d25 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s \u5206\u6790 Spiderpool \u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6574\u4e2a\u96c6\u7fa4\u8282\u70b9\u7684\u6240\u6709 Pod \u90fd\u4ece\u540c\u4e00\u4e2a CIDR \u4e2d\u5206\u914d IP\uff0c\u6240\u4ee5 IP \u5206\u914d\u548c\u91ca\u653e\u9700\u8981\u9762\u4e34\u6fc0\u70c8\u7684\u7ade\u4e89\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u66f4\u5927\uff1bWhereabouts \u548c Calico \u3001Cilium\u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u5c0f\u7684 IP \u96c6\u5408\uff0c\u6240\u4ee5 IP \u5206\u914d\u7684\u7ade\u4e89\u6bd4\u8f83\u5c0f\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u5c0f\u3002\u4f46\u4ece\u4e0a\u8ff0\u5b9e\u9a8c\u6570\u636e\u4e0a\u770b\uff0c\u867d\u7136 Spdierpool \u7684 IPAM \u539f\u7406\u662f \"\u5403\u4e8f\" \u7684\uff0c\u4f46\u662f\u5206\u914d IP \u7684\u6027\u80fd\u5374\u662f\u5f88\u597d\u7684\u3002 \u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u9047\u5230\u5982\u4e0b\u73b0\u8c61\uff1a Whereabouts based on macvlan\uff1a\u5728 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u573a\u666f\u4e0b\uff0c\u5728 300s \u5185 261 \u4e2a Pod \u4ee5\u8f83\u4e3a\u5300\u901f\u7684\u72b6\u6001\u8fbe\u5230\u4e86 Running \u72b6\u6001\uff0c\u5728 1080s \u65f6\uff0c\u5206\u914d 768 \u4e2a IP \u5730\u5740\u3002\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u5728 2280s \u65f6\u8fbe\u5230 845 \u4e2a\uff0c\u540e\u7eed Whereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002\u7531\u4e8e IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u7b49\u91cf\uff0c\u5982\u679c IPAM \u7ec4\u4ef6\u672a\u80fd\u6b63\u786e\u7684\u56de\u6536 IP\uff0c\u65b0 Pod \u5c06\u56e0\u4e3a\u7f3a\u5c11 IP \u8d44\u6e90\uff0c\u4e14\u65e0\u6cd5\u83b7\u53d6\u5230\u53ef\u7528\u7684 IP\uff0c\u4ece\u800c\u65e0\u6cd5\u542f\u52a8\u3002\u5e76\u4e14\u89c2\u5bdf\u5230\u5728\u542f\u52a8\u5931\u8d25\u7684 Pod \u4e2d\uff0c\u51fa\u73b0\u4e86\u5982\u4e0b\u7684\u4e00\u4e9b\u9519\u8bef\uff1a [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\" \u603b\u7ed3 \u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e Underlay \u7f51\u7edc\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684 IPAM \u80fd\u529b\uff0c\u5176 IP \u5206\u914d\u4ee5\u53ca\u56de\u6536\u7684\u7279\u70b9\u76f8\u8f83\u4e8e\u4e3b\u6d41 Overlay CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u3001\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u7684\u6027\u80fd\u8868\u73b0\u9886\u5148\u4e8e\u540e\u8005\u3002","title":"IPAM \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/ipam-performance-zh_CN/#ipam","text":"\u7b80\u4f53\u4e2d\u6587 | English Spiderpool \u662f\u4e00\u4e2a underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684 IPAM \u548c CNI \u6574\u5408\u80fd\u529b\uff0c\u6b64\u6587\u5c06\u5bf9\u6bd4\u5176\u4e0e\u5e02\u9762\u4e0a\u4e3b\u6d41\u8fd0\u884c\u5728 underlay \u573a\u666f\u4e0b\u7684 IPAM CNI \u63d2\u4ef6\uff08\u5982 Whereabouts \uff0c Kube-OVN \uff09\u4ee5\u53ca\u88ab\u5e7f\u6cdb\u4f7f\u7528\u7684 overlay IPAM CNI \u63d2\u4ef6 calico-ipam \u3001 cilium \u5728 1000 Pod \u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002","title":"IPAM \u6027\u80fd\u6d4b\u8bd5"},{"location":"concepts/ipam-performance-zh_CN/#_1","text":"\u4e3a\u4ec0\u4e48\u8981\u505a underlay IPAM CNI \u63d2\u4ef6\u7684\u6027\u80fd\u6d4b\u8bd5\uff1f IPAM \u5206\u914d IP \u5730\u5740\u7684\u901f\u5ea6\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7684\u51b3\u5b9a\u4e86\u5e94\u7528\u53d1\u5e03\u7684\u901f\u5ea6\u3002 \u5927\u89c4\u6a21\u7684 Kubernetes \u96c6\u7fa4\u5728\u6545\u969c\u6062\u590d\u65f6\uff0cunderlay IPAM \u5f80\u5f80\u4f1a\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002 underlay \u7f51\u7edc\u4e0b\uff0c\u79c1\u6709\u7684 IPv4 \u5730\u5740\u6709\u9650\u3002\u5728\u6709\u9650\u7684 IP \u5730\u5740\u8303\u56f4\u5185\uff0c\u5e76\u53d1\u7684\u521b\u5efa Pod \u4f1a\u6d89\u53ca IP \u5730\u5740\u7684\u62a2\u5360\u4e0e\u51b2\u7a81\uff0c\u80fd\u5426\u5feb\u901f\u7684\u8c03\u8282\u597d\u6709\u9650\u7684 IP \u5730\u5740\u8d44\u6e90\u5177\u6709\u6311\u6218\u3002","title":"\u80cc\u666f"},{"location":"concepts/ipam-performance-zh_CN/#_2","text":"Kubernetes: v1.26.7 container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi","title":"\u73af\u5883"},{"location":"concepts/ipam-performance-zh_CN/#_3","text":"\u672c\u6b21\u6d4b\u8bd5\u57fa\u4e8e 0.3.1 \u7248\u672c\u7684 CNI Specification \uff0c\u4ee5 macvlan \u642d\u914d Spiderpool \u4f5c\u4e3a\u6d4b\u8bd5\u65b9\u6848\uff0c\u5e76\u9009\u62e9\u4e86\u5f00\u6e90\u793e\u533a\u4e2d\u5176\u5b83\u51e0\u79cd\u5e38\u89c1\u7684\u7f51\u7edc\u65b9\u6848\u4f5c\u4e3a\u5bf9\u6bd4\uff1a \u6d4b\u8bd5\u5bf9\u8c61 \u7248\u672c Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3","title":"\u6d4b\u8bd5\u5bf9\u8c61"},{"location":"concepts/ipam-performance-zh_CN/#_4","text":"\u6d4b\u8bd5\u601d\u8def\u4e3b\u8981\u662f\uff1a Underlay IP \u8d44\u6e90\u6709\u9650\uff0cIP \u7684\u6cc4\u9732\u548c\u5206\u914d\u91cd\u590d\uff0c\u5bb9\u6613\u9020\u6210\u5e72\u6270 \uff0c\u56e0\u6b64 IP \u5206\u914d\u7684\u51c6\u786e\u6027\u975e\u5e38\u91cd\u8981\u3002 \u5728\u5927\u91cf Pod \u542f\u52a8\u65f6\u7ade\u4e89\u5206\u914d IP\uff0cIPAM \u7684\u5206\u914d\u7b97\u6cd5\u8981\u9ad8\u6548\uff0c\u624d\u80fd\u4fdd\u969c Pod \u5feb\u901f\u53d1\u5e03\u6210\u529f\u3002 \u56e0\u6b64\uff0c\u8bbe\u8ba1\u4e86 IP \u8d44\u6e90\u548c Pod \u8d44\u6e90\u6570\u91cf\u76f8\u540c\u7684\u6781\u9650\u6d4b\u8bd5\uff0c\u8ba1\u65f6 Pod \u4ece\u521b\u5efa\u5230 Running \u7684\u65f6\u95f4\uff0c\u6765\u53d8\u76f8\u6d4b\u8bd5 IPAM \u7684\u7cbe\u786e\u6027\u548c\u5065\u58ee\u6027\u3002\u6d4b\u8bd5\u7684\u6761\u4ef6\u5982\u4e0b\uff1a IPv4 \u5355\u6808\u548c IPv4/IPv6 \u53cc\u6808\u573a\u666f\u3002 \u521b\u5efa 100 \u4e2a Deployment\uff0c\u6bcf\u4e2a Deployment \u7684\u526f\u672c\u6570\u4e3a 10\u3002","title":"\u65b9\u6848"},{"location":"concepts/ipam-performance-zh_CN/#_5","text":"\u5982\u4e0b\u5c55\u793a\u4e86 IPAM \u6027\u80fd\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5176\u4e2d\uff0c\u5305\u542b\u4e86 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u548c \u4e0d\u9650\u5236 IP \u6570\u91cf \u4e24\u79cd\u573a\u666f\uff0c\u6765\u5206\u522b\u6d4b\u8bd5\u6bcf\u4e2a CNI \uff0c\u800c Calico \u548c Cilium \u7b49\u662f\u57fa\u4e8e IP block \u9884\u5206\u914d\u673a\u5236\u5206\u914d IP\uff0c\u56e0\u6b64\u6ca1\u6cd5\u76f8\u5bf9 \"\u516c\u5e73\" \u7684\u8fdb\u884c \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u6d4b\u8bd5\uff0c\u53ea\u8fdb\u884c \u4e0d\u9650\u5236 IP \u6570\u91cf \u573a\u666f\u6d4b\u8bd5\u3002 \u6d4b\u8bd5\u5bf9\u8c61 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u4e0d\u9650\u5236 IP \u6570\u91cf Spiderpool based on macvlan 207s 182 Whereabouts based on macvlan \u5931\u8d25 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s","title":"\u6d4b\u8bd5\u7ed3\u679c"},{"location":"concepts/ipam-performance-zh_CN/#_6","text":"Spiderpool \u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6574\u4e2a\u96c6\u7fa4\u8282\u70b9\u7684\u6240\u6709 Pod \u90fd\u4ece\u540c\u4e00\u4e2a CIDR \u4e2d\u5206\u914d IP\uff0c\u6240\u4ee5 IP \u5206\u914d\u548c\u91ca\u653e\u9700\u8981\u9762\u4e34\u6fc0\u70c8\u7684\u7ade\u4e89\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u66f4\u5927\uff1bWhereabouts \u548c Calico \u3001Cilium\u7684 IPAM \u5206\u914d\u539f\u7406\uff0c\u662f\u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u5c0f\u7684 IP \u96c6\u5408\uff0c\u6240\u4ee5 IP \u5206\u914d\u7684\u7ade\u4e89\u6bd4\u8f83\u5c0f\uff0cIP \u5206\u914d\u6027\u80fd\u7684\u6311\u6218\u4f1a\u5c0f\u3002\u4f46\u4ece\u4e0a\u8ff0\u5b9e\u9a8c\u6570\u636e\u4e0a\u770b\uff0c\u867d\u7136 Spdierpool \u7684 IPAM \u539f\u7406\u662f \"\u5403\u4e8f\" \u7684\uff0c\u4f46\u662f\u5206\u914d IP \u7684\u6027\u80fd\u5374\u662f\u5f88\u597d\u7684\u3002 \u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u9047\u5230\u5982\u4e0b\u73b0\u8c61\uff1a Whereabouts based on macvlan\uff1a\u5728 \u9650\u5236 IP \u4e0e Pod \u7b49\u91cf \u573a\u666f\u4e0b\uff0c\u5728 300s \u5185 261 \u4e2a Pod \u4ee5\u8f83\u4e3a\u5300\u901f\u7684\u72b6\u6001\u8fbe\u5230\u4e86 Running \u72b6\u6001\uff0c\u5728 1080s \u65f6\uff0c\u5206\u914d 768 \u4e2a IP \u5730\u5740\u3002\u81ea\u6b64\u4e4b\u540e\u7684 Pod \u589e\u957f\u901f\u7387\u5927\u5e45\u964d\u4f4e\uff0c\u5728 2280s \u65f6\u8fbe\u5230 845 \u4e2a\uff0c\u540e\u7eed Whereabouts \u5c31\u57fa\u672c\u4e0d\u5de5\u4f5c\u4e86\uff0c\u8017\u65f6\u7c7b\u6bd4\u4e8e\u6b63\u65e0\u7a77\u3002\u7531\u4e8e IP \u5730\u5740\u6570\u91cf\u4e0e Pod \u6570\u7b49\u91cf\uff0c\u5982\u679c IPAM \u7ec4\u4ef6\u672a\u80fd\u6b63\u786e\u7684\u56de\u6536 IP\uff0c\u65b0 Pod \u5c06\u56e0\u4e3a\u7f3a\u5c11 IP \u8d44\u6e90\uff0c\u4e14\u65e0\u6cd5\u83b7\u53d6\u5230\u53ef\u7528\u7684 IP\uff0c\u4ece\u800c\u65e0\u6cd5\u542f\u52a8\u3002\u5e76\u4e14\u89c2\u5bdf\u5230\u5728\u542f\u52a8\u5931\u8d25\u7684 Pod \u4e2d\uff0c\u51fa\u73b0\u4e86\u5982\u4e0b\u7684\u4e00\u4e9b\u9519\u8bef\uff1a [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\"","title":"\u5206\u6790"},{"location":"concepts/ipam-performance-zh_CN/#_7","text":"\u867d\u7136 Spiderpool \u662f\u4e00\u79cd\u9002\u7528\u4e8e Underlay \u7f51\u7edc\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684 IPAM \u80fd\u529b\uff0c\u5176 IP \u5206\u914d\u4ee5\u53ca\u56de\u6536\u7684\u7279\u70b9\u76f8\u8f83\u4e8e\u4e3b\u6d41 Overlay CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u9762\u4e34\u7740\u66f4\u591a\u7684\u3001\u590d\u6742\u7684 IP \u5730\u5740\u62a2\u5360\u4e0e\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u7684\u6027\u80fd\u8868\u73b0\u9886\u5148\u4e8e\u540e\u8005\u3002","title":"\u603b\u7ed3"},{"location":"concepts/ipam-performance/","text":"IPAM Performance Testing English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is an underlay networking solution that provides rich IPAM and CNI integration capabilities, this article will compare it with the mainstream IPAM CNI plug-ins (e.g. Whereabouts , Kube-OVN ) and the widely used IPAM CNI plug-ins that are running in underlay scenarios. This article will compare it with the mainstream IPAM CNI plug-ins running in underlay scenarios (e.g., Whereabouts , Kube-OVN ) and the widely-used overlay IPAM CNI plugins calico-ipam \u3001 cilium in 1000 Pod scenarios. Background Why do we need to do performance testing on the underlay IPAM CNI plugin? The speed at which IPAM allocates IP addresses largely determines the speed of application publishing. Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures. Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources. ENV Kubernetes: v1.26.7 container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi Objects This test is based on the 0.3.1 version of CNI Specification , with macvlan and Spiderpool as the test object, and selected several other common network solutions in the open source community as a comparison: test object version Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3 Plan The test ideas are mainly: Underlay IP resources are limited, IP leakage and duplication of IP allocation can easily cause interference, so the accuracy of IP allocation is very important. 2. When a large number of Pods start up and compete for IP allocation, the IPAM allocation algorithm should be efficient in order to ensure that the Pods are released quickly and successfully. Therefore, we designed a limit test with the same number of IP resources and Pod resources, and timed the time from Pod creation to Running to test the accuracy and robustness of IPAM in disguise. The test conditions are as follows: IPv4 single-stack and IPv4/IPv6 dual-stack scenarios. Create 100 Deployments, each with 10 replicas. Result The following shows the results of the IPAM performance test, which includes two scenarios, The number of IPs is equal to the number of Pods and IP sufficient , to test each CNI, whereas Calico and Cilium, for example, are based on the IP block pre-allocation mechanism to allocate IPs, and therefore can't perform the The number of IPs is equal to the number of Pods test in a relatively fair way, and only perform the IP sufficient scenario. We can only test unlimited IPs scenarios. test object Limit IP to Pod Equivalents IP sufficient Spiderpool based on macvlan 207s 182 Whereabouts based on macvlan failure 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s analyze Spiderpool allocate IP addresses from the same CIDR range to all Pods in the whole cluster. Consequently, IP allocation and release face intense competition, presenting larger challenges in terms of IP allocation performance. By comparison, Whereabouts, Calico, and Cilium adopt an IPAM allocation principle where each node has a small IP address pool. This reduces the competition for IP allocation and mitigates the associated performance challenges. However, experimental data shows that despite Spiderpool's \"lossy\" IPAM principle, its IP allocation performance is actually quite good. During testing, the following phenomenon was encountered: Whereabouts based on macvlan\uff1aWe tested the combination of macvlan and Whereabouts in a scenario where the available number of IP addresses matches the number of Pods in a 1:1 ratio. Within 300 seconds, 261 Pods reached the \"Running\" state at a relatively steady pace. By the 1080-second mark, 768 IP addresses were allocated. Afterward, the growth rate of Pods significantly slowed down, reaching 845 Pods by 2280 seconds. Subsequently, Whereabouts essentially stopped working, resulting in a positively near-infinite amount of time needed for further allocation. In our testing scenario, where the number of IP addresses matches the number of Pods in a 1:1 ratio, if the IPAM component fails to properly reclaim IP addresses, new Pods will fail to start due to a lack of available IP resources. And observed some of the following errors in the Pod that failed to start: [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\" Summary Although Spiderpool is primarily designed for underlay networks, it provides powerful IPAM capabilities. Its IP allocation and reclamation features face more intricate challenges, including IP address contention and conflicts, compared to the popular Overlay CNI IPAM plugins. However, Spiderpool's performance is ahead of the latter.","title":"IPAM Performance"},{"location":"concepts/ipam-performance/#ipam-performance-testing","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is an underlay networking solution that provides rich IPAM and CNI integration capabilities, this article will compare it with the mainstream IPAM CNI plug-ins (e.g. Whereabouts , Kube-OVN ) and the widely used IPAM CNI plug-ins that are running in underlay scenarios. This article will compare it with the mainstream IPAM CNI plug-ins running in underlay scenarios (e.g., Whereabouts , Kube-OVN ) and the widely-used overlay IPAM CNI plugins calico-ipam \u3001 cilium in 1000 Pod scenarios.","title":"IPAM Performance Testing"},{"location":"concepts/ipam-performance/#background","text":"Why do we need to do performance testing on the underlay IPAM CNI plugin? The speed at which IPAM allocates IP addresses largely determines the speed of application publishing. Underlay IPAM often becomes a performance bottleneck when a large-scale Kubernetes cluster recovers from failures. Under underlay networks, private IPv4 addresses are limited. Within a limited range of IP addresses, concurrent creation of Pods can involve IP address preemption and conflict, and it is challenging to quickly adjust the limited IP address resources.","title":"Background"},{"location":"concepts/ipam-performance/#env","text":"Kubernetes: v1.26.7 container runtime: containerd v1.7.2 OS: Ubuntu 22.04 LTS kernel: 5.15.0-33-generic Node Role CPU Memory master1 control-plane, worker 3C 8Gi master2 control-plane, worker 3C 8Gi master3 control-plane, worker 3C 8Gi worker4 worker 3C 8Gi worker5 worker 3C 8Gi worker6 worker 3C 8Gi worker7 worker 3C 8Gi worker8 worker 3C 8Gi worker9 worker 3C 8Gi worker10 worker 3C 8Gi","title":"ENV"},{"location":"concepts/ipam-performance/#objects","text":"This test is based on the 0.3.1 version of CNI Specification , with macvlan and Spiderpool as the test object, and selected several other common network solutions in the open source community as a comparison: test object version Spiderpool based on macvlan v0.8.0 Whereabouts based on macvlan v0.6.2 Kube-OVN v1.12.2 Cilium v1.14.3 Calico v3.26.3","title":"Objects"},{"location":"concepts/ipam-performance/#plan","text":"The test ideas are mainly: Underlay IP resources are limited, IP leakage and duplication of IP allocation can easily cause interference, so the accuracy of IP allocation is very important. 2. When a large number of Pods start up and compete for IP allocation, the IPAM allocation algorithm should be efficient in order to ensure that the Pods are released quickly and successfully. Therefore, we designed a limit test with the same number of IP resources and Pod resources, and timed the time from Pod creation to Running to test the accuracy and robustness of IPAM in disguise. The test conditions are as follows: IPv4 single-stack and IPv4/IPv6 dual-stack scenarios. Create 100 Deployments, each with 10 replicas.","title":"Plan"},{"location":"concepts/ipam-performance/#result","text":"The following shows the results of the IPAM performance test, which includes two scenarios, The number of IPs is equal to the number of Pods and IP sufficient , to test each CNI, whereas Calico and Cilium, for example, are based on the IP block pre-allocation mechanism to allocate IPs, and therefore can't perform the The number of IPs is equal to the number of Pods test in a relatively fair way, and only perform the IP sufficient scenario. We can only test unlimited IPs scenarios. test object Limit IP to Pod Equivalents IP sufficient Spiderpool based on macvlan 207s 182 Whereabouts based on macvlan failure 2529s Kube-OVN 405s 343s Cilium NA 215s Calico NA 322s","title":"Result"},{"location":"concepts/ipam-performance/#analyze","text":"Spiderpool allocate IP addresses from the same CIDR range to all Pods in the whole cluster. Consequently, IP allocation and release face intense competition, presenting larger challenges in terms of IP allocation performance. By comparison, Whereabouts, Calico, and Cilium adopt an IPAM allocation principle where each node has a small IP address pool. This reduces the competition for IP allocation and mitigates the associated performance challenges. However, experimental data shows that despite Spiderpool's \"lossy\" IPAM principle, its IP allocation performance is actually quite good. During testing, the following phenomenon was encountered: Whereabouts based on macvlan\uff1aWe tested the combination of macvlan and Whereabouts in a scenario where the available number of IP addresses matches the number of Pods in a 1:1 ratio. Within 300 seconds, 261 Pods reached the \"Running\" state at a relatively steady pace. By the 1080-second mark, 768 IP addresses were allocated. Afterward, the growth rate of Pods significantly slowed down, reaching 845 Pods by 2280 seconds. Subsequently, Whereabouts essentially stopped working, resulting in a positively near-infinite amount of time needed for further allocation. In our testing scenario, where the number of IP addresses matches the number of Pods in a 1:1 ratio, if the IPAM component fails to properly reclaim IP addresses, new Pods will fail to start due to a lack of available IP resources. And observed some of the following errors in the Pod that failed to start: [ default/whereabout-9-5c658db57b-xtjx7:k8s-pod-network ] : error adding container to network \"k8s-pod-network\" : error at storage engine: time limit exceeded while waiting to become leader name \"whereabout-9-5c658db57b-tdlms_default_e1525b95-f433-4dbe-81d9-6c85fd02fa70_1\" is reserved for \"38e7139658f37e40fa7479c461f84ec2777e29c9c685f6add6235fd0dba6e175\"","title":"analyze"},{"location":"concepts/ipam-performance/#summary","text":"Although Spiderpool is primarily designed for underlay networks, it provides powerful IPAM capabilities. Its IP allocation and reclamation features face more intricate challenges, including IP address contention and conflicts, compared to the popular Overlay CNI IPAM plugins. However, Spiderpool's performance is ahead of the latter.","title":"Summary"},{"location":"develop/CODE-OF-CONDUCT/","text":"Code of Conduct This project follows the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"develop/CODE-OF-CONDUCT/#code-of-conduct","text":"This project follows the CNCF Code of Conduct .","title":"Code of Conduct"},{"location":"develop/contributing/","text":"Contribution unitest run the following command to check unitest make unitest-tests setup cluster and run test check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow before start the test, you shoud know there are test scenes as following kind setup cluster test test underlay CNI make e2e_init_underlay make e2e_test_underlay test overlay CNI for calico make e2e_init_overlay_calico make e2e_test_overlay_calico test overlay CNI for cilium make e2e_init_overlay_cilium make e2e_test_overlay_cilium if you are in China, it could add -e E2E_CHINA_IMAGE_REGISTRY=true to pull images from china image registry, add -e HTTP_PROXY=http://${ADDR} to get chart build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init_underlay ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv6 # for china developer not able access ghcr.io # it pulls images from another image registry and just use http proxy to pull chart $ make e2e_init_underlay -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with calico cni $ make e2e_init_calico -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with cilium cni $ make e2e_init_cilium -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test_underlay # run all e2e test on ipv4-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test_underlay -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test_underlay -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test_underlay -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_calico # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_cilium $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${MULTUS_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init_underlay -e E2E_SPIDERPOOL_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} \\ -e E2E_MULTUS_IMAGE_NAME=${MULTUS_IMAGE_NAME} # run all e2e test $ make e2e_test finally, you could visit \" http://HostIp:4040 \" the in the browser of your desktop, and get flamegraph clean make clean_e2e Submit Pull Request A pull request will be checked by following workflow, which is required for merging. Action: your PR should be signed off When you commit your modification, add -s in your commit command git commit -s Action: check yaml files If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml . Action: check golang source code It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label Action: check licenses Any golang or shell file should be licensed correctly. Action: check markdown file Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling . Action: lint yaml file If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml . Action: lint chart Action: lint openapi.yaml Action: check code spell Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case. Changelog How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Contribution Guide"},{"location":"develop/contributing/#contribution","text":"","title":"Contribution"},{"location":"develop/contributing/#unitest","text":"run the following command to check unitest make unitest-tests","title":"unitest"},{"location":"develop/contributing/#setup-cluster-and-run-test","text":"check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow before start the test, you shoud know there are test scenes as following kind setup cluster test test underlay CNI make e2e_init_underlay make e2e_test_underlay test overlay CNI for calico make e2e_init_overlay_calico make e2e_test_overlay_calico test overlay CNI for cilium make e2e_init_overlay_cilium make e2e_test_overlay_cilium if you are in China, it could add -e E2E_CHINA_IMAGE_REGISTRY=true to pull images from china image registry, add -e HTTP_PROXY=http://${ADDR} to get chart build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init_underlay ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=$(pwd)/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init_underlay -e E2E_IP_FAMILY=ipv6 # for china developer not able access ghcr.io # it pulls images from another image registry and just use http proxy to pull chart $ make e2e_init_underlay -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with calico cni $ make e2e_init_calico -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} # setup cluster with cilium cni $ make e2e_init_cilium -e E2E_CHINA_IMAGE_REGISTRY=true -e HTTP_PROXY=http://${ADDR} run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test_underlay # run all e2e test on ipv4-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test_underlay -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test_underlay -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test_underlay -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test_underlay -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_calico # Run all e2e tests for enableSpiderSubnet=false cluster $ make e2e_test_cilium $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${MULTUS_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init_underlay -e E2E_SPIDERPOOL_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} \\ -e E2E_MULTUS_IMAGE_NAME=${MULTUS_IMAGE_NAME} # run all e2e test $ make e2e_test finally, you could visit \" http://HostIp:4040 \" the in the browser of your desktop, and get flamegraph clean make clean_e2e","title":"setup cluster and run test"},{"location":"develop/contributing/#submit-pull-request","text":"A pull request will be checked by following workflow, which is required for merging.","title":"Submit Pull Request"},{"location":"develop/contributing/#action-your-pr-should-be-signed-off","text":"When you commit your modification, add -s in your commit command git commit -s","title":"Action: your PR should be signed off"},{"location":"develop/contributing/#action-check-yaml-files","text":"If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml .","title":"Action: check yaml files"},{"location":"develop/contributing/#action-check-golang-source-code","text":"It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label","title":"Action: check golang source code"},{"location":"develop/contributing/#action-check-licenses","text":"Any golang or shell file should be licensed correctly.","title":"Action: check licenses"},{"location":"develop/contributing/#action-check-markdown-file","text":"Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling .","title":"Action: check markdown file"},{"location":"develop/contributing/#action-lint-yaml-file","text":"If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml .","title":"Action: lint yaml file"},{"location":"develop/contributing/#action-lint-chart","text":"","title":"Action: lint chart"},{"location":"develop/contributing/#action-lint-openapiyaml","text":"","title":"Action: lint openapi.yaml"},{"location":"develop/contributing/#action-check-code-spell","text":"Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Action: check code spell"},{"location":"develop/contributing/#changelog","text":"How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Changelog"},{"location":"develop/release/","text":"workflow for release pre-steps update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml' update version in '/VERSION' a version tag should be set on right branch. The version should go with v0.1.0-rc0 v0.1.0-rc1 v0.1.0 v0.1.1 v0.1.2 v0.2.0-rc0 v0.2.0 update roadmap push a version tag If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"release/feature-new\" to be classified to \"New Features\" label \"release/feature-changed\" to be classified to \"Changed Features\" label \"release/bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml post Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"Release workflow"},{"location":"develop/release/#workflow-for-release","text":"","title":"workflow for release"},{"location":"develop/release/#pre-steps","text":"update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml' update version in '/VERSION' a version tag should be set on right branch. The version should go with v0.1.0-rc0 v0.1.0-rc1 v0.1.0 v0.1.1 v0.1.2 v0.2.0-rc0 v0.2.0 update roadmap","title":"pre-steps"},{"location":"develop/release/#push-a-version-tag","text":"If a tag vx.x.x is pushed , the following steps will automatically run: check the tag name is same with https://github.com/spidernet-io/spiderpool/blob/main/VERSION create a branch named 'release-vx.x.x' build the images with the pushed tag, and push to ghcr registry https://github.com/orgs/spidernet-io/packages?repo_name=spiderpool generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". changelogs is generated by historical PR label: label \"release/feature-new\" to be classified to \"New Features\" label \"release/feature-changed\" to be classified to \"Changed Features\" label \"release/bug\" to be classified to \"Fixes\" build the chart package with the pushed tag, and submit a PR to branch 'github_pages' you cloud get the chart with command helm repo add spiderpool https://spidernet-io.github.io/spiderpool submit '/docs' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\" For the detail, refer to https://github.com/spidernet-io/spiderpool/blob/main/.github/workflows/auto-version-release.yaml","title":"push a version tag"},{"location":"develop/release/#post","text":"Submit a issue of the version update to the documentation site --> https://github.com/DaoCloud/DaoCloud-docs","title":"post"},{"location":"develop/roadmap/","text":"roadmap feature description Alpha release Beta release GA release ippool ip settings v0.2.0 v0.4.0 v0.6.0 namespace affinity v0.4.0 v0.6.0 application affinity v0.4.0 v0.6.0 multiple default ippool v0.6.0 multusname affinity v0.6.0 nodename affinity v0.6.0 v0.6.0 default cluster ippool v0.2.0 v0.4.0 v0.6.0 default namespace ippool v0.4.0 v0.5.0 default CNI ippool v0.4.0 v0.4.0 annotation ippool v0.2.0 v0.5.0 annotation route v0.2.0 v0.5.0 subnet automatically create ippool v0.4.0 automatically scaling and deletion ip according to application v0.4.0 automatically delete ippool v0.5.0 annotation for multiple interface v0.4.0 keep ippool after deleting application v0.5.0 support deployment, statefulset, job, replicaset v0.4.0 support operator controller v0.4.0 flexible ip number v0.5.0 ippool inherit route and gateway attribute from its subnet v0.6.0 reservedIP reservedIP v0.4.0 v0.6.0 fixed ip fixed ip for each pod of statefulset v0.5.0 fixed ip ranges for statefulset, deployment, replicaset v0.4.0 v0.6.0 fixed ip for kubevirt v0.8.0 support calico v0.5.0 v0.6.0 support weave v0.5.0 v0.6.0 spidermultusconfig support macvlan ipvlan sriov custom v0.6.0 v0.7.0 support ovs-cni v0.7.0 ipam plugin cni v1.0.0 v0.4.0 v0.5.0 ifacer plugin bond interface v0.6.0 v0.8.0 vlan interface v0.6.0 v0.8.0 coordinator plugin support underlay mode v0.6.0 v0.7.0 support overlay mode v0.6.0 v0.8.0 CRD spidercoordinators for multus configuration v0.6.0 v0.8.0 detect ip conflict and gateway v0.6.0 v0.6.0 specify the MAC of pod v0.6.0 v0.8.0 tune the default route of pod multiple interfaces v0.6.0 v0.8.0 ovs/macvlan/sriov/ipvlan visit service based on kube-proxy v0.6.0 v0.7.0 visit local node to guarantee the pod health check v0.6.0 v0.7.0 visit nodePort with spec.externalTrafficPolicy=local or spec.externalTrafficPolicy=cluster v0.6.0 bandwidth In plan observability eBPF: pod stats In plan network policy ipvlan v0.8.0 macvlan In plan sriov In plan bandwidth ipvlan v0.8.0 macvlan In plan sriov In plan accelerate eBPF: visit service based on the kube-proxy replacement In plan eBPF: accelerate communication of pods on a same node In plan recycle IP recycle IP taken by deleted pod v0.4.0 v0.6.0 recycle IP taken by deleting pod v0.4.0 v0.6.0 dual-stack dual-stack v0.2.0 v0.4.0 CLI debug and operate. check which pod an IP is taken by, check IP usage , trigger GC In plan multi-cluster a broker cluster could synchronize ippool resource within a same subnet from all member clusters, which could help avoid IP conflict In plan support submariner In plan dual CNI underlay cooperate with cilium v0.7.0 underlay cooperate with calico v0.7.0 RDMA support macvlan and ipvlan CNI for RoCE device v0.8.0 support sriov CNI for RoCE device v0.8.0 support ipoib CNI for infiniband device In plan support ib-sriov CNI for infiniband device In plan egressGateway egressGateway v0.8.0","title":"Roadmap"},{"location":"develop/roadmap/#roadmap","text":"feature description Alpha release Beta release GA release ippool ip settings v0.2.0 v0.4.0 v0.6.0 namespace affinity v0.4.0 v0.6.0 application affinity v0.4.0 v0.6.0 multiple default ippool v0.6.0 multusname affinity v0.6.0 nodename affinity v0.6.0 v0.6.0 default cluster ippool v0.2.0 v0.4.0 v0.6.0 default namespace ippool v0.4.0 v0.5.0 default CNI ippool v0.4.0 v0.4.0 annotation ippool v0.2.0 v0.5.0 annotation route v0.2.0 v0.5.0 subnet automatically create ippool v0.4.0 automatically scaling and deletion ip according to application v0.4.0 automatically delete ippool v0.5.0 annotation for multiple interface v0.4.0 keep ippool after deleting application v0.5.0 support deployment, statefulset, job, replicaset v0.4.0 support operator controller v0.4.0 flexible ip number v0.5.0 ippool inherit route and gateway attribute from its subnet v0.6.0 reservedIP reservedIP v0.4.0 v0.6.0 fixed ip fixed ip for each pod of statefulset v0.5.0 fixed ip ranges for statefulset, deployment, replicaset v0.4.0 v0.6.0 fixed ip for kubevirt v0.8.0 support calico v0.5.0 v0.6.0 support weave v0.5.0 v0.6.0 spidermultusconfig support macvlan ipvlan sriov custom v0.6.0 v0.7.0 support ovs-cni v0.7.0 ipam plugin cni v1.0.0 v0.4.0 v0.5.0 ifacer plugin bond interface v0.6.0 v0.8.0 vlan interface v0.6.0 v0.8.0 coordinator plugin support underlay mode v0.6.0 v0.7.0 support overlay mode v0.6.0 v0.8.0 CRD spidercoordinators for multus configuration v0.6.0 v0.8.0 detect ip conflict and gateway v0.6.0 v0.6.0 specify the MAC of pod v0.6.0 v0.8.0 tune the default route of pod multiple interfaces v0.6.0 v0.8.0 ovs/macvlan/sriov/ipvlan visit service based on kube-proxy v0.6.0 v0.7.0 visit local node to guarantee the pod health check v0.6.0 v0.7.0 visit nodePort with spec.externalTrafficPolicy=local or spec.externalTrafficPolicy=cluster v0.6.0 bandwidth In plan observability eBPF: pod stats In plan network policy ipvlan v0.8.0 macvlan In plan sriov In plan bandwidth ipvlan v0.8.0 macvlan In plan sriov In plan accelerate eBPF: visit service based on the kube-proxy replacement In plan eBPF: accelerate communication of pods on a same node In plan recycle IP recycle IP taken by deleted pod v0.4.0 v0.6.0 recycle IP taken by deleting pod v0.4.0 v0.6.0 dual-stack dual-stack v0.2.0 v0.4.0 CLI debug and operate. check which pod an IP is taken by, check IP usage , trigger GC In plan multi-cluster a broker cluster could synchronize ippool resource within a same subnet from all member clusters, which could help avoid IP conflict In plan support submariner In plan dual CNI underlay cooperate with cilium v0.7.0 underlay cooperate with calico v0.7.0 RDMA support macvlan and ipvlan CNI for RoCE device v0.8.0 support sriov CNI for RoCE device v0.8.0 support ipoib CNI for infiniband device In plan support ib-sriov CNI for infiniband device In plan egressGateway egressGateway v0.8.0","title":"roadmap"},{"location":"develop/swagger_openapi/","text":"SWAGGER OPENAPI Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes. Features Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs. Usages There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR . validate spec Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec generate source codes with the given spec To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen verify the spec with current source codes to make sure whether the current source codes is out of date To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify clean the generated source codes To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code Use swagger-ui To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web. Steps For Developers Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Swagger OpenAPI"},{"location":"develop/swagger_openapi/#swagger-openapi","text":"Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#features","text":"Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs.","title":"Features"},{"location":"develop/swagger_openapi/#usages","text":"There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR .","title":"Usages"},{"location":"develop/swagger_openapi/#validate-spec","text":"Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec","title":"validate spec"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","text":"To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen","title":"generate source codes with the given spec"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","text":"To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify","title":"verify the spec with current source codes to make sure whether the current source codes is out of date"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","text":"To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code","title":"clean the generated source codes"},{"location":"develop/swagger_openapi/#use-swagger-ui","text":"To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.","title":"Use swagger-ui"},{"location":"develop/swagger_openapi/#steps-for-developers","text":"Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Steps For Developers"},{"location":"reference/_example/","text":"CRD \u57fa\u672c\u63cf\u8ff0 \u672cCRD \u662f\u505a\u4ec0\u4e48\u7684 \u914d\u7f6e\u8bf4\u660e \u8868\u683c\uff08\u5b57\u6bb5\u3001\u63cf\u8ff0\u3001\u7f3a\u7701\u503c\uff09\uff0c\u5305\u62ec\u4e86 status \u7684\u4fe1\u606f\u8bf4\u660e \u4f7f\u7528\u4f8b\u5b50 \u7ed9\u51fa\u4e00\u4e9b\u573a\u666f\u573a\u666f\u4e0b\u7684 CR yaml","title":"CRD"},{"location":"reference/_example/#crd","text":"","title":"CRD"},{"location":"reference/_example/#_1","text":"\u672cCRD \u662f\u505a\u4ec0\u4e48\u7684","title":"\u57fa\u672c\u63cf\u8ff0"},{"location":"reference/_example/#_2","text":"\u8868\u683c\uff08\u5b57\u6bb5\u3001\u63cf\u8ff0\u3001\u7f3a\u7701\u503c\uff09\uff0c\u5305\u62ec\u4e86 status \u7684\u4fe1\u606f\u8bf4\u660e","title":"\u914d\u7f6e\u8bf4\u660e"},{"location":"reference/_example/#_3","text":"\u7ed9\u51fa\u4e00\u4e9b\u573a\u666f\u573a\u666f\u4e0b\u7684 CR yaml","title":"\u4f7f\u7528\u4f8b\u5b50"},{"location":"reference/annotation/","text":"Annotations Spiderpool provides annotations for configuring custom IPPools and routes. Pod annotations After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations. ipam.spidernet.io/subnet Specify the Subnets used to generate IPPools and allocate IP addresses. ipam.spidernet.io/subnet : |- { \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] } ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/subnets ipam.spidernet.io/subnets : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-subnet2\"], \"ipv6\": [\"demo-v6-subnet2\"] }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/ippool-ip-number This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1'). ipam.spidernet.io/ippool-ip-number : +1 ipam.spidernet.io/ippool-reclaim This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true'). ipam.spidernet.io/ippool-reclaim : true ipam.spidernet.io/ippool Specify the IPPools used to allocate IP addresses. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"] } ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. ipam.spidernet.io/ippools It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. cleangateway (bool, optional): If set to true, the IPAM plugin will not return the default route (generated by spec.gateway ) recorded in the IPPool. ipam.spidernet.io/routes You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address. Namespace annotations A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace. ipam.spidernet.io/default-ipv4-ippool ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'","title":"Annotations"},{"location":"reference/annotation/#annotations","text":"Spiderpool provides annotations for configuring custom IPPools and routes.","title":"Annotations"},{"location":"reference/annotation/#pod-annotations","text":"After enabling the feature SpiderSubnet (default enabled after v0.4.0), the annotations related to Subnet will take effect. They always have higher priority than IPPool related annotations.","title":"Pod annotations"},{"location":"reference/annotation/#ipamspidernetiosubnet","text":"Specify the Subnets used to generate IPPools and allocate IP addresses. ipam.spidernet.io/subnet : |- { \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] } ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/subnet"},{"location":"reference/annotation/#ipamspidernetiosubnets","text":"ipam.spidernet.io/subnets : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-subnet1\"], \"ipv6\": [\"demo-v6-subnet1\"] },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-subnet2\"], \"ipv6\": [\"demo-v6-subnet2\"] }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which Subnet is used to generate IPPool and allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/subnets"},{"location":"reference/annotation/#ipamspidernetioippool-ip-number","text":"This annotation is used with SpiderSubnet feature enabled. It specifies the IP numbers of the corresponding SpiderIPPool (fixed and flexible mode, optional and default '+1'). ipam.spidernet.io/ippool-ip-number : +1","title":"ipam.spidernet.io/ippool-ip-number"},{"location":"reference/annotation/#ipamspidernetioippool-reclaim","text":"This annotation is used with SpiderSubnet feature enabled. It specifies the corresponding SpiderIPPool to delete or not once the application was deleted (optional and default 'true'). ipam.spidernet.io/ippool-reclaim : true","title":"ipam.spidernet.io/ippool-reclaim"},{"location":"reference/annotation/#ipamspidernetioippool","text":"Specify the IPPools used to allocate IP addresses. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\", \"demo-v6-ippool2\"] } ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required.","title":"ipam.spidernet.io/ippool"},{"location":"reference/annotation/#ipamspidernetioippools","text":"It is similar to ipam.spidernet.io/ippool but could be used in the case with multiple interfaces. Note that ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] interface (string, required): Since the CNI request only carries the information of one interface, the field interface shall be specified to distinguish in the case of multiple interfaces. ipv4 (array, optional): Specify which IPPool is used to allocate the IPv4 address. When enableIPv4 in the ConfigMap spiderpool-conf is set to true, this field is required. ipv6 (array, optional): Specify which IPPool is used to allocate the IPv6 address. When enableIPv6 in the ConfigMap spiderpool-conf is set to true, this field is required. cleangateway (bool, optional): If set to true, the IPAM plugin will not return the default route (generated by spec.gateway ) recorded in the IPPool.","title":"ipam.spidernet.io/ippools"},{"location":"reference/annotation/#ipamspidernetioroutes","text":"You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }] dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address.","title":"ipam.spidernet.io/routes"},{"location":"reference/annotation/#namespace-annotations","text":"A Namespace can set the following annotations to specify default IPPools which are effective for all Pods under the Namespace.","title":"Namespace annotations"},{"location":"reference/annotation/#ipamspidernetiodefault-ipv4-ippool","text":"ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]'","title":"ipam.spidernet.io/default-ipv4-ippool"},{"location":"reference/annotation/#ipamspidernetiodefault-ipv6-ippool","text":"ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]'","title":"ipam.spidernet.io/default-ipv6-ippool"},{"location":"reference/configmap/","text":"Configuration Instructions for global configuration and environment arguments of Spiderpool. Configmap Configuration Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : kube-system data : conf.yml : | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock enableIPv4: true enableIPv6: true enableStatefulSet: true enableKubevirtStaticIP: true enableSpiderSubnet: true clusterSubnetDefaultFlexibleIPNumber: 1 ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet static IP capability of Spiderpool. false : Disable StatefulSet static IP capability of Spiderpool. enableKubevirtStaticIP (bool): true : Enable kubevirt VM static IP capability of Spiderpool. false : Disable kubevirt VM static IP capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster.","title":"Configmap"},{"location":"reference/configmap/#configuration","text":"Instructions for global configuration and environment arguments of Spiderpool.","title":"Configuration"},{"location":"reference/configmap/#configmap-configuration","text":"Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion : v1 kind : ConfigMap metadata : name : spiderpool-conf namespace : kube-system data : conf.yml : | ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock enableIPv4: true enableIPv6: true enableStatefulSet: true enableKubevirtStaticIP: true enableSpiderSubnet: true clusterSubnetDefaultFlexibleIPNumber: 1 ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handles IPAM requests from IPAM plugin. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. enableStatefulSet (bool): true : Enable StatefulSet static IP capability of Spiderpool. false : Disable StatefulSet static IP capability of Spiderpool. enableKubevirtStaticIP (bool): true : Enable kubevirt VM static IP capability of Spiderpool. false : Disable kubevirt VM static IP capability of Spiderpool. enableSpiderSubnet (bool): true : Enable SpiderSubnet capability of Spiderpool. false : Disable SpiderSubnet capability of Spiderpool. clusterSubnetDefaultFlexibleIPNumber (int): Global SpiderSubnet default flexible IP number. It takes effect across the cluster.","title":"Configmap Configuration"},{"location":"reference/crd-spidercoordinator/","text":"Spidercoordinator A Spidercoordinator resource represents the global default configuration of the cni meta-plugin: coordinator. There is only one instance of this resource, which is automatically generated while you install Spiderpool and does not need to be created manually. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderCoordinator metadata : name : default spec : detectGateway : false detectIPConflict : false hostRPFilter : 0 hostRuleTable : 500 mode : underlay podCIDRType : auto podDefaultRouteNIC : eth0 podMACPrefix : \"\" tunePodRoutes : true status : overlayPodCIDR : - 10.233.64.0/18 - fd85:ee78:d8a6:8607::1:0000/112 phase : Synced serviceCIDR : - 10.233.0.0/18 - fd85:ee78:d8a6:8607::1000/116 Spidercoordinators definition Metadata Field Description Schema Validation name The name of this Spidercoordinators resource string required Spec This is the Spidercoordinators spec for users to configure. Field Description Schema Validation Values Default mode The mode in which the coordinator. auto: automatically determine if it's overlay or underlay. underlay: coordinator creates veth devices to solve the problem that CNIs such as macvlan cannot communicate with clusterIP. overlay: fix the problem that CNIs such as Macvlan cannot access ClusterIP through the Calico network card attached to the pod,coordinate policy route between interfaces to ensure consistence data path of request and reply packets string require auto,underlay,overlay auto podCIDRType The ways to fetch the CIDR of the cluster. auto(default), This means that it will automatically switch podCIDRType to cluster or calico or cilium. based on cluster CNI. calico: auto fetch the subnet of the pod from the ip pools of calico, This only works if the cluster CNI is calico; cilium: Auto fetch the pod's subnet from cilium's configMap or ip pools. Supported IPAM modes: [\"cluster-pool\",\"kubernetes\",\"multi-pool\"]; cluster: auto fetch the subnet of the pod from the kubeadm-config configmap, This is useful if there is only a globally unique default pod's subnet; none: don't get the subnet of the pod, which is useful for some special cases. In this case,you can manually configure the hijackCIDR field string require auto,cluster,calico,cilium,none auto tunePodRoutes tune pod's route while the pod is attached to multiple NICs bool optional true,false true podDefaultRouteNIC The NIC where the pod's default route resides string optional \"\",eth0,net1... underlay: eth0,overlay: net1 detectGateway enable detect gateway while launching pod, If the gateway is unreachable, pod will be failed to created; Note: We use ARP probes to detect if the gateway is reachable, and some gateway routers may warn about this boolean optional true,false false detectIPConflict enable the pod's ip if is conflicting while launching pod. If an IP conflict of the pod is detected, pod will be failed to created boolean optional true,false false podMACPrefix fix the pod's mac address with this prefix + 4 bytes IP string optional a invalid mac address prefix \"\" hostRPFilter sysctls: rp_filter in host int required 0,1,2;suggest to be 0 0 hostRuleTable The directly routing table of the host accessing the pod's underlay IP will be placed in this policy routing table int required int 500 Status (subresource) The Spidercoordinators status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation overlayPodCIDR the cluster pod cidr []string required serviceCIDR the cluster service cidr []string required phase Represents the status of synchronization string required","title":"CRD Spidercoordinator"},{"location":"reference/crd-spidercoordinator/#spidercoordinator","text":"A Spidercoordinator resource represents the global default configuration of the cni meta-plugin: coordinator. There is only one instance of this resource, which is automatically generated while you install Spiderpool and does not need to be created manually.","title":"Spidercoordinator"},{"location":"reference/crd-spidercoordinator/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderCoordinator metadata : name : default spec : detectGateway : false detectIPConflict : false hostRPFilter : 0 hostRuleTable : 500 mode : underlay podCIDRType : auto podDefaultRouteNIC : eth0 podMACPrefix : \"\" tunePodRoutes : true status : overlayPodCIDR : - 10.233.64.0/18 - fd85:ee78:d8a6:8607::1:0000/112 phase : Synced serviceCIDR : - 10.233.0.0/18 - fd85:ee78:d8a6:8607::1000/116","title":"Sample YAML"},{"location":"reference/crd-spidercoordinator/#spidercoordinators-definition","text":"","title":"Spidercoordinators definition"},{"location":"reference/crd-spidercoordinator/#metadata","text":"Field Description Schema Validation name The name of this Spidercoordinators resource string required","title":"Metadata"},{"location":"reference/crd-spidercoordinator/#spec","text":"This is the Spidercoordinators spec for users to configure. Field Description Schema Validation Values Default mode The mode in which the coordinator. auto: automatically determine if it's overlay or underlay. underlay: coordinator creates veth devices to solve the problem that CNIs such as macvlan cannot communicate with clusterIP. overlay: fix the problem that CNIs such as Macvlan cannot access ClusterIP through the Calico network card attached to the pod,coordinate policy route between interfaces to ensure consistence data path of request and reply packets string require auto,underlay,overlay auto podCIDRType The ways to fetch the CIDR of the cluster. auto(default), This means that it will automatically switch podCIDRType to cluster or calico or cilium. based on cluster CNI. calico: auto fetch the subnet of the pod from the ip pools of calico, This only works if the cluster CNI is calico; cilium: Auto fetch the pod's subnet from cilium's configMap or ip pools. Supported IPAM modes: [\"cluster-pool\",\"kubernetes\",\"multi-pool\"]; cluster: auto fetch the subnet of the pod from the kubeadm-config configmap, This is useful if there is only a globally unique default pod's subnet; none: don't get the subnet of the pod, which is useful for some special cases. In this case,you can manually configure the hijackCIDR field string require auto,cluster,calico,cilium,none auto tunePodRoutes tune pod's route while the pod is attached to multiple NICs bool optional true,false true podDefaultRouteNIC The NIC where the pod's default route resides string optional \"\",eth0,net1... underlay: eth0,overlay: net1 detectGateway enable detect gateway while launching pod, If the gateway is unreachable, pod will be failed to created; Note: We use ARP probes to detect if the gateway is reachable, and some gateway routers may warn about this boolean optional true,false false detectIPConflict enable the pod's ip if is conflicting while launching pod. If an IP conflict of the pod is detected, pod will be failed to created boolean optional true,false false podMACPrefix fix the pod's mac address with this prefix + 4 bytes IP string optional a invalid mac address prefix \"\" hostRPFilter sysctls: rp_filter in host int required 0,1,2;suggest to be 0 0 hostRuleTable The directly routing table of the host accessing the pod's underlay IP will be placed in this policy routing table int required int 500","title":"Spec"},{"location":"reference/crd-spidercoordinator/#status-subresource","text":"The Spidercoordinators status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation overlayPodCIDR the cluster pod cidr []string required serviceCIDR the cluster service cidr []string required phase Represents the status of synchronization string required","title":"Status (subresource)"},{"location":"reference/crd-spiderendpoint/","text":"SpiderEndpoint A SpiderEndpoint resource represents IP address allocation details for the corresponding pod. This resource one to one pod, and it will inherit the pod name and pod namespace. Notice: For kubevirt VM static IP feature, the SpiderEndpoint object would inherit the kubevirt VM/VMI resource name and namespace. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderEndpoint metadata : name : test-app-1-9dc78fb9-rs99d status : current : ips : - cleanGateway : false interface : eth0 ipv4 : 172.31.199.193/20 ipv4Gateway : 172.31.207.253 ipv4Pool : worker-172 vlan : 0 node : dc-test02 uid : e7b50a38-25c2-41d0-b332-7f619c69194e ownerControllerName : test-app-1 ownerControllerType : Deployment SpiderEndpoint definition Metadata Field Description Schema Validation name the name of this SpiderEndpoint resource string required namespace the namespace of this SpiderEndpoint resource string required Status (subresource) The IPPool status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation current the IP allocation details of the corresponding pod PodIPAllocation required ownerControllerType the corresponding pod top owner controller type string required ownerControllerName the corresponding pod top owner controller name string required PodIPAllocation This property describes the SpiderEndpoint corresponding pod details. Field Description Schema Validation uid corresponding pod uid string required node total IP counts of this pool to use string required ips current allocated IP counts list of IPAllocationDetail required IPAllocationDetail This property describes single Interface allocation details. Field Description Schema Validation Default interface single interface name string required ipv4 single IPv4 allocated IP address string optional ipv6 single IPv6 allocated IP address string optional ipv4Pool the IPv4 allocated IP address corresponding pool string optional ipv6Pool the IPv6 allocated IP address corresponding pool string optional vlan vlan ID int optional 0 ipv4Gateway the IPv4 gateway IP address string optional ipv6Gateway the IPv6 gateway IP address string optional cleanGateway a flag to choose whether need default route by the gateway boolean optional routes the allocation routes list if Route optional","title":"CRD SpiderEndpoint"},{"location":"reference/crd-spiderendpoint/#spiderendpoint","text":"A SpiderEndpoint resource represents IP address allocation details for the corresponding pod. This resource one to one pod, and it will inherit the pod name and pod namespace. Notice: For kubevirt VM static IP feature, the SpiderEndpoint object would inherit the kubevirt VM/VMI resource name and namespace.","title":"SpiderEndpoint"},{"location":"reference/crd-spiderendpoint/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderEndpoint metadata : name : test-app-1-9dc78fb9-rs99d status : current : ips : - cleanGateway : false interface : eth0 ipv4 : 172.31.199.193/20 ipv4Gateway : 172.31.207.253 ipv4Pool : worker-172 vlan : 0 node : dc-test02 uid : e7b50a38-25c2-41d0-b332-7f619c69194e ownerControllerName : test-app-1 ownerControllerType : Deployment","title":"Sample YAML"},{"location":"reference/crd-spiderendpoint/#spiderendpoint-definition","text":"","title":"SpiderEndpoint definition"},{"location":"reference/crd-spiderendpoint/#metadata","text":"Field Description Schema Validation name the name of this SpiderEndpoint resource string required namespace the namespace of this SpiderEndpoint resource string required","title":"Metadata"},{"location":"reference/crd-spiderendpoint/#status-subresource","text":"The IPPool status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema Validation current the IP allocation details of the corresponding pod PodIPAllocation required ownerControllerType the corresponding pod top owner controller type string required ownerControllerName the corresponding pod top owner controller name string required","title":"Status (subresource)"},{"location":"reference/crd-spiderendpoint/#podipallocation","text":"This property describes the SpiderEndpoint corresponding pod details. Field Description Schema Validation uid corresponding pod uid string required node total IP counts of this pool to use string required ips current allocated IP counts list of IPAllocationDetail required","title":"PodIPAllocation"},{"location":"reference/crd-spiderendpoint/#ipallocationdetail","text":"This property describes single Interface allocation details. Field Description Schema Validation Default interface single interface name string required ipv4 single IPv4 allocated IP address string optional ipv6 single IPv6 allocated IP address string optional ipv4Pool the IPv4 allocated IP address corresponding pool string optional ipv6Pool the IPv6 allocated IP address corresponding pool string optional vlan vlan ID int optional 0 ipv4Gateway the IPv4 gateway IP address string optional ipv6Gateway the IPv6 gateway IP address string optional cleanGateway a flag to choose whether need default route by the gateway boolean optional routes the allocation routes list if Route optional","title":"IPAllocationDetail"},{"location":"reference/crd-spiderippool/","text":"SpiderIPPool A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned. Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : ipVersion : 4 subnet : 172.31.192.0/20 ips : - 172.31.199.180-172.31.199.189 - 172.31.199.205-172.31.199.209 excludeIPs : - 172.31.199.186-172.31.199.188 - 172.31.199.207 gateway : 172.31.207.253 default : true disable : false SpiderIPPool definition Metadata Field Description Schema Validation name the name of this SpiderIPPool resource string required Spec This is the IPPool spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this pool int optional 4,6 subnet subnet of this pool string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this pool to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this pool to filter list of strings optional array of IP ranges and single IP address gateway gateway for this pool string optional an IP address vlan vlan ID(deprecated) int optional [0,4094] 0 routes custom routes in this pool (please don't set default route 0.0.0.0/0 if property gateway exists) list of route optional podAffinity specify which pods can use this pool labelSelector optional kubernetes LabelSelector namespaceAffinity specify which namespaces pods can use this pool labelSelector optional kubernetes LabelSelector namespaceName specify which namespaces pods can use this pool (The priority is higher than property namespaceAffinity ) list of strings optional nodeAffinity specify which nodes pods can use this pool labelSelector optional kubernetes LabelSelector nodeName specify which nodes pods can use this pool (The priority is higher than property nodeAffinity ) list of strings optional multusName specify which multus net-attach-def objects can use this pool list of strings optional default configure this resource as a default pool for pods boolean optional true,false false disable configure whether the pool is usable boolean optional true,false false Status (subresource) The IPPool status is a subresource that processed automatically by the system to summarize the current state Field Description Schema allocatedIPs current IP allocations in this pool string totalIPCount total IP counts of this pool to use int allocatedIPCount current allocated IP counts int Route Field Description Schema Validation dst destination of this route string required gw gateway of this route string required Pod Affinity For details on configuring SpiderIPPool podAffinity, please read the Pod Affinity of IPPool . Namespace Affinity For details on configuring SpiderIPPool namespaceAffinity or namespaceName, please read the Namespace Affinity of IPPool . Notice: namespaceName has higher priority than namespaceAffinity . Node Affinity For details on configuring SpiderIPPool nodeAffinity or nodeName, please read the Node Affinity of IPPool and Network topology allocation . Notice: nodeName has higher priority than nodeAffinity . Multus Affinity For details on configuring SpiderIPPool multusName, please read the multus Affinity of IPPool .","title":"CRD SpiderIPPool"},{"location":"reference/crd-spiderippool/#spiderippool","text":"A SpiderIPPool resource represents a collection of IP addresses from which Spiderpool expects endpoint IPs to be assigned.","title":"SpiderIPPool"},{"location":"reference/crd-spiderippool/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : ipVersion : 4 subnet : 172.31.192.0/20 ips : - 172.31.199.180-172.31.199.189 - 172.31.199.205-172.31.199.209 excludeIPs : - 172.31.199.186-172.31.199.188 - 172.31.199.207 gateway : 172.31.207.253 default : true disable : false","title":"Sample YAML"},{"location":"reference/crd-spiderippool/#spiderippool-definition","text":"","title":"SpiderIPPool definition"},{"location":"reference/crd-spiderippool/#metadata","text":"Field Description Schema Validation name the name of this SpiderIPPool resource string required","title":"Metadata"},{"location":"reference/crd-spiderippool/#spec","text":"This is the IPPool spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this pool int optional 4,6 subnet subnet of this pool string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this pool to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this pool to filter list of strings optional array of IP ranges and single IP address gateway gateway for this pool string optional an IP address vlan vlan ID(deprecated) int optional [0,4094] 0 routes custom routes in this pool (please don't set default route 0.0.0.0/0 if property gateway exists) list of route optional podAffinity specify which pods can use this pool labelSelector optional kubernetes LabelSelector namespaceAffinity specify which namespaces pods can use this pool labelSelector optional kubernetes LabelSelector namespaceName specify which namespaces pods can use this pool (The priority is higher than property namespaceAffinity ) list of strings optional nodeAffinity specify which nodes pods can use this pool labelSelector optional kubernetes LabelSelector nodeName specify which nodes pods can use this pool (The priority is higher than property nodeAffinity ) list of strings optional multusName specify which multus net-attach-def objects can use this pool list of strings optional default configure this resource as a default pool for pods boolean optional true,false false disable configure whether the pool is usable boolean optional true,false false","title":"Spec"},{"location":"reference/crd-spiderippool/#status-subresource","text":"The IPPool status is a subresource that processed automatically by the system to summarize the current state Field Description Schema allocatedIPs current IP allocations in this pool string totalIPCount total IP counts of this pool to use int allocatedIPCount current allocated IP counts int","title":"Status (subresource)"},{"location":"reference/crd-spiderippool/#route","text":"Field Description Schema Validation dst destination of this route string required gw gateway of this route string required","title":"Route"},{"location":"reference/crd-spiderippool/#pod-affinity","text":"For details on configuring SpiderIPPool podAffinity, please read the Pod Affinity of IPPool .","title":"Pod Affinity"},{"location":"reference/crd-spiderippool/#namespace-affinity","text":"For details on configuring SpiderIPPool namespaceAffinity or namespaceName, please read the Namespace Affinity of IPPool . Notice: namespaceName has higher priority than namespaceAffinity .","title":"Namespace Affinity"},{"location":"reference/crd-spiderippool/#node-affinity","text":"For details on configuring SpiderIPPool nodeAffinity or nodeName, please read the Node Affinity of IPPool and Network topology allocation . Notice: nodeName has higher priority than nodeAffinity .","title":"Node Affinity"},{"location":"reference/crd-spiderippool/#multus-affinity","text":"For details on configuring SpiderIPPool multusName, please read the multus Affinity of IPPool .","title":"Multus Affinity"},{"location":"reference/crd-spidermultusconfig/","text":"SpiderMultusConfig A SpiderMultusConfig resource represents a best practice to generate a multus net-attach-def CR object for spiderpool to use. For details on using this CRD, please read the SpiderMultusConfig guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : demo namespace : default annotations : multus.spidernet.io/cr-name : \"macvlan-100\" multus.spidernet.io/cni-version : 0.4.0 spec : cniType : macvlan macvlan : master : [ \"eth0\" ] vlanID : 100 ippools : ipv4 : [ \"default-pool-v4\" ] ipv6 : [ \"default-pool-v6\" ] SpiderMultusConfig definition Metadata Field Description Schema Validation name The name of this SpiderMultusConfig resource string required namespace The namespace of this SpiderMultusConfig resource string required annotations The annotations of this SpiderMultusConfig resource map optional Metadata.annotations You can also set annotations for this SpiderMultusConfig resource, then the corresponding Multus net-attach-def resource will inherit these annotations too. And you can also use special annotation multus.spidernet.io/cr-name and multus.spidernet.io/cni-version to customize the corresponding Multus net-attach-def resource name and CNI version. Field Description Schema Validation Default multus.spidernet.io/cr-name The customized Multus net-attach-def resource name string optional multus.spidernet.io/cni-version The customized Multus net-attach-def resource CNI version string optional 0.3.1 Spec This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values Default cniType expected main CNI type string require macvlan,ipvlan,sriov,ovs,custom macvlan macvlan CNI configuration SpiderMacvlanCniConfig optional ipvlan ipvlan CNI configuration SpiderIPvlanCniConfig optional sriov sriov CNI configuration SpiderSRIOVCniConfig optional ovs ovs CNI configuration SpiderOvsCniConfig optional enableCoordinator enable coordinator or not boolean optional true,false true disableIPAM disable IPAM or not boolean optional true,false false coordinator coordinator CNI configuration CoordinatorSpec optional customCNI a string that represents custom CNI configuration string optional SpiderMacvlanCniConfig Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderIPvlanCniConfig Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional SpiderSRIOVCniConfig Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with SRIOV string required vlanID vlan ID int optional minTxRateMbps change the allowed minimum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting. The min_tx_rate value should be <= max_tx_rate. Support of this feature depends on NICs and drivers int optional maxTxRateMbps change the allowed maximum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting int optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional enableRdma enable rdma chain cni to isolate the rdma device bool optional SpiderOvsCniConfig Field Description Schema Validation bridge name of the bridge to use string required vlan vlan ID of attached port. Trunk port if not specified int optional trunk List of VLAN ID's and/or ranges of accepted VLAN ID's Trunk optional deviceID PCI address of a VF in valid sysfs format string optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional BondConfig Field Description Schema Validation Values Name the expected bond interface name string required Mode bond interface mode int required [0,6] Options expected bond Interface configurations string optional Trunk Field Description Schema Validation Values minID the min value of vlan ID int optional [0,4094] maxID the max value of vlan ID int optional [0,4094] id the value of vlan ID int optional [0,4094] SpiderpoolPools Field Description Schema Validation ipv4 the default IPv4 IPPools in your CNI configurations list of strings optional ipv6 the default IPv6 IPPools in your CNI configurations list of strings optional","title":"CRD Spidermultusconfig"},{"location":"reference/crd-spidermultusconfig/#spidermultusconfig","text":"A SpiderMultusConfig resource represents a best practice to generate a multus net-attach-def CR object for spiderpool to use. For details on using this CRD, please read the SpiderMultusConfig guide .","title":"SpiderMultusConfig"},{"location":"reference/crd-spidermultusconfig/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : demo namespace : default annotations : multus.spidernet.io/cr-name : \"macvlan-100\" multus.spidernet.io/cni-version : 0.4.0 spec : cniType : macvlan macvlan : master : [ \"eth0\" ] vlanID : 100 ippools : ipv4 : [ \"default-pool-v4\" ] ipv6 : [ \"default-pool-v6\" ]","title":"Sample YAML"},{"location":"reference/crd-spidermultusconfig/#spidermultusconfig-definition","text":"","title":"SpiderMultusConfig definition"},{"location":"reference/crd-spidermultusconfig/#metadata","text":"Field Description Schema Validation name The name of this SpiderMultusConfig resource string required namespace The namespace of this SpiderMultusConfig resource string required annotations The annotations of this SpiderMultusConfig resource map optional","title":"Metadata"},{"location":"reference/crd-spidermultusconfig/#metadataannotations","text":"You can also set annotations for this SpiderMultusConfig resource, then the corresponding Multus net-attach-def resource will inherit these annotations too. And you can also use special annotation multus.spidernet.io/cr-name and multus.spidernet.io/cni-version to customize the corresponding Multus net-attach-def resource name and CNI version. Field Description Schema Validation Default multus.spidernet.io/cr-name The customized Multus net-attach-def resource name string optional multus.spidernet.io/cni-version The customized Multus net-attach-def resource CNI version string optional 0.3.1","title":"Metadata.annotations"},{"location":"reference/crd-spidermultusconfig/#spec","text":"This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values Default cniType expected main CNI type string require macvlan,ipvlan,sriov,ovs,custom macvlan macvlan CNI configuration SpiderMacvlanCniConfig optional ipvlan ipvlan CNI configuration SpiderIPvlanCniConfig optional sriov sriov CNI configuration SpiderSRIOVCniConfig optional ovs ovs CNI configuration SpiderOvsCniConfig optional enableCoordinator enable coordinator or not boolean optional true,false true disableIPAM disable IPAM or not boolean optional true,false false coordinator coordinator CNI configuration CoordinatorSpec optional customCNI a string that represents custom CNI configuration string optional","title":"Spec"},{"location":"reference/crd-spidermultusconfig/#spidermacvlancniconfig","text":"Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderMacvlanCniConfig"},{"location":"reference/crd-spidermultusconfig/#spideripvlancniconfig","text":"Field Description Schema Validation Values master the Interfaces on your master, you could specify a single one Interface or multiple Interfaces to generate one bond Interface list of strings required vlanID vlan ID int optional [0,4094] bond expected bond Interface configurations BondConfig optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderIPvlanCniConfig"},{"location":"reference/crd-spidermultusconfig/#spidersriovcniconfig","text":"Field Description Schema Validation resourceName this property will create an annotation for Multus net-attach-def to cooperate with SRIOV string required vlanID vlan ID int optional minTxRateMbps change the allowed minimum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting. The min_tx_rate value should be <= max_tx_rate. Support of this feature depends on NICs and drivers int optional maxTxRateMbps change the allowed maximum transmit bandwidth, in Mbps, for the VF. Setting this to 0 disables rate limiting int optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional enableRdma enable rdma chain cni to isolate the rdma device bool optional","title":"SpiderSRIOVCniConfig"},{"location":"reference/crd-spidermultusconfig/#spiderovscniconfig","text":"Field Description Schema Validation bridge name of the bridge to use string required vlan vlan ID of attached port. Trunk port if not specified int optional trunk List of VLAN ID's and/or ranges of accepted VLAN ID's Trunk optional deviceID PCI address of a VF in valid sysfs format string optional ippools the default IPPools in your CNI configurations SpiderpoolPools optional","title":"SpiderOvsCniConfig"},{"location":"reference/crd-spidermultusconfig/#bondconfig","text":"Field Description Schema Validation Values Name the expected bond interface name string required Mode bond interface mode int required [0,6] Options expected bond Interface configurations string optional","title":"BondConfig"},{"location":"reference/crd-spidermultusconfig/#trunk","text":"Field Description Schema Validation Values minID the min value of vlan ID int optional [0,4094] maxID the max value of vlan ID int optional [0,4094] id the value of vlan ID int optional [0,4094]","title":"Trunk"},{"location":"reference/crd-spidermultusconfig/#spiderpoolpools","text":"Field Description Schema Validation ipv4 the default IPv4 IPPools in your CNI configurations list of strings optional ipv6 the default IPv6 IPPools in your CNI configurations list of strings optional","title":"SpiderpoolPools"},{"location":"reference/crd-spiderreservedip/","text":"SpiderReservedIP A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. For details on using this CRD, please read the SpiderReservedIP guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : exclude-ips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50 SpiderReservedIP definition Metadata Field Description Schema Validation name the name of this SpiderReservedIP resource string required Spec This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values ipVersion IP version of this resource int optional 4,6 ips IP ranges for this resource that we expect not to use list of strings optional array of IP ranges and single IP address","title":"CRD SpiderReservedIP"},{"location":"reference/crd-spiderreservedip/#spiderreservedip","text":"A SpiderReservedIP resource represents a collection of IP addresses that Spiderpool expects not to be allocated. For details on using this CRD, please read the SpiderReservedIP guide .","title":"SpiderReservedIP"},{"location":"reference/crd-spiderreservedip/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : exclude-ips spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.44 - 172.18.41.46-172.18.41.50","title":"Sample YAML"},{"location":"reference/crd-spiderreservedip/#spiderreservedip-definition","text":"","title":"SpiderReservedIP definition"},{"location":"reference/crd-spiderreservedip/#metadata","text":"Field Description Schema Validation name the name of this SpiderReservedIP resource string required","title":"Metadata"},{"location":"reference/crd-spiderreservedip/#spec","text":"This is the SpiderReservedIP spec for users to configure. Field Description Schema Validation Values ipVersion IP version of this resource int optional 4,6 ips IP ranges for this resource that we expect not to use list of strings optional array of IP ranges and single IP address","title":"Spec"},{"location":"reference/crd-spidersubnet/","text":"SpiderSubnet A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. For details on using this CRD, please read the SpiderSubnet guide . Sample YAML apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : default-v4-subnet spec : ipVersion : 4 ips : - 172.22.40.2-172.22.40.254 subnet : 172.22.0.0/16 excludeIPs : - 172.22.40.10-172.22.40.20 gateway : 172.22.40.1 SpiderSubnet definition Metadata Field Description Schema Validation name the name of this SpiderSubnet resource string required Spec This is the SpiderSubnet spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this subnet int optional 4,6 subnet subnet of this resource string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this resource to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this resource to filter list of strings optional array of IP ranges and single IP address gateway gateway for this resource string optional an IP address vlan vlan ID(deprecated) int optional [0,4094] 0 routes custom routes in this resource list of Route optional Status (subresource) The Subnet status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema controlledIPPools current IP allocations in this subnet resource string totalIPCount total IP addresses counts of this subnet resource to use int allocatedIPCount current allocated IP addresses counts int","title":"CRD SpiderSubnet"},{"location":"reference/crd-spidersubnet/#spidersubnet","text":"A SpiderSubnet resource represents a collection of IP addresses from which Spiderpool expects SpiderIPPool IPs to be assigned. For details on using this CRD, please read the SpiderSubnet guide .","title":"SpiderSubnet"},{"location":"reference/crd-spidersubnet/#sample-yaml","text":"apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderSubnet metadata : name : default-v4-subnet spec : ipVersion : 4 ips : - 172.22.40.2-172.22.40.254 subnet : 172.22.0.0/16 excludeIPs : - 172.22.40.10-172.22.40.20 gateway : 172.22.40.1","title":"Sample YAML"},{"location":"reference/crd-spidersubnet/#spidersubnet-definition","text":"","title":"SpiderSubnet definition"},{"location":"reference/crd-spidersubnet/#metadata","text":"Field Description Schema Validation name the name of this SpiderSubnet resource string required","title":"Metadata"},{"location":"reference/crd-spidersubnet/#spec","text":"This is the SpiderSubnet spec for users to configure. Field Description Schema Validation Values Default ipVersion IP version of this subnet int optional 4,6 subnet subnet of this resource string required IPv4 or IPv6 CIDR. Must not overlap ips IP ranges for this resource to use list of strings optional array of IP ranges and single IP address excludeIPs isolated IP ranges for this resource to filter list of strings optional array of IP ranges and single IP address gateway gateway for this resource string optional an IP address vlan vlan ID(deprecated) int optional [0,4094] 0 routes custom routes in this resource list of Route optional","title":"Spec"},{"location":"reference/crd-spidersubnet/#status-subresource","text":"The Subnet status is a subresource that processed automatically by the system to summarize the current state. Field Description Schema controlledIPPools current IP allocations in this subnet resource string totalIPCount total IP addresses counts of this subnet resource to use int allocatedIPCount current allocated IP addresses counts int","title":"Status (subresource)"},{"location":"reference/metrics/","text":"Metric Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller. spiderpool controller The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 spiderpool agent The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711 Get Started Enable Metric support Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true . Metric reference Spiderpool Agent Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric) Spiderpool Controller Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric)","title":"Metrics"},{"location":"reference/metrics/#metric","text":"Spiderpool can be configured to serve Opentelemetry metrics. And spiderpool metrics provide the insight of Spiderpool Agent and Spiderpool Controller.","title":"Metric"},{"location":"reference/metrics/#spiderpool-controller","text":"The metrics of spiderpool controller is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721","title":"spiderpool controller"},{"location":"reference/metrics/#spiderpool-agent","text":"The metrics of spiderpool agent is set by the following pod environment: environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_ENABLED_DEBUG_METRIC enable debug level metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5711","title":"spiderpool agent"},{"location":"reference/metrics/#get-started","text":"","title":"Get Started"},{"location":"reference/metrics/#enable-metric-support","text":"Check the environment variable SPIDERPOOL_ENABLED_METRIC of the daemonset spiderpool-agent for whether it is already set to true or not. Check the environment variable SPIDERPOOL_ENABLED_METRIC of deployment spiderpool-controller for whether it is already set to true or not. kubectl -n kube-system get daemonset spiderpool-agent -o yaml ------ kubectl -n kube-system get deployment spiderpool-controller -o yaml You can set one or both of them to true . For example, let's enable spiderpool agent metrics by running helm upgrade --set spiderpoolAgent.prometheus.enabled=true .","title":"Enable Metric support"},{"location":"reference/metrics/#metric-reference","text":"","title":"Metric reference"},{"location":"reference/metrics/#spiderpool-agent_1","text":"Spiderpool agent exports some metrics related with IPAM allocation and release. Currently, those include: Name description spiderpool_ipam_allocation_counts Number of IPAM allocation requests that Spiderpool Agent received , prometheus type: counter spiderpool_ipam_allocation_failure_counts Number of Spiderpool Agent IPAM allocation failures, prometheus type: counter spiderpool_ipam_allocation_update_ippool_conflict_counts Number of Spiderpool Agent IPAM allocation update IPPool conflicts, prometheus type: counter spiderpool_ipam_allocation_err_internal_counts Number of Spiderpool Agent IPAM allocation internal errors, prometheus type: counter spiderpool_ipam_allocation_err_no_available_pool_counts Number of Spiderpool Agent IPAM allocation no available IPPool errors, prometheus type: counter spiderpool_ipam_allocation_err_retries_exhausted_counts Number of Spiderpool Agent IPAM allocation retries exhausted errors, prometheus type: counter spiderpool_ipam_allocation_err_ip_used_out_counts Number of Spiderpool Agent IPAM allocation IP addresses used out errors, prometheus type: counter spiderpool_ipam_allocation_average_duration_seconds The average duration of all Spiderpool Agent allocation processes, prometheus type: gauge spiderpool_ipam_allocation_max_duration_seconds The maximum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_min_duration_seconds The minimum duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_latest_duration_seconds The latest duration of Spiderpool Agent allocation process (per-process), prometheus type: gauge spiderpool_ipam_allocation_duration_seconds Histogram of IPAM allocation duration in seconds, prometheus type: histogram spiderpool_ipam_allocation_average_limit_duration_seconds The average duration of all Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_max_limit_duration_seconds The maximum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_min_limit_duration_seconds The minimum duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_latest_limit_duration_seconds The latest duration of Spiderpool Agent allocation queuing, prometheus type: gauge spiderpool_ipam_allocation_limit_duration_seconds Histogram of IPAM allocation queuing duration in seconds, prometheus type: histogram spiderpool_ipam_release_counts Count of the number of Spiderpool Agent received the IPAM release requests, prometheus type: counter spiderpool_ipam_release_failure_counts Number of Spiderpool Agent IPAM release failure, prometheus type: counter spiderpool_ipam_release_update_ippool_conflict_counts Number of Spiderpool Agent IPAM release update IPPool conflicts, prometheus type: counter spiderpool_ipam_release_err_internal_counts Number of Spiderpool Agent IPAM releasing internal error, prometheus type: counter spiderpool_ipam_release_err_retries_exhausted_counts Number of Spiderpool Agent IPAM releasing retries exhausted error, prometheus type: counter spiderpool_ipam_release_average_duration_seconds The average duration of all Spiderpool Agent release processes, prometheus type: gauge spiderpool_ipam_release_max_duration_seconds The maximum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_min_duration_seconds The minimum duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_latest_duration_seconds The latest duration of Spiderpool Agent release process (per-process), prometheus type: gauge spiderpool_ipam_release_duration_seconds Histogram of IPAM release duration in seconds, prometheus type: histogram spiderpool_ipam_release_average_limit_duration_seconds The average duration of all Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_max_limit_duration_seconds The maximum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_min_limit_duration_seconds The minimum duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_latest_limit_duration_seconds The latest duration of Spiderpool Agent release queuing, prometheus type: gauge spiderpool_ipam_release_limit_duration_seconds Histogram of IPAM release queuing duration in seconds, prometheus type: histogram spiderpool_debug_auto_pool_waited_for_available_counts Number of Spiderpool Agent IPAM allocation wait for auto-created IPPool available, prometheus type: counter. (debug level metric)","title":"Spiderpool Agent"},{"location":"reference/metrics/#spiderpool-controller_1","text":"Spiderpool controller exports some metrics related with SpiderIPPool IP garbage collection. Currently, those include: Name description spiderpool_ip_gc_counts Number of Spiderpool Controller IP garbage collection, prometheus type: counter. spiderpool_ip_gc_failure_counts Number of Spiderpool Controller IP garbage collection failures, prometheus type: counter. spiderpool_total_ippool_counts Number of Spiderpool IPPools, prometheus type: gauge. spiderpool_debug_ippool_total_ip_counts Number of Spiderpool IPPool corresponding total IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_debug_ippool_available_ip_counts Number of Spiderpool IPPool corresponding availbale IPs (per-IPPool), prometheus type: gauge. (debug level metric) spiderpool_total_subnet_counts Number of Spiderpool Subnets, prometheus type: gauge. spiderpool_debug_subnet_ippool_counts Number of Spiderpool Subnet corresponding IPPools (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_total_ip_counts Number of Spiderpool Subnet corresponding total IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_subnet_available_ip_counts Number of Spiderpool Subnet corresponding availbale IPs (per-Subnet), prometheus type: gauge. (debug level metric) spiderpool_debug_auto_pool_waited_for_available_counts Number of waiting for auto-created IPPool available, prometheus type: couter. (debug level metric)","title":"Spiderpool Controller"},{"location":"reference/plugin-ifacer/","text":"CNI meta-plugin: ifacer Introduction When Pods use VLAN networks, network administrators may need to manually configure various VLAN or Bond interfaces on the nodes in advance. This process can be tedious and error-prone. Spiderpool provides a CNI meta-plugin called ifacer . This plugin dynamically creates VLAN sub-interfaces or Bond interfaces on the nodes during Pod creation, based on the provided ifacer configuration, greatly simplifying the configuration workload. In the following sections, we will delve into this plugin. Feature Support dynamic creation of VLAN sub-interfaces Support dynamic creation of Bond interfaces Notes The VLAN/Bond interfaces created by this plugin will be lost when the node restarts, but they will be automatically recreated upon the Pod restarts. Deleting existed VLAN/Bond interfaces is not supported. Configuring the address of VLAN/Bond interfaces during creation is not supported. If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, Highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with Vlan and Bond interfaces created by Ifacer : ~# INTERFACE = <your_interface_name> ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${INTERFACE} > EOF ~# systemctl restart NetworkManager Prerequisite There are no specific requirements including Kubernetes or Kernel versions for using this plugin. During the installation of Spiderpool, the plugin will be automatically installed in the /opt/cni/bin/ directory on each host. You can verify by checking for the presence of the ifacer binary in that directory on each host. How to use Examples please see Ifacer Configuration","title":"Ifacer plugin"},{"location":"reference/plugin-ifacer/#cni-meta-plugin-ifacer","text":"","title":"CNI meta-plugin: ifacer"},{"location":"reference/plugin-ifacer/#introduction","text":"When Pods use VLAN networks, network administrators may need to manually configure various VLAN or Bond interfaces on the nodes in advance. This process can be tedious and error-prone. Spiderpool provides a CNI meta-plugin called ifacer . This plugin dynamically creates VLAN sub-interfaces or Bond interfaces on the nodes during Pod creation, based on the provided ifacer configuration, greatly simplifying the configuration workload. In the following sections, we will delve into this plugin.","title":"Introduction"},{"location":"reference/plugin-ifacer/#feature","text":"Support dynamic creation of VLAN sub-interfaces Support dynamic creation of Bond interfaces","title":"Feature"},{"location":"reference/plugin-ifacer/#notes","text":"The VLAN/Bond interfaces created by this plugin will be lost when the node restarts, but they will be automatically recreated upon the Pod restarts. Deleting existed VLAN/Bond interfaces is not supported. Configuring the address of VLAN/Bond interfaces during creation is not supported. If your OS(such as Fedora, CentOS, etc.) uses NetworkManager, Highly recommend configuring following configuration file at /etc/NetworkManager/conf.d/spidernet.conf to prevent interference from NetworkManager with Vlan and Bond interfaces created by Ifacer : ~# INTERFACE = <your_interface_name> ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"Notes"},{"location":"reference/plugin-ifacer/#prerequisite","text":"There are no specific requirements including Kubernetes or Kernel versions for using this plugin. During the installation of Spiderpool, the plugin will be automatically installed in the /opt/cni/bin/ directory on each host. You can verify by checking for the presence of the ifacer binary in that directory on each host.","title":"Prerequisite"},{"location":"reference/plugin-ifacer/#how-to-use","text":"Examples please see Ifacer Configuration","title":"How to use"},{"location":"reference/plugin-ipam/","text":"IPAM Plugin Configuration Here is an example of IPAM configuration. { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-pod-network\" , \"plugins\" :[ { \"name\" : \"macvlan-pod-network\" , \"type\" : \"macvlan\" , \"master\" : \"ens256\" , \"mode\" : \"bridge\" , \"mtu\" : 1500 , \"ipam\" :{ \"type\" : \"spiderpool\" , \"log_file_path\" : \"/var/log/spidernet/spiderpool.log\" , \"log_file_max_size\" : \"100\" , \"log_file_max_age\" : \"30\" , \"log_file_max_count\" : 7 , \"log_level\" : \"INFO\" , \"default_ipv4_ippool\" : [ \"default-ipv4-pool1\" , \"default-ipv4-pool2\" ], \"default_ipv6_ippool\" : [ \"default-ipv6-pool1\" , \"default-ipv6-pool2\" ] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM plugin"},{"location":"reference/plugin-ipam/#ipam-plugin-configuration","text":"Here is an example of IPAM configuration. { \"cniVersion\" : \"0.3.1\" , \"name\" : \"macvlan-pod-network\" , \"plugins\" :[ { \"name\" : \"macvlan-pod-network\" , \"type\" : \"macvlan\" , \"master\" : \"ens256\" , \"mode\" : \"bridge\" , \"mtu\" : 1500 , \"ipam\" :{ \"type\" : \"spiderpool\" , \"log_file_path\" : \"/var/log/spidernet/spiderpool.log\" , \"log_file_max_size\" : \"100\" , \"log_file_max_age\" : \"30\" , \"log_file_max_count\" : 7 , \"log_level\" : \"INFO\" , \"default_ipv4_ippool\" : [ \"default-ipv4-pool1\" , \"default-ipv4-pool2\" ], \"default_ipv6_ippool\" : [ \"default-ipv6-pool1\" , \"default-ipv6-pool2\" ] } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100\" (unit MByte). log_file_max_age (string, optional): Max age of each rotated file, default to \"30\" (unit Day). log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . default_ipv4_ippool (string array, optional): Default IPAM IPv4 Pool to use. default_ipv6_ippool (string array, optional): Default IPAM IPv6 Pool to use.","title":"IPAM Plugin Configuration"},{"location":"reference/spiderpool-agent/","text":"spiderpool-agent This page describes CLI options and ENV of spiderpool-agent. spiderpool-agent daemon Run the spiderpool agent daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin ENV env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide. spiderpool-agent shutdown Notify of stopping the spiderpool-agent daemon. spiderpool-agent metric Get local metrics. Options --port string http server port of local metric (default to 5711)","title":"spiderpool-agent"},{"location":"reference/spiderpool-agent/#spiderpool-agent","text":"This page describes CLI options and ENV of spiderpool-agent.","title":"spiderpool-agent"},{"location":"reference/spiderpool-agent/#spiderpool-agent-daemon","text":"Run the spiderpool agent daemon.","title":"spiderpool-agent daemon"},{"location":"reference/spiderpool-agent/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin","title":"Options"},{"location":"reference/spiderpool-agent/#env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listening on. Disabled if empty. SPIDERPOOL_UPDATE_CR_MAX_RETRIES 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide.","title":"ENV"},{"location":"reference/spiderpool-agent/#spiderpool-agent-shutdown","text":"Notify of stopping the spiderpool-agent daemon.","title":"spiderpool-agent shutdown"},{"location":"reference/spiderpool-agent/#spiderpool-agent-metric","text":"Get local metrics.","title":"spiderpool-agent metric"},{"location":"reference/spiderpool-agent/#options_1","text":"--port string http server port of local metric (default to 5711)","title":"Options"},{"location":"reference/spiderpool-controller/","text":"spiderpool-controller This page describes CLI options and ENV of spiderpool-controller. spiderpool-controller daemon Run the spiderpool controller daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) ENV env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_ENABLED_DEBUG_METRIC false Enable spiderpool agent to collect debug level metrics. SPIDERPOOL_METRIC_HTTP_PORT false The metrics port of spiderpool agent. SPIDERPOOL_GOPS_LISTEN_PORT 5724 The gops port of spiderpool Controller. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_HEALTH_PORT 5720 The http Port for spiderpoolController, for health checking and http service. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED true Enable/disable IP GC for Terminating pod. SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY true The gc delay seconds after the pod times out of deleting graceful period. SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION true The gc all interval duration. SPIDERPOOL_MULTUS_CONFIG_ENABLED true Enable/disable SpiderMultusConfig. SPIDERPOOL_CNI_CONFIG_DIR true The host path of the cni config directory. SPIDERPOOL_CILIUM_CONFIGMAP_NAMESPACE_NAME true The cilium's configMap, default is kube-system/cilium-config. spiderpool-controller shutdown Notify of stopping spiderpool-controller daemon. spiderpool-controller metric Get local metrics. Options --port string http server port of local metric (default to 5721) spiderpool-controller status Show status: Whether local is controller leader ... Options --port string http server port of local metric (default to 5720)","title":"spiderpool-controller"},{"location":"reference/spiderpool-controller/#spiderpool-controller","text":"This page describes CLI options and ENV of spiderpool-controller.","title":"spiderpool-controller"},{"location":"reference/spiderpool-controller/#spiderpool-controller-daemon","text":"Run the spiderpool controller daemon.","title":"spiderpool-controller daemon"},{"location":"reference/spiderpool-controller/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map)","title":"Options"},{"location":"reference/spiderpool-controller/#env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Enable/disable metrics. SPIDERPOOL_ENABLED_DEBUG_METRIC false Enable spiderpool agent to collect debug level metrics. SPIDERPOOL_METRIC_HTTP_PORT false The metrics port of spiderpool agent. SPIDERPOOL_GOPS_LISTEN_PORT 5724 The gops port of spiderpool Controller. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_HEALTH_PORT 5720 The http Port for spiderpoolController, for health checking and http service. SPIDERPOOL_GC_IP_ENABLED true Enable/disable IP GC. SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED true Enable/disable IP GC for Terminating pod. SPIDERPOOL_GC_ADDITIONAL_GRACE_DELAY true The gc delay seconds after the pod times out of deleting graceful period. SPIDERPOOL_GC_DEFAULT_INTERVAL_DURATION true The gc all interval duration. SPIDERPOOL_MULTUS_CONFIG_ENABLED true Enable/disable SpiderMultusConfig. SPIDERPOOL_CNI_CONFIG_DIR true The host path of the cni config directory. SPIDERPOOL_CILIUM_CONFIGMAP_NAMESPACE_NAME true The cilium's configMap, default is kube-system/cilium-config.","title":"ENV"},{"location":"reference/spiderpool-controller/#spiderpool-controller-shutdown","text":"Notify of stopping spiderpool-controller daemon.","title":"spiderpool-controller shutdown"},{"location":"reference/spiderpool-controller/#spiderpool-controller-metric","text":"Get local metrics.","title":"spiderpool-controller metric"},{"location":"reference/spiderpool-controller/#options_1","text":"--port string http server port of local metric (default to 5721)","title":"Options"},{"location":"reference/spiderpool-controller/#spiderpool-controller-status","text":"Show status: Whether local is controller leader ...","title":"spiderpool-controller status"},{"location":"reference/spiderpool-controller/#options_2","text":"--port string http server port of local metric (default to 5720)","title":"Options"},{"location":"reference/spiderpoolctl/","text":"spiderpoolctl This page describes CLI usage of spiderpoolctl for debug. spiderpoolctl gc Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address) spiderpoolctl ip show Show a pod that is taking this IP. Options --ip string [required] ip spiderpoolctl ip release Try to release an IP. Options --ip string [optional] ip --force [optional] force release ip spiderpoolctl ip set Set IP to be taken by a pod. This will update ippool and workload endpoint resource. Options --ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"spiderpoolctl"},{"location":"reference/spiderpoolctl/#spiderpoolctl","text":"This page describes CLI usage of spiderpoolctl for debug.","title":"spiderpoolctl"},{"location":"reference/spiderpoolctl/#spiderpoolctl-gc","text":"Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address)","title":"spiderpoolctl gc"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-show","text":"Show a pod that is taking this IP.","title":"spiderpoolctl ip show"},{"location":"reference/spiderpoolctl/#options","text":"--ip string [required] ip","title":"Options"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-release","text":"Try to release an IP.","title":"spiderpoolctl ip release"},{"location":"reference/spiderpoolctl/#options_1","text":"--ip string [optional] ip --force [optional] force release ip","title":"Options"},{"location":"reference/spiderpoolctl/#spiderpoolctl-ip-set","text":"Set IP to be taken by a pod. This will update ippool and workload endpoint resource.","title":"spiderpoolctl ip set"},{"location":"reference/spiderpoolctl/#options_2","text":"--ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"Options"},{"location":"usage/_feature_example_zh/","text":"\u67d0\u529f\u80fd \u4ecb\u7ecd \u672c\u6587\u4e3a\u4e86\u6f14\u793a\u4ec0\u4e48\uff0c\u5b83\u7684\u5e94\u7528\u573a\u666f \u9879\u76ee\u529f\u80fd \u672c\u9879\u76ee\u6709\u4ec0\u4e48\u529f\u80fd\uff0c\u5b83\u4e3a\u4ec0\u4e48\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u529f\u80fd\u7684\u5e94\u7528\u573a\u666f\uff0c\u529f\u80fd\u5b9e\u65bd\u7684\u9650\u5236\u6709\u54ea\u4e9b \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49\uff0c\u672c\u9879\u76ee\u5b89\u88c5\u65f6\u54ea\u4e9b\u9009\u578b\u8981\u6253\u5f00\u6216\u5173\u95ed \u6b65\u9aa4 step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b(\u7528\u4e8e\u6392\u969c)\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6","title":"\u67d0\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_1","text":"","title":"\u67d0\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_2","text":"\u672c\u6587\u4e3a\u4e86\u6f14\u793a\u4ec0\u4e48\uff0c\u5b83\u7684\u5e94\u7528\u573a\u666f","title":"\u4ecb\u7ecd"},{"location":"usage/_feature_example_zh/#_3","text":"\u672c\u9879\u76ee\u6709\u4ec0\u4e48\u529f\u80fd\uff0c\u5b83\u4e3a\u4ec0\u4e48\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u529f\u80fd\u7684\u5e94\u7528\u573a\u666f\uff0c\u529f\u80fd\u5b9e\u65bd\u7684\u9650\u5236\u6709\u54ea\u4e9b","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/_feature_example_zh/#_4","text":"\u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49\uff0c\u672c\u9879\u76ee\u5b89\u88c5\u65f6\u54ea\u4e9b\u9009\u578b\u8981\u6253\u5f00\u6216\u5173\u95ed","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/_feature_example_zh/#_5","text":"step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b(\u7528\u4e8e\u6392\u969c)\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6","title":"\u6b65\u9aa4"},{"location":"usage/_install_example_zh/","text":"\u5b89\u88c5\u6587\u6863 \u4ecb\u7ecd \u672c\u6587\u8bf4\u660e\u4e3a\u4e86\u5b89\u88c5\u51fa\u4ec0\u4e48\u6837\u7684\u4e00\u5957\u96c6\u7fa4\uff0c\u5b83\u7684\u4ef7\u503c\u662f\u4ec0\u4e48 \u5b9e\u65bd\u8981\u6c42 \u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49 \u6b65\u9aa4 step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6 \u53ea\u8c08\u5b89\u88c5\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u72b6\u6001","title":"\u5b89\u88c5\u6587\u6863"},{"location":"usage/_install_example_zh/#_1","text":"","title":"\u5b89\u88c5\u6587\u6863"},{"location":"usage/_install_example_zh/#_2","text":"\u672c\u6587\u8bf4\u660e\u4e3a\u4e86\u5b89\u88c5\u51fa\u4ec0\u4e48\u6837\u7684\u4e00\u5957\u96c6\u7fa4\uff0c\u5b83\u7684\u4ef7\u503c\u662f\u4ec0\u4e48","title":"\u4ecb\u7ecd"},{"location":"usage/_install_example_zh/#_3","text":"\u5b89\u88c5\u8981\u6c42\uff0c\u5982\u5185\u6838\u9650\u5236\u3001K8s \u7248\u672c\u3001\u7b2c\u4e09\u65b9\u9879\u76ee\u7248\u672c\u7b49","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/_install_example_zh/#_4","text":"step by step \u5c0f\u767d\u53ef\u5b9e\u65bd\uff0c\u6bcf\u4e00\u6b65\u9aa4\u7684\u7ed3\u679c\u786e\u8ba4\u548c\u72b6\u6001\u67e5\u770b\uff0c\u7279\u6b8a\u8bf4\u660e\uff0cyaml \u6709\u5bf9\u5e94\u7684\u5de5\u7a0b\u6587\u4ef6 \u53ea\u8c08\u5b89\u88c5\uff0c\u786e\u8ba4\u5b89\u88c5\u6210\u529f\u72b6\u6001","title":"\u6b65\u9aa4"},{"location":"usage/cilium-chaining-zh_CN/","text":"Cilium \u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u652f\u6301 English | \u7b80\u4f53\u4e2d\u6587 \u4ecb\u7ecd \u672c\u6587\u4ecb\u7ecd IPVlan \u5982\u4f55\u4e0e Cilium \u96c6\u6210\uff0c\u4e3a IPVlan CNI \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002 \u80cc\u666f \u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI \u5982 IPVlan\u3001Macvlan \u7b49, \u5e76\u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u501f\u52a9 Cilium chaining-mode \u529f\u80fd\u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002 \u4f46 Cilium \u5728 1.12 \u7248\u672c\u6b63\u5f0f\u79fb\u9664\u4e86\u5bf9 IPVlan Dataplane \u7684\u652f\u6301, \u8be6\u89c1 removed-options \u3002 \u7531\u4e8e\u53d7\u5230 Terway \u7684\u542f\u53d1\uff0c cilium-chaining \u9879\u76ee\u57fa\u4e8e Cilium v1.12.7 \u7248\u672c\u4fee\u6539 IPVlan Dataplane \u90e8\u5206, \u4f7f Cilium \u80fd\u591f\u4ee5 chaining-mode \u7684\u65b9\u5f0f\u4e0e IPVlan \u4e00\u8d77\u5de5\u4f5c\u3002\u89e3\u51b3 IPVlan \u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002 \u73af\u5883\u51c6\u5907 \u8981\u6c42\u8282\u70b9\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4\uff0c\u5e76\u6ce8\u610f\u4e0d\u80fd\u5b89\u88c5 Cilium \u5df2\u5b89\u88c5 Helm \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5b89\u88c5 Cilium-chaining \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 cilium-chaining \u7ec4\u4ef6: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system \u9a8c\u8bc1\u5b89\u88c5: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s \u914d\u7f6e CNI \u521b\u5efa Multus NetworkAttachmentDefinition CR, \u5982\u4e0b\u662f\u521b\u5efa IPvlan NetworkAttachmentDefinition \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u5728\u5982\u4e0b\u7684\u914d\u7f6e\u4e2d\uff0c\u6307\u5b9a master \u4e3a ens192, ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u8282\u70b9\u4e0a \u5c06 cilium \u5d4c\u5165\u5230 CNI \u914d\u7f6e\u4e2d\uff0c\u653e\u7f6e\u4e8e ipvlan plugin \u4e4b\u540e CNI \u7684 name \u5fc5\u987b\u548c\u5b89\u88c5 cilium-chaining \u65f6\u7684 cniChainingMode \u4fdd\u6301\u4e00\u81f4\uff0c\u5426\u5219\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF \u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\u4f7f\u7528 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u6587\u4ef6: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none> \u9a8c\u8bc1\u7f51\u7edc\u7b56\u7565\u662f\u5426\u751f\u6548 \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms \u521b\u5efa\u7981\u6b62 Pod \u4e0e\u5916\u90e8\u901a\u4fe1\u7684\u7f51\u7edc\u7b56\u7565 ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all \u6839\u636e label \u5339\u914d\u6240\u6709 pod, \u8be5\u7b56\u7565\u7981\u6b62 Pod \u5bf9\u5916\u901a\u4fe1 \u518d\u6b21\u9a8c\u8bc1 Pod \u5bf9\u5916\u901a\u4fe1 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss \u603b\u7ed3 \u901a\u8fc7\u6d4b\u8bd5\u53ef\u4ee5\u770b\u51fa\uff0cPod \u8bbf\u95ee\u5916\u90e8\u7684\u6d41\u91cf\u88ab\u7981\u6b62\uff0c\u7f51\u7edc\u7b56\u7565\u751f\u6548\uff0c\u8bc1\u660e\u901a\u8fc7 Cilium-chaining \u9879\u76ee \u5e2e\u52a9 IPVlan \u5b9e\u73b0\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"Cilium \u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u652f\u6301"},{"location":"usage/cilium-chaining-zh_CN/#cilium-ipvlan","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Cilium \u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u652f\u6301"},{"location":"usage/cilium-chaining-zh_CN/#_1","text":"\u672c\u6587\u4ecb\u7ecd IPVlan \u5982\u4f55\u4e0e Cilium \u96c6\u6210\uff0c\u4e3a IPVlan CNI \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/cilium-chaining-zh_CN/#_2","text":"\u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI \u5982 IPVlan\u3001Macvlan \u7b49, \u5e76\u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u501f\u52a9 Cilium chaining-mode \u529f\u80fd\u4e3a IPVlan \u63d0\u4f9b\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002 \u4f46 Cilium \u5728 1.12 \u7248\u672c\u6b63\u5f0f\u79fb\u9664\u4e86\u5bf9 IPVlan Dataplane \u7684\u652f\u6301, \u8be6\u89c1 removed-options \u3002 \u7531\u4e8e\u53d7\u5230 Terway \u7684\u542f\u53d1\uff0c cilium-chaining \u9879\u76ee\u57fa\u4e8e Cilium v1.12.7 \u7248\u672c\u4fee\u6539 IPVlan Dataplane \u90e8\u5206, \u4f7f Cilium \u80fd\u591f\u4ee5 chaining-mode \u7684\u65b9\u5f0f\u4e0e IPVlan \u4e00\u8d77\u5de5\u4f5c\u3002\u89e3\u51b3 IPVlan \u4e0d\u652f\u6301 Kubernetes \u539f\u751f\u7684\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"\u80cc\u666f"},{"location":"usage/cilium-chaining-zh_CN/#_3","text":"\u8981\u6c42\u8282\u70b9\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4\uff0c\u5e76\u6ce8\u610f\u4e0d\u80fd\u5b89\u88c5 Cilium \u5df2\u5b89\u88c5 Helm","title":"\u73af\u5883\u51c6\u5907"},{"location":"usage/cilium-chaining-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/cilium-chaining-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool.","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/cilium-chaining-zh_CN/#cilium-chaining","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 cilium-chaining \u7ec4\u4ef6: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system \u9a8c\u8bc1\u5b89\u88c5: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s","title":"\u5b89\u88c5 Cilium-chaining"},{"location":"usage/cilium-chaining-zh_CN/#cni","text":"\u521b\u5efa Multus NetworkAttachmentDefinition CR, \u5982\u4e0b\u662f\u521b\u5efa IPvlan NetworkAttachmentDefinition \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u5728\u5982\u4e0b\u7684\u914d\u7f6e\u4e2d\uff0c\u6307\u5b9a master \u4e3a ens192, ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u8282\u70b9\u4e0a \u5c06 cilium \u5d4c\u5165\u5230 CNI \u914d\u7f6e\u4e2d\uff0c\u653e\u7f6e\u4e8e ipvlan plugin \u4e4b\u540e CNI \u7684 name \u5fc5\u987b\u548c\u5b89\u88c5 cilium-chaining \u65f6\u7684 cniChainingMode \u4fdd\u6301\u4e00\u81f4\uff0c\u5426\u5219\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF","title":"\u914d\u7f6e CNI"},{"location":"usage/cilium-chaining-zh_CN/#_5","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\u4f7f\u7528 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u6587\u4ef6: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none>","title":"\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528"},{"location":"usage/cilium-chaining-zh_CN/#_6","text":"\u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms \u521b\u5efa\u7981\u6b62 Pod \u4e0e\u5916\u90e8\u901a\u4fe1\u7684\u7f51\u7edc\u7b56\u7565 ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all \u6839\u636e label \u5339\u914d\u6240\u6709 pod, \u8be5\u7b56\u7565\u7981\u6b62 Pod \u5bf9\u5916\u901a\u4fe1 \u518d\u6b21\u9a8c\u8bc1 Pod \u5bf9\u5916\u901a\u4fe1 ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss","title":"\u9a8c\u8bc1\u7f51\u7edc\u7b56\u7565\u662f\u5426\u751f\u6548"},{"location":"usage/cilium-chaining-zh_CN/#_7","text":"\u901a\u8fc7\u6d4b\u8bd5\u53ef\u4ee5\u770b\u51fa\uff0cPod \u8bbf\u95ee\u5916\u90e8\u7684\u6d41\u91cf\u88ab\u7981\u6b62\uff0c\u7f51\u7edc\u7b56\u7565\u751f\u6548\uff0c\u8bc1\u660e\u901a\u8fc7 Cilium-chaining \u9879\u76ee \u5e2e\u52a9 IPVlan \u5b9e\u73b0\u7f51\u7edc\u7b56\u7565\u80fd\u529b\u3002","title":"\u603b\u7ed3"},{"location":"usage/cilium-chaining/","text":"Cilium provides network policy support for IPVlan English | \u7b80\u4f53\u4e2d\u6587 Introduction This article describes how IPVlan integrates with Cilium to provide network policy capabilities for IPVlan CNI. background Currently, most Underlay type CNIs in the community, such as IPVlan, Macvlan, etc., do not support Kubernetes' native network policy capabilities. We can use the Cilium chaining-mode function to provide network policy capabilities for IPVlan.However, Cilium officially removed support for IPVlan Dataplane in version 1.12. For details, see removed-options . Inspired by Terway , the cilium-chaining project is based on Cilium The v1.12.7 version modifies the IPVlan Dataplane part to enable Cilium to work with IPVlan in chaining-mode. Solve the problem that IPVlan does not support Kubernetes\u2019 native network policy capabilities. Prerequisites The node kernel version is required to be at least greater than 4.19 Prepare a Kubernetes cluster and be careful not to install Cilium Installed Helm Steps Install Spiderpool Refer to Installation to install Spiderpool. Install Cilium-chaining Install the cilium-chaining component using the following command: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system Verify installation: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s Configure CNI Create Multus NetworkAttachmentDefinition CR. The following is an example of creating an IPvlan NetworkAttachmentDefinition configuration: In the following configuration, specify master as ens192, ens192 must exist on the node Embed cilium into CNI configuration, placed after ipvlan plugin The name of CNI must be consistent with the cniChainingMode when installing cilium-chaining, otherwise it will not work properly IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF Create a test application In the following example Yaml, a set of DaemonSet applications will be created, using v1.multus-cni.io/default-network : used to specify the CNI configuration file used by the application: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF Check Pod running status: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none> Verify whether the network policy is effective Test the communication between Pods and Pods across nodes and subnets ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms Create a network policy that prohibits Pods from communicating with the outside world ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all matches all pods based on label. This policy prohibits pods from communicating with others. Verify Pod external communication again ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss Conclusion From the results, it can be seen that the Pod's access to external traffic is prohibited and the network policy takes effect, proving that the Cilium-chaining project helps IPVlan achieve network policy capabilities.","title":"Network Policy Support"},{"location":"usage/cilium-chaining/#cilium-provides-network-policy-support-for-ipvlan","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Cilium provides network policy support for IPVlan"},{"location":"usage/cilium-chaining/#introduction","text":"This article describes how IPVlan integrates with Cilium to provide network policy capabilities for IPVlan CNI.","title":"Introduction"},{"location":"usage/cilium-chaining/#background","text":"Currently, most Underlay type CNIs in the community, such as IPVlan, Macvlan, etc., do not support Kubernetes' native network policy capabilities. We can use the Cilium chaining-mode function to provide network policy capabilities for IPVlan.However, Cilium officially removed support for IPVlan Dataplane in version 1.12. For details, see removed-options . Inspired by Terway , the cilium-chaining project is based on Cilium The v1.12.7 version modifies the IPVlan Dataplane part to enable Cilium to work with IPVlan in chaining-mode. Solve the problem that IPVlan does not support Kubernetes\u2019 native network policy capabilities.","title":"background"},{"location":"usage/cilium-chaining/#prerequisites","text":"The node kernel version is required to be at least greater than 4.19 Prepare a Kubernetes cluster and be careful not to install Cilium Installed Helm","title":"Prerequisites"},{"location":"usage/cilium-chaining/#steps","text":"","title":"Steps"},{"location":"usage/cilium-chaining/#install-spiderpool","text":"Refer to Installation to install Spiderpool.","title":"Install Spiderpool"},{"location":"usage/cilium-chaining/#install-cilium-chaining","text":"Install the cilium-chaining component using the following command: helm repo add cilium-chaining https://spidernet-io.github.io/cilium-chaining helm repo update cilium-chaining helm install cilium-chaining/cilium-chaining --namespace kube-system Verify installation: ~# kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE cilium-chaining-4xnnm 1 /1 Running 0 5m48s cilium-chaining-82ptj 1 /1 Running 0 5m48s","title":"Install Cilium-chaining"},{"location":"usage/cilium-chaining/#configure-cni","text":"Create Multus NetworkAttachmentDefinition CR. The following is an example of creating an IPvlan NetworkAttachmentDefinition configuration: In the following configuration, specify master as ens192, ens192 must exist on the node Embed cilium into CNI configuration, placed after ipvlan plugin The name of CNI must be consistent with the cniChainingMode when installing cilium-chaining, otherwise it will not work properly IPVLAN_MASTER_INTERFACE = \"ens192\" CNI_CHAINING_MODE = \"terway-chainer\" cat <<EOF | kubectl apply -f - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: ipvlan-ens192 namespace: kube-system spec: config: | { \"cniVersion\": \"0.4.0\", \"name\": \"${CNI_CHAINING_MODE}\", \"plugins\": [ { \"type\": \"ipvlan\", \"mode\": \"l2\", \"master\": \"${IPVLAN_MASTER_INTERFACE}\", \"ipam\": { \"type\": \"spiderpool\" } }, { \"type\": \"cilium-cni\" }, { \"type\": \"coordinator\" }] } EOF","title":"Configure CNI"},{"location":"usage/cilium-chaining/#create-a-test-application","text":"In the following example Yaml, a set of DaemonSet applications will be created, using v1.multus-cni.io/default-network : used to specify the CNI configuration file used by the application: APP_NAME = test cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ${APP_NAME} name: ${APP_NAME} namespace: default spec: selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-ens192 spec: containers: - image: docker.io/centos/tools imagePullPolicy: IfNotPresent name: ${APP_NAME} ports: - name: http containerPort: 80 protocol: TCP EOF Check Pod running status: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-55c97ccfd8-l4h5w 1 /1 Running 0 3m50s 10 .6.185.217 worker1 <none> <none> test-55c97ccfd8-w62k7 1 /1 Running 0 3m50s 10 .6.185.206 controller1 <none> <none>","title":"Create a test application"},{"location":"usage/cilium-chaining/#verify-whether-the-network-policy-is-effective","text":"Test the communication between Pods and Pods across nodes and subnets ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.30 PING 10 .6.185.30 ( 10 .6.185.30 ) : 56 data bytes 64 bytes from 10 .6.185.30: seq = 0 ttl = 64 time = 1 .917 ms 64 bytes from 10 .6.185.30: seq = 1 ttl = 64 time = 1 .406 ms --- 10 .6.185.30 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 1 .406/1.661/1.917 ms ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes 64 bytes from 10 .6.185.206: seq = 0 ttl = 64 time = 1 .608 ms 64 bytes from 10 .6.185.206: seq = 1 ttl = 64 time = 0 .647 ms --- 10 .6.185.206 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .647/1.127/1.608 ms Create a network policy that prohibits Pods from communicating with the outside world ~# cat << EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all spec: podSelector: matchLabels: app: test policyTypes: - E gress - Ingress deny-all matches all pods based on label. This policy prohibits pods from communicating with others. Verify Pod external communication again ~# kubectl exec -it test-55c97ccfd8-l4h5w -- ping -c2 10 .6.185.206 kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. PING 10 .6.185.206 ( 10 .6.185.206 ) : 56 data bytes --- 10 .6.185.206 ping statistics --- 14 packets transmitted, 0 packets received, 100 % packet loss","title":"Verify whether the network policy is effective"},{"location":"usage/cilium-chaining/#conclusion","text":"From the results, it can be seen that the Pod's access to external traffic is prohibited and the network policy takes effect, proving that the Cilium-chaining project helps IPVlan achieve network policy capabilities.","title":"Conclusion"},{"location":"usage/cli/","text":"Command line tool (spiderpoolctl) TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/cli/#command-line-tool-spiderpoolctl","text":"TODO.","title":"Command line tool (spiderpoolctl)"},{"location":"usage/egress-zh_CN/","text":"Egress Policy English | \u7b80\u4f53\u4e2d\u6587 \u4ecb\u7ecd Spiderpool \u662f\u4e00\u4e2a Kubernetes \u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728 Kubernetes \u96c6\u7fa4\u4e2d\uff0cPod \u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u5176\u51fa\u53e3 IP \u5730\u5740\u4e0d\u662f\u56fa\u5b9a\u7684\u3002\u5728 Overlay \u7f51\u7edc\u4e2d\uff0c\u51fa\u53e3 IP \u5730\u5740\u4e3a Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u800c\u5728 Underlay \u7f51\u7edc\u4e2d\uff0cPod \u76f4\u63a5\u4f7f\u7528\u81ea\u8eab\u7684 IP \u5730\u5740\u4e0e\u5916\u90e8\u901a\u4fe1\u3002\u56e0\u6b64\uff0c\u5f53 Pod \u53d1\u751f\u65b0\u7684\u8c03\u5ea6\u65f6\uff0c\u65e0\u8bba\u54ea\u79cd\u7f51\u7edc\u6a21\u5f0f\uff0cPod \u4e0e\u5916\u90e8\u901a\u4fe1\u65f6\u7684 IP \u5730\u5740\u90fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7ed9\u7cfb\u7edf\u7ef4\u62a4\u4eba\u5458\u5e26\u6765\u4e86 IP \u5730\u5740\u7ba1\u7406\u7684\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\u4ee5\u53ca\u9700\u8981\u8fdb\u884c\u7f51\u7edc\u6545\u969c\u8bca\u65ad\u65f6\uff0c\u5728\u96c6\u7fa4\u5916\u90e8\uff0c\u57fa\u4e8e Pod \u539f\u672c\u7684\u51fa\u53e3 IP \u6765\u7ba1\u63a7\u51fa\u53e3\u6d41\u91cf\u5f88\u96be\u5b9e\u73b0\u3002\u800c Spiderpool \u53ef\u4ee5\u642d\u914d\u7ec4\u4ef6 EgressGateway \u5b8c\u7f8e\u89e3\u51b3 Underlay \u7f51\u7edc\u4e0b Pod \u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u7684\u95ee\u9898\u3002 \u9879\u76ee\u529f\u80fd EgressGateway \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 Egress \u7f51\u5173\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u4e0d\u540c CNI \u7f51\u7edc\u6a21\u5f0f\u4e0b\uff08Spiderpool\u3001Calico\u3001Flannel\u3001Weave\uff09\u51fa\u53e3 Egress IP \u5730\u5740\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7075\u6d3b\u914d\u7f6e\u548c\u7ba1\u7406\u51fa\u53e3\u7b56\u7565\uff0c\u4e3a\u79df\u6237\u7ea7\u6216\u96c6\u7fa4\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u7f6e Egress IP\uff0c\u4f7f\u5f97 Pod \u8bbf\u95ee\u5916\u90e8\u7f51\u7edc\u65f6\uff0c\u7cfb\u7edf\u4f1a\u7edf\u4e00\u4f7f\u7528\u8fd9\u4e2a\u8bbe\u7f6e\u7684 Egress IP \u4f5c\u4e3a\u51fa\u53e3\u5730\u5740\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002\u4f46 EgressGateway \u6240\u6709\u7684\u89c4\u5219\u90fd\u662f\u751f\u6548\u5728\u4e3b\u673a\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e0a\u7684\uff0c\u8981\u4f7f EgressGateway \u7b56\u7565\u751f\u6548\uff0c\u5219 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7684\u6d41\u91cf\uff0c\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u3002\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7 Spiderpool \u7684 spidercoordinators \u7684 spec.hijackCIDR \u5b57\u6bb5\u914d\u7f6e\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\uff0c\u518d\u901a\u8fc7 coordinator \u5c06\u5339\u914d\u7684\u6d41\u91cf\u4ece veth pair \u8f6c\u53d1\u5230\u4e3b\u673a\u4e0a\u3002\u4f7f\u5f97\u6240\u8bbf\u95ee\u7684\u5916\u90e8\u6d41\u91cf\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\uff0c\u501f\u6b64\u5b9e\u73b0 underlay \u7f51\u7edc\u4e0b\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u3002 Spiderpool \u642d\u914d EgressGateway \u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u529f\u80fd\uff1a \u89e3\u51b3 IPv4/IPv6 \u53cc\u6808\u8fde\u63a5\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u5728\u4e0d\u540c\u534f\u8bae\u6808\u4e0b\u7684\u65e0\u7f1d\u8fde\u63a5\u3002 \u89e3\u51b3 Egress \u8282\u70b9\u7684\u9ad8\u53ef\u7528\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u8fde\u901a\u6027\u4e0d\u53d7\u5355\u70b9\u6545\u969c\u7684\u5e72\u6270\u3002 \u5141\u8bb8\u66f4\u7cbe\u7ec6\u7684\u7b56\u7565\u63a7\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7 EgressGateway \u7075\u6d3b\u5730\u8fc7\u6ee4 Pods \u7684 Egress \u7b56\u7565\uff0c\u5305\u62ec Destination CIDR\u3002 \u5141\u8bb8\u8fc7\u6ee4 Egress \u5e94\u7528\uff08Pod\uff09\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u7ba1\u7406\u7279\u5b9a\u5e94\u7528\u7684\u51fa\u53e3\u6d41\u91cf\u3002 \u652f\u6301\u591a\u4e2a\u51fa\u53e3\u7f51\u5173\u5b9e\u4f8b\uff0c\u80fd\u591f\u5904\u7406\u591a\u4e2a\u7f51\u7edc\u5206\u533a\u6216\u96c6\u7fa4\u4e4b\u95f4\u7684\u901a\u4fe1\u3002 \u652f\u6301\u79df\u6237\u7ea7\u522b\u7684 Egress IP\u3002 \u652f\u6301\u81ea\u52a8\u68c0\u6d4b\u96c6\u7fa4\u6d41\u91cf\u7684 Egress \u7f51\u5173\u7b56\u7565\u3002 \u652f\u6301\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 Egress \u5b9e\u4f8b\u3002 \u53ef\u7528\u4e8e\u8f83\u4f4e\u5185\u6838\u7248\u672c\uff0c\u9002\u7528\u4e8e\u5404\u79cd Kubernetes \u90e8\u7f72\u73af\u5883\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool\uff0c\u5e76\u521b\u5efa SpiderMultusConfig CR \u4e0e IPPool CR. \u5728\u5b89\u88c5\u5b8c Spiderpool \u540e\uff0c\u5c06\u96c6\u7fa4\u5916\u7684\u670d\u52a1\u5730\u5740\u6dfb\u52a0\u5230 spiderpool.spidercoordinators \u7684 'default' \u5bf9\u8c61\u7684 'hijackCIDR' \u4e2d\uff0c\u4f7f Pod \u8bbf\u95ee\u8fd9\u4e9b\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u6d41\u91cf\u5148\u7ecf\u8fc7 Pod \u6240\u5728\u7684\u4e3b\u673a\uff0c\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\u3002 # \"10.6.168.63/32\" \u4e3a\u5916\u90e8\u670d\u52a1\u5730\u5740\u3002\u5bf9\u4e8e\u5df2\u7ecf\u8fd0\u884c\u7684 Pod\uff0c\u9700\u8981\u91cd\u542f Pod\uff0c\u8fd9\u4e9b\u8def\u7531\u89c4\u5219\u624d\u4f1a\u5728 Pod \u4e2d\u751f\u6548\u3002 ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}' \u5b89\u88c5 EgressGateway \u901a\u8fc7 helm \u5b89\u88c5 EgressGateway helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug \u5982\u679c\u9700\u8981\u4f7f\u7528 IPv6 \uff0c\u53ef\u4f7f\u7528\u9009\u9879 --set feature.enableIPv6=true \u5f00\u542f\uff0c\u5e76\u8bbe\u7f6e feature.tunnelIpv6Subnet , \u503c\u5f97\u6ce8\u610f\u7684\u662f\u5728\u901a\u8fc7 feature.tunnelIpv4Subnet \u4e0e feature.tunnelIpv6Subnet \u914d\u7f6e IPv4 \u6216 IPv6 \u7f51\u6bb5\u65f6\uff0c\u9700\u8981\u4fdd\u8bc1\u7f51\u6bb5\u548c\u96c6\u7fa4\u5185\u7684\u5176\u4ed6\u5730\u5740\u4e0d\u51b2\u7a81\u3002 feature.enableGatewayReplyRoute \u4e3a true \u65f6\uff0c\u5c06\u5f00\u542f\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u8fd4\u56de\u8def\u7531\u89c4\u5219\uff0c\u5728\u4e0e Spiderpool \u642d\u914d\u652f\u6301 underlay CNI \u65f6\uff0c\u5fc5\u987b\u5f00\u542f\u8be5\u9009\u9879\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u8fd8\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d EgressGateway \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u9a8c\u8bc1 EgressGateway \u5b89\u88c5 ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m \u66f4\u591a\u5b89\u88c5\u7ec6\u8282\uff0c\u53c2\u8003 EgressGateway \u5b89\u88c5 \u521b\u5efa EgressGateway \u5b9e\u4f8b EgressGateway \u5b9a\u4e49\u4e86\u4e00\u7ec4\u8282\u70b9\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u96c6\u7fa4\u5185\u7684 egress \u6d41\u91cf\u5c06\u4f1a\u901a\u8fc7\u8fd9\u7ec4\u8282\u70b9\u8f6c\u53d1\u800c\u51fa\u96c6\u7fa4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u9884\u5148\u5b9a\u4e49 EgressGateway \u5b9e\u4f8b\uff0c\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a EgressGateway \u5b9e\u4f8b\uff0c\u5176\u4e2d\uff1a spec.ippools.ipv4 \uff1a\u5b9a\u4e49\u4e86\u4e00\u7ec4 egress \u7684\u51fa\u53e3 IP \u5730\u5740\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u73af\u5883\u7684\u5b9e\u9645\u60c5\u51b5\u8c03\u6574\u3002\u5e76\u4e14 spec.ippools.ipv4 \u7684 CIDR \u5e94\u8be5\u4e0e\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u51fa\u53e3\u7f51\u5361\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\u662f\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361\uff09\u7684\u5b50\u7f51\u76f8\u540c\uff0c\u5426\u5219\uff0c\u53ef\u80fd\u5bfc\u81f4 egress \u8bbf\u95ee\u4e0d\u901a\u3002 spec.nodeSelector \uff1aEgressGateway \u63d0\u4f9b\u7684\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u5339\u914d\u65f6\uff0c\u8be5\u8282\u70b9\u5c06\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u4e0d\u5339\u914d\u65f6\uff0cEgressGateway \u4f1a\u7565\u8fc7\u8be5\u8282\u70b9\uff0c\u5b83\u5c06\u4e0d\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5b83\u652f\u6301\u9009\u62e9\u591a\u4e2a\u8282\u70b9\u6765\u5b9e\u73b0\u9ad8\u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF \u7ed9\u8282\u70b9\u6253\u4e0a\u4e0a\u8ff0\u4e2d nodeSelector.selector.matchLabels \u6240\u6307\u5b9a\u7684 Label\uff0c\u4f7f\u8282\u70b9\u80fd\u88ab EgressGateway \u9009\u4e2d\uff0c\u4f5c\u4e3a\u51fa\u53e3\u7f51\u5173\u3002 ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressGateway \u72b6\u6001\u3002\u5176\u4e2d\uff1a spec.ippools.ipv4DefaultEIP \u4ee3\u8868\u8be5\u7ec4 EgressGateway \u7684\u9ed8\u8ba4 VIP\uff0c\u5b83\u662f\u4f1a\u4ece spec.ippools.ipv4 \u4e2d\u968f\u673a\u9009\u62e9\u7684\u4e00\u4e2a IP \u5730\u5740\uff0c\u5b83\u7684\u4f5c\u7528\u662f\uff1a\u5f53\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy \u5bf9\u8c61\u65f6\uff0c\u5982\u679c\u672a\u6307\u5b9a VIP \u5730\u5740\uff0c\u5219\u4f7f\u7528\u8be5\u9ed8\u8ba4 VIP status.nodeList \u4ee3\u8868\u8bc6\u522b\u5230\u4e86\u7b26\u5408 spec.nodeSelector \u7684\u8282\u70b9\u53ca\u8be5\u8282\u70b9\u5bf9\u5e94\u7684 EgressTunnel \u5bf9\u8c61\u7684\u72b6\u6001\u3002 ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready \u521b\u5efa\u5e94\u7528\u548c\u51fa\u53e3\u7b56\u7565 \u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u5b83\u5c06\u7528\u4e8e\u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7528\u9014\uff0c\u5e76\u7ed9\u5b83\u6253\u4e0a label \u4ee5\u4fbf\u4e0e EgressPolicy \u5173\u8054\uff0c\u5982\u4e0b\u662f\u793a\u4f8b Yaml\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u8be5\u503c\u5bf9\u5e94\u7684 Multus CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b\u7684 SpiderIPPool \u8d44\u6e90, \u8be5\u503c\u5bf9\u5e94\u7684 SpiderIPPool CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF EgressPolicy \u5b9e\u4f8b\u7528\u4e8e\u5b9a\u4e49\u54ea\u4e9b Pod \u7684\u51fa\u53e3\u6d41\u91cf\u8981\u7ecf\u8fc7 EgressGateway \u8282\u70b9\u8f6c\u53d1\uff0c\u4ee5\u53ca\u5176\u5b83\u7684\u914d\u7f6e\u7ec6\u8282\u3002\u5982\u4e0b\u662f\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy CR \u5bf9\u8c61\u7684\u793a\u4f8b\uff0c\u5176\u4e2d\u3002 spec.egressGatewayName \u7528\u4e8e\u6307\u5b9a\u4e86\u4f7f\u7528\u54ea\u4e00\u7ec4 EgressGateway \u3002 spec.appliedTo.podSelector \u7528\u4e8e\u6307\u5b9a\u672c\u7b56\u7565\u5728\u96c6\u7fa4\u5185\u7684\u54ea\u4e9b Pod \u4e0a\u751f\u6548\u3002 namespace \u7528\u4e8e\u6307\u5b9a EgressPolicy \u5bf9\u8c61\u6240\u5728\u79df\u6237\uff0c\u56e0\u4e3a EgressPolicy \u662f\u79df\u6237\u7ea7\u522b\u7684\uff0c\u6240\u4ee5\u5b83\u52a1\u5fc5\u521b\u5efa\u5728\u4e0a\u8ff0\u5173\u8054\u5e94\u7528\u7684\u76f8\u540c namespace \u4e0b\uff0c\u8fd9\u6837\u5339\u914d\u7684 Pod \u8bbf\u95ee\u4efb\u610f\u96c6\u7fa4\u5916\u90e8\u7684\u5730\u5740\u65f6\uff0c\u624d\u80fd\u88ab EgressGateway Node \u8f6c\u53d1\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressPolicy \u7684\u72b6\u6001\u3002\u5176\u4e2d\uff1a status.eip \u5c55\u793a\u4e86\u8be5\u7ec4\u5e94\u7528\u51fa\u96c6\u7fa4\u65f6\u4f7f\u7528\u7684\u51fa\u53e3 IP \u5730\u5740\u3002 status.node \u5c55\u793a\u4e86\u54ea\u4e00\u4e2a EgressGateway \u7684\u8282\u70b9\u8d1f\u8d23\u8be5 EgressPolicy \u51fa\u53e3\u6d41\u91cf\u7684\u8f6c\u53d1\u3002 ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 \u521b\u5efa EgressPolicy \u5bf9\u8c61\u540e\uff0c\u4f1a\u6839\u636e EgressPolicy \u9009\u62e9\u7684\u5e94\u7528\u751f\u6210\u4e00\u4e2a\u5305\u542b\u6240\u6709\u5e94\u7528\u7684 IP \u5730\u5740\u96c6\u5408\u7684 EgressEndpointSlices \u5bf9\u8c61\uff0c\u5f53\u5e94\u7528\u65e0\u6cd5\u51fa\u53e3\u8bbf\u95ee\u65f6\uff0c\u53ef\u4ee5\u67e5\u770b EgressEndpointSlices \u5bf9\u8c61\u4e2d\u7684 IP \u5730\u5740\u662f\u5426\u6b63\u5e38\u3002 ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default \u6d4b\u8bd5 \u5728\u96c6\u7fa4\u5916\u90e8\u7f72\u5e94\u7528 nettools \uff0c\u7528\u4e8e\u6a21\u62df\u4e00\u4e2a\u96c6\u7fa4\u5916\u90e8\u7684\u670d\u52a1\uff0c\u800c nettools \u4f1a\u5728 http \u56de\u590d\u4e2d\u8fd4\u56de\u8bf7\u6c42\u8005\u7684\u6e90 IP \u5730\u5740\u3002 ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 \u5728\u96c6\u7fa4\u5185\u7684\u6d4b\u8bd5\u5e94\u7528\uff1atest-app \u4e2d\uff0c\u9a8c\u8bc1\u51fa\u53e3\u6d41\u91cf\u7684\u6548\u679c\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5728\u8be5\u5e94\u7528\u5bf9\u5e94 Pod \u4e2d\u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c nettools \u8fd4\u56de\u7684\u6e90 IP \u7b26\u5408\u4e86 EgressPolicy.status.eip \u7684\u6548\u679c\u3002 ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.1.92:8080 # \u96c6\u7fa4\u5916\u8282\u70b9\u7684 IP \u5730\u5740 + webPort Remote IP: 10 .6.168.201","title":"Egress Policy"},{"location":"usage/egress-zh_CN/#egress-policy","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Egress Policy"},{"location":"usage/egress-zh_CN/#_1","text":"Spiderpool \u662f\u4e00\u4e2a Kubernetes \u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728 Kubernetes \u96c6\u7fa4\u4e2d\uff0cPod \u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u5176\u51fa\u53e3 IP \u5730\u5740\u4e0d\u662f\u56fa\u5b9a\u7684\u3002\u5728 Overlay \u7f51\u7edc\u4e2d\uff0c\u51fa\u53e3 IP \u5730\u5740\u4e3a Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u800c\u5728 Underlay \u7f51\u7edc\u4e2d\uff0cPod \u76f4\u63a5\u4f7f\u7528\u81ea\u8eab\u7684 IP \u5730\u5740\u4e0e\u5916\u90e8\u901a\u4fe1\u3002\u56e0\u6b64\uff0c\u5f53 Pod \u53d1\u751f\u65b0\u7684\u8c03\u5ea6\u65f6\uff0c\u65e0\u8bba\u54ea\u79cd\u7f51\u7edc\u6a21\u5f0f\uff0cPod \u4e0e\u5916\u90e8\u901a\u4fe1\u65f6\u7684 IP \u5730\u5740\u90fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7ed9\u7cfb\u7edf\u7ef4\u62a4\u4eba\u5458\u5e26\u6765\u4e86 IP \u5730\u5740\u7ba1\u7406\u7684\u6311\u6218\u3002\u7279\u522b\u662f\u5728\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\u4ee5\u53ca\u9700\u8981\u8fdb\u884c\u7f51\u7edc\u6545\u969c\u8bca\u65ad\u65f6\uff0c\u5728\u96c6\u7fa4\u5916\u90e8\uff0c\u57fa\u4e8e Pod \u539f\u672c\u7684\u51fa\u53e3 IP \u6765\u7ba1\u63a7\u51fa\u53e3\u6d41\u91cf\u5f88\u96be\u5b9e\u73b0\u3002\u800c Spiderpool \u53ef\u4ee5\u642d\u914d\u7ec4\u4ef6 EgressGateway \u5b8c\u7f8e\u89e3\u51b3 Underlay \u7f51\u7edc\u4e0b Pod \u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u7684\u95ee\u9898\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/egress-zh_CN/#_2","text":"EgressGateway \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 Egress \u7f51\u5173\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u4e0d\u540c CNI \u7f51\u7edc\u6a21\u5f0f\u4e0b\uff08Spiderpool\u3001Calico\u3001Flannel\u3001Weave\uff09\u51fa\u53e3 Egress IP \u5730\u5740\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7075\u6d3b\u914d\u7f6e\u548c\u7ba1\u7406\u51fa\u53e3\u7b56\u7565\uff0c\u4e3a\u79df\u6237\u7ea7\u6216\u96c6\u7fa4\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u7f6e Egress IP\uff0c\u4f7f\u5f97 Pod \u8bbf\u95ee\u5916\u90e8\u7f51\u7edc\u65f6\uff0c\u7cfb\u7edf\u4f1a\u7edf\u4e00\u4f7f\u7528\u8fd9\u4e2a\u8bbe\u7f6e\u7684 Egress IP \u4f5c\u4e3a\u51fa\u53e3\u5730\u5740\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002\u4f46 EgressGateway \u6240\u6709\u7684\u89c4\u5219\u90fd\u662f\u751f\u6548\u5728\u4e3b\u673a\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u4e0a\u7684\uff0c\u8981\u4f7f EgressGateway \u7b56\u7565\u751f\u6548\uff0c\u5219 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7684\u6d41\u91cf\uff0c\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u3002\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7 Spiderpool \u7684 spidercoordinators \u7684 spec.hijackCIDR \u5b57\u6bb5\u914d\u7f6e\u4ece\u4e3b\u673a\u8f6c\u53d1\u7684\u5b50\u7f51\u8def\u7531\uff0c\u518d\u901a\u8fc7 coordinator \u5c06\u5339\u914d\u7684\u6d41\u91cf\u4ece veth pair \u8f6c\u53d1\u5230\u4e3b\u673a\u4e0a\u3002\u4f7f\u5f97\u6240\u8bbf\u95ee\u7684\u5916\u90e8\u6d41\u91cf\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\uff0c\u501f\u6b64\u5b9e\u73b0 underlay \u7f51\u7edc\u4e0b\u51fa\u53e3\u6d41\u91cf\u7ba1\u7406\u3002 Spiderpool \u642d\u914d EgressGateway \u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u529f\u80fd\uff1a \u89e3\u51b3 IPv4/IPv6 \u53cc\u6808\u8fde\u63a5\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u901a\u4fe1\u5728\u4e0d\u540c\u534f\u8bae\u6808\u4e0b\u7684\u65e0\u7f1d\u8fde\u63a5\u3002 \u89e3\u51b3 Egress \u8282\u70b9\u7684\u9ad8\u53ef\u7528\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u7f51\u7edc\u8fde\u901a\u6027\u4e0d\u53d7\u5355\u70b9\u6545\u969c\u7684\u5e72\u6270\u3002 \u5141\u8bb8\u66f4\u7cbe\u7ec6\u7684\u7b56\u7565\u63a7\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7 EgressGateway \u7075\u6d3b\u5730\u8fc7\u6ee4 Pods \u7684 Egress \u7b56\u7565\uff0c\u5305\u62ec Destination CIDR\u3002 \u5141\u8bb8\u8fc7\u6ee4 Egress \u5e94\u7528\uff08Pod\uff09\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u7ba1\u7406\u7279\u5b9a\u5e94\u7528\u7684\u51fa\u53e3\u6d41\u91cf\u3002 \u652f\u6301\u591a\u4e2a\u51fa\u53e3\u7f51\u5173\u5b9e\u4f8b\uff0c\u80fd\u591f\u5904\u7406\u591a\u4e2a\u7f51\u7edc\u5206\u533a\u6216\u96c6\u7fa4\u4e4b\u95f4\u7684\u901a\u4fe1\u3002 \u652f\u6301\u79df\u6237\u7ea7\u522b\u7684 Egress IP\u3002 \u652f\u6301\u81ea\u52a8\u68c0\u6d4b\u96c6\u7fa4\u6d41\u91cf\u7684 Egress \u7f51\u5173\u7b56\u7565\u3002 \u652f\u6301\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4 Egress \u5b9e\u4f8b\u3002 \u53ef\u7528\u4e8e\u8f83\u4f4e\u5185\u6838\u7248\u672c\uff0c\u9002\u7528\u4e8e\u5404\u79cd Kubernetes \u90e8\u7f72\u73af\u5883\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/egress-zh_CN/#_3","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/egress-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/egress-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool\uff0c\u5e76\u521b\u5efa SpiderMultusConfig CR \u4e0e IPPool CR. \u5728\u5b89\u88c5\u5b8c Spiderpool \u540e\uff0c\u5c06\u96c6\u7fa4\u5916\u7684\u670d\u52a1\u5730\u5740\u6dfb\u52a0\u5230 spiderpool.spidercoordinators \u7684 'default' \u5bf9\u8c61\u7684 'hijackCIDR' \u4e2d\uff0c\u4f7f Pod \u8bbf\u95ee\u8fd9\u4e9b\u5916\u90e8\u670d\u52a1\u65f6\uff0c\u6d41\u91cf\u5148\u7ecf\u8fc7 Pod \u6240\u5728\u7684\u4e3b\u673a\uff0c\u4ece\u800c\u88ab EgressGateway \u89c4\u5219\u5339\u914d\u3002 # \"10.6.168.63/32\" \u4e3a\u5916\u90e8\u670d\u52a1\u5730\u5740\u3002\u5bf9\u4e8e\u5df2\u7ecf\u8fd0\u884c\u7684 Pod\uff0c\u9700\u8981\u91cd\u542f Pod\uff0c\u8fd9\u4e9b\u8def\u7531\u89c4\u5219\u624d\u4f1a\u5728 Pod \u4e2d\u751f\u6548\u3002 ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}'","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/egress-zh_CN/#egressgateway","text":"\u901a\u8fc7 helm \u5b89\u88c5 EgressGateway helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug \u5982\u679c\u9700\u8981\u4f7f\u7528 IPv6 \uff0c\u53ef\u4f7f\u7528\u9009\u9879 --set feature.enableIPv6=true \u5f00\u542f\uff0c\u5e76\u8bbe\u7f6e feature.tunnelIpv6Subnet , \u503c\u5f97\u6ce8\u610f\u7684\u662f\u5728\u901a\u8fc7 feature.tunnelIpv4Subnet \u4e0e feature.tunnelIpv6Subnet \u914d\u7f6e IPv4 \u6216 IPv6 \u7f51\u6bb5\u65f6\uff0c\u9700\u8981\u4fdd\u8bc1\u7f51\u6bb5\u548c\u96c6\u7fa4\u5185\u7684\u5176\u4ed6\u5730\u5740\u4e0d\u51b2\u7a81\u3002 feature.enableGatewayReplyRoute \u4e3a true \u65f6\uff0c\u5c06\u5f00\u542f\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u8fd4\u56de\u8def\u7531\u89c4\u5219\uff0c\u5728\u4e0e Spiderpool \u642d\u914d\u652f\u6301 underlay CNI \u65f6\uff0c\u5fc5\u987b\u5f00\u542f\u8be5\u9009\u9879\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u8fd8\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d EgressGateway \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u9a8c\u8bc1 EgressGateway \u5b89\u88c5 ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m \u66f4\u591a\u5b89\u88c5\u7ec6\u8282\uff0c\u53c2\u8003 EgressGateway \u5b89\u88c5","title":"\u5b89\u88c5 EgressGateway"},{"location":"usage/egress-zh_CN/#egressgateway_1","text":"EgressGateway \u5b9a\u4e49\u4e86\u4e00\u7ec4\u8282\u70b9\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u96c6\u7fa4\u5185\u7684 egress \u6d41\u91cf\u5c06\u4f1a\u901a\u8fc7\u8fd9\u7ec4\u8282\u70b9\u8f6c\u53d1\u800c\u51fa\u96c6\u7fa4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u9884\u5148\u5b9a\u4e49 EgressGateway \u5b9e\u4f8b\uff0c\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a EgressGateway \u5b9e\u4f8b\uff0c\u5176\u4e2d\uff1a spec.ippools.ipv4 \uff1a\u5b9a\u4e49\u4e86\u4e00\u7ec4 egress \u7684\u51fa\u53e3 IP \u5730\u5740\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u73af\u5883\u7684\u5b9e\u9645\u60c5\u51b5\u8c03\u6574\u3002\u5e76\u4e14 spec.ippools.ipv4 \u7684 CIDR \u5e94\u8be5\u4e0e\u7f51\u5173\u8282\u70b9\u4e0a\u7684\u51fa\u53e3\u7f51\u5361\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\u662f\u9ed8\u8ba4\u8def\u7531\u7684\u7f51\u5361\uff09\u7684\u5b50\u7f51\u76f8\u540c\uff0c\u5426\u5219\uff0c\u53ef\u80fd\u5bfc\u81f4 egress \u8bbf\u95ee\u4e0d\u901a\u3002 spec.nodeSelector \uff1aEgressGateway \u63d0\u4f9b\u7684\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u5339\u914d\u65f6\uff0c\u8be5\u8282\u70b9\u5c06\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5f53 selector.matchLabels \u4e0e\u8282\u70b9\u4e0d\u5339\u914d\u65f6\uff0cEgressGateway \u4f1a\u7565\u8fc7\u8be5\u8282\u70b9\uff0c\u5b83\u5c06\u4e0d\u4f1a\u88ab\u4f5c\u4e3a\u96c6\u7fa4\u7684\u51fa\u53e3\u7f51\u5173\uff0c\u5b83\u652f\u6301\u9009\u62e9\u591a\u4e2a\u8282\u70b9\u6765\u5b9e\u73b0\u9ad8\u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF \u7ed9\u8282\u70b9\u6253\u4e0a\u4e0a\u8ff0\u4e2d nodeSelector.selector.matchLabels \u6240\u6307\u5b9a\u7684 Label\uff0c\u4f7f\u8282\u70b9\u80fd\u88ab EgressGateway \u9009\u4e2d\uff0c\u4f5c\u4e3a\u51fa\u53e3\u7f51\u5173\u3002 ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressGateway \u72b6\u6001\u3002\u5176\u4e2d\uff1a spec.ippools.ipv4DefaultEIP \u4ee3\u8868\u8be5\u7ec4 EgressGateway \u7684\u9ed8\u8ba4 VIP\uff0c\u5b83\u662f\u4f1a\u4ece spec.ippools.ipv4 \u4e2d\u968f\u673a\u9009\u62e9\u7684\u4e00\u4e2a IP \u5730\u5740\uff0c\u5b83\u7684\u4f5c\u7528\u662f\uff1a\u5f53\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy \u5bf9\u8c61\u65f6\uff0c\u5982\u679c\u672a\u6307\u5b9a VIP \u5730\u5740\uff0c\u5219\u4f7f\u7528\u8be5\u9ed8\u8ba4 VIP status.nodeList \u4ee3\u8868\u8bc6\u522b\u5230\u4e86\u7b26\u5408 spec.nodeSelector \u7684\u8282\u70b9\u53ca\u8be5\u8282\u70b9\u5bf9\u5e94\u7684 EgressTunnel \u5bf9\u8c61\u7684\u72b6\u6001\u3002 ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready","title":"\u521b\u5efa EgressGateway \u5b9e\u4f8b"},{"location":"usage/egress-zh_CN/#_5","text":"\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u5b83\u5c06\u7528\u4e8e\u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5916\u90e8\u7528\u9014\uff0c\u5e76\u7ed9\u5b83\u6253\u4e0a label \u4ee5\u4fbf\u4e0e EgressPolicy \u5173\u8054\uff0c\u5982\u4e0b\u662f\u793a\u4f8b Yaml\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u8be5\u503c\u5bf9\u5e94\u7684 Multus CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b\u7684 SpiderIPPool \u8d44\u6e90, \u8be5\u503c\u5bf9\u5e94\u7684 SpiderIPPool CR \u9700\u53c2\u8003 \u5b89\u88c5 \u6587\u6863\u63d0\u524d\u521b\u5efa\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF EgressPolicy \u5b9e\u4f8b\u7528\u4e8e\u5b9a\u4e49\u54ea\u4e9b Pod \u7684\u51fa\u53e3\u6d41\u91cf\u8981\u7ecf\u8fc7 EgressGateway \u8282\u70b9\u8f6c\u53d1\uff0c\u4ee5\u53ca\u5176\u5b83\u7684\u914d\u7f6e\u7ec6\u8282\u3002\u5982\u4e0b\u662f\u4e3a\u5e94\u7528\u521b\u5efa EgressPolicy CR \u5bf9\u8c61\u7684\u793a\u4f8b\uff0c\u5176\u4e2d\u3002 spec.egressGatewayName \u7528\u4e8e\u6307\u5b9a\u4e86\u4f7f\u7528\u54ea\u4e00\u7ec4 EgressGateway \u3002 spec.appliedTo.podSelector \u7528\u4e8e\u6307\u5b9a\u672c\u7b56\u7565\u5728\u96c6\u7fa4\u5185\u7684\u54ea\u4e9b Pod \u4e0a\u751f\u6548\u3002 namespace \u7528\u4e8e\u6307\u5b9a EgressPolicy \u5bf9\u8c61\u6240\u5728\u79df\u6237\uff0c\u56e0\u4e3a EgressPolicy \u662f\u79df\u6237\u7ea7\u522b\u7684\uff0c\u6240\u4ee5\u5b83\u52a1\u5fc5\u521b\u5efa\u5728\u4e0a\u8ff0\u5173\u8054\u5e94\u7528\u7684\u76f8\u540c namespace \u4e0b\uff0c\u8fd9\u6837\u5339\u914d\u7684 Pod \u8bbf\u95ee\u4efb\u610f\u96c6\u7fa4\u5916\u90e8\u7684\u5730\u5740\u65f6\uff0c\u624d\u80fd\u88ab EgressGateway Node \u8f6c\u53d1\u3002 cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u67e5\u770b EgressPolicy \u7684\u72b6\u6001\u3002\u5176\u4e2d\uff1a status.eip \u5c55\u793a\u4e86\u8be5\u7ec4\u5e94\u7528\u51fa\u96c6\u7fa4\u65f6\u4f7f\u7528\u7684\u51fa\u53e3 IP \u5730\u5740\u3002 status.node \u5c55\u793a\u4e86\u54ea\u4e00\u4e2a EgressGateway \u7684\u8282\u70b9\u8d1f\u8d23\u8be5 EgressPolicy \u51fa\u53e3\u6d41\u91cf\u7684\u8f6c\u53d1\u3002 ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 \u521b\u5efa EgressPolicy \u5bf9\u8c61\u540e\uff0c\u4f1a\u6839\u636e EgressPolicy \u9009\u62e9\u7684\u5e94\u7528\u751f\u6210\u4e00\u4e2a\u5305\u542b\u6240\u6709\u5e94\u7528\u7684 IP \u5730\u5740\u96c6\u5408\u7684 EgressEndpointSlices \u5bf9\u8c61\uff0c\u5f53\u5e94\u7528\u65e0\u6cd5\u51fa\u53e3\u8bbf\u95ee\u65f6\uff0c\u53ef\u4ee5\u67e5\u770b EgressEndpointSlices \u5bf9\u8c61\u4e2d\u7684 IP \u5730\u5740\u662f\u5426\u6b63\u5e38\u3002 ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default","title":"\u521b\u5efa\u5e94\u7528\u548c\u51fa\u53e3\u7b56\u7565"},{"location":"usage/egress-zh_CN/#_6","text":"\u5728\u96c6\u7fa4\u5916\u90e8\u7f72\u5e94\u7528 nettools \uff0c\u7528\u4e8e\u6a21\u62df\u4e00\u4e2a\u96c6\u7fa4\u5916\u90e8\u7684\u670d\u52a1\uff0c\u800c nettools \u4f1a\u5728 http \u56de\u590d\u4e2d\u8fd4\u56de\u8bf7\u6c42\u8005\u7684\u6e90 IP \u5730\u5740\u3002 ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 \u5728\u96c6\u7fa4\u5185\u7684\u6d4b\u8bd5\u5e94\u7528\uff1atest-app \u4e2d\uff0c\u9a8c\u8bc1\u51fa\u53e3\u6d41\u91cf\u7684\u6548\u679c\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5728\u8be5\u5e94\u7528\u5bf9\u5e94 Pod \u4e2d\u8bbf\u95ee\u5916\u90e8\u670d\u52a1\u65f6\uff0c nettools \u8fd4\u56de\u7684\u6e90 IP \u7b26\u5408\u4e86 EgressPolicy.status.eip \u7684\u6548\u679c\u3002 ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.1.92:8080 # \u96c6\u7fa4\u5916\u8282\u70b9\u7684 IP \u5730\u5740 + webPort Remote IP: 10 .6.168.201","title":"\u6d4b\u8bd5"},{"location":"usage/egress/","text":"Egress Policy English | \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool is an Underlay networking solution for Kubernetes, but the egress IP address is not fixed in a Kubernetes cluster when a Pod accesses an external service. In an Overlay network, the egress IP address is the address of the node on which the Pod resides, whereas in an Underlay network, the Pod communicates directly with the outside world using its own IP address. Therefore, when a Pod undergoes new scheduling, the IP address of the Pod when communicating with the outside world will change regardless of the network mode. This instability creates IP address management challenges for system maintainers. Especially when the cluster scale increases and network troubleshooting is required, it is difficult to control the egress traffic based on the Pod's original egress IP outside the cluster. Spiderpool can be used with the component EgressGateway to solve the problem of Pod egress traffic management in Underlay network. Features of EgressGateway EgressGateway is an open source Egress gateway designed to solve the problem of exporting Egress IP addresses in different CNI network modes (Spiderpool, Calico, Flannel, Weave). By flexibly configuring and managing egress policies, Egress IP is set for tenant-level or cluster-level workloads, so that when a Pod accesses an external network, the system will uniformly use this set Egress IP as the egress address, thus providing a stable egress traffic management solution. However, all EgressGateway rules are effective on the host's network namespace. To make the EgressGateway policy effective, the traffic of Pods accessing the outside of the cluster has to go through the host's network namespace. Therefore, you can configure the subnet routes forwarded from the host via the spec.hijackCIDR field of spidercoordinators in Spiderpool, and then configure the subnet routes forwarded from the host via coordinator to forward matching traffic from the veth pair to the host. This enables egress traffic management on an underlay network by allowing access to external traffic to be matched by EgressGateway rules. Some of the features and benefits of Spiderpool with EgressGateway are as follows: Solve IPv4 IPv6 dual-stack connectivity,ensuring seamless communication across different protocol stacks. Solve the high availability of Egress Nodes, ensuring network connectivity remains unaffected by single-point failures. Support finer-grained policy control, allowing flexible filtering of Pods' Egress policies, including Destination CIDR. Support application-level control, allowing EgressGateway to filter Egress applications (Pods) for precise management of specific application outbound traffic. Support multiple egress gateways instance,capable of handling communication between multiple network partitions or clusters. Support namespaced egress IP. Support automatic detection of cluster traffic for egress gateways policies. Support namespace default egress instances. Can be used in low kernel version, making EgressGateway suitable for various Kubernetes deployment environments. Prerequisites A ready Kubernetes kubernetes. Helm has been already installed. Steps Install Spiderpool Refer to Installation to install Spiderpool and create SpiderMultusConfig CR and IPPool CR. After installing Spiderpool, Add the service addresses outside the cluster to the 'hijackCIDR' field in the 'default' object of spiderpool.spidercoordinators. This ensures that when Pods access these external services, the traffic is routed through the host where the Pod is located, allowing the EgressGateway rules to match. # For running Pods, you need to restart them for these routing rules to take effect within the Pods. ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}' Install EgressGateway Installing EgressGateway via helm helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug If IPv6 is required, enable it with the option -set feature.enableIPv6=true and set feature.tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature. tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature.tunnelIpv6Subnet , you need to make sure that the segments don't conflict with any other addresses in the cluster. feature.enableGatewayReplyRoute is true to enable return routing rules on gateway nodes, which must be enabled when pairing with Spiderpool to support underlay CNI. If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for EgressGateway. Verifying EgressGateway Installation ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m For more installation details, refer to EgressGateway Installation Creating an instance of EgressGateway An EgressGateway defines a set of nodes that act as an egress gateway for the cluster, through which egress traffic within the cluster will be forwarded out of the cluster. Therefore, an EgressGateway instance needs to be pre-defined. The following example Yaml creates an EgressGateway instance. spec.ippools.ipv4 : defines a set of egress egress IP addresses, which need to be adjusted according to the actual situation of the specific environment. The CIDR of spec.ippools.ipv4 should be the same as the subnet of the egress NIC on the gateway node (usually the NIC of the default route), or else the egress access may not work. spec.nodeSelector : the node affinity method provided by EgressGateway, when selector.matchLabels matches with a node, the node will be used as the egress gateway for the cluster, when selector.matchLabels does not match with a node, the When selector.matchLabels does not match with a node, the EgressGateway skips that node and it will not be used as an egress gateway for the cluster, which supports selecting multiple nodes for high availability. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF Label the node with the Label specified in nodeSelector.selector.matchLabels above so that the node can be selected by the EgressGateway to act as an egress gateway. ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" When creation is complete, check the EgressGateway status. spec.ippools.ipv4DefaultEIP represents the default VIP of the EgressGateway for the group, which is an IP address that will be randomly selected from spec.ippools.ipv4 , and its function is: when creating an EgressPolicy object for an application, if no VIP address is specified, the is used if no VIP address is specified when creating an EgressPolicy object for the application. status.nodeList represents the status of the nodes identified as matching the spec.nodeSelector and the corresponding EgressTunnel object for that node. ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready Create Applications and Egress Policies Create an application that will be used to test Pod access for external cluster purposes and label it to be associated with the EgressPolicy, as shown in the following example Yaml. v1.multus-cni.io/default-network : used to specify the subnet used by the application, the Multus CR corresponding to this value needs to be created in advance by referring to the installation document to create it in advance. ipam.spidernet.io/ippool : Specify which SpiderIPPool resources are used by the Pod, the corresponding SpiderIPPool CR should be created in advance by referring to the Installation . cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"], } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF The EgressPolicy instance is used to define which Pods' egress traffic is to be forwarded through the EgressGateway node, as well as other configuration details. The following is an example of creating an EgressPolicy CR object for an application. spec.egressGatewayName is used to specify which set of EgressGateways to use. spec.appliedTo.podSelector is used to specify on which Pods within the cluster this policy takes effect. namespace is used to specify the tenant where the EgressPolicy object resides. Because EgressPolicy is tenant-level, it must be created under the same namespace as the associated application, so that when the matching Pod accesses any address outside the cluster, it can be forwarded by the EgressGateway Node. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF When creation is complete, check the status of the EgressPolicy. status.eip shows the egress IP address used by the group when applying out of the cluster. status.node shows which EgressGateway node is responsible for forwarding traffic out of the EgressPolicy. ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 After creating the EgressPolicy object, an EgressEndpointSlices object containing a collection of IP addresses of all the applications will be generated according to the application selected by the EgressPolicy, so that you can check whether the IP addresses in the EgressEndpointSlices object are normal or not when the application cannot be accessed by export. ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default Test Results Deploy the application nettools outside the cluster to emulate a service outside the cluster, and nettools will return the source IP address of the requester in the http reply. ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 Verifying the effect of egress traffic in a test app within the cluster: test-app, we can see that the source IP returned by nettools complies with EgressPolicy.status.eip when accessing an external service in the Pod corresponding to this app. ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.168.63:8080 # IP address of the node outside the cluster + webPort Remote IP: 10 .6.168.201","title":"Egress Policy"},{"location":"usage/egress/#egress-policy","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Egress Policy"},{"location":"usage/egress/#introduction","text":"Spiderpool is an Underlay networking solution for Kubernetes, but the egress IP address is not fixed in a Kubernetes cluster when a Pod accesses an external service. In an Overlay network, the egress IP address is the address of the node on which the Pod resides, whereas in an Underlay network, the Pod communicates directly with the outside world using its own IP address. Therefore, when a Pod undergoes new scheduling, the IP address of the Pod when communicating with the outside world will change regardless of the network mode. This instability creates IP address management challenges for system maintainers. Especially when the cluster scale increases and network troubleshooting is required, it is difficult to control the egress traffic based on the Pod's original egress IP outside the cluster. Spiderpool can be used with the component EgressGateway to solve the problem of Pod egress traffic management in Underlay network.","title":"Introduction"},{"location":"usage/egress/#features-of-egressgateway","text":"EgressGateway is an open source Egress gateway designed to solve the problem of exporting Egress IP addresses in different CNI network modes (Spiderpool, Calico, Flannel, Weave). By flexibly configuring and managing egress policies, Egress IP is set for tenant-level or cluster-level workloads, so that when a Pod accesses an external network, the system will uniformly use this set Egress IP as the egress address, thus providing a stable egress traffic management solution. However, all EgressGateway rules are effective on the host's network namespace. To make the EgressGateway policy effective, the traffic of Pods accessing the outside of the cluster has to go through the host's network namespace. Therefore, you can configure the subnet routes forwarded from the host via the spec.hijackCIDR field of spidercoordinators in Spiderpool, and then configure the subnet routes forwarded from the host via coordinator to forward matching traffic from the veth pair to the host. This enables egress traffic management on an underlay network by allowing access to external traffic to be matched by EgressGateway rules. Some of the features and benefits of Spiderpool with EgressGateway are as follows: Solve IPv4 IPv6 dual-stack connectivity,ensuring seamless communication across different protocol stacks. Solve the high availability of Egress Nodes, ensuring network connectivity remains unaffected by single-point failures. Support finer-grained policy control, allowing flexible filtering of Pods' Egress policies, including Destination CIDR. Support application-level control, allowing EgressGateway to filter Egress applications (Pods) for precise management of specific application outbound traffic. Support multiple egress gateways instance,capable of handling communication between multiple network partitions or clusters. Support namespaced egress IP. Support automatic detection of cluster traffic for egress gateways policies. Support namespace default egress instances. Can be used in low kernel version, making EgressGateway suitable for various Kubernetes deployment environments.","title":"Features of EgressGateway"},{"location":"usage/egress/#prerequisites","text":"A ready Kubernetes kubernetes. Helm has been already installed.","title":"Prerequisites"},{"location":"usage/egress/#steps","text":"","title":"Steps"},{"location":"usage/egress/#install-spiderpool","text":"Refer to Installation to install Spiderpool and create SpiderMultusConfig CR and IPPool CR. After installing Spiderpool, Add the service addresses outside the cluster to the 'hijackCIDR' field in the 'default' object of spiderpool.spidercoordinators. This ensures that when Pods access these external services, the traffic is routed through the host where the Pod is located, allowing the EgressGateway rules to match. # For running Pods, you need to restart them for these routing rules to take effect within the Pods. ~# kubectl patch spidercoordinators default --type = 'merge' -p '{\"spec\": {\"hijackCIDR\": [\"10.6.168.63/32\"]}}'","title":"Install Spiderpool"},{"location":"usage/egress/#install-egressgateway","text":"Installing EgressGateway via helm helm repo add egressgateway https://spidernet-io.github.io/egressgateway/ helm repo update egressgateway helm install egressgateway egressgateway/egressgateway -n kube-system --set feature.tunnelIpv4Subnet = \"192.200.0.1/16\" --set feature.enableGatewayReplyRoute = true --wait --debug If IPv6 is required, enable it with the option -set feature.enableIPv6=true and set feature.tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature. tunnelIpv6Subnet , it is worth noting that when configuring IPv4 or IPv6 segments via feature.tunnelIpv4Subnet and feature.tunnelIpv6Subnet , you need to make sure that the segments don't conflict with any other addresses in the cluster. feature.enableGatewayReplyRoute is true to enable return routing rules on gateway nodes, which must be enabled when pairing with Spiderpool to support underlay CNI. If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for EgressGateway. Verifying EgressGateway Installation ~# kubectl get pod -n kube-system | grep egressgateway egressgateway-agent-4s8lt 1 /1 Running 0 29m egressgateway-agent-thzth 1 /1 Running 0 29m egressgateway-controller-77698899df-tln7j 1 /1 Running 0 29m For more installation details, refer to EgressGateway Installation","title":"Install EgressGateway"},{"location":"usage/egress/#creating-an-instance-of-egressgateway","text":"An EgressGateway defines a set of nodes that act as an egress gateway for the cluster, through which egress traffic within the cluster will be forwarded out of the cluster. Therefore, an EgressGateway instance needs to be pre-defined. The following example Yaml creates an EgressGateway instance. spec.ippools.ipv4 : defines a set of egress egress IP addresses, which need to be adjusted according to the actual situation of the specific environment. The CIDR of spec.ippools.ipv4 should be the same as the subnet of the egress NIC on the gateway node (usually the NIC of the default route), or else the egress access may not work. spec.nodeSelector : the node affinity method provided by EgressGateway, when selector.matchLabels matches with a node, the node will be used as the egress gateway for the cluster, when selector.matchLabels does not match with a node, the When selector.matchLabels does not match with a node, the EgressGateway skips that node and it will not be used as an egress gateway for the cluster, which supports selecting multiple nodes for high availability. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressGateway metadata: name: default spec: ippools: ipv4: - \"10.6.168.201-10.6.168.205\" nodeSelector: selector: matchLabels: egressgateway: \"true\" EOF Label the node with the Label specified in nodeSelector.selector.matchLabels above so that the node can be selected by the EgressGateway to act as an egress gateway. ~# kubectl get node NAME STATUS ROLES AGE VERSION controller-node-1 Ready control-plane 5d17h v1.26.7 worker-node-1 Ready <none> 5d17h v1.26.7 ~# kubectl label node worker-node-1 egressgateway = \"true\" When creation is complete, check the EgressGateway status. spec.ippools.ipv4DefaultEIP represents the default VIP of the EgressGateway for the group, which is an IP address that will be randomly selected from spec.ippools.ipv4 , and its function is: when creating an EgressPolicy object for an application, if no VIP address is specified, the is used if no VIP address is specified when creating an EgressPolicy object for the application. status.nodeList represents the status of the nodes identified as matching the spec.nodeSelector and the corresponding EgressTunnel object for that node. ~# kubectl get EgressGateway default -o yaml ... spec: ippools: ipv4DefaultEIP: 10 .6.168.201 ... status: nodeList: - name: worker-node-1 status: Ready","title":"Creating an instance of EgressGateway"},{"location":"usage/egress/#create-applications-and-egress-policies","text":"Create an application that will be used to test Pod access for external cluster purposes and label it to be associated with the EgressPolicy, as shown in the following example Yaml. v1.multus-cni.io/default-network : used to specify the subnet used by the application, the Multus CR corresponding to this value needs to be created in advance by referring to the installation document to create it in advance. ipam.spidernet.io/ippool : Specify which SpiderIPPool resources are used by the Pod, the corresponding SpiderIPPool CR should be created in advance by referring to the Installation . cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: test-app name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"v4-pool\"], } v1.multus-cni.io/default-network: kube-system/macvlan-conf spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app ports: - name: http containerPort: 80 protocol: TCP EOF The EgressPolicy instance is used to define which Pods' egress traffic is to be forwarded through the EgressGateway node, as well as other configuration details. The following is an example of creating an EgressPolicy CR object for an application. spec.egressGatewayName is used to specify which set of EgressGateways to use. spec.appliedTo.podSelector is used to specify on which Pods within the cluster this policy takes effect. namespace is used to specify the tenant where the EgressPolicy object resides. Because EgressPolicy is tenant-level, it must be created under the same namespace as the associated application, so that when the matching Pod accesses any address outside the cluster, it can be forwarded by the EgressGateway Node. cat <<EOF | kubectl apply -f - apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: egressGatewayName: default appliedTo: podSelector: matchLabels: app: \"test-app\" EOF When creation is complete, check the status of the EgressPolicy. status.eip shows the egress IP address used by the group when applying out of the cluster. status.node shows which EgressGateway node is responsible for forwarding traffic out of the EgressPolicy. ~# kubectl get EgressPolicy -A NAMESPACE NAME GATEWAY IPV4 IPV6 EGRESSNODE default test default 10 .6.168.201 worker-node-1 ~# kubectl get EgressPolicy test -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 kind: EgressPolicy metadata: name: test namespace: default spec: appliedTo: podSelector: matchLabels: app: test-app egressIP: allocatorPolicy: default useNodeIP: false status: eip: ipv4: 10 .6.168.201 node: worker-node-1 After creating the EgressPolicy object, an EgressEndpointSlices object containing a collection of IP addresses of all the applications will be generated according to the application selected by the EgressPolicy, so that you can check whether the IP addresses in the EgressEndpointSlices object are normal or not when the application cannot be accessed by export. ~# kubectl get egressendpointslices -A NAMESPACE NAME AGE default test-4vbqf 41s ~# kubectl get egressendpointslices test-kvlp6 -o yaml apiVersion: egressgateway.spidernet.io/v1beta1 endpoints: - ipv4: - 10 .6.168.208 node: worker-node-1 ns: default pod: test-app-f44846544-8dnzp kind: EgressEndpointSlice metadata: name: test-4vbqf namespace: default","title":"Create Applications and Egress Policies"},{"location":"usage/egress/#test-results","text":"Deploy the application nettools outside the cluster to emulate a service outside the cluster, and nettools will return the source IP address of the requester in the http reply. ~# docker run -d --net = host ghcr.io/spidernet-io/egressgateway-nettools:latest /usr/bin/nettools-server -protocol web -webPort 8080 Verifying the effect of egress traffic in a test app within the cluster: test-app, we can see that the source IP returned by nettools complies with EgressPolicy.status.eip when accessing an external service in the Pod corresponding to this app. ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f44846544-8dnzp 1 /1 Running 0 4m27s 10 .6.168.208 worker-node-1 <none> <none> ~# kubectl exec -it test-app-f44846544-8dnzp bash ~# curl 10 .6.168.63:8080 # IP address of the node outside the cluster + webPort Remote IP: 10 .6.168.201","title":"Test Results"},{"location":"usage/faq-zh_CN/","text":"FAQ \u7ecf\u5e38\u88ab\u95ee\u5230\u7684\u95ee\u9898 English | \u7b80\u4f53\u4e2d\u6587 \u6574\u4f53 \u4ec0\u4e48\u662f Spiderpool \uff1f Spiderpool \u9879\u76ee\u7531\u591a\u4e2a\u5b50\u63d2\u4ef6\u9879\u76ee\u7ec4\u6210\uff0c\u5305\u62ec\u6709\uff1a spiderpool , coordinator , ifacer . \u8fd9\u91cc\u7684 spiderpool \u63d2\u4ef6\u662f\u4e00\u6b3e\u670d\u52a1\u4e8e main CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u53ef\u4e3a\u60a8\u7684\u96c6\u7fa4\u7ba1\u7406 IP\u3002 coordinator \u63d2\u4ef6\u80fd\u4e3a\u4f60\u534f\u540c\u8def\u7531\u3002 ifacer \u63d2\u4ef6\u53ef\u4e3a\u4f60\u521b\u5efa vlan \u5b50\u63a5\u53e3\u4ee5\u53ca\u521b\u5efa bond \u7f51\u5361\u3002\u5176\u4e2d\uff0c coordinator \u548c ifacer \u63d2\u4ef6\u4ee5 CNI \u534f\u8bae\u4e2d\u7684\u94fe\u5f0f\u8c03\u7528\u65b9\u5f0f\u6765\u4f7f\u7528\uff0c\u4e14\u53ef\u9009\u5e76\u975e\u5f3a\u5236\u4f7f\u7528\u3002 \u914d\u7f6e \u4e3a\u4ec0\u4e48\u66f4\u6539 configmap \u7684\u914d\u7f6e\u540e\u5374\u65e0\u6cd5\u751f\u6548\uff1f \u4fee\u6539 configmap \u8d44\u6e90 spiderpool-conf \u914d\u7f6e\u540e\uff0c\u9700\u8981\u91cd\u542f spiderpool-agent \u548c spiderpool-controller \u7ec4\u4ef6\u3002 \u4f7f\u7528 SpiderSubnet\u529f\u80fd\u4f7f\u7528\u4e0d\u6b63\u5e38 \u5982\u679c\u9047\u5230\u62a5\u9519 Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource \uff0c\u8bf7\u68c0\u67e5 configmap spiderpool-conf \u786e\u4fdd SpiderSubnet \u529f\u80fd\u5df2\u542f\u52a8\u3002 \u82e5\u9047\u5230\u62a5\u9519 failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables \uff0c\u8bf7\u68c0\u67e5 spiderpool-controller \u7684\u65e5\u5fd7\u3002","title":"FAQ"},{"location":"usage/faq-zh_CN/#faq","text":"\u7ecf\u5e38\u88ab\u95ee\u5230\u7684\u95ee\u9898 English | \u7b80\u4f53\u4e2d\u6587","title":"FAQ"},{"location":"usage/faq-zh_CN/#_1","text":"","title":"\u6574\u4f53"},{"location":"usage/faq-zh_CN/#spiderpool","text":"Spiderpool \u9879\u76ee\u7531\u591a\u4e2a\u5b50\u63d2\u4ef6\u9879\u76ee\u7ec4\u6210\uff0c\u5305\u62ec\u6709\uff1a spiderpool , coordinator , ifacer . \u8fd9\u91cc\u7684 spiderpool \u63d2\u4ef6\u662f\u4e00\u6b3e\u670d\u52a1\u4e8e main CNI \u7684 IPAM \u63d2\u4ef6\uff0c\u53ef\u4e3a\u60a8\u7684\u96c6\u7fa4\u7ba1\u7406 IP\u3002 coordinator \u63d2\u4ef6\u80fd\u4e3a\u4f60\u534f\u540c\u8def\u7531\u3002 ifacer \u63d2\u4ef6\u53ef\u4e3a\u4f60\u521b\u5efa vlan \u5b50\u63a5\u53e3\u4ee5\u53ca\u521b\u5efa bond \u7f51\u5361\u3002\u5176\u4e2d\uff0c coordinator \u548c ifacer \u63d2\u4ef6\u4ee5 CNI \u534f\u8bae\u4e2d\u7684\u94fe\u5f0f\u8c03\u7528\u65b9\u5f0f\u6765\u4f7f\u7528\uff0c\u4e14\u53ef\u9009\u5e76\u975e\u5f3a\u5236\u4f7f\u7528\u3002","title":"\u4ec0\u4e48\u662f Spiderpool \uff1f"},{"location":"usage/faq-zh_CN/#_2","text":"","title":"\u914d\u7f6e"},{"location":"usage/faq-zh_CN/#configmap","text":"\u4fee\u6539 configmap \u8d44\u6e90 spiderpool-conf \u914d\u7f6e\u540e\uff0c\u9700\u8981\u91cd\u542f spiderpool-agent \u548c spiderpool-controller \u7ec4\u4ef6\u3002","title":"\u4e3a\u4ec0\u4e48\u66f4\u6539 configmap \u7684\u914d\u7f6e\u540e\u5374\u65e0\u6cd5\u751f\u6548\uff1f"},{"location":"usage/faq-zh_CN/#_3","text":"","title":"\u4f7f\u7528"},{"location":"usage/faq-zh_CN/#spidersubnet","text":"\u5982\u679c\u9047\u5230\u62a5\u9519 Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource \uff0c\u8bf7\u68c0\u67e5 configmap spiderpool-conf \u786e\u4fdd SpiderSubnet \u529f\u80fd\u5df2\u542f\u52a8\u3002 \u82e5\u9047\u5230\u62a5\u9519 failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables \uff0c\u8bf7\u68c0\u67e5 spiderpool-controller \u7684\u65e5\u5fd7\u3002","title":"SpiderSubnet\u529f\u80fd\u4f7f\u7528\u4e0d\u6b63\u5e38"},{"location":"usage/faq/","text":"FAQ Frequently asked questions English | \u7b80\u4f53\u4e2d\u6587 General What is Spiderpool? Spiderpool project is consist of several plugins include: spiderpool , coordinator , ifacer . The spiderpool basically is a IPAM plugin works for CNI main plugin to manage IP addresses for the container. The coordinator is a plugin that coordinate the routes. The ifacer plugin help you to create vlan sub-interface or create bond interfaces. The coordinator and ifacer plugin are used in CNI plugin chaining and they also optional to use. Configuration Why doesn't changing configmap configuration update the behavior of Spiderpool? If you change the configmap spiderpool-conf configurations, you need to restart spiderpool-agent and spiderpool-controller components Operation Why SpiderSubnet feature not works well? For error like Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource , you need to update configmap spiderpool-conf to enable SpiderSubnet feature and restart spiderpool-agent and spiderpool-controller components. For error like failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables , you should check spiderpool-controller logs.","title":"FAQ"},{"location":"usage/faq/#faq","text":"Frequently asked questions English | \u7b80\u4f53\u4e2d\u6587","title":"FAQ"},{"location":"usage/faq/#general","text":"","title":"General"},{"location":"usage/faq/#what-is-spiderpool","text":"Spiderpool project is consist of several plugins include: spiderpool , coordinator , ifacer . The spiderpool basically is a IPAM plugin works for CNI main plugin to manage IP addresses for the container. The coordinator is a plugin that coordinate the routes. The ifacer plugin help you to create vlan sub-interface or create bond interfaces. The coordinator and ifacer plugin are used in CNI plugin chaining and they also optional to use.","title":"What is Spiderpool?"},{"location":"usage/faq/#configuration","text":"","title":"Configuration"},{"location":"usage/faq/#why-doesnt-changing-configmap-configuration-update-the-behavior-of-spiderpool","text":"If you change the configmap spiderpool-conf configurations, you need to restart spiderpool-agent and spiderpool-controller components","title":"Why doesn't changing configmap configuration update the behavior of Spiderpool?"},{"location":"usage/faq/#operation","text":"","title":"Operation"},{"location":"usage/faq/#why-spidersubnet-feature-not-works-well","text":"For error like Internal error occurred: failed calling webhook \"spidersubnet.spiderpool.spidernet.io\": the server could not find the requested resource , you need to update configmap spiderpool-conf to enable SpiderSubnet feature and restart spiderpool-agent and spiderpool-controller components. For error like failed to get IPPool candidates from Subnet: no matching auto-created IPPool candidate with matchLables , you should check spiderpool-controller logs.","title":"Why SpiderSubnet feature not works well?"},{"location":"usage/kubevirt-zh_CN/","text":"KubeVirt \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u80fd\u4fdd\u8bc1 kubevirt VM \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 KubeVirt \u7f51\u7edc\u642d\u914d Spiderpool underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u53ef\u7ed9 KubeVirt \u8d4b\u4e88\u4ecb\u5165 underlay \u7684\u80fd\u529b: \u5bf9\u4e8e KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d Spiderpool macvlan \u96c6\u6210\u65b9\u6848\u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u652f\u6301 Service Mesh \u7684\u6240\u6709\u529f\u80fd\uff0c\u4e0d\u8fc7\u53ea\u80fd\u4f7f\u7528**\u5355\u7f51\u5361**\uff0c\u4e14\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 \u5bf9\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d OVS CNI \u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u4e0d\u652f\u6301 Service Mesh \u529f\u80fd\uff0c\u53ef\u4f7f\u7528**\u591a\u7f51\u5361**\uff0c\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 KubeVirt VM \u56fa\u5b9a\u5730\u5740 KubeVirt VM \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a VM \u7684\u70ed\u8fc1\u79fb\uff0c\u671f\u671b\u8fc1\u79fb\u8fc7\u540e\u7684 VM \u4ecd\u80fd\u7ee7\u627f\u4e4b\u524d\u7684 IP \u5730\u5740\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u4e86\u91cd\u542f\u7684\u60c5\u51b5\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 VMI(VirtualMachineInstance) \u8d44\u6e90\u88ab\u5220\u9664\u7684\u60c5\u666f\u3002 \u6b64\u5916\uff0cKubeVirt VM \u56fa\u5b9a IP \u5730\u5740\u4e0e StatefulSet \u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e VM \uff0cPod \u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u7684\u540d\u5b57\u662f\u4f1a\u53d1\u751f\u53d8\u5316\u7684\uff0c\u4f46\u662f\u5176\u5bf9\u5e94\u7684 VMI \u4e0d\u8bba\u91cd\u542f\u4e0e\u5426\uff0c\u5176\u540d\u5b57\u90fd\u4e0d\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u4f1a\u4ee5 VM \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740(\u6211\u4eec\u7684 SpiderEndpoint \u8d44\u6e90\u5c06\u4f1a\u7ee7\u627f\u4f7f\u7528 VM \u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4ee5\u53ca\u540d\u5b57)\u3002 \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u6211\u4eec Spiderpool \u4f1a\u56e0\u6b64\u4ee5 Pod \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cVM \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba VM \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u5230\u76f8\u540c IP\u3002 \u82e5\u5173\u95ed\uff0cVM \u5bf9\u5e94\u7684 Pod \u5c06\u88ab\u5f53\u4f5c\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u901a\u8fc7 --set ipam.enableKubevirtStaticIP=false \u5173\u95ed\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u4ee5\u4e0b\u6d41\u7a0b\u5c06\u4f1a\u6f14\u793a KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan CNI \u4ee5\u4f7f\u5f97 VM \u83b7\u5f97 underlay \u63a5\u5165\u80fd\u529b\uff0c\u5e76\u901a\u8fc7 Spiderpool \u5b9e\u73b0\u5206\u914d\u56fa\u5b9a IP \u7684\u529f\u80fd\u3002 Notice\uff1a\u5f53\u524d macvlan \u548c ipvlan \u5e76\u4e0d\u9002\u7528\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u56e0\u4e3a\u5bf9\u4e8e bridge \u7f51\u7edc\u6a21\u5f0f\u4f1a\u5c06 Pod \u7f51\u5361\u7684 MAC \u5730\u5740\u79fb\u52a8\u5230 VM\uff0c\u4f7f\u5f97 Pod \u4f7f\u7528\u53e6\u4e00\u4e2a\u4e0d\u540c\u7684\u5730\u5740\u3002\u800c macvlan \u548c ipvlan CNI \u8981\u6c42 Pod \u7684\u7f51\u5361\u63a5\u53e3\u5177\u6709\u539f\u59cb MAC \u5730\u5740\u3002 \u5b89\u88c5 Spiderpool \u8bf7\u53c2\u8003 Macvlan Quick Start \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u53ef\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableKubevirtStaticIP=true \u521b\u5efa KubeVirt VM \u5e94\u7528 underlay \u5355\u7f51\u5361\u573a\u666f \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u9009\u62e9\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u7684 CNI \u914d\u7f6e\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-cirros labels : kubevirt.io/vm : vm-cirros spec : runStrategy : Always template : metadata : annotations : v1.multus-cni.io/default-network : kube-system/macvlan-ens192 labels : kubevirt.io/vm : vm-cirros spec : domain : devices : disks : - name : containerdisk disk : bus : virtio - name : cloudinitdisk disk : bus : virtio interfaces : - name : default passt : {} resources : requests : memory : 64M networks : - name : default pod : {} volumes : - name : containerdisk containerDisk : image : quay.io/kubevirt/cirros-container-disk-demo - name : cloudinitdisk cloudInitNoCloud : userData : | #!/bin/sh echo 'printed from cloud-init userdata' \u6700\u7ec8\uff0c\u5728 KubeVirt VM \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VM Pod, \u89c2\u5bdf\u5230\u65b0\u7684 Pod \u7684 IP \u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VMI\uff0c\u89c2\u5bdf\u5230\u540e\u7eed\u65b0\u7684 Pod \u7684IP \u4e5f\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 VM \u4e5f\u53ef\u4e0e\u5176\u4ed6 underlay Pod \u7684\u901a\u4fe1\u3002 ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms VM \u4e5f\u53ef\u8bbf\u95ee cluster IP\u3002 ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes underlay \u591a\u7f51\u5361\u573a\u666f \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt bridge \u7f51\u7edc\u6a21\u5f0f\u642d\u914d ovs-cni \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippools : \u4e3a\u5e94\u7528\u6307\u5b9a\u6bcf\u5f20\u7f51\u5361\u9009\u62e9\u4f7f\u7528\u54ea\u4e9b IP \u6c60\u3002(\u4f60\u4e5f\u53ef\u4f7f\u7528 multus \u5b9e\u4f8b\u4e2d\u6307\u5b9a\u7684 CNI \u914d\u7f6e\u6587\u4ef6\u7ea7\u522b\u9ed8\u8ba4 IP \u6c60) \u8981\u6c42 multus \u5b9e\u4f8b kube-system/ovs-vlan30 \u548c kube-system/ovs-vlan40 \u542f\u7528 coordinator \u63d2\u4ef6\u6765\u534f\u8c03\u591a\u7f51\u5361\u9ed8\u8ba4\u8def\u7531\u3002 ovs-cni \u4e0d\u652f\u6301 clusterIP \u8bbf\u95ee\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk \u603b\u7ed3 Spiderpool \u80fd\u4fdd\u8bc1 KubeVirt VM Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 KubeVirt \u865a\u62df\u673a\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002\u5e76\u53ef\u914d\u5408 macvlan \u6216 OVS CNI \u4e0e KubeVirt \u7684\u591a\u79cd\u7f51\u7edc\u6a21\u5f0f\u5b9e\u73b0 VM underlay \u63a5\u5165\u80fd\u529b\u3002","title":"KubeVirt"},{"location":"usage/kubevirt-zh_CN/#kubevirt","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"KubeVirt"},{"location":"usage/kubevirt-zh_CN/#_1","text":"Spiderpool \u80fd\u4fdd\u8bc1 kubevirt VM \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/kubevirt-zh_CN/#kubevirt_1","text":"Spiderpool underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u53ef\u7ed9 KubeVirt \u8d4b\u4e88\u4ecb\u5165 underlay \u7684\u80fd\u529b: \u5bf9\u4e8e KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d Spiderpool macvlan \u96c6\u6210\u65b9\u6848\u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u652f\u6301 Service Mesh \u7684\u6240\u6709\u529f\u80fd\uff0c\u4e0d\u8fc7\u53ea\u80fd\u4f7f\u7528**\u5355\u7f51\u5361**\uff0c\u4e14\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002 \u5bf9\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u53ef\u642d\u914d OVS CNI \u4f7f\u7528\u3002\u5728\u8be5\u7f51\u7edc\u6a21\u5f0f\u4e0b\uff0c \u4e0d\u652f\u6301 Service Mesh \u529f\u80fd\uff0c\u53ef\u4f7f\u7528**\u591a\u7f51\u5361**\uff0c\u4e0d\u652f\u6301\u70ed\u8fc1\u79fb\u3002","title":"KubeVirt \u7f51\u7edc\u642d\u914d"},{"location":"usage/kubevirt-zh_CN/#kubevirt-vm","text":"KubeVirt VM \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a VM \u7684\u70ed\u8fc1\u79fb\uff0c\u671f\u671b\u8fc1\u79fb\u8fc7\u540e\u7684 VM \u4ecd\u80fd\u7ee7\u627f\u4e4b\u524d\u7684 IP \u5730\u5740\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u4e86\u91cd\u542f\u7684\u60c5\u51b5\u3002 VM \u8d44\u6e90\u5bf9\u5e94\u7684 VMI(VirtualMachineInstance) \u8d44\u6e90\u88ab\u5220\u9664\u7684\u60c5\u666f\u3002 \u6b64\u5916\uff0cKubeVirt VM \u56fa\u5b9a IP \u5730\u5740\u4e0e StatefulSet \u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e VM \uff0cPod \u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u7684\u540d\u5b57\u662f\u4f1a\u53d1\u751f\u53d8\u5316\u7684\uff0c\u4f46\u662f\u5176\u5bf9\u5e94\u7684 VMI \u4e0d\u8bba\u91cd\u542f\u4e0e\u5426\uff0c\u5176\u540d\u5b57\u90fd\u4e0d\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u4f1a\u4ee5 VM \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740(\u6211\u4eec\u7684 SpiderEndpoint \u8d44\u6e90\u5c06\u4f1a\u7ee7\u627f\u4f7f\u7528 VM \u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4ee5\u53ca\u540d\u5b57)\u3002 \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u6211\u4eec Spiderpool \u4f1a\u56e0\u6b64\u4ee5 Pod \u4e3a\u5355\u4f4d\u6765\u8bb0\u5f55\u5176\u56fa\u5b9a\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cVM \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba VM \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u5230\u76f8\u540c IP\u3002 \u82e5\u5173\u95ed\uff0cVM \u5bf9\u5e94\u7684 Pod \u5c06\u88ab\u5f53\u4f5c\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u901a\u8fc7 --set ipam.enableKubevirtStaticIP=false \u5173\u95ed\u3002","title":"KubeVirt VM \u56fa\u5b9a\u5730\u5740"},{"location":"usage/kubevirt-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/kubevirt-zh_CN/#_3","text":"\u4ee5\u4e0b\u6d41\u7a0b\u5c06\u4f1a\u6f14\u793a KubeVirt \u7684 passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan CNI \u4ee5\u4f7f\u5f97 VM \u83b7\u5f97 underlay \u63a5\u5165\u80fd\u529b\uff0c\u5e76\u901a\u8fc7 Spiderpool \u5b9e\u73b0\u5206\u914d\u56fa\u5b9a IP \u7684\u529f\u80fd\u3002 Notice\uff1a\u5f53\u524d macvlan \u548c ipvlan \u5e76\u4e0d\u9002\u7528\u4e8e KubeVirt \u7684 bridge \u7f51\u7edc\u6a21\u5f0f\uff0c\u56e0\u4e3a\u5bf9\u4e8e bridge \u7f51\u7edc\u6a21\u5f0f\u4f1a\u5c06 Pod \u7f51\u5361\u7684 MAC \u5730\u5740\u79fb\u52a8\u5230 VM\uff0c\u4f7f\u5f97 Pod \u4f7f\u7528\u53e6\u4e00\u4e2a\u4e0d\u540c\u7684\u5730\u5740\u3002\u800c macvlan \u548c ipvlan CNI \u8981\u6c42 Pod \u7684\u7f51\u5361\u63a5\u53e3\u5177\u6709\u539f\u59cb MAC \u5730\u5740\u3002","title":"\u6b65\u9aa4"},{"location":"usage/kubevirt-zh_CN/#spiderpool","text":"\u8bf7\u53c2\u8003 Macvlan Quick Start \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u53ef\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableKubevirtStaticIP=true","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/kubevirt-zh_CN/#kubevirt-vm_1","text":"","title":"\u521b\u5efa KubeVirt VM \u5e94\u7528"},{"location":"usage/kubevirt-zh_CN/#underlay","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt passt \u7f51\u7edc\u6a21\u5f0f\u642d\u914d macvlan \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u9009\u62e9\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u7684 CNI \u914d\u7f6e\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-cirros labels : kubevirt.io/vm : vm-cirros spec : runStrategy : Always template : metadata : annotations : v1.multus-cni.io/default-network : kube-system/macvlan-ens192 labels : kubevirt.io/vm : vm-cirros spec : domain : devices : disks : - name : containerdisk disk : bus : virtio - name : cloudinitdisk disk : bus : virtio interfaces : - name : default passt : {} resources : requests : memory : 64M networks : - name : default pod : {} volumes : - name : containerdisk containerDisk : image : quay.io/kubevirt/cirros-container-disk-demo - name : cloudinitdisk cloudInitNoCloud : userData : | #!/bin/sh echo 'printed from cloud-init userdata' \u6700\u7ec8\uff0c\u5728 KubeVirt VM \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VM Pod, \u89c2\u5bdf\u5230\u65b0\u7684 Pod \u7684 IP \u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 \u91cd\u542f KubeVirt VMI\uff0c\u89c2\u5bdf\u5230\u540e\u7eed\u65b0\u7684 Pod \u7684IP \u4e5f\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 VM \u4e5f\u53ef\u4e0e\u5176\u4ed6 underlay Pod \u7684\u901a\u4fe1\u3002 ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms VM \u4e5f\u53ef\u8bbf\u95ee cluster IP\u3002 ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes","title":"underlay \u5355\u7f51\u5361\u573a\u666f"},{"location":"usage/kubevirt-zh_CN/#underlay_1","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a\u4f7f\u7528 KubeVirt bridge \u7f51\u7edc\u6a21\u5f0f\u642d\u914d ovs-cni \u7684 KubeVirt VM \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippools : \u4e3a\u5e94\u7528\u6307\u5b9a\u6bcf\u5f20\u7f51\u5361\u9009\u62e9\u4f7f\u7528\u54ea\u4e9b IP \u6c60\u3002(\u4f60\u4e5f\u53ef\u4f7f\u7528 multus \u5b9e\u4f8b\u4e2d\u6307\u5b9a\u7684 CNI \u914d\u7f6e\u6587\u4ef6\u7ea7\u522b\u9ed8\u8ba4 IP \u6c60) \u8981\u6c42 multus \u5b9e\u4f8b kube-system/ovs-vlan30 \u548c kube-system/ovs-vlan40 \u542f\u7528 coordinator \u63d2\u4ef6\u6765\u534f\u8c03\u591a\u7f51\u5361\u9ed8\u8ba4\u8def\u7531\u3002 ovs-cni \u4e0d\u652f\u6301 clusterIP \u8bbf\u95ee\u3002 apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk","title":"underlay \u591a\u7f51\u5361\u573a\u666f"},{"location":"usage/kubevirt-zh_CN/#_4","text":"Spiderpool \u80fd\u4fdd\u8bc1 KubeVirt VM Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 KubeVirt \u865a\u62df\u673a\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002\u5e76\u53ef\u914d\u5408 macvlan \u6216 OVS CNI \u4e0e KubeVirt \u7684\u591a\u79cd\u7f51\u7edc\u6a21\u5f0f\u5b9e\u73b0 VM underlay \u63a5\u5165\u80fd\u529b\u3002","title":"\u603b\u7ed3"},{"location":"usage/kubevirt/","text":"KubeVirt English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool ensures that KubeVirt VM Pods consistently obtain the same IP addresses during restart and rebuild processes. Integrate with KubeVirt Networking The Spiderpool underlay networking solution provides the ability to integrate with KubeVirt: For KubeVirt's passt network mode, it can be used in conjunction with the Spiderpool macvlan integrated solution. In this network mode, all Service Mesh functionalities are supported . However, only single NIC is supported, and live migration is not available. For KubeVirt's bridge network mode, it can be used in conjunction with OVS CNI. In this network mode, Service Mesh functionalities are not supported , but multiple NICs can be used, and live migration is not available. Fix IP Address for KubeVirt VMs KubeVirt VMs may require fixed IP addresses in the following scenarios: During VM live migration, it is expected that the VM retains its previous IP address after migration. When the Pod associated with the VM resource undergoes a restart. When the VMI (VirtualMachineInstance) resource corresponding to the VM resource is deleted. It is important to note that the pattern of fixed IP addresses for KubeVirt VMs differs from that of StatefulSets: For VMs, the Pod name changes between restarts, but the VMI name remains unchanged regardless of restarts. Therefore, the fixed IPs will be recorded based on VMs. Specifically, SpiderEndpoint resource will be associated with the VM resource's namespace and name to record its fixed IP address. For StatefulSets, the Pod name remains the same between restarts, so Spiderpool records the fixed IP addresses based on Pods. This feature is enabled by default. When enabled, there are no restrictions. VMs can use a limited set of IP addresses from an IP pool to assign fixed IPs. However, regardless of whether a VM uses a fixed IP pool, its Pod can consistently obtain the same IP address. If disabled, Pods associated with VMs will be treated as stateless. During installation of Spiderpool using Helm, you can disable it by using --set ipam.enableKubevirtStaticIP=false . Prerequisites A ready Kubernetes cluster. Helm has been installed. Steps The following steps demonstrate how to use the passt network mode of KubeVirt with macvlan CNI to enable VMs to access the underlay network and assign fixed IPs using Spiderpool. Currently, macvlan and ipvlan are not suitable for KubeVirt's bridge network mode because in bridge network mode, the MAC address of the Pod interface is moved to the VM, causing the Pod to use a different address. However, macvlan and ipvlan CNI require the Pod's network interface to have the original MAC address. Install Spiderpool Please refer to Macvlan Quick Start for installing Spiderpool. Make sure to set the Helm installation option ipam.enableKubevirtStaticIP=true . Create KubeVirt VM Applications underlay single NIC situation In the example YAML below, we create 1 KubeVirt VM application with KubeVirt passt network mode + macvlan: v1.multus-cni.io/default-network : select the default network CNI configuration for the application. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: vm-cirros labels: kubevirt.io/vm: vm-cirros spec: runStrategy: Always template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default passt: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userData: | #!/bin/sh echo 'printed from cloud-init userdata' When creating a KubeVirt VM application, Spiderpool randomly selects an IP from the specified IP pool to establish a binding with the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 Upon restarting a Kubevirt VM Pod, the new Pod retains its original IP address as expected. ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 When restarting a Kubevirt VMI, new Pods also maintain their assigned IP addresses as expected. ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 The VM can communicate with other underlay Pods ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms The VM can access cluster IP addresses. ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes underlay multiple NICs situation In the example YAML below, we create 1 KubeVirt VM application with KubeVirt bridge network mode + ovs-cni : ipam.spidernet.io/ippools : select IPPools for every NIC.(You can also use the multus resource CNI configuration level default IPPools) The multus resource kube-system/ovs-vlan30 and kube-system/ovs-vlan40 must enable the coordinator plugin to solve the multiple interfaces default route problem. ovs-cni doesn't support clusterIP network connectivity access. apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk Summary Spiderpool guarantees that KubeVirt VM Pods consistently acquire the same IP addresses during restart and rebuild processes, meeting the fixed IP address requirements for Kubevirt VMs. It seamlessly integrates with macvlan or OVS CNI to enable VMs to access underlay networks.","title":"Kubevirt"},{"location":"usage/kubevirt/#kubevirt","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"KubeVirt"},{"location":"usage/kubevirt/#introduction","text":"Spiderpool ensures that KubeVirt VM Pods consistently obtain the same IP addresses during restart and rebuild processes.","title":"Introduction"},{"location":"usage/kubevirt/#integrate-with-kubevirt-networking","text":"The Spiderpool underlay networking solution provides the ability to integrate with KubeVirt: For KubeVirt's passt network mode, it can be used in conjunction with the Spiderpool macvlan integrated solution. In this network mode, all Service Mesh functionalities are supported . However, only single NIC is supported, and live migration is not available. For KubeVirt's bridge network mode, it can be used in conjunction with OVS CNI. In this network mode, Service Mesh functionalities are not supported , but multiple NICs can be used, and live migration is not available.","title":"Integrate with KubeVirt Networking"},{"location":"usage/kubevirt/#fix-ip-address-for-kubevirt-vms","text":"KubeVirt VMs may require fixed IP addresses in the following scenarios: During VM live migration, it is expected that the VM retains its previous IP address after migration. When the Pod associated with the VM resource undergoes a restart. When the VMI (VirtualMachineInstance) resource corresponding to the VM resource is deleted. It is important to note that the pattern of fixed IP addresses for KubeVirt VMs differs from that of StatefulSets: For VMs, the Pod name changes between restarts, but the VMI name remains unchanged regardless of restarts. Therefore, the fixed IPs will be recorded based on VMs. Specifically, SpiderEndpoint resource will be associated with the VM resource's namespace and name to record its fixed IP address. For StatefulSets, the Pod name remains the same between restarts, so Spiderpool records the fixed IP addresses based on Pods. This feature is enabled by default. When enabled, there are no restrictions. VMs can use a limited set of IP addresses from an IP pool to assign fixed IPs. However, regardless of whether a VM uses a fixed IP pool, its Pod can consistently obtain the same IP address. If disabled, Pods associated with VMs will be treated as stateless. During installation of Spiderpool using Helm, you can disable it by using --set ipam.enableKubevirtStaticIP=false .","title":"Fix IP Address for KubeVirt VMs"},{"location":"usage/kubevirt/#prerequisites","text":"A ready Kubernetes cluster. Helm has been installed.","title":"Prerequisites"},{"location":"usage/kubevirt/#steps","text":"The following steps demonstrate how to use the passt network mode of KubeVirt with macvlan CNI to enable VMs to access the underlay network and assign fixed IPs using Spiderpool. Currently, macvlan and ipvlan are not suitable for KubeVirt's bridge network mode because in bridge network mode, the MAC address of the Pod interface is moved to the VM, causing the Pod to use a different address. However, macvlan and ipvlan CNI require the Pod's network interface to have the original MAC address.","title":"Steps"},{"location":"usage/kubevirt/#install-spiderpool","text":"Please refer to Macvlan Quick Start for installing Spiderpool. Make sure to set the Helm installation option ipam.enableKubevirtStaticIP=true .","title":"Install Spiderpool"},{"location":"usage/kubevirt/#create-kubevirt-vm-applications","text":"","title":"Create KubeVirt VM Applications"},{"location":"usage/kubevirt/#underlay-single-nic-situation","text":"In the example YAML below, we create 1 KubeVirt VM application with KubeVirt passt network mode + macvlan: v1.multus-cni.io/default-network : select the default network CNI configuration for the application. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: vm-cirros labels: kubevirt.io/vm: vm-cirros spec: runStrategy: Always template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default passt: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userData: | #!/bin/sh echo 'printed from cloud-init userdata' When creating a KubeVirt VM application, Spiderpool randomly selects an IP from the specified IP pool to establish a binding with the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-rg6fs 2 /2 Running 0 3m43s 10 .6.168.105 node2 <none> 1 /1 Upon restarting a Kubevirt VM Pod, the new Pod retains its original IP address as expected. ~# kubectl delete pod virt-launcher-vm-cirros-rg6fs pod \"virt-launcher-vm-cirros-rg6fs\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-d68l2 2 /2 Running 0 1m21s 10 .6.168.105 node2 <none> 1 /1 When restarting a Kubevirt VMI, new Pods also maintain their assigned IP addresses as expected. ~# kubectl delete vmi vm-cirros virtualmachineinstance.kubevirt.io \"vm-cirros\" deleted ~# kubectl get po -l vm.kubevirt.io/name = vm-cirros -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-vm-cirros-jjgrl 2 /2 Running 0 104s 10 .6.168.105 node2 <none> 1 /1 The VM can communicate with other underlay Pods ~# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES daocloud-2048-5855b45f44-bvmdr 1 /1 Running 0 5m55s 10 .6.168.108 spider-worker <none> <none> ~# kubectl virtctl console vm-cirros $ ping -c 1 10 .6.168.108 PING 10 .6.168.108 ( 10 .6.168.108 ) : 56 data bytes 64 bytes from 10 .6.168.108: seq = 0 ttl = 255 time = 70 .554 ms --- 10 .6.168.108 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 70 .554/70.554/70.554 ms The VM can access cluster IP addresses. ~# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR daocloud-2048-svc ClusterIP 10 .233.36.38 <none> 80 /TCP 3m50s app = daocloud-2048 ~# curl -I 10 .233.36.38:80 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Tue, 17 Oct 2023 06 :50:04 GMT Content-Type: text/html Content-Length: 4090 Last-Modified: Tue, 17 Oct 2023 06 :40:53 GMT Connection: keep-alive ETag: \"652e2c75-ffa\" Accept-Ranges: bytes","title":"underlay single NIC situation"},{"location":"usage/kubevirt/#underlay-multiple-nics-situation","text":"In the example YAML below, we create 1 KubeVirt VM application with KubeVirt bridge network mode + ovs-cni : ipam.spidernet.io/ippools : select IPPools for every NIC.(You can also use the multus resource CNI configuration level default IPPools) The multus resource kube-system/ovs-vlan30 and kube-system/ovs-vlan40 must enable the coordinator plugin to solve the multiple interfaces default route problem. ovs-cni doesn't support clusterIP network connectivity access. apiVersion : kubevirt.io/v1 kind : VirtualMachine metadata : name : vm-centos spec : runStrategy : Always template : metadata : annotations : ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"vlan30-v4-ippool\"], \"ipv6\": [\"vlan30-v6-ippool\"] },{ \"ipv4\": [\"vlan40-v4-ippool\"], \"ipv6\": [\"vlan40-v6-ippool\"] }] spec : architecture : amd64 domain : cpu : cores : 1 model : host-model sockets : 2 threads : 1 devices : disks : - disk : bus : virtio name : containerdisk - disk : bus : virtio name : cloudinitdisk interfaces : - bridge : {} name : ovs-bridge1 - bridge : {} name : ovs-bridge2 features : acpi : enabled : true machine : type : q35 resources : requests : memory : 1Gi networks : - multus : default : true networkName : kube-system/ovs-vlan30 name : ovs-bridge1 - multus : networkName : kube-system/ovs-vlan40 name : ovs-bridge2 volumes : - name : containerdisk containerDisk : image : release-ci.daocloud.io/virtnest/system-images/centos-7.9-x86_64:v1 - cloudInitNoCloud : networkData : | version: 2 ethernets: eth0: dhcp4: true eth1: dhcp4: true userData : | #cloud-config ssh_pwauth: true disable_root: false chpasswd: {\"list\": \"root:dangerous\", expire: False} runcmd: - sed -i \"/#\\?PermitRootLogin/s/^.*$/PermitRootLogin yes/g\" /etc/ssh/sshd_config name : cloudinitdisk","title":"underlay multiple NICs situation"},{"location":"usage/kubevirt/#summary","text":"Spiderpool guarantees that KubeVirt VM Pods consistently acquire the same IP addresses during restart and rebuild processes, meeting the fixed IP address requirements for Kubevirt VMs. It seamlessly integrates with macvlan or OVS CNI to enable VMs to access underlay networks.","title":"Summary"},{"location":"usage/multi-interfaces-annotation/","text":"Pod annotation of multi-NIC When assigning multiple NICs to a Pod with Multus CNI , Spiderpool supports to specify the IP pools for each interface. This feature supports to implement by annotation ipam.spidernet.io/subnets and ipam.spidernet.io/ippools Get Started The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a Pod with two NICs with IP in different subnets. Set up Spiderpool Follow the guide installation to install Spiderpool. Set up Multus Configuration In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed: Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use. In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service . ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf-ens192 20s macvlan-conf-ens224 22s multiple NICs by subnet Create two Subnets to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-test-ens192 4 10 .6.0.1/16 0 10 subnet-test-ens224 4 10 .7.0.1/16 0 10 In the following example Yaml, 2 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-test-app-v4-eth0-b1a361c7e9df 4 10 .6.0.1/16 2 3 false false auto-test-app-v4-net1-b1a361c7e9df 4 10 .7.0.1/16 2 3 false false ~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.6.168.171-10.6.168.173\" ] ~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.7.168.171-10.7.168.173\" ] ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f4594ff67-fkqbw 1 /1 Running 0 40s 10 .6.168.172 node2 <none> <none> test-app-6f4594ff67-gwlx8 1 /1 Running 0 40s 10 .6.168.173 node1 <none> <none> ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a 3 : eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.168.172/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 26 :6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 5 : net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .7.168.173/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.173 lookup 100 32765 : from all to 10 .7.168.173/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 proto kernel scope link src 10 .7.168.173 10 .96.0.0/12 via 10 .6.168.123 dev veth0 multiple NICs by IPPool Create two IPPools to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 0 5 false false ippool-test-ens224 4 10 .7.0.1/16 0 5 false false In the following example Yaml, 1 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 1 5 false false ippool-test-ens224 4 10 .7.0.1/16 1 5 false false ~# kubectl get po -l app = ippool-test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-65f646574c-mpr47 1 /1 Running 0 6m18s 10 .6.168.175 node2 <none> <none> ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a ... 3 : eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff inet 10 .6.168.175/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 86 :ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff 5 : net1@eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff inet 10 .7.168.177/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.177 lookup 100 32765 : from all to 10 .7.168.177/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 scope link src 10 .7.168.177 10 .96.0.0/12 via 10 .6.168.123 dev veth0 Clean up Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\ --ignore-not-found = true","title":"MultipleInterfaces"},{"location":"usage/multi-interfaces-annotation/#pod-annotation-of-multi-nic","text":"When assigning multiple NICs to a Pod with Multus CNI , Spiderpool supports to specify the IP pools for each interface. This feature supports to implement by annotation ipam.spidernet.io/subnets and ipam.spidernet.io/ippools","title":"Pod annotation of multi-NIC"},{"location":"usage/multi-interfaces-annotation/#get-started","text":"The example will create two Multus CNI Configuration object and create two underlay subnets. Then run a Pod with two NICs with IP in different subnets.","title":"Get Started"},{"location":"usage/multi-interfaces-annotation/#set-up-spiderpool","text":"Follow the guide installation to install Spiderpool.","title":"Set up Spiderpool"},{"location":"usage/multi-interfaces-annotation/#set-up-multus-configuration","text":"In this example, Macvlan will be used as the main CNI, Create two network-attachment-definitions\uff0cThe following parameters need to be confirmed: Confirm the host machine parent interface required for Macvlan. This example takes the ens192 and ens224 network cards of the host machine as examples to create a Macvlan sub interface for Pod to use. In order to use the Veth plugin for clusterIP communication, you need to confirm the serviceIP CIDR of the cluster service, e.g. by using the command kubectl -n kube-system get configmap kubeadm-config -oyaml | grep service . ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf-ens192 20s macvlan-conf-ens224 22s","title":"Set up Multus Configuration"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-subnet","text":"Create two Subnets to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-test-ens192 4 10 .6.0.1/16 0 10 subnet-test-ens224 4 10 .7.0.1/16 0 10 In the following example Yaml, 2 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool will select random IPs from the specified subnet to create two fixed IP pools to bind to each of the Deployment Pod's two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE auto-test-app-v4-eth0-b1a361c7e9df 4 10 .6.0.1/16 2 3 false false auto-test-app-v4-net1-b1a361c7e9df 4 10 .7.0.1/16 2 3 false false ~# kubectl get spiderippool auto-test-app-v4-eth0-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.6.168.171-10.6.168.173\" ] ~# kubectl get spiderippool auto-test-app-v4-net1-b1a361c7e9df -o jsonpath = '{.spec.ips}' [ \"10.7.168.171-10.7.168.173\" ] ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f4594ff67-fkqbw 1 /1 Running 0 40s 10 .6.168.172 node2 <none> <none> test-app-6f4594ff67-gwlx8 1 /1 Running 0 40s 10 .6.168.173 node1 <none> <none> ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip a 3 : eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether ae:fa:5e:d9:79:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.168.172/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 26 :6f:22:91:22:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 5 : net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether d6:4b:c2:6a:62:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .7.168.173/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.173 lookup 100 32765 : from all to 10 .7.168.173/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti test-app-6f4594ff67-fkqbw -- ip route show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 proto kernel scope link src 10 .7.168.173 10 .96.0.0/12 via 10 .6.168.123 dev veth0","title":"multiple NICs by subnet"},{"location":"usage/multi-interfaces-annotation/#multiple-nics-by-ippool","text":"Create two IPPools to provide IP addresses for different interfaces. ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 0 5 false false ippool-test-ens224 4 10 .7.0.1/16 0 5 false false In the following example Yaml, 1 copies of the Deployment are created\uff1a ~# kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml Eventually, when the Deployment is created, Spiderpool randomly selects IPs from the two specified IPPools to form bindings to each of the two NICs. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test-ens192 4 10 .6.0.1/16 1 5 false false ippool-test-ens224 4 10 .7.0.1/16 1 5 false false ~# kubectl get po -l app = ippool-test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ippool-test-app-65f646574c-mpr47 1 /1 Running 0 6m18s 10 .6.168.175 node2 <none> <none> ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip a ... 3 : eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 2a:ca:ce:06:1e:91 brd ff:ff:ff:ff:ff:ff inet 10 .6.168.175/16 brd 10 .6.255.255 scope global eth0 valid_lft forever preferred_lft forever 4 : veth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue link/ether 86 :ba:6f:97:ae:1b brd ff:ff:ff:ff:ff:ff 5 : net1@eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue link/ether f2:12:b5:8c:ff:4f brd ff:ff:ff:ff:ff:ff inet 10 .7.168.177/16 brd 10 .7.255.255 scope global net1 valid_lft forever preferred_lft forever The following command shows the multi-NIC routing information in the Pod. The Veth plug-in can automatically coordinate the policy routing between multiple NICs and solve the communication problems between multiple NICs. ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip rule show 0 : from all lookup local 32764 : from 10 .7.168.177 lookup 100 32765 : from all to 10 .7.168.177/16 lookup 100 32766 : from all lookup main 32767 : from all lookup default ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show main default via 10 .6.0.1 dev eth0 ~# kubectl exec -ti ippool-test-app-65f646574c-mpr47 -- ip r show table 100 default via 10 .7.0.1 dev net1 10 .6.168.123 dev veth0 scope link 10 .7.0.0/16 dev net1 scope link src 10 .7.168.177 10 .96.0.0/12 via 10 .6.168.123 dev veth0","title":"multiple NICs by IPPool"},{"location":"usage/multi-interfaces-annotation/#clean-up","text":"Clean the relevant resources so that you can run this tutorial again. kubectl delete \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/multus-conf.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-subnets.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/subnet-test-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/different-segment-ipv4-ippools.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-dual-ippool-deploy.yaml \\ -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/multi-interfaces-annotation/ippool-test-deploy.yaml \\ --ignore-not-found = true","title":"Clean up"},{"location":"usage/network-topology-zh_CN/","text":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u968f\u7740\u6570\u636e\u4e2d\u5fc3\u79c1\u6709\u4e91\u7684\u4e0d\u65ad\u666e\u53ca\uff0cUnderlay \u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u4f20\u8f93\u548c\u66f4\u597d\u7684\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u80fd\u529b\u3002\u7531\u4e8e\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9760\u3001\u5b89\u5168\u7b49\u7279\u6027\uff0cUnderlay \u5728\u4e0b\u5217\u573a\u666f\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7684\u5e94\u7528\uff1a \u5ef6\u65f6\u654f\u611f\u7684\u5e94\u7528\uff1a\u67d0\u4e9b\u7279\u5b9a\u884c\u4e1a\u6216\u5e94\u7528\uff08\u5982\u91d1\u878d\u4ea4\u6613\u3001\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u7b49\uff09\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u975e\u5e38\u654f\u611f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cUnderlay \u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u7269\u7406\u548c\u94fe\u8def\u5c42\u7684\u8fde\u63a5\u6765\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u65f6\u95f4\u3002\u8fd9\u79cd\u4f4e\u5ef6\u8fdf\u7684\u7279\u6027\u4f7f\u5f97 Underlay \u7f51\u7edc\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u5e94\u7528\u9700\u6c42\u7684\u7406\u60f3\u9009\u62e9\u3002 \u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff1a\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7 Underlay \u7f51\u7edc\u7684 IPAM \u5bf9\u5e94\u7528\u56fa\u5b9a\u51fa\u53e3 IP \u5730\u5740\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u63a7\u5236\u96c6\u7fa4\u4e0e\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002 \u5728 Underlay \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5f53\u5b83\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u800c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u5b50\u7f51\u65f6\uff0c\u5c06\u5bf9 IP \u5730\u5740\u7ba1\u7406\uff08IPAM\uff09\u5e26\u6765\u6311\u6218\uff0c\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u80fd\u5b9e\u73b0\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u7684\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u3002 \u4e00\u4e2a\u96c6\u7fa4\uff0c\u4f46\u96c6\u7fa4\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 10.6.1.0/24\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.16.2.0/24\u3002 \u5728\u4e0a\u8ff0\u7684\u573a\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u4e0e\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\uff0cSpiderpool \u7684 CR\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5b9e\u73b0 IP \u6c60\u4e0e\u8282\u70b9\u4e4b\u95f4\u7684\u4eb2\u548c\u6027\uff0c\u8ba9 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e00\u8282\u70b9\u65f6\uff0c\u80fd\u4ece\u8282\u70b9\u6240\u5728\u7684 Underlay \u5b50\u7f51\u4e2d\u83b7\u5f97 IP \u5730\u5740\uff0c\u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd. \u5b9e\u65bd\u8981\u6c42 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4 \u51c6\u5907\u4e00\u5957\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4\uff0c\u5982\u8282\u70b9 1 \u4f7f\u7528 10.6.0.0/16 \uff0c\u8282\u70b9 2 \u4f7f\u7528 10.7.0.0/16 \u5b50\u7f51\uff0c\u4ee5\u4e0b\u662f\u6240\u4f7f\u7528\u7684\u96c6\u7fa4\u4fe1\u606f\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u56fe\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 eth0 \u4f5c\u4e3a master \u7684\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u5e94\u4e0e\u96c6\u7fa4\u8de8\u8d8a\u7f51\u7edc\u533a\u7684\u8282\u70b9\u4e0a\u7684\u63a5\u53e3\u540d\u79f0\u5339\u914d\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m \u521b\u5efa IPPools Spiderpool \u7684 CRD\uff1aSpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a daemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0cSpiderpool \u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u8282\u70b9\u8de8\u7f51\u7edc\u533a\u57df\u7684\u573a\u666f\u5206\u914d IP \u65f6\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684 IPPool.spec.nodeAffinity \u6ce8\u89e3\uff0c Pod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u53ef\u4ee5\u901a\u8fc7\u5907\u9009\u6c60 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u3002 v1.multus-cni.io/default-network\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5b8c\u6210\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0 Pod \u7684 IP \u5c5e\u4e8e Pod \u6240\u5728\u8282\u70b9\u7684\u5b50\u7f51\u5185\uff0c\u6240\u5bf9\u5e94\u7684 IP \u6c60\u4e3a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e86 IP \u5730\u5740\u3002 ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false \u8de8\u7f51\u7edc\u533a\u57df\u7684 Pod \u4e0e Pod \u4e4b\u95f4\u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms \u603b\u7ed3 \u4e0d\u540c\u7f51\u7edc\u533a\u57df\u7684 Pod \u80fd\u591f\u6b63\u5e38\u901a\u4fe1\uff0cSpiderpool \u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u9700\u6c42\u3002","title":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d"},{"location":"usage/network-topology-zh_CN/#ip","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d"},{"location":"usage/network-topology-zh_CN/#_1","text":"\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u79c1\u6709\u4e91\u7684\u4e0d\u65ad\u666e\u53ca\uff0cUnderlay \u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u4f20\u8f93\u548c\u66f4\u597d\u7684\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u80fd\u529b\u3002\u7531\u4e8e\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9760\u3001\u5b89\u5168\u7b49\u7279\u6027\uff0cUnderlay \u5728\u4e0b\u5217\u573a\u666f\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7684\u5e94\u7528\uff1a \u5ef6\u65f6\u654f\u611f\u7684\u5e94\u7528\uff1a\u67d0\u4e9b\u7279\u5b9a\u884c\u4e1a\u6216\u5e94\u7528\uff08\u5982\u91d1\u878d\u4ea4\u6613\u3001\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u7b49\uff09\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u975e\u5e38\u654f\u611f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cUnderlay \u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u7269\u7406\u548c\u94fe\u8def\u5c42\u7684\u8fde\u63a5\u6765\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u65f6\u95f4\u3002\u8fd9\u79cd\u4f4e\u5ef6\u8fdf\u7684\u7279\u6027\u4f7f\u5f97 Underlay \u7f51\u7edc\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e9b\u5e94\u7528\u9700\u6c42\u7684\u7406\u60f3\u9009\u62e9\u3002 \u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff1a\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7 Underlay \u7f51\u7edc\u7684 IPAM \u5bf9\u5e94\u7528\u56fa\u5b9a\u51fa\u53e3 IP \u5730\u5740\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u63a7\u5236\u96c6\u7fa4\u4e0e\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002 \u5728 Underlay \u7f51\u7edc\u7684\u96c6\u7fa4\uff0c\u5f53\u5b83\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u800c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u7279\u5b9a\u5b50\u7f51\u65f6\uff0c\u5c06\u5bf9 IP \u5730\u5740\u7ba1\u7406\uff08IPAM\uff09\u5e26\u6765\u6311\u6218\uff0c\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u80fd\u5b9e\u73b0\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u7684\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/network-topology-zh_CN/#_2","text":"Spiderpool \u63d0\u4f9b\u4e86\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u3002 \u4e00\u4e2a\u96c6\u7fa4\uff0c\u4f46\u96c6\u7fa4\u7684\u8282\u70b9\u5206\u5e03\u5728\u4e0d\u540c\u5730\u533a\u6216\u6570\u636e\u4e2d\u5fc3\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 10.6.1.0/24\uff0c\u4e00\u4e9b\u8282\u70b9\u7684\u533a\u57df\u53ea\u80fd\u4f7f\u7528\u5b50\u7f51 172.16.2.0/24\u3002 \u5728\u4e0a\u8ff0\u7684\u573a\u666f\u4e0b\uff0c\u5f53\u4e00\u4e2a\u5e94\u7528\u8de8\u5b50\u7f51\u90e8\u7f72\u526f\u672c\u65f6\uff0c\u8981\u6c42 IPAM \u80fd\u591f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\uff0c\u4e3a\u540c\u4e00\u4e2a\u5e94\u7528\u4e0b\u7684\u4e0d\u540c Pod \u5206\u914d\u51fa\u4e0e\u5b50\u7f51\u5339\u914d\u7684 IP \u5730\u5740\uff0cSpiderpool \u7684 CR\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5b9e\u73b0 IP \u6c60\u4e0e\u8282\u70b9\u4e4b\u95f4\u7684\u4eb2\u548c\u6027\uff0c\u8ba9 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e00\u8282\u70b9\u65f6\uff0c\u80fd\u4ece\u8282\u70b9\u6240\u5728\u7684 Underlay \u5b50\u7f51\u4e2d\u83b7\u5f97 IP \u5730\u5740\uff0c\u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd.","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/network-topology-zh_CN/#_3","text":"\u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/network-topology-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/network-topology-zh_CN/#_5","text":"\u51c6\u5907\u4e00\u5957\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4\uff0c\u5982\u8282\u70b9 1 \u4f7f\u7528 10.6.0.0/16 \uff0c\u8282\u70b9 2 \u4f7f\u7528 10.7.0.0/16 \u5b50\u7f51\uff0c\u4ee5\u4e0b\u662f\u6240\u4f7f\u7528\u7684\u96c6\u7fa4\u4fe1\u606f\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u56fe\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none>","title":"\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684\u96c6\u7fa4"},{"location":"usage/network-topology-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u68c0\u67e5\u5b89\u88c5\u5b8c\u6210 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/network-topology-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 eth0 \u4f5c\u4e3a master \u7684\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u5e94\u4e0e\u96c6\u7fa4\u8de8\u8d8a\u7f51\u7edc\u533a\u7684\u8282\u70b9\u4e0a\u7684\u63a5\u53e3\u540d\u79f0\u5339\u914d\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/network-topology-zh_CN/#ippools","text":"Spiderpool \u7684 CRD\uff1aSpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u5b57\u6bb5\uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF","title":"\u521b\u5efa IPPools"},{"location":"usage/network-topology-zh_CN/#_6","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4f1a\u521b\u5efa\u4e00\u4e2a daemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool\uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\u7528\u4f5c\u5907\u9009\u6c60\uff0cSpiderpool \u4f1a\u6309\u7167 \"IP \u6c60\u6570\u7ec4\" \u4e2d\u5143\u7d20\u7684\u987a\u5e8f\u4f9d\u6b21\u5c1d\u8bd5\u5206\u914d IP \u5730\u5740\uff0c\u5728\u8282\u70b9\u8de8\u7f51\u7edc\u533a\u57df\u7684\u573a\u666f\u5206\u914d IP \u65f6\uff0c\u5982\u679c\u5e94\u7528\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u7684\u8282\u70b9\uff0c\u7b26\u5408\u7b2c\u4e00\u4e2a IP \u6c60\u7684 IPPool.spec.nodeAffinity \u6ce8\u89e3\uff0c Pod \u4f1a\u4ece\u8be5\u6c60\u4e2d\u83b7\u5f97 IP \u5206\u914d\uff0c\u5982\u679c\u4e0d\u6ee1\u8db3\uff0cSpiderpool \u4f1a\u5c1d\u8bd5\u4ece\u5907\u9009\u6c60\u4e2d\u9009\u62e9 IP \u6c60\u7ee7\u7eed\u4e3a Pod \u5206\u914d IP \uff0c\u76f4\u5230\u6240\u6709\u5907\u9009\u6c60\u5168\u90e8\u7b5b\u9009\u5931\u8d25\u3002\u53ef\u4ee5\u901a\u8fc7\u5907\u9009\u6c60 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u3002 v1.multus-cni.io/default-network\uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u5b8c\u6210\u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0 Pod \u7684 IP \u5c5e\u4e8e Pod \u6240\u5728\u8282\u70b9\u7684\u5b50\u7f51\u5185\uff0c\u6240\u5bf9\u5e94\u7684 IP \u6c60\u4e3a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e86 IP \u5730\u5740\u3002 ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false \u8de8\u7f51\u7edc\u533a\u57df\u7684 Pod \u4e0e Pod \u4e4b\u95f4\u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/network-topology-zh_CN/#_7","text":"\u4e0d\u540c\u7f51\u7edc\u533a\u57df\u7684 Pod \u80fd\u591f\u6b63\u5e38\u901a\u4fe1\uff0cSpiderpool \u53ef\u4ee5\u5f88\u597d\u7684\u5b9e\u73b0\u57fa\u4e8e\u8de8\u8d8a\u7f51\u7edc\u533a\u57df\u7684 IP \u5206\u914d\u9700\u6c42\u3002","title":"\u603b\u7ed3"},{"location":"usage/network-topology/","text":"Based on IP allocation across network zones English | \u7b80\u4f53\u4e2d\u6587 Introduce The rising popularity of private cloud data centers has made underlay networks essential components of data center network architecture by offering efficient network transmission and improved network topology management capabilities. Underlay is widely used in the following scenarios due to its low latency, reliability, and security: Latency-sensitive applications: applications in specific industries, such as financial trading and real-time video transmission, are highly sensitive to network latency. Underlay networks directly control physical and link-layer connections to reduce data transmission time, providing an ideal solution for these applications. Firewall security control and management: firewalls are often used to manage north-south traffic, namely communication between internal and external networks, by checking, filtering, and restricting communication traffic. IP address management (IPAM) solutions of underlay networks that allocate fixed egress IP addresses for applications can provide better communication management and control between the cluster and external networks, further enhancing overall network security. In the cluster of the Underlay network, when its nodes are distributed in different regions or data centers, and some node regions can only use specific subnets, it will bring challenges to IP address management (IPAM). This article will introduce a method that can realize A complete underlay network solution for IP allocation across network regions. Project Functions Spiderpool provides the function of node topology, which can help solve the problem of IP allocation across network regions. Its implementation principle is as follows. A cluster, but the nodes of the cluster are distributed in different regions or data centers, some nodes' regions can only use the subnet 10.6.1.0/24, and some nodes' regions can only use the subnet 172.16.2.0/24. In the appeal scenario, when an application deploys copies across subnets, IPAM is required to assign IP addresses that match the subnet to different Pods under the same application on different nodes. Spiderpool's CR: SpiderIPPool The nodeName field is provided to realize the affinity between the IP pool and the node, so that when the Pod is scheduled to a certain node, it can obtain the IP address from the Underlay subnet where the node is located, and realize the node topology function. Implementation Requirements Installed Helm . Steps Clusters spanning network regions Prepare a set of clusters that span the network area. For example, node 1 uses 10.6.0.0/16 and node 2 uses 10.7.0.0/16 subnet. The following is the cluster information and network topology used: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> ~# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS controller-node-1 Ready control-plane,master 1h v1.25.3 node-subnet = subnet-6, ... worker-node-1 Ready <none> 1h v1.25.3 node-subnet = subnet-7, ... Install Spiderpool Install Spiderpool via helm. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Verify the installation\uff1a ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m Install CNI configuration To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an IPvlan SpiderMultusConfig configuration: master: In this example the interface eth0 is used as the parameter for master, this parameter should match the interface name on the nodes where the cluster spans network zones. MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } In the example of this article, use the above configuration to create the following two Macvlan SpiderMultusConfig, which will be automatically generated based on the Multus NetworkAttachmentDefinition CR, which corresponds to the eth0 network card of the host. ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m Create IPPools CRD of Spiderpool: SpiderIPPool provides nodeName field. When nodeName is not empty, when Pod starts on a node and tries to allocate IP from SpiderIPPool, if the node where Pod is located matches the nodeName setting, it can be retrieved from SpiderIPPool The IP is allocated successfully, otherwise the IP cannot be allocated from the SpiderIPPool. When nodeName is empty, Spiderpool does not enforce any allocation limit on Pods. As above, using the following Yaml, create 2 SpiderIPPools that will provide IP addresses to Pods on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF Create the application The following sample yaml creates a daemonSet application where: ipam.spidernet.io/ippool: It is used to specify the IP pool of Spiderpool. Multiple IP pools can be set as alternative pools. Spiderpool will try to allocate IP addresses in sequence according to the order of elements in the \"IP pool array\". When assigning IP in the network area scenario, if the node to which the application copy is scheduled meets the IPPool.spec.nodeAffinity annotation of the first IP pool, the Pod will obtain the IP allocation from the pool. Select the IP pool in the selected pool and continue to allocate IPs for Pods until all candidate pools fail to be screened. You can learn more about usage with alternative pools. v1.multus-cni.io/default-network: Used to specify the NetworkAttachmentDefinition configuration of Multus, which will create a default network card for the application. ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF After creating an application, it can be observed that each Pod's IP address is assigned from an IP pool belonging to the same subnet as its node. ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false Communication between Pods across network zones: ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms Summarize Pods in different network areas can communicate normally, and Spiderpool can well meet the IP allocation requirements based on cross-network areas.","title":"Node-based Topology"},{"location":"usage/network-topology/#based-on-ip-allocation-across-network-zones","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Based on IP allocation across network zones"},{"location":"usage/network-topology/#introduce","text":"The rising popularity of private cloud data centers has made underlay networks essential components of data center network architecture by offering efficient network transmission and improved network topology management capabilities. Underlay is widely used in the following scenarios due to its low latency, reliability, and security: Latency-sensitive applications: applications in specific industries, such as financial trading and real-time video transmission, are highly sensitive to network latency. Underlay networks directly control physical and link-layer connections to reduce data transmission time, providing an ideal solution for these applications. Firewall security control and management: firewalls are often used to manage north-south traffic, namely communication between internal and external networks, by checking, filtering, and restricting communication traffic. IP address management (IPAM) solutions of underlay networks that allocate fixed egress IP addresses for applications can provide better communication management and control between the cluster and external networks, further enhancing overall network security. In the cluster of the Underlay network, when its nodes are distributed in different regions or data centers, and some node regions can only use specific subnets, it will bring challenges to IP address management (IPAM). This article will introduce a method that can realize A complete underlay network solution for IP allocation across network regions.","title":"Introduce"},{"location":"usage/network-topology/#project-functions","text":"Spiderpool provides the function of node topology, which can help solve the problem of IP allocation across network regions. Its implementation principle is as follows. A cluster, but the nodes of the cluster are distributed in different regions or data centers, some nodes' regions can only use the subnet 10.6.1.0/24, and some nodes' regions can only use the subnet 172.16.2.0/24. In the appeal scenario, when an application deploys copies across subnets, IPAM is required to assign IP addresses that match the subnet to different Pods under the same application on different nodes. Spiderpool's CR: SpiderIPPool The nodeName field is provided to realize the affinity between the IP pool and the node, so that when the Pod is scheduled to a certain node, it can obtain the IP address from the Underlay subnet where the node is located, and realize the node topology function.","title":"Project Functions"},{"location":"usage/network-topology/#implementation-requirements","text":"Installed Helm .","title":"Implementation Requirements"},{"location":"usage/network-topology/#steps","text":"","title":"Steps"},{"location":"usage/network-topology/#clusters-spanning-network-regions","text":"Prepare a set of clusters that span the network area. For example, node 1 uses 10.6.0.0/16 and node 2 uses 10.7.0.0/16 subnet. The following is the cluster information and network topology used: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP controller-node-1 Ready control-plane 1h v1.25.3 10 .6.168.71 <none> worker-node-1 Ready <none> 1h v1.25.3 10 .7.168.73 <none> ~# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS controller-node-1 Ready control-plane,master 1h v1.25.3 node-subnet = subnet-6, ... worker-node-1 Ready <none> 1h v1.25.3 node-subnet = subnet-7, ...","title":"Clusters spanning network regions"},{"location":"usage/network-topology/#install-spiderpool","text":"Install Spiderpool via helm. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Verify the installation\uff1a ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m","title":"Install Spiderpool"},{"location":"usage/network-topology/#install-cni-configuration","text":"To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an IPvlan SpiderMultusConfig configuration: master: In this example the interface eth0 is used as the parameter for master, this parameter should match the interface name on the nodes where the cluster spans network zones. MACVLAN_MASTER_INTERFACE = \"eth0\" MACVLAN_MULTUS_NAME = \"macvlan-conf\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME } namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${ MACVLAN_MASTER_INTERFACE } In the example of this article, use the above configuration to create the following two Macvlan SpiderMultusConfig, which will be automatically generated based on the Multus NetworkAttachmentDefinition CR, which corresponds to the eth0 network card of the host. ~# ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"Install CNI configuration"},{"location":"usage/network-topology/#create-ippools","text":"CRD of Spiderpool: SpiderIPPool provides nodeName field. When nodeName is not empty, when Pod starts on a node and tries to allocate IP from SpiderIPPool, if the node where Pod is located matches the nodeName setting, it can be retrieved from SpiderIPPool The IP is allocated successfully, otherwise the IP cannot be allocated from the SpiderIPPool. When nodeName is empty, Spiderpool does not enforce any allocation limit on Pods. As above, using the following Yaml, create 2 SpiderIPPools that will provide IP addresses to Pods on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-6 spec: subnet: 10.6.0.0/16 ips: - 10.6.168.60-10.6.168.69 gateway: 10.6.0.1 nodeName: - controller-node-1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool-7 spec: subnet: 10.7.0.0/16 ips: - 10.7.168.60-10.7.168.69 gateway: 10.7.0.1 nodeName: - worker-node-1 EOF","title":"Create IPPools"},{"location":"usage/network-topology/#create-the-application","text":"The following sample yaml creates a daemonSet application where: ipam.spidernet.io/ippool: It is used to specify the IP pool of Spiderpool. Multiple IP pools can be set as alternative pools. Spiderpool will try to allocate IP addresses in sequence according to the order of elements in the \"IP pool array\". When assigning IP in the network area scenario, if the node to which the application copy is scheduled meets the IPPool.spec.nodeAffinity annotation of the first IP pool, the Pod will obtain the IP allocation from the pool. Select the IP pool in the selected pool and continue to allocate IPs for Pods until all candidate pools fail to be screened. You can learn more about usage with alternative pools. v1.multus-cni.io/default-network: Used to specify the NetworkAttachmentDefinition configuration of Multus, which will create a default network card for the application. ~# cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app spec: selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool-6\", \"test-ippool-7\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF After creating an application, it can be observed that each Pod's IP address is assigned from an IP pool belonging to the same subnet as its node. ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-j9ftl 1 /1 Running 0 45s 10 .6.168.65 controller-node-1 <none> <none> test-app-nkq5h 1 /1 Running 0 45s 10 .7.168.61 worker-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE test-ippool-6 4 10 .6.0.0/16 1 10 false false test-ippool-7 4 10 .7.0.0/16 1 10 false false Communication between Pods across network zones: ~# kubectl exec -ti test-app-j9ftl -- ping 10 .7.168.61 -c 2 PING 10 .7.168.61 ( 10 .7.168.61 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .7.168.61: icmp_seq = 1 ttl = 63 time = 1 .06 ms 64 bytes from 10 .7.168.61: icmp_seq = 2 ttl = 63 time = 0 .515 ms --- 10 .7.168.61 ping statistics --- 2 packets transmitted, 2 received, 0 % packet loss, time 1002ms rtt min/avg/max/mdev = 0 .515/0.789/1.063/0.274 ms","title":"Create the application"},{"location":"usage/network-topology/#summarize","text":"Pods in different network areas can communicate normally, and Spiderpool can well meet the IP allocation requirements based on cross-network areas.","title":"Summarize"},{"location":"usage/rdma-zh_CN/","text":"RDMA \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u8d4b\u80fd\u4e86 macvlan\u3001ipvlan \u548c SR-IOV CNI\uff0c \u8fd9\u4e9b CNI \u80fd\u8ba9\u5bbf\u4e3b\u673a\u7684 RDMA \u7f51\u5361\u66b4\u9732\u7ed9 Pod \u6765\u4f7f\u7528\uff0c\u672c\u7ae0\u8282\u5c06\u4ecb\u7ecd\u5728 Spiderpool \u4e0b\u5982\u4f55 RDMA \u7f51\u5361\u3002 \u529f\u80fd RDMA \u8bbe\u5907\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u5177\u5907 shared \u548c exclusive \u4e24\u79cd\u6a21\u5f0f\uff0c\u5bb9\u5668\u56e0\u6b64\u53ef\u4ee5\u5b9e\u73b0\u5171\u4eab RDMA \u7f51\u5361\uff0c\u6216\u8005\u72ec\u4eab RDMA \u7f51\u5361\u3002\u5728 kubernetes \u4e0b\uff0c\u53ef\u57fa\u4e8e macvlan \u6216 ipvlan CNI \u6765\u4f7f\u7528 shared \u6a21\u5f0f\u7684 RDMA \u7f51\u5361\uff0c\u4e5f\u53ef\u4ee5\u57fa\u4e8e SR-IOV CNI \u6765\u4f7f\u7528 exclusive \u6a21\u5f0f\u7684\u7f51\u5361\u3002 \u5728 shared \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f7f\u7528\u4e86 macvlan \u6216 ipvlan CNI \u6765\u66b4\u9732\u5bbf\u4e3b\u673a\u4e0a\u7684 RoCE \u7f51\u5361\u7ed9 Pod \u4f7f\u7528\uff0c\u4f7f\u7528 RDMA shared device plugin \u6765\u5b8c\u6210 RDMA \u7f51\u5361\u8d44\u6e90\u7684\u66b4\u9732\u548c Pod \u8c03\u5ea6\u3002 \u5728 exclusive \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f7f\u7528\u4e86 SR-IOV CNI \u6765\u66b4\u9732\u5bbf\u4e3b\u673a\u4e0a\u7684 RDMA \u7f51\u5361\u7ed9 Pod \u4f7f\u7528\uff0c\u66b4\u9732 RDMA \u8d44\u6e90\u3002\u4f7f\u7528 RDMA CNI \u6765\u5b8c\u6210 RDMA \u8bbe\u5907\u9694\u79bb\u3002 \u57fa\u4e8e macvlan \u6216 ipvlan \u5171\u4eab\u4f7f\u7528\u5177\u5907 RoCE \u529f\u80fd\u7684\u7f51\u5361 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e macvlan CNI \u4f7f\u5f97 Pod \u5171\u4eab\u4f7f\u7528 RDMA \u8bbe\u5907\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\uff0c\u786e\u4fdd\u4e3b\u673a\u62e5\u6709 RDMA \u7f51\u5361\uff0c\u4e14\u5b89\u88c5\u597d\u9a71\u52a8\uff0c\u786e\u4fdd RDMA \u529f\u80fd\u5de5\u4f5c\u6b63\u5e38\u3002 \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u5177\u5907 RoCE \u529f\u80fd\u7684 mellanox ConnectX 5 \u7f51\u5361\uff0c\u53ef\u6309\u7167 NVIDIA \u5b98\u65b9\u6307\u5bfc \u5b89\u88c5\u6700\u65b0\u7684 OFED \u9a71\u52a8\u3002\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u53ef\u67e5\u8be2\u5230 RDMA \u8bbe\u5907\uff1a ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 \u786e\u8ba4\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u5de5\u4f5c\u5728 shared \u6a21\u5f0f\u4e0b\uff0c\u5426\u5219\uff0c\u8bf7\u5207\u6362\u5230 shared \u6a21\u5f0f\u3002 ~# rdma system netns shared copy-on-fork on # \u5207\u6362\u5230 shared \u6a21\u5f0f ~# rdma system set netns shared \u786e\u8ba4 RDMA \u7f51\u5361\u7684\u4fe1\u606f\uff0c\u7528\u4e8e\u540e\u7eed device plugin \u53d1\u73b0\u8bbe\u5907\u8d44\u6e90\u3002 \u672c\u6f14\u793a\u73af\u5883\uff0c\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u7f51\u5361 vendors \u4e3a 15b3\uff0c\u7f51\u5361 deviceIDs \u4e3a 1017 ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] \u5b89\u88c5 Spiderpool \u5e76\u914d\u7f6e sriov-network-operator\uff1a helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName=\"macvlan-ens6f0np0\" \\ --set rdma.rdmaSharedDevicePlugin.install=true \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourcePrefix=\"spidernet.io\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourceName=\"hca_shared_devices\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.rdmaHcaMax=500 \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.vendors=\"15b3\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.deviceIDs=\"1017\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u5b8c\u6210 Spiderpool \u5b89\u88c5\u540e\uff0c\u53ef\u4ee5\u624b\u52a8\u7f16\u8f91 configmap spiderpool-rdma-shared-device-plugin \u6765\u91cd\u65b0\u914d\u7f6e RDMA shared device plugin\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m spiderpool-rdma-shared-device-plugin-dr7w8 1/1 Running 0 1m spiderpool-rdma-shared-device-plugin-zj65g 1/1 Running 0 1m \u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e0a\u62a5\u7684 RDMA \u8bbe\u5907\u8d44\u6e90\uff1a ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"memory\": \"263518036Ki\", \"pods\": \"110\", \"spidernet.io/hca_shared_devices\": \"500\", ... } }, ... ] \u5982\u679c\u4e0a\u62a5\u7684\u8d44\u6e90\u6570\u4e3a 0\uff0c\u53ef\u80fd\u7684\u539f\u56e0\uff1a (1) \u8bf7\u786e\u8ba4 configmap spiderpool-rdma-shared-device-plugin \u4e2d\u7684 vendors \u548c deviceID \u4e0e\u5b9e\u9645\u76f8\u7b26 (2) \u67e5\u770b rdma-shared-device-plugin \u7684\u65e5\u5fd7\uff0c\u5bf9\u4e8e\u652f\u6301 RDMA \u7f51\u5361\u62a5\u9519\u5982\u4e0b\u65e5\u5fd7\uff0c\u53ef\u5c1d\u8bd5\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5 apt-get install rdma-core \u6216 dnf install rdma-core error creating new device: \"missing RDMA device spec for device 0000:04:00.0, RDMA device \\\"issm\\\" not found\" \u57fa\u4e8e RDMA \u7f51\u5361\u4f5c\u4e3a master \u8282\u70b9\uff0c\u521b\u5efa macvlan \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens6f0np0 namespace: kube-system spec: cniType: macvlan macvlan: master: - \"ens6f0np0\" ippools: ipv4: [\"v4-81\"] EOF \u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u7684\u914d\u7f6e\uff0c\u6765\u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\uff1a ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/macvlan-ens6f0np0\" RESOURCE=\"spidernet.io/hca_shared_devices\" NAME=rdma-macvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38\u3002 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1\uff1a # \u80fd\u770b\u5230\u5bbf\u4e3b\u673a\u4e0a\u7684\u6240\u6709 RDMA \u7f51\u5361 ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # \u542f\u52a8\u4e00\u4e2a RDMA \u670d\u52a1 ~# ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # \u80fd\u770b\u5230\u5bbf\u4e3b\u673a\u4e0a\u7684\u6240\u6709 RDMA \u7f51\u5361 ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # \u8bbf\u95ee\u5bf9\u65b9 Pod \u7684\u670d\u52a1 ~# ib_read_lat 172.81.0.120 --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 12 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0107 PSN 0x79dd10 OUT 0x10 RKey 0x1fddbc VAddr 0x000000023bd000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:119 remote address: LID 0000 QPN 0x0107 PSN 0x40001a OUT 0x10 RKey 0x1fddbc VAddr 0x00000000bf9000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:120 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1040.353000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 1849.351000. CPU Frequency is not max. 2 1000 6.88 16.81 7.04 7.06 0.31 7.38 16.81 --------------------------------------------------------------------------------------- \u57fa\u4e8e SR-IOV \u9694\u79bb\u4f7f\u7528 RDMA \u7f51\u5361 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e SR-IOV CNI \u4f7f\u5f97 Pod \u9694\u79bb\u4f7f\u7528 RDMA \u8bbe\u5907\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\uff0c\u786e\u4fdd\u4e3b\u673a\u62e5\u6709 RDMA \u548c SR-IOV \u529f\u80fd\u7684\u7f51\u5361\uff0c\u4e14\u5b89\u88c5\u597d\u9a71\u52a8\uff0c\u786e\u4fdd RDMA \u529f\u80fd\u5de5\u4f5c\u6b63\u5e38\u3002 \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u5177\u5907 RoCE \u529f\u80fd\u7684 mellanox ConnectX 5 \u7f51\u5361\uff0c\u53ef\u6309\u7167 NVIDIA \u5b98\u65b9\u6307\u5bfc \u5b89\u88c5\u6700\u65b0\u7684 OFED \u9a71\u52a8\u3002 \u8981\u9694\u79bb\u4f7f\u7528 RDMA \u7f51\u5361\uff0c\u52a1\u5fc5\u6ee1\u8db3\u5982\u4e0b\u5176\u4e2d\u4e00\u4e2a\u6761\u4ef6\uff1a (1) \u5185\u6838\u7248\u672c\u8981\u6c42 5.3.0 \u6216\u66f4\u9ad8\u7248\u672c\uff0c\u5e76\u5728\u7cfb\u7edf\u4e2d\u52a0\u8f7d RDMA \u6a21\u5757\u3002rdma-core \u8f6f\u4ef6\u5305\u63d0\u4f9b\u4e86\u5728\u7cfb\u7edf\u542f\u52a8\u65f6\u81ea\u52a8\u52a0\u8f7d\u76f8\u5173\u6a21\u5757\u7684\u529f\u80fd\u3002 (2) Mellanox OFED \u8981\u6c42 4.7 \u6216\u66f4\u9ad8\u7248\u672c\u3002\u6b64\u65f6\u4e0d\u9700\u8981\u4f7f\u7528 5.3.0 \u6216\u66f4\u65b0\u7248\u672c\u7684\u5185\u6838\u3002 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u53ef\u67e5\u8be2\u5230 RDMA \u8bbe\u5907\uff1a ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 \u786e\u8ba4\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u5de5\u4f5c\u5728 exclusive \u6a21\u5f0f\u4e0b\uff0c\u5426\u5219\uff0c\u8bf7\u5207\u6362\u5230 exclusive \u6a21\u5f0f\u3002 # \u5207\u6362\u5230 exclusive \u6a21\u5f0f\uff0c\u91cd\u542f\u4e3b\u673a\u5931\u6548 ~# rdma system set netns exclusive # \u6301\u4e45\u5316\u914d\u7f6e ~# echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf ~# rdma system netns exclusive copy-on-fork on \u786e\u8ba4\u7f51\u5361\u5177\u5907 SR-IOV \u529f\u80fd\uff0c\u67e5\u770b\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\uff1a ~# cat /sys/class/net/ens6f0np0/device/sriov_totalvfs 127 \uff08\u53ef\u9009\uff09SR-IOV \u573a\u666f\u4e0b\uff0c\u5e94\u7528\u53ef\u4f7f NVIDIA \u7684 GPUDirect RMDA \u529f\u80fd\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u5b89\u88c5\u5185\u6838\u6a21\u5757\u3002 \u786e\u8ba4 RDMA \u7f51\u5361\u7684\u4fe1\u606f\uff0c\u7528\u4e8e\u540e\u7eed device plugin \u53d1\u73b0\u8bbe\u5907\u8d44\u6e90\u3002 \u672c\u6f14\u793a\u73af\u5883\uff0c\u8f93\u5165\u5982\u4e0b\uff0c\u7f51\u5361 vendors \u4e3a 15b3\uff0c\u7f51\u5361 deviceIDs \u4e3a 1017 ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] \u5b89\u88c5 Spiderpool helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set sriov.install=true \\ --set plugins.installRdmaCNI=true=true \\ --set multus.multusCNI.defaultCniCRName=\"sriov-rdma\" \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u5b8c\u6210 Spiderpool \u5b89\u88c5\u540e\uff0c\u53ef\u4ee5\u624b\u52a8\u7f16\u8f91 configmap spiderpool-rdma-shared-device-plugin \u6765\u91cd\u65b0\u914d\u7f6e RDMA shared device plugin \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m \u914d\u7f6e SR-IOV operator \u5982\u4e0b\u914d\u7f6e\uff0c\u4f7f\u5f97 SR-IOV operator \u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u521b\u5efa\u51fa VF\uff0c\u5e76\u4e0a\u62a5\u8d44\u6e90 cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policyrdma namespace: kube-system spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: mellanoxrdma priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" rootDevices: - 0000:af:00.0 vendor: \"15b3\" deviceType: netdevice isRdma: true EOF \u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e0a\u62a5\u7684 SR-IOV \u8bbe\u5907\u8d44\u6e90 ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"pods\": \"110\", \"spidernet.io/mellanoxrdma\": \"12\", ... } }, ... ] \u521b\u5efa SR-IOV \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/mellanoxrdma enableRdma: true ippools: ipv4: [\"v4-81\"] EOF \u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u7684\u914d\u7f6e\uff0c\u6765\u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528 ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/sriov-rdma\" RESOURCE=\"spidernet.io/mellanoxrdma\" NAME=rdma-sriov cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1\uff1a # \u53ea\u80fd\u770b\u5230\u5206\u914d\u7ed9 Pod \u7684\u4e00\u4e2a RDMA \u8bbe\u5907 ~# rdma link 7/1: mlx5_3/1: state ACTIVE physical_state LINK_UP netdev eth0 # \u542f\u52a8\u4e00\u4e2a RDMA \u670d\u52a1 ~# ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # \u80fd\u770b\u5230\u5bbf\u4e3b\u673a\u4e0a\u7684\u6240\u6709 RDMA \u7f51\u5361 ~# rdma link 10/1: mlx5_5/1: state ACTIVE physical_state LINK_UP netdev eth0 # \u8bbf\u95ee\u5bf9\u65b9 Pod \u7684\u670d\u52a1 ~# ib_read_lat 172.81.0.118 libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_4'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_2'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_0'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_3'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_1'. --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_5 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 2 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0b69 PSN 0xd476c2 OUT 0x10 RKey 0x006f00 VAddr 0x00000001f91000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:105 remote address: LID 0000 QPN 0x0d69 PSN 0xbe5c89 OUT 0x10 RKey 0x004f00 VAddr 0x0000000160d000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:118 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1338.151000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 2881.668000. CPU Frequency is not max. 2 1000 6.66 20.37 6.74 6.82 0.78 7.15 20.37 ---------------------------------------------------------------------------------------","title":"RDMA"},{"location":"usage/rdma-zh_CN/#rdma","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"RDMA"},{"location":"usage/rdma-zh_CN/#_1","text":"Spiderpool \u8d4b\u80fd\u4e86 macvlan\u3001ipvlan \u548c SR-IOV CNI\uff0c \u8fd9\u4e9b CNI \u80fd\u8ba9\u5bbf\u4e3b\u673a\u7684 RDMA \u7f51\u5361\u66b4\u9732\u7ed9 Pod \u6765\u4f7f\u7528\uff0c\u672c\u7ae0\u8282\u5c06\u4ecb\u7ecd\u5728 Spiderpool \u4e0b\u5982\u4f55 RDMA \u7f51\u5361\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/rdma-zh_CN/#_2","text":"RDMA \u8bbe\u5907\u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u5177\u5907 shared \u548c exclusive \u4e24\u79cd\u6a21\u5f0f\uff0c\u5bb9\u5668\u56e0\u6b64\u53ef\u4ee5\u5b9e\u73b0\u5171\u4eab RDMA \u7f51\u5361\uff0c\u6216\u8005\u72ec\u4eab RDMA \u7f51\u5361\u3002\u5728 kubernetes \u4e0b\uff0c\u53ef\u57fa\u4e8e macvlan \u6216 ipvlan CNI \u6765\u4f7f\u7528 shared \u6a21\u5f0f\u7684 RDMA \u7f51\u5361\uff0c\u4e5f\u53ef\u4ee5\u57fa\u4e8e SR-IOV CNI \u6765\u4f7f\u7528 exclusive \u6a21\u5f0f\u7684\u7f51\u5361\u3002 \u5728 shared \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f7f\u7528\u4e86 macvlan \u6216 ipvlan CNI \u6765\u66b4\u9732\u5bbf\u4e3b\u673a\u4e0a\u7684 RoCE \u7f51\u5361\u7ed9 Pod \u4f7f\u7528\uff0c\u4f7f\u7528 RDMA shared device plugin \u6765\u5b8c\u6210 RDMA \u7f51\u5361\u8d44\u6e90\u7684\u66b4\u9732\u548c Pod \u8c03\u5ea6\u3002 \u5728 exclusive \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f7f\u7528\u4e86 SR-IOV CNI \u6765\u66b4\u9732\u5bbf\u4e3b\u673a\u4e0a\u7684 RDMA \u7f51\u5361\u7ed9 Pod \u4f7f\u7528\uff0c\u66b4\u9732 RDMA \u8d44\u6e90\u3002\u4f7f\u7528 RDMA CNI \u6765\u5b8c\u6210 RDMA \u8bbe\u5907\u9694\u79bb\u3002","title":"\u529f\u80fd"},{"location":"usage/rdma-zh_CN/#macvlan-ipvlan-roce","text":"\u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e macvlan CNI \u4f7f\u5f97 Pod \u5171\u4eab\u4f7f\u7528 RDMA \u8bbe\u5907\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\uff0c\u786e\u4fdd\u4e3b\u673a\u62e5\u6709 RDMA \u7f51\u5361\uff0c\u4e14\u5b89\u88c5\u597d\u9a71\u52a8\uff0c\u786e\u4fdd RDMA \u529f\u80fd\u5de5\u4f5c\u6b63\u5e38\u3002 \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u5177\u5907 RoCE \u529f\u80fd\u7684 mellanox ConnectX 5 \u7f51\u5361\uff0c\u53ef\u6309\u7167 NVIDIA \u5b98\u65b9\u6307\u5bfc \u5b89\u88c5\u6700\u65b0\u7684 OFED \u9a71\u52a8\u3002\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u53ef\u67e5\u8be2\u5230 RDMA \u8bbe\u5907\uff1a ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 \u786e\u8ba4\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u5de5\u4f5c\u5728 shared \u6a21\u5f0f\u4e0b\uff0c\u5426\u5219\uff0c\u8bf7\u5207\u6362\u5230 shared \u6a21\u5f0f\u3002 ~# rdma system netns shared copy-on-fork on # \u5207\u6362\u5230 shared \u6a21\u5f0f ~# rdma system set netns shared \u786e\u8ba4 RDMA \u7f51\u5361\u7684\u4fe1\u606f\uff0c\u7528\u4e8e\u540e\u7eed device plugin \u53d1\u73b0\u8bbe\u5907\u8d44\u6e90\u3002 \u672c\u6f14\u793a\u73af\u5883\uff0c\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\uff0c\u7f51\u5361 vendors \u4e3a 15b3\uff0c\u7f51\u5361 deviceIDs \u4e3a 1017 ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] \u5b89\u88c5 Spiderpool \u5e76\u914d\u7f6e sriov-network-operator\uff1a helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName=\"macvlan-ens6f0np0\" \\ --set rdma.rdmaSharedDevicePlugin.install=true \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourcePrefix=\"spidernet.io\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourceName=\"hca_shared_devices\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.rdmaHcaMax=500 \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.vendors=\"15b3\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.deviceIDs=\"1017\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u5b8c\u6210 Spiderpool \u5b89\u88c5\u540e\uff0c\u53ef\u4ee5\u624b\u52a8\u7f16\u8f91 configmap spiderpool-rdma-shared-device-plugin \u6765\u91cd\u65b0\u914d\u7f6e RDMA shared device plugin\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m spiderpool-rdma-shared-device-plugin-dr7w8 1/1 Running 0 1m spiderpool-rdma-shared-device-plugin-zj65g 1/1 Running 0 1m \u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e0a\u62a5\u7684 RDMA \u8bbe\u5907\u8d44\u6e90\uff1a ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"memory\": \"263518036Ki\", \"pods\": \"110\", \"spidernet.io/hca_shared_devices\": \"500\", ... } }, ... ] \u5982\u679c\u4e0a\u62a5\u7684\u8d44\u6e90\u6570\u4e3a 0\uff0c\u53ef\u80fd\u7684\u539f\u56e0\uff1a (1) \u8bf7\u786e\u8ba4 configmap spiderpool-rdma-shared-device-plugin \u4e2d\u7684 vendors \u548c deviceID \u4e0e\u5b9e\u9645\u76f8\u7b26 (2) \u67e5\u770b rdma-shared-device-plugin \u7684\u65e5\u5fd7\uff0c\u5bf9\u4e8e\u652f\u6301 RDMA \u7f51\u5361\u62a5\u9519\u5982\u4e0b\u65e5\u5fd7\uff0c\u53ef\u5c1d\u8bd5\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5 apt-get install rdma-core \u6216 dnf install rdma-core error creating new device: \"missing RDMA device spec for device 0000:04:00.0, RDMA device \\\"issm\\\" not found\" \u57fa\u4e8e RDMA \u7f51\u5361\u4f5c\u4e3a master \u8282\u70b9\uff0c\u521b\u5efa macvlan \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens6f0np0 namespace: kube-system spec: cniType: macvlan macvlan: master: - \"ens6f0np0\" ippools: ipv4: [\"v4-81\"] EOF \u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u7684\u914d\u7f6e\uff0c\u6765\u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\uff1a ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/macvlan-ens6f0np0\" RESOURCE=\"spidernet.io/hca_shared_devices\" NAME=rdma-macvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38\u3002 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1\uff1a # \u80fd\u770b\u5230\u5bbf\u4e3b\u673a\u4e0a\u7684\u6240\u6709 RDMA \u7f51\u5361 ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # \u542f\u52a8\u4e00\u4e2a RDMA \u670d\u52a1 ~# ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # \u80fd\u770b\u5230\u5bbf\u4e3b\u673a\u4e0a\u7684\u6240\u6709 RDMA \u7f51\u5361 ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # \u8bbf\u95ee\u5bf9\u65b9 Pod \u7684\u670d\u52a1 ~# ib_read_lat 172.81.0.120 --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 12 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0107 PSN 0x79dd10 OUT 0x10 RKey 0x1fddbc VAddr 0x000000023bd000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:119 remote address: LID 0000 QPN 0x0107 PSN 0x40001a OUT 0x10 RKey 0x1fddbc VAddr 0x00000000bf9000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:120 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1040.353000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 1849.351000. CPU Frequency is not max. 2 1000 6.88 16.81 7.04 7.06 0.31 7.38 16.81 ---------------------------------------------------------------------------------------","title":"\u57fa\u4e8e macvlan \u6216 ipvlan \u5171\u4eab\u4f7f\u7528\u5177\u5907 RoCE \u529f\u80fd\u7684\u7f51\u5361"},{"location":"usage/rdma-zh_CN/#sr-iov-rdma","text":"\u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e SR-IOV CNI \u4f7f\u5f97 Pod \u9694\u79bb\u4f7f\u7528 RDMA \u8bbe\u5907\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\uff0c\u786e\u4fdd\u4e3b\u673a\u62e5\u6709 RDMA \u548c SR-IOV \u529f\u80fd\u7684\u7f51\u5361\uff0c\u4e14\u5b89\u88c5\u597d\u9a71\u52a8\uff0c\u786e\u4fdd RDMA \u529f\u80fd\u5de5\u4f5c\u6b63\u5e38\u3002 \u672c\u793a\u4f8b\u73af\u5883\u4e2d\uff0c\u5bbf\u4e3b\u673a\u4e0a\u5177\u5907 RoCE \u529f\u80fd\u7684 mellanox ConnectX 5 \u7f51\u5361\uff0c\u53ef\u6309\u7167 NVIDIA \u5b98\u65b9\u6307\u5bfc \u5b89\u88c5\u6700\u65b0\u7684 OFED \u9a71\u52a8\u3002 \u8981\u9694\u79bb\u4f7f\u7528 RDMA \u7f51\u5361\uff0c\u52a1\u5fc5\u6ee1\u8db3\u5982\u4e0b\u5176\u4e2d\u4e00\u4e2a\u6761\u4ef6\uff1a (1) \u5185\u6838\u7248\u672c\u8981\u6c42 5.3.0 \u6216\u66f4\u9ad8\u7248\u672c\uff0c\u5e76\u5728\u7cfb\u7edf\u4e2d\u52a0\u8f7d RDMA \u6a21\u5757\u3002rdma-core \u8f6f\u4ef6\u5305\u63d0\u4f9b\u4e86\u5728\u7cfb\u7edf\u542f\u52a8\u65f6\u81ea\u52a8\u52a0\u8f7d\u76f8\u5173\u6a21\u5757\u7684\u529f\u80fd\u3002 (2) Mellanox OFED \u8981\u6c42 4.7 \u6216\u66f4\u9ad8\u7248\u672c\u3002\u6b64\u65f6\u4e0d\u9700\u8981\u4f7f\u7528 5.3.0 \u6216\u66f4\u65b0\u7248\u672c\u7684\u5185\u6838\u3002 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u53ef\u67e5\u8be2\u5230 RDMA \u8bbe\u5907\uff1a ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 \u786e\u8ba4\u4e3b\u673a\u4e0a\u7684 RDMA \u5b50\u7cfb\u7edf\u5de5\u4f5c\u5728 exclusive \u6a21\u5f0f\u4e0b\uff0c\u5426\u5219\uff0c\u8bf7\u5207\u6362\u5230 exclusive \u6a21\u5f0f\u3002 # \u5207\u6362\u5230 exclusive \u6a21\u5f0f\uff0c\u91cd\u542f\u4e3b\u673a\u5931\u6548 ~# rdma system set netns exclusive # \u6301\u4e45\u5316\u914d\u7f6e ~# echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf ~# rdma system netns exclusive copy-on-fork on \u786e\u8ba4\u7f51\u5361\u5177\u5907 SR-IOV \u529f\u80fd\uff0c\u67e5\u770b\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\uff1a ~# cat /sys/class/net/ens6f0np0/device/sriov_totalvfs 127 \uff08\u53ef\u9009\uff09SR-IOV \u573a\u666f\u4e0b\uff0c\u5e94\u7528\u53ef\u4f7f NVIDIA \u7684 GPUDirect RMDA \u529f\u80fd\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u5b89\u88c5\u5185\u6838\u6a21\u5757\u3002 \u786e\u8ba4 RDMA \u7f51\u5361\u7684\u4fe1\u606f\uff0c\u7528\u4e8e\u540e\u7eed device plugin \u53d1\u73b0\u8bbe\u5907\u8d44\u6e90\u3002 \u672c\u6f14\u793a\u73af\u5883\uff0c\u8f93\u5165\u5982\u4e0b\uff0c\u7f51\u5361 vendors \u4e3a 15b3\uff0c\u7f51\u5361 deviceIDs \u4e3a 1017 ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] \u5b89\u88c5 Spiderpool helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set sriov.install=true \\ --set plugins.installRdmaCNI=true=true \\ --set multus.multusCNI.defaultCniCRName=\"sriov-rdma\" \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u5b8c\u6210 Spiderpool \u5b89\u88c5\u540e\uff0c\u53ef\u4ee5\u624b\u52a8\u7f16\u8f91 configmap spiderpool-rdma-shared-device-plugin \u6765\u91cd\u65b0\u914d\u7f6e RDMA shared device plugin \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m \u914d\u7f6e SR-IOV operator \u5982\u4e0b\u914d\u7f6e\uff0c\u4f7f\u5f97 SR-IOV operator \u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u521b\u5efa\u51fa VF\uff0c\u5e76\u4e0a\u62a5\u8d44\u6e90 cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policyrdma namespace: kube-system spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: mellanoxrdma priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" rootDevices: - 0000:af:00.0 vendor: \"15b3\" deviceType: netdevice isRdma: true EOF \u67e5\u770b node \u7684\u53ef\u7528\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e0a\u62a5\u7684 SR-IOV \u8bbe\u5907\u8d44\u6e90 ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"pods\": \"110\", \"spidernet.io/mellanoxrdma\": \"12\", ... } }, ... ] \u521b\u5efa SR-IOV \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/mellanoxrdma enableRdma: true ippools: ipv4: [\"v4-81\"] EOF \u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u7684\u914d\u7f6e\uff0c\u6765\u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528 ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/sriov-rdma\" RESOURCE=\"spidernet.io/mellanoxrdma\" NAME=rdma-sriov cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF \u5728\u8de8\u8282\u70b9\u7684 Pod \u4e4b\u95f4\uff0c\u786e\u8ba4 RDMA \u6536\u53d1\u6570\u636e\u6b63\u5e38 \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u4e00\u4e2a Pod \u542f\u52a8\u670d\u52a1\uff1a # \u53ea\u80fd\u770b\u5230\u5206\u914d\u7ed9 Pod \u7684\u4e00\u4e2a RDMA \u8bbe\u5907 ~# rdma link 7/1: mlx5_3/1: state ACTIVE physical_state LINK_UP netdev eth0 # \u542f\u52a8\u4e00\u4e2a RDMA \u670d\u52a1 ~# ib_read_lat \u5f00\u542f\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u53e6\u4e00\u4e2a Pod \u8bbf\u95ee\u670d\u52a1\uff1a # \u80fd\u770b\u5230\u5bbf\u4e3b\u673a\u4e0a\u7684\u6240\u6709 RDMA \u7f51\u5361 ~# rdma link 10/1: mlx5_5/1: state ACTIVE physical_state LINK_UP netdev eth0 # \u8bbf\u95ee\u5bf9\u65b9 Pod \u7684\u670d\u52a1 ~# ib_read_lat 172.81.0.118 libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_4'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_2'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_0'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_3'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_1'. --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_5 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 2 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0b69 PSN 0xd476c2 OUT 0x10 RKey 0x006f00 VAddr 0x00000001f91000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:105 remote address: LID 0000 QPN 0x0d69 PSN 0xbe5c89 OUT 0x10 RKey 0x004f00 VAddr 0x0000000160d000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:118 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1338.151000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 2881.668000. CPU Frequency is not max. 2 1000 6.66 20.37 6.74 6.82 0.78 7.15 20.37 ---------------------------------------------------------------------------------------","title":"\u57fa\u4e8e SR-IOV \u9694\u79bb\u4f7f\u7528 RDMA \u7f51\u5361"},{"location":"usage/rdma/","text":"RDMA English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool employs macvlan, ipvlan, and SR-IOV CNI to expose RDMA network cards on the host machine for Pod. This page provides an overview of how to utilize RDMA network cards in Spiderpool. Features RDMA devices' network namespaces have two modes: shared and exclusive. Containers can either share or exclusively access RDMA network cards. In Kubernetes, shared cards can be utilized with macvlan or ipvlan CNI, while the exclusive one can be used with SR-IOV CNI. In shared mode, Spiderpool leverages macvlan or ipvlan CNI to expose RoCE network cards on the host machine for Pod. The RDMA shared device plugin is employed for exposing RDMA card resources and scheduling Pods. In exclusive mode, Spiderpool utilizes SR-IOV CNI to expose RDMA cards on the host machine for Pods, providing access to RDMA resources. RDMA CNI is used to ensure isolation of RDMA devices. Shared usage of RoCE-capable NIC with macvlan or ipvlan The following steps demonstrate how to enable shared usage of RDMA devices by Pods in a cluster with two nodes via macvlan CNI: Ensure that the host machine has an RDMA card installed and the driver is properly installed, ensuring proper RDMA functioning. In our demo environment, the host machine is equipped with a Mellanox ConnectX-5 NIC with RoCE capabilities. Follow the official NVIDIA guide to install the latest OFED driver. To confirm the presence of RDMA devices, use the following command: ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 Make sure that the RDMA subsystem on the host is operating in shared mode. If not, switch to shared mode. ~# rdma system netns shared copy-on-fork on # switch to shared mode ~# rdma system set netns shared Verify the details of the RDMA card for subsequent device resource discovery by the device plugin. Enter the following command with NIC vendors being 15b3 and its deviceIDs being 1017: ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] Install Spiderpool and configure sriov-network-operator: helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName=\"macvlan-ens6f0np0\" \\ --set rdma.rdmaSharedDevicePlugin.install=true \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourcePrefix=\"spidernet.io\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourceName=\"hca_shared_devices\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.rdmaHcaMax=500 \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.vendors=\"15b3\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.deviceIDs=\"1017\" If Macvlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install Macvlan in your cluster. If you are a user from China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. After completing the installation of Spiderpool, you can manually edit the spiderpool-rdma-shared-device-plugin configmap to reconfigure the RDMA shared device plugin. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Once the installation is complete, the following components will be installed: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m spiderpool-rdma-shared-device-plugin-dr7w8 1/1 Running 0 1m spiderpool-rdma-shared-device-plugin-zj65g 1/1 Running 0 1m View the available resources on a node, including the reported RDMA device resources: ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"memory\": \"263518036Ki\", \"pods\": \"110\", \"spidernet.io/hca_shared_devices\": \"500\", ... } }, ... ] If the reported resource count is 0, it may be due to the following reasons: (1) Verify that the vendors and deviceID in the spiderpool-rdma-shared-device-plugin configmap match the actual values. (2) Check the logs of the rdma-shared-device-plugin. If you encounter errors related to RDMA NIC support, try installing apt-get install rdma-core or dnf install rdma-core on the host machine. error creating new device: \"missing RDMA device spec for device 0000:04:00.0, RDMA device \\\"issm\\\" not found\" Create macvlan-related multus configurations using an RDMA card as the master node and set up the corresponding ippool resources: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens6f0np0 namespace: kube-system spec: cniType: macvlan macvlan: master: - \"ens6f0np0\" ippools: ipv4: [\"v4-81\"] EOF Following the configurations from the previous step, create a DaemonSet application that spans across nodes: ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/macvlan-ens6f0np0\" RESOURCE=\"spidernet.io/hca_shared_devices\" NAME=rdma-macvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF Verify that RDMA data transmission is working correctly between the Pods across nodes. Open a terminal and access one Pod to launch a service: # You are able to see all the RDMA cards on the host machine ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # Start an RDMA service ~# ib_read_lat Open a terminal and access another Pod to launch a service: # You are able to see all the RDMA cards on the host machine ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # Access the service running in the other Pod ~# ib_read_lat 172.81.0.120 --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 12 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0107 PSN 0x79dd10 OUT 0x10 RKey 0x1fddbc VAddr 0x000000023bd000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:119 remote address: LID 0000 QPN 0x0107 PSN 0x40001a OUT 0x10 RKey 0x1fddbc VAddr 0x00000000bf9000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:120 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1040.353000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 1849.351000. CPU Frequency is not max. 2 1000 6.88 16.81 7.04 7.06 0.31 7.38 16.81 --------------------------------------------------------------------------------------- Isolated usage of RoCE-capable NIC with SR-IOV The following steps demonstrate how to enable isolated usage of RDMA devices by Pods in a cluster with two nodes via SR-IOV CNI: Ensure that the host machine has an RDMA and SR-IOV enabled card and the driver is properly installed, ensuring proper RDMA functioning. In our demo environment, the host machine is equipped with a Mellanox ConnectX-5 NIC with RoCE capabilities. Follow the official NVIDIA guide to install the latest OFED driver. To isolate the usage of an RDMA network card, ensure that at least one of the following conditions is met: (1) Kernel based on 5.3.0 or newer, RDMA modules loaded in the system. rdma-core package provides means to automatically load relevant modules on system start (2) Mellanox OFED version 4.7 or newer is required. In this case it is not required to use a Kernel based on 5.3.0 or newer. To confirm the presence of RDMA devices, use the following command: ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 Make sure that the RDMA subsystem on the host is operating in exclusive mode. If not, switch to exclusive mode. # switch to exclusive mode and fail to restart the host ~# rdma system set netns exclusive # apply persistent settings: ~# echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf ~# rdma system netns exclusive copy-on-fork on To verify if the network card has SR-IOV functionality, check the maximum number of supported VFs: ~# cat /sys/class/net/ens6f0np0/device/sriov_totalvfs 127 (Optional) in an SR-IOV scenario, applications can enable NVIDIA's GPUDirect RDMA feature. For instructions on installing the kernel module, please refer to the official documentation . Verify the details of the RDMA card for subsequent device resource discovery by the device plugin. Enter the following command with NIC vendors being 15b3 and its deviceIDs being 1017: ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] Install Spiderpool helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set sriov.install=true \\ --set plugins.installRdmaCNI=true If you are a user from China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. After completing the installation of Spiderpool, you can manually edit the spiderpool-rdma-shared-device-plugin configmap to reconfigure the RDMA shared device plugin. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Once the installation is complete, the following components will be installed: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m Configure SR-IOV operator With the following configuration, the SR-IOV operator can create VFs on the host and report the resources: cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policyrdma namespace: kube-system spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: mellanoxrdma priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" rootDevices: - 0000:af:00.0 vendor: \"15b3\" deviceType: netdevice isRdma: true EOF Verify the available resources on the node, including the reported SR-IOV device resources: ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"pods\": \"110\", \"spidernet.io/mellanoxrdma\": \"12\", ... } }, ... ] Create multus configurations related to SR-IOV and create corresponding ippool resources. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/mellanoxrdma enableRdma: true ippools: ipv4: [\"v4-81\"] EOF Following the configurations from the previous step, create a DaemonSet application that spans across nodes: ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/sriov-rdma\" RESOURCE=\"spidernet.io/mellanoxrdma\" NAME=rdma-sriov cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF Verify that RDMA data transmission is working correctly between the Pods across nodes. Open a terminal and access one Pod to launch a service: # Only one RDMA device allocated to the Pod can be found ~# rdma link 7/1: mlx5_3/1: state ACTIVE physical_state LINK_UP netdev eth0 # launch an RDMA service ~# ib_read_lat Open a terminal and access another Pod to launch a service: # You are able to see all the RDMA cards on the host machine ~# rdma link 10/1: mlx5_5/1: state ACTIVE physical_state LINK_UP netdev eth0 # Access the service running in the other Pod ~# ib_read_lat 172.81.0.118 libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_4'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_2'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_0'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_3'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_1'. --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_5 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 2 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0b69 PSN 0xd476c2 OUT 0x10 RKey 0x006f00 VAddr 0x00000001f91000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:105 remote address: LID 0000 QPN 0x0d69 PSN 0xbe5c89 OUT 0x10 RKey 0x004f00 VAddr 0x0000000160d000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:118 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1338.151000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 2881.668000. CPU Frequency is not max. 2 1000 6.66 20.37 6.74 6.82 0.78 7.15 20.37 ---------------------------------------------------------------------------------------","title":"RDMA"},{"location":"usage/rdma/#rdma","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"RDMA"},{"location":"usage/rdma/#introduction","text":"Spiderpool employs macvlan, ipvlan, and SR-IOV CNI to expose RDMA network cards on the host machine for Pod. This page provides an overview of how to utilize RDMA network cards in Spiderpool.","title":"Introduction"},{"location":"usage/rdma/#features","text":"RDMA devices' network namespaces have two modes: shared and exclusive. Containers can either share or exclusively access RDMA network cards. In Kubernetes, shared cards can be utilized with macvlan or ipvlan CNI, while the exclusive one can be used with SR-IOV CNI. In shared mode, Spiderpool leverages macvlan or ipvlan CNI to expose RoCE network cards on the host machine for Pod. The RDMA shared device plugin is employed for exposing RDMA card resources and scheduling Pods. In exclusive mode, Spiderpool utilizes SR-IOV CNI to expose RDMA cards on the host machine for Pods, providing access to RDMA resources. RDMA CNI is used to ensure isolation of RDMA devices.","title":"Features"},{"location":"usage/rdma/#shared-usage-of-roce-capable-nic-with-macvlan-or-ipvlan","text":"The following steps demonstrate how to enable shared usage of RDMA devices by Pods in a cluster with two nodes via macvlan CNI: Ensure that the host machine has an RDMA card installed and the driver is properly installed, ensuring proper RDMA functioning. In our demo environment, the host machine is equipped with a Mellanox ConnectX-5 NIC with RoCE capabilities. Follow the official NVIDIA guide to install the latest OFED driver. To confirm the presence of RDMA devices, use the following command: ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 Make sure that the RDMA subsystem on the host is operating in shared mode. If not, switch to shared mode. ~# rdma system netns shared copy-on-fork on # switch to shared mode ~# rdma system set netns shared Verify the details of the RDMA card for subsequent device resource discovery by the device plugin. Enter the following command with NIC vendors being 15b3 and its deviceIDs being 1017: ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] Install Spiderpool and configure sriov-network-operator: helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName=\"macvlan-ens6f0np0\" \\ --set rdma.rdmaSharedDevicePlugin.install=true \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourcePrefix=\"spidernet.io\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.resourceName=\"hca_shared_devices\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.rdmaHcaMax=500 \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.vendors=\"15b3\" \\ --set rdma.rdmaSharedDevicePlugin.deviceConfig.deviceIDs=\"1017\" If Macvlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install Macvlan in your cluster. If you are a user from China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. After completing the installation of Spiderpool, you can manually edit the spiderpool-rdma-shared-device-plugin configmap to reconfigure the RDMA shared device plugin. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Once the installation is complete, the following components will be installed: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m spiderpool-rdma-shared-device-plugin-dr7w8 1/1 Running 0 1m spiderpool-rdma-shared-device-plugin-zj65g 1/1 Running 0 1m View the available resources on a node, including the reported RDMA device resources: ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"memory\": \"263518036Ki\", \"pods\": \"110\", \"spidernet.io/hca_shared_devices\": \"500\", ... } }, ... ] If the reported resource count is 0, it may be due to the following reasons: (1) Verify that the vendors and deviceID in the spiderpool-rdma-shared-device-plugin configmap match the actual values. (2) Check the logs of the rdma-shared-device-plugin. If you encounter errors related to RDMA NIC support, try installing apt-get install rdma-core or dnf install rdma-core on the host machine. error creating new device: \"missing RDMA device spec for device 0000:04:00.0, RDMA device \\\"issm\\\" not found\" Create macvlan-related multus configurations using an RDMA card as the master node and set up the corresponding ippool resources: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens6f0np0 namespace: kube-system spec: cniType: macvlan macvlan: master: - \"ens6f0np0\" ippools: ipv4: [\"v4-81\"] EOF Following the configurations from the previous step, create a DaemonSet application that spans across nodes: ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/macvlan-ens6f0np0\" RESOURCE=\"spidernet.io/hca_shared_devices\" NAME=rdma-macvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF Verify that RDMA data transmission is working correctly between the Pods across nodes. Open a terminal and access one Pod to launch a service: # You are able to see all the RDMA cards on the host machine ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # Start an RDMA service ~# ib_read_lat Open a terminal and access another Pod to launch a service: # You are able to see all the RDMA cards on the host machine ~# rdma link 0/1: mlx5_0/1: state ACTIVE physical_state LINK_UP 1/1: mlx5_1/1: state ACTIVE physical_state LINK_UP # Access the service running in the other Pod ~# ib_read_lat 172.81.0.120 --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 12 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0107 PSN 0x79dd10 OUT 0x10 RKey 0x1fddbc VAddr 0x000000023bd000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:119 remote address: LID 0000 QPN 0x0107 PSN 0x40001a OUT 0x10 RKey 0x1fddbc VAddr 0x00000000bf9000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:120 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1040.353000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 1849.351000. CPU Frequency is not max. 2 1000 6.88 16.81 7.04 7.06 0.31 7.38 16.81 ---------------------------------------------------------------------------------------","title":"Shared usage of RoCE-capable NIC with macvlan or ipvlan"},{"location":"usage/rdma/#isolated-usage-of-roce-capable-nic-with-sr-iov","text":"The following steps demonstrate how to enable isolated usage of RDMA devices by Pods in a cluster with two nodes via SR-IOV CNI: Ensure that the host machine has an RDMA and SR-IOV enabled card and the driver is properly installed, ensuring proper RDMA functioning. In our demo environment, the host machine is equipped with a Mellanox ConnectX-5 NIC with RoCE capabilities. Follow the official NVIDIA guide to install the latest OFED driver. To isolate the usage of an RDMA network card, ensure that at least one of the following conditions is met: (1) Kernel based on 5.3.0 or newer, RDMA modules loaded in the system. rdma-core package provides means to automatically load relevant modules on system start (2) Mellanox OFED version 4.7 or newer is required. In this case it is not required to use a Kernel based on 5.3.0 or newer. To confirm the presence of RDMA devices, use the following command: ~# rdma link show link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev ens6f0np0 link mlx5_1/1 state ACTIVE physical_state LINK_UP netdev ens6f1np1 Make sure that the RDMA subsystem on the host is operating in exclusive mode. If not, switch to exclusive mode. # switch to exclusive mode and fail to restart the host ~# rdma system set netns exclusive # apply persistent settings: ~# echo \"options ib_core netns_mode=0\" >> /etc/modprobe.d/ib_core.conf ~# rdma system netns exclusive copy-on-fork on To verify if the network card has SR-IOV functionality, check the maximum number of supported VFs: ~# cat /sys/class/net/ens6f0np0/device/sriov_totalvfs 127 (Optional) in an SR-IOV scenario, applications can enable NVIDIA's GPUDirect RDMA feature. For instructions on installing the kernel module, please refer to the official documentation . Verify the details of the RDMA card for subsequent device resource discovery by the device plugin. Enter the following command with NIC vendors being 15b3 and its deviceIDs being 1017: ~# lspci -nn | grep Ethernet af:00.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] af:00.1 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017] Install Spiderpool helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set sriov.install=true \\ --set plugins.installRdmaCNI=true If you are a user from China, you can specify the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. After completing the installation of Spiderpool, you can manually edit the spiderpool-rdma-shared-device-plugin configmap to reconfigure the RDMA shared device plugin. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Once the installation is complete, the following components will be installed: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1/1 Running 0 1m spiderpool-agent-h92bv 1/1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1/1 Running 0 1m spiderpool-init 0/1 Completed 0 1m Configure SR-IOV operator With the following configuration, the SR-IOV operator can create VFs on the host and report the resources: cat <<EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policyrdma namespace: kube-system spec: nodeSelector: kubernetes.io/os: \"linux\" resourceName: mellanoxrdma priority: 99 numVfs: 12 nicSelector: deviceID: \"1017\" rootDevices: - 0000:af:00.0 vendor: \"15b3\" deviceType: netdevice isRdma: true EOF Verify the available resources on the node, including the reported SR-IOV device resources: ~# kubectl get no -o json | jq -r '[.items[] | {name:.metadata.name, allocable:.status.allocatable}]' [ { \"name\": \"10-20-1-10\", \"allocable\": { \"cpu\": \"40\", \"pods\": \"110\", \"spidernet.io/mellanoxrdma\": \"12\", ... } }, ... ] Create multus configurations related to SR-IOV and create corresponding ippool resources. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-81 spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/mellanoxrdma enableRdma: true ippools: ipv4: [\"v4-81\"] EOF Following the configurations from the previous step, create a DaemonSet application that spans across nodes: ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network: kube-system/sriov-rdma\" RESOURCE=\"spidernet.io/mellanoxrdma\" NAME=rdma-sriov cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - image: docker.io/mellanox/rping-test imagePullPolicy: IfNotPresent name: mofed-test securityContext: capabilities: add: [ \"IPC_LOCK\" ] resources: limits: ${RESOURCE}: 1 command: - sh - -c - | ls -l /dev/infiniband /sys/class/net sleep 1000000 EOF Verify that RDMA data transmission is working correctly between the Pods across nodes. Open a terminal and access one Pod to launch a service: # Only one RDMA device allocated to the Pod can be found ~# rdma link 7/1: mlx5_3/1: state ACTIVE physical_state LINK_UP netdev eth0 # launch an RDMA service ~# ib_read_lat Open a terminal and access another Pod to launch a service: # You are able to see all the RDMA cards on the host machine ~# rdma link 10/1: mlx5_5/1: state ACTIVE physical_state LINK_UP netdev eth0 # Access the service running in the other Pod ~# ib_read_lat 172.81.0.118 libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_4'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_2'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_0'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_3'. libibverbs: Warning: couldn't stat '/sys/class/infiniband/mlx5_1'. --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_5 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF TX depth : 1 Mtu : 1024[B] Link type : Ethernet GID index : 2 Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0000 QPN 0x0b69 PSN 0xd476c2 OUT 0x10 RKey 0x006f00 VAddr 0x00000001f91000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:105 remote address: LID 0000 QPN 0x0d69 PSN 0xbe5c89 OUT 0x10 RKey 0x004f00 VAddr 0x0000000160d000 GID: 00:00:00:00:00:00:00:00:00:00:255:255:172:81:00:118 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 2200.000000 != 1338.151000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2200.000000 != 2881.668000. CPU Frequency is not max. 2 1000 6.66 20.37 6.74 6.82 0.78 7.15 20.37 ---------------------------------------------------------------------------------------","title":"Isolated usage of RoCE-capable NIC with SR-IOV"},{"location":"usage/readme-zh_CN/","text":"\u4f7f\u7528 English | \u7b80\u4f53\u4e2d\u6587 \u5b89\u88c5 Spiderpool \u57fa\u4e8e Underlay CNI \u5b89\u88c5 Spiderpool \u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u591a\u4e2a Underlay CNI \u7684\u7f51\u7edc\uff0c\u4ece\u800c\u8ba9 Pod \u5177\u5907\u63a5\u5165 underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 \u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e kind \u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e SR-IOV CNI \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e ovs \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e weave CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e underlay CNI \u4e3a\u5e94\u7528\u63d0\u4f9b RDMA \u901a\u4fe1\u8bbe\u5907 \u57fa\u4e8e Overlay CNI \u548c Underlay CNI \u5b89\u88c5 Spiderpool \u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u540c\u65f6\u63a5\u5165\u4e00\u5f20 Overlay CNI \u7f51\u5361\u548c\u591a\u4e2a Underlay CNI \u7684\u7f51\u5361\uff0c\u4ece\u800c\u8ba9 Pod \u540c\u65f6\u5177\u5907\u63a5\u5165 overlay \u548c underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 overlay CNI \u548c underlay CNI \u534f\u540c \u3002\u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e kind \u96c6\u7fa4\u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e Cilium \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc \u5728\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883\u4e0a\u57fa\u4e8e Underlay CNI \u5b89\u88c5 Spiderpool \u521b\u5efa\u96c6\u7fa4\uff1a\u5728\u963f\u91cc\u4e91\u4e0a\u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1aVMware vsphere \u521b\u5efa\u96c6\u7fa4\uff1a\u5728 AWS \u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u5728 VMware vSphere \u5e73\u53f0\u4e0a\u8fd0\u884c ipvlan CNI\uff0c\u65e0\u9700\u6253\u5f00 vSwitch \u7684 \"\u6df7\u6742\"\u8f6c\u53d1\u6a21\u5f0f \uff0c\u4ece\u800c\u786e\u4fdd vSphere \u5e73\u53f0\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53c2\u8003 \u5b89\u88c5 TLS \u8bc1\u4e66 \u5b89\u88c5 spiderpool \u65f6\uff0c\u53ef\u6307\u5b9a TLS \u8bc1\u4e66\u7684\u751f\u6210\u65b9\u5f0f\uff0c\u53ef\u53c2\u8003 \u6587\u7ae0 \u5378\u8f7d Spiderpool \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5378\u8f7d Spiderpool \uff1a helm uninstall spiderpool -n kube-system Spiderpool \u7684\u67d0\u4e9b CR \u8d44\u6e90\u4e2d\u5b58\u5728 finalizers \uff0c\u5bfc\u81f4 helm uninstall \u547d\u4ee4\u65e0\u6cd5\u6e05\u7406\u5e72\u51c0\u3002 \u53ef\u83b7\u53d6\u5982\u4e0b\u6e05\u7406\u811a\u672c\u6765\u5b8c\u6210\u6e05\u7406\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh \u5347\u7ea7 Spiderpool \u53ef\u53c2\u8003 \u5347\u7ea7 \u4f7f\u7528 Spiderpool IPAM \u529f\u80fd \u5e94\u7528\u53ef\u4ee5\u5171\u4eab\u4e00\u4e2a IP \u6c60\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u53ef\u4ee5\u72ec\u4eab\u4e00\u4e2a IP \u5730\u5740\u6c60\uff0c\u5e76\u56fa\u5b9a\u6240\u6709 Pod \u7684 IP \u4f7f\u7528\u8303\u56f4\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u6709\u72b6\u6001\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u6bcf\u4e00\u4e2a Pod \u6301\u4e45\u5316\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\uff0c\u540c\u65f6\u5728\u6269\u7f29\u65f6\u53ef\u63a7\u5236\u6240\u6709 Pod \u6240\u4f7f\u7528\u7684 IP \u8303\u56f4\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301\u4e3a kubevirt \u63d0\u4f9b underlay \u7f51\u7edc\uff0c\u56fa\u5b9a\u865a\u62df\u673a\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u5bf9\u4e8e\u4e00\u4e2a\u8de8\u5b50\u7f51\u90e8\u7f72\u7684\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u5176\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 Subnet \u529f\u80fd\uff0c\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7ba1\u7406\u5458\u7684\u804c\u8d23\u5206\u79bb\uff0c \u53e6\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u4e3a\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\u81ea\u52a8\u7ba1\u7406 IP \u6c60\uff0c\u5305\u62ec\u81ea\u52a8\u521b\u5efa\u3001\u6269\u7f29\u5bb9 IP\u3001\u5220\u9664 \u56fa\u5b9a IP \u6c60\uff0c \u8fd9\u80fd\u591f\u51cf\u5c11\u5927\u91cf\u7684\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8be5\u529f\u80fd\u9664\u4e86\u652f\u6301 K8S \u539f\u751f\u7684\u5e94\u7528\u63a7\u5236\u5668\uff0c\u540c\u65f6\u652f\u6301\u57fa\u4e8e operator \u5b9e\u73b0\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u63a7\u5236\u5668\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u8bbe\u7f6e\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\uff0c\u4e5f\u53ef\u79df\u6237\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\u3002\u540c\u65f6\uff0cIP \u6c60\u65e2\u53ef\u4ee5\u88ab\u6574\u4e2a\u96c6\u7fa4\u5171\u4eab\uff0c \u4e5f\u53ef\u88ab\u9650\u5b9a\u4e3a\u88ab\u4e00\u4e2a\u79df\u6237\u4f7f\u7528\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u57fa\u4e8e\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u6ee1\u8db3\u6bcf\u4e2a\u8282\u70b9\u7cbe\u7ec6\u5316\u7684\u5b50\u7f51\u89c4\u5212\u9700\u6c42\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u53ef\u4ee5\u901a\u8fc7 IP \u6c60\u548c Pod annotaiton \u7b49\u591a\u79cd\u65b9\u5f0f\u5b9a\u5236\u81ea\u5b9a\u4e49\u8def\u7531\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e94\u7528\u53ef\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\uff0c\u5b9e\u73b0 IP \u8d44\u6e90\u7684\u5907\u7528\u6548\u679c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8bbe\u7f6e\u5168\u5c40\u7684\u9884\u7559 IP\uff0c\u8ba9 IPAM \u4e0d\u5206\u914d\u51fa\u8fd9\u4e9b IP \u5730\u5740\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u5df2\u7528 IP \u51b2\u7a81\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u62a5\u544a \u3002 \u5408\u7406\u7684 IP \u56de\u6536\u673a\u5236\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6216\u5e94\u7528\u5728\u6545\u969c\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u53ca\u65f6\u5206\u914d\u5230 IP \u5730\u5740\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u591a\u7f51\u5361\u529f\u80fd \u652f\u6301\u4e3a Pod \u591a\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff1b\u5e2e\u52a9\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u534f\u8c03\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u4e22\u5305\uff1b\u652f\u6301\u5b9a\u5236\u54ea\u5f20\u7f51\u5361\u7684\u7f51\u5173\u4f5c\u4e3a\u7f3a\u7701\u8def\u7531\u3002 \u5bf9\u4e8e Pod \u5177\u5907\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e Pod \u5177\u5907\u4e00\u4e2a overlay \u7f51\u5361\u548c\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8fde\u901a\u6027\u529f\u80fd \u652f\u6301 RDMA \u7f51\u5361\u7684 shared \u548c exclusive \u6a21\u5f0f\uff0c\u80fd\u57fa\u4e8e maclan\u3001ipvlan \u548c SR-IOV CNI \u4e3a\u5e94\u7528\u63d0\u4f9b RDMA \u901a\u4fe1\u8bbe\u5907\u3002\u5177\u4f53\u53ef\u53c2\u8003 \u4f8b\u5b50 coordinator \u63d2\u4ef6\u80fd\u591f\u4f9d\u636e\u7f51\u5361\u7684 IP \u5730\u5740\u6765\u91cd\u65b0\u914d\u7f6e MAC \u5730\u5740\uff0c\u4f7f\u4e24\u8005\u4e00\u4e00\u5bf9\u5e94\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u907f\u514d\u7f51\u7edc\u4e2d\u7684\u4ea4\u6362\u8def\u7531\u8bbe\u5907\u66f4\u65b0 ARP \u8f6c\u53d1\u89c4\u5219\uff0c\u907f\u514d\u4e22\u5305\u3002\u53ef\u53c2\u8003 \u6587\u7ae0 \u3002 \u5bf9 Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\uff0c \u63d0\u4f9b\u4e86\u57fa\u4e8e kube-proxy \u548c eBPF kube-proxy replacement \u8bbf\u95ee ClusterIP \u8bbf\u95ee\uff0c\u5e76\u8054\u901a Pod \u548c\u5bbf\u4e3b\u673a\u901a\u4fe1\uff0c\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u67e5\u95ee\u9898\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u80fd\u591f\u5e2e\u52a9\u5b9e\u65bd IP \u5730\u5740\u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u8bc1 Pod \u901a\u4fe1\u6b63\u5e38\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8fd0\u7ef4\u7ba1\u7406\u529f\u80fd \u5728 Pod \u542f\u52a8\u65f6\uff0c\u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u52a8\u6001\u521b\u5efa BOND \u63a5\u53e3\u548c VLAN \u5b50\u63a5\u53e3\uff0c\u4ee5\u5e2e\u52a9 Macvlan CNI \u548c ipvlan CNI \u51c6\u5907 master \u63a5\u53e3\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u4ee5\u6700\u4f73\u5b9e\u8df5\u7684 CNI \u914d\u7f6e\u6765\u4fbf\u6377\u5730\u751f\u6210 Multus NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5e76\u4e14\u4fdd\u8bc1\u5176\u6b63\u786e\u7684 JSON \u683c\u5f0f\u6765\u63d0\u9ad8\u4f7f\u7528\u4f53\u9a8c\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5176\u5b83\u529f\u80fd \u6307\u6807 \u652f\u6301 AMD64 \u548c ARM64 \u6240\u6709\u7684\u529f\u80fd\u90fd\u80fd\u591f\u5728 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u4e0b\u5de5\u4f5c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u4f7f\u7528"},{"location":"usage/readme-zh_CN/#_1","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"\u4f7f\u7528"},{"location":"usage/readme-zh_CN/#spiderpool","text":"","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#underlay-cni-spiderpool","text":"\u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u63a5\u5165\u4e00\u4e2a\u6216\u591a\u4e2a Underlay CNI \u7684\u7f51\u7edc\uff0c\u4ece\u800c\u8ba9 Pod \u5177\u5907\u63a5\u5165 underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 \u4e00\u4e2a\u6216\u591a\u4e2a underlay CNI \u534f\u540c \u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e kind \u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e SR-IOV CNI \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e ovs \u7f51\u7edc\u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e weave CNI \u63d0\u4f9b\u56fa\u5b9a IP \u7684\u96c6\u7fa4 \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e underlay CNI \u4e3a\u5e94\u7528\u63d0\u4f9b RDMA \u901a\u4fe1\u8bbe\u5907","title":"\u57fa\u4e8e Underlay CNI \u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#overlay-cni-underlay-cni-spiderpool","text":"\u96c6\u7fa4\u7f51\u7edc\u53ef\u4ee5\u4e3a Pod \u540c\u65f6\u63a5\u5165\u4e00\u5f20 Overlay CNI \u7f51\u5361\u548c\u591a\u4e2a Underlay CNI \u7684\u7f51\u5361\uff0c\u4ece\u800c\u8ba9 Pod \u540c\u65f6\u5177\u5907\u63a5\u5165 overlay \u548c underlay \u7f51\u7edc\u7684\u80fd\u529b\uff0c\u5177\u4f53\u53ef\u53c2\u8003 overlay CNI \u548c underlay CNI \u534f\u540c \u3002\u4ee5\u4e0b\u662f\u5b89\u88c5\u793a\u4f8b\uff1a \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e kind \u96c6\u7fa4\u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e calico \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1a\u57fa\u4e8e Cilium \u548c macvlan CNI \u7684\u53cc\u7f51\u7edc","title":"\u57fa\u4e8e Overlay CNI \u548c Underlay CNI \u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#underlay-cni-spiderpool_1","text":"\u521b\u5efa\u96c6\u7fa4\uff1a\u5728\u963f\u91cc\u4e91\u4e0a\u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u521b\u5efa\u96c6\u7fa4\uff1aVMware vsphere \u521b\u5efa\u96c6\u7fa4\uff1a\u5728 AWS \u57fa\u4e8e ipvlan \u7684\u7f51\u7edc \u5728 VMware vSphere \u5e73\u53f0\u4e0a\u8fd0\u884c ipvlan CNI\uff0c\u65e0\u9700\u6253\u5f00 vSwitch \u7684 \"\u6df7\u6742\"\u8f6c\u53d1\u6a21\u5f0f \uff0c\u4ece\u800c\u786e\u4fdd vSphere \u5e73\u53f0\u7684\u8f6c\u53d1\u6027\u80fd\u3002\u53c2\u8003 \u5b89\u88c5","title":"\u5728\u865a\u62df\u673a\u548c\u516c\u6709\u4e91\u73af\u5883\u4e0a\u57fa\u4e8e Underlay CNI \u5b89\u88c5 Spiderpool"},{"location":"usage/readme-zh_CN/#tls","text":"\u5b89\u88c5 spiderpool \u65f6\uff0c\u53ef\u6307\u5b9a TLS \u8bc1\u4e66\u7684\u751f\u6210\u65b9\u5f0f\uff0c\u53ef\u53c2\u8003 \u6587\u7ae0","title":"TLS \u8bc1\u4e66"},{"location":"usage/readme-zh_CN/#spiderpool_1","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5378\u8f7d Spiderpool \uff1a helm uninstall spiderpool -n kube-system Spiderpool \u7684\u67d0\u4e9b CR \u8d44\u6e90\u4e2d\u5b58\u5728 finalizers \uff0c\u5bfc\u81f4 helm uninstall \u547d\u4ee4\u65e0\u6cd5\u6e05\u7406\u5e72\u51c0\u3002 \u53ef\u83b7\u53d6\u5982\u4e0b\u6e05\u7406\u811a\u672c\u6765\u5b8c\u6210\u6e05\u7406\uff0c\u4ee5\u786e\u4fdd\u4e0b\u6b21\u90e8\u7f72 Spiderpool \u65f6\u4e0d\u4f1a\u51fa\u73b0\u610f\u5916\u9519\u8bef\u3002 wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"\u5378\u8f7d Spiderpool"},{"location":"usage/readme-zh_CN/#spiderpool_2","text":"\u53ef\u53c2\u8003 \u5347\u7ea7","title":"\u5347\u7ea7 Spiderpool"},{"location":"usage/readme-zh_CN/#spiderpool_3","text":"","title":"\u4f7f\u7528 Spiderpool"},{"location":"usage/readme-zh_CN/#ipam","text":"\u5e94\u7528\u53ef\u4ee5\u5171\u4eab\u4e00\u4e2a IP \u6c60\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u65e0\u72b6\u6001\u5e94\u7528\uff0c\u53ef\u4ee5\u72ec\u4eab\u4e00\u4e2a IP \u5730\u5740\u6c60\uff0c\u5e76\u56fa\u5b9a\u6240\u6709 Pod \u7684 IP \u4f7f\u7528\u8303\u56f4\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e\u6709\u72b6\u6001\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u6bcf\u4e00\u4e2a Pod \u6301\u4e45\u5316\u5206\u914d\u56fa\u5b9a IP \u5730\u5740\uff0c\u540c\u65f6\u5728\u6269\u7f29\u65f6\u53ef\u63a7\u5236\u6240\u6709 Pod \u6240\u4f7f\u7528\u7684 IP \u8303\u56f4\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u652f\u6301\u4e3a kubevirt \u63d0\u4f9b underlay \u7f51\u7edc\uff0c\u56fa\u5b9a\u865a\u62df\u673a\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u5bf9\u4e8e\u4e00\u4e2a\u8de8\u5b50\u7f51\u90e8\u7f72\u7684\u5e94\u7528\uff0c\u652f\u6301\u4e3a\u5176\u4e0d\u540c\u526f\u672c\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 Subnet \u529f\u80fd\uff0c\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7ba1\u7406\u5458\u7684\u804c\u8d23\u5206\u79bb\uff0c \u53e6\u4e00\u65b9\u9762\uff0c\u80fd\u591f\u4e3a\u6709\u56fa\u5b9a IP \u9700\u6c42\u7684\u5e94\u7528\u81ea\u52a8\u7ba1\u7406 IP \u6c60\uff0c\u5305\u62ec\u81ea\u52a8\u521b\u5efa\u3001\u6269\u7f29\u5bb9 IP\u3001\u5220\u9664 \u56fa\u5b9a IP \u6c60\uff0c \u8fd9\u80fd\u591f\u51cf\u5c11\u5927\u91cf\u7684\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8be5\u529f\u80fd\u9664\u4e86\u652f\u6301 K8S \u539f\u751f\u7684\u5e94\u7528\u63a7\u5236\u5668\uff0c\u540c\u65f6\u652f\u6301\u57fa\u4e8e operator \u5b9e\u73b0\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u63a7\u5236\u5668\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u53ef\u4ee5\u8bbe\u7f6e\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\uff0c\u4e5f\u53ef\u79df\u6237\u7ea7\u522b\u7684\u9ed8\u8ba4 IP \u6c60\u3002\u540c\u65f6\uff0cIP \u6c60\u65e2\u53ef\u4ee5\u88ab\u6574\u4e2a\u96c6\u7fa4\u5171\u4eab\uff0c \u4e5f\u53ef\u88ab\u9650\u5b9a\u4e3a\u88ab\u4e00\u4e2a\u79df\u6237\u4f7f\u7528\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u57fa\u4e8e\u8282\u70b9\u62d3\u6251\u7684 IP \u6c60\u529f\u80fd\uff0c\u6ee1\u8db3\u6bcf\u4e2a\u8282\u70b9\u7cbe\u7ec6\u5316\u7684\u5b50\u7f51\u89c4\u5212\u9700\u6c42\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u53ef\u4ee5\u901a\u8fc7 IP \u6c60\u548c Pod annotaiton \u7b49\u591a\u79cd\u65b9\u5f0f\u5b9a\u5236\u81ea\u5b9a\u4e49\u8def\u7531\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5e94\u7528\u53ef\u8bbe\u7f6e\u591a\u4e2a IP \u6c60\uff0c\u5b9e\u73b0 IP \u8d44\u6e90\u7684\u5907\u7528\u6548\u679c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u8bbe\u7f6e\u5168\u5c40\u7684\u9884\u7559 IP\uff0c\u8ba9 IPAM \u4e0d\u5206\u914d\u51fa\u8fd9\u4e9b IP \u5730\u5740\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u4e0e\u96c6\u7fa4\u5916\u90e8\u7684\u5df2\u7528 IP \u51b2\u7a81\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5206\u914d\u548c\u91ca\u653e IP \u5730\u5740\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u53ef\u53c2\u8003 \u62a5\u544a \u3002 \u5408\u7406\u7684 IP \u56de\u6536\u673a\u5236\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6216\u5e94\u7528\u5728\u6545\u969c\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u53ca\u65f6\u5206\u914d\u5230 IP \u5730\u5740\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"IPAM \u529f\u80fd"},{"location":"usage/readme-zh_CN/#_2","text":"\u652f\u6301\u4e3a Pod \u591a\u7f51\u5361\u5206\u914d\u4e0d\u540c\u5b50\u7f51\u7684 IP \u5730\u5740\uff1b\u5e2e\u52a9\u6240\u6709\u7f51\u5361\u4e4b\u95f4\u534f\u8c03\u7b56\u7565\u8def\u7531\uff0c\u4ee5\u786e\u4fdd\u8bf7\u6c42\u5411\u548c\u56de\u590d\u5411\u6570\u636e\u8def\u5f84\u4e00\u81f4\uff0c\u907f\u514d\u4e22\u5305\uff1b\u652f\u6301\u5b9a\u5236\u54ea\u5f20\u7f51\u5361\u7684\u7f51\u5173\u4f5c\u4e3a\u7f3a\u7701\u8def\u7531\u3002 \u5bf9\u4e8e Pod \u5177\u5907\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u5bf9\u4e8e Pod \u5177\u5907\u4e00\u4e2a overlay \u7f51\u5361\u548c\u591a\u4e2a underlay CNI \u7f51\u5361\u573a\u666f\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u591a\u7f51\u5361\u529f\u80fd"},{"location":"usage/readme-zh_CN/#_3","text":"\u652f\u6301 RDMA \u7f51\u5361\u7684 shared \u548c exclusive \u6a21\u5f0f\uff0c\u80fd\u57fa\u4e8e maclan\u3001ipvlan \u548c SR-IOV CNI \u4e3a\u5e94\u7528\u63d0\u4f9b RDMA \u901a\u4fe1\u8bbe\u5907\u3002\u5177\u4f53\u53ef\u53c2\u8003 \u4f8b\u5b50 coordinator \u63d2\u4ef6\u80fd\u591f\u4f9d\u636e\u7f51\u5361\u7684 IP \u5730\u5740\u6765\u91cd\u65b0\u914d\u7f6e MAC \u5730\u5740\uff0c\u4f7f\u4e24\u8005\u4e00\u4e00\u5bf9\u5e94\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u907f\u514d\u7f51\u7edc\u4e2d\u7684\u4ea4\u6362\u8def\u7531\u8bbe\u5907\u66f4\u65b0 ARP \u8f6c\u53d1\u89c4\u5219\uff0c\u907f\u514d\u4e22\u5305\u3002\u53ef\u53c2\u8003 \u6587\u7ae0 \u3002 \u5bf9 Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI \u7b49\uff0c \u63d0\u4f9b\u4e86\u57fa\u4e8e kube-proxy \u548c eBPF kube-proxy replacement \u8bbf\u95ee ClusterIP \u8bbf\u95ee\uff0c\u5e76\u8054\u901a Pod \u548c\u5bbf\u4e3b\u673a\u901a\u4fe1\uff0c\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u67e5\u95ee\u9898\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u80fd\u591f\u5e2e\u52a9\u5b9e\u65bd IP \u5730\u5740\u51b2\u7a81\u68c0\u6d4b\u3001\u7f51\u5173\u53ef\u8fbe\u6027\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u8bc1 Pod \u901a\u4fe1\u6b63\u5e38\uff0c\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u8fde\u901a\u6027\u529f\u80fd"},{"location":"usage/readme-zh_CN/#_4","text":"\u5728 Pod \u542f\u52a8\u65f6\uff0c\u80fd\u591f\u5728\u5bbf\u4e3b\u673a\u4e0a\u52a8\u6001\u521b\u5efa BOND \u63a5\u53e3\u548c VLAN \u5b50\u63a5\u53e3\uff0c\u4ee5\u5e2e\u52a9 Macvlan CNI \u548c ipvlan CNI \u51c6\u5907 master \u63a5\u53e3\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002 \u4ee5\u6700\u4f73\u5b9e\u8df5\u7684 CNI \u914d\u7f6e\u6765\u4fbf\u6377\u5730\u751f\u6210 Multus NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5e76\u4e14\u4fdd\u8bc1\u5176\u6b63\u786e\u7684 JSON \u683c\u5f0f\u6765\u63d0\u9ad8\u4f7f\u7528\u4f53\u9a8c\u3002 \u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u8fd0\u7ef4\u7ba1\u7406\u529f\u80fd"},{"location":"usage/readme-zh_CN/#_5","text":"\u6307\u6807 \u652f\u6301 AMD64 \u548c ARM64 \u6240\u6709\u7684\u529f\u80fd\u90fd\u80fd\u591f\u5728 ipv4-only\u3001ipv6-only\u3001dual-stack \u573a\u666f\u4e0b\u5de5\u4f5c\u3002\u53ef\u53c2\u8003 \u4f8b\u5b50 \u3002","title":"\u5176\u5b83\u529f\u80fd"},{"location":"usage/readme/","text":"Usage English \uff5c \u7b80\u4f53\u4e2d\u6587 Install Spiderpool Install Spiderpool with Underlay CNI With Spiderpool on your cluster, Pods can have one or multiple underlay CNI networks, allowing them to connect to the underlay network. Refer to Spiderpool Architecture for details. Please refer to the following examples for installation: Create a cluster using kind Create a cluster using SR-IOV CNI Create a cluster using ovs Create a cluster: using calico CNI for fixed IP addresses Create a cluster: using weave CNI for fixed IP addresses Create a cluster: using underlay CNI for RDMA communication Install Spiderpool with Overlay CNI and Underlay CNI With Spiderpool on your cluster, Pods can simultaneously access overlay CNI interfaces and multiple underlay CNI interfaces, allowing them to connect to both overlay and underlay networks. Refer to Spiderpool Architecture for details. Please refer to the following examples for installation: Create a dual-network cluster using kind Create a dual-network cluster using calico and macvlan CNI Create a dual-network cluster using Cilium and macvlan CNI Install Spiderpool with Underlay CNI on VMs and Public Cloud Environments Please refer to the following examples for installation: Create a cluster on Alibaba Cloud with ipvlan-based networking Create a cluster on VMware vSphere Create a cluster on AWS with ipvlan-based networking For VMware vSphere platform, you can run ipvlan CNI without enabling \"promiscuous\" mode for vSwitch to ensure forwarding performance. Refer to the installation guide for details. TLS Certificate During the Spiderpool installation, you can choose a method for TLS certificate generation. For more information, refer to the article . Uninstall Spiderpool To uninstall Spiderpool, you can use the following methods: helm uninstall spiderpool -n kube-system Some CR resources having finalizers prevents complete cleanup via helm uninstall . You can download the cleaning script below to perform the necessary cleanup and avoid any unexpected errors during future deployments of Spiderpool. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh Upgrade Spiderpool For instructions on how to upgrade Spiderpool, please refer to the upgrade guide . Use Spiderpool IPAM Applications can share an IP pool. See the example for reference. Stateless applications can have a dedicated IP address pool with fixed IP usage range for all Pods. Refer to the example for more details. For stateful applications, each Pod can be allocated a persistent fixed IP address. It also provides control over the IP range used by all Pods during scaling operations. Refer to the example for details. Underlay networking support is available for kubevirt, allowing fixed IP addresses for virtual machines. Refer to the example for details. Applications deployed across subnets can be assigned different subnet IP addresses for each replica. Refer to the example for details. The Subnet feature separates responsibilities between infrastructure administrators and application ones. It automates IP pool management for applications with fixed IP requirements, enabling automatic creation, scaling, and deletion of fixed IP pools. This greatly reduces operational burden. See the example for practical use cases. In addition to supporting native Kubernetes application controllers, Spiderpool's Subnet feature complements third-party application controllers implemented using operators. Refer to the example for more details. Default IP pools can be set at either the cluster-level or tenant-level. IP pools can be shared throughout the entire cluster or restricted to specific tenants. Check out the example for details. IP pool based on node topology caters to fine-grained subnet planning requirements for each node. Refer to the example for details. Custom routing can be achieved through IP pools, Pod annotations, and other methods. Refer to the example for details. Multiple IP pools can be configured by applications to provide redundancy for IP resources. Refer to the example for details.. Global reserved IP addresses can be specified to prevent IPAM from allocating those addresses, thereby avoiding conflicts with externally used IPs. Refer to the example for details. Efficient performance in IP address allocation and release is ensured. Refer to the report for details.. Well-designed IP reclamation mechanisms promptly allocate IP addresses during cluster or application recovery processes. Refer to the example for details. Multiple Network Interfaces Features Spiderpool offers the ability to assign IP addresses from different subnets to multiple network interfaces of a Pod. This feature ensures coordinated policy routing among all interfaces, guaranteeing consistent data paths for outgoing and incoming requests and mitigating packet loss. Moreover, it allows for customization of the default route using a specific network interface's gateway. For Pods with multiple underlay CNI network interfaces, you can refer to the example . For Pods with one overlay network interface and multiple underlay interfaces, you can refer to the example . Connectivity Support for shared and exclusive modes of RDMA network cards enables applications to utilize RDMA communication devices via maclan, ipvlan, and SR-IOV CNI. For more details, see the example . coordinator plugin facilitates MAC address reconfiguration based on the IP address of the network interface, ensuring a one-to-one correspondence between them. This approach prevents the need to update ARP forwarding rules in network switches and routers, thus eliminating packet loss. Read the article for further information. Spiderpool enables access to ClusterIP through kube-proxy and eBPF kube-proxy replacement for plugins such as Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI . This allows seamless communication between Pods and the host machine, thereby resolving Pod health check issues. Refer to the example for details. Spiderpool assists in IP address conflict detection and gateway reachability checks, ensuring uninterrupted Pod communication. Refer to the example for details. Operations and Management Spiderpool dynamically creates BOND interfaces and VLAN sub-interfaces on the host machine during Pod startup. This feature assists in setting up master interfaces for Macvlan CNI and ipvlan CNI . Check out the example for implementation details. Convenient generation of Multus NetworkAttachmentDefinition instances with optimized CNI configurations. Spiderpool ensures correct JSON formatting to enhance user experience. See the example for details. Other Features Metrics Support for AMD64 and ARM64 architectures All features are compatible with ipv4-only, ipv6-only, and dual-stack scenarios. Refer to the example for use cases.","title":"Index"},{"location":"usage/readme/#usage","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"Usage"},{"location":"usage/readme/#install-spiderpool","text":"","title":"Install Spiderpool"},{"location":"usage/readme/#install-spiderpool-with-underlay-cni","text":"With Spiderpool on your cluster, Pods can have one or multiple underlay CNI networks, allowing them to connect to the underlay network. Refer to Spiderpool Architecture for details. Please refer to the following examples for installation: Create a cluster using kind Create a cluster using SR-IOV CNI Create a cluster using ovs Create a cluster: using calico CNI for fixed IP addresses Create a cluster: using weave CNI for fixed IP addresses Create a cluster: using underlay CNI for RDMA communication","title":"Install Spiderpool with Underlay CNI"},{"location":"usage/readme/#install-spiderpool-with-overlay-cni-and-underlay-cni","text":"With Spiderpool on your cluster, Pods can simultaneously access overlay CNI interfaces and multiple underlay CNI interfaces, allowing them to connect to both overlay and underlay networks. Refer to Spiderpool Architecture for details. Please refer to the following examples for installation: Create a dual-network cluster using kind Create a dual-network cluster using calico and macvlan CNI Create a dual-network cluster using Cilium and macvlan CNI","title":"Install Spiderpool with Overlay CNI and Underlay CNI"},{"location":"usage/readme/#install-spiderpool-with-underlay-cni-on-vms-and-public-cloud-environments","text":"Please refer to the following examples for installation: Create a cluster on Alibaba Cloud with ipvlan-based networking Create a cluster on VMware vSphere Create a cluster on AWS with ipvlan-based networking For VMware vSphere platform, you can run ipvlan CNI without enabling \"promiscuous\" mode for vSwitch to ensure forwarding performance. Refer to the installation guide for details.","title":"Install Spiderpool with Underlay CNI on VMs and Public Cloud Environments"},{"location":"usage/readme/#tls-certificate","text":"During the Spiderpool installation, you can choose a method for TLS certificate generation. For more information, refer to the article .","title":"TLS Certificate"},{"location":"usage/readme/#uninstall-spiderpool","text":"To uninstall Spiderpool, you can use the following methods: helm uninstall spiderpool -n kube-system Some CR resources having finalizers prevents complete cleanup via helm uninstall . You can download the cleaning script below to perform the necessary cleanup and avoid any unexpected errors during future deployments of Spiderpool. wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/scripts/cleanCRD.sh chmod +x cleanCRD.sh && ./cleanCRD.sh","title":"Uninstall Spiderpool"},{"location":"usage/readme/#upgrade-spiderpool","text":"For instructions on how to upgrade Spiderpool, please refer to the upgrade guide .","title":"Upgrade Spiderpool"},{"location":"usage/readme/#use-spiderpool","text":"","title":"Use Spiderpool"},{"location":"usage/readme/#ipam","text":"Applications can share an IP pool. See the example for reference. Stateless applications can have a dedicated IP address pool with fixed IP usage range for all Pods. Refer to the example for more details. For stateful applications, each Pod can be allocated a persistent fixed IP address. It also provides control over the IP range used by all Pods during scaling operations. Refer to the example for details. Underlay networking support is available for kubevirt, allowing fixed IP addresses for virtual machines. Refer to the example for details. Applications deployed across subnets can be assigned different subnet IP addresses for each replica. Refer to the example for details. The Subnet feature separates responsibilities between infrastructure administrators and application ones. It automates IP pool management for applications with fixed IP requirements, enabling automatic creation, scaling, and deletion of fixed IP pools. This greatly reduces operational burden. See the example for practical use cases. In addition to supporting native Kubernetes application controllers, Spiderpool's Subnet feature complements third-party application controllers implemented using operators. Refer to the example for more details. Default IP pools can be set at either the cluster-level or tenant-level. IP pools can be shared throughout the entire cluster or restricted to specific tenants. Check out the example for details. IP pool based on node topology caters to fine-grained subnet planning requirements for each node. Refer to the example for details. Custom routing can be achieved through IP pools, Pod annotations, and other methods. Refer to the example for details. Multiple IP pools can be configured by applications to provide redundancy for IP resources. Refer to the example for details.. Global reserved IP addresses can be specified to prevent IPAM from allocating those addresses, thereby avoiding conflicts with externally used IPs. Refer to the example for details. Efficient performance in IP address allocation and release is ensured. Refer to the report for details.. Well-designed IP reclamation mechanisms promptly allocate IP addresses during cluster or application recovery processes. Refer to the example for details.","title":"IPAM"},{"location":"usage/readme/#multiple-network-interfaces-features","text":"Spiderpool offers the ability to assign IP addresses from different subnets to multiple network interfaces of a Pod. This feature ensures coordinated policy routing among all interfaces, guaranteeing consistent data paths for outgoing and incoming requests and mitigating packet loss. Moreover, it allows for customization of the default route using a specific network interface's gateway. For Pods with multiple underlay CNI network interfaces, you can refer to the example . For Pods with one overlay network interface and multiple underlay interfaces, you can refer to the example .","title":"Multiple Network Interfaces Features"},{"location":"usage/readme/#connectivity","text":"Support for shared and exclusive modes of RDMA network cards enables applications to utilize RDMA communication devices via maclan, ipvlan, and SR-IOV CNI. For more details, see the example . coordinator plugin facilitates MAC address reconfiguration based on the IP address of the network interface, ensuring a one-to-one correspondence between them. This approach prevents the need to update ARP forwarding rules in network switches and routers, thus eliminating packet loss. Read the article for further information. Spiderpool enables access to ClusterIP through kube-proxy and eBPF kube-proxy replacement for plugins such as Macvlan CNI , vlan CNI , ipvlan CNI , SR-IOV CNI , ovs CNI . This allows seamless communication between Pods and the host machine, thereby resolving Pod health check issues. Refer to the example for details. Spiderpool assists in IP address conflict detection and gateway reachability checks, ensuring uninterrupted Pod communication. Refer to the example for details.","title":"Connectivity"},{"location":"usage/readme/#operations-and-management","text":"Spiderpool dynamically creates BOND interfaces and VLAN sub-interfaces on the host machine during Pod startup. This feature assists in setting up master interfaces for Macvlan CNI and ipvlan CNI . Check out the example for implementation details. Convenient generation of Multus NetworkAttachmentDefinition instances with optimized CNI configurations. Spiderpool ensures correct JSON formatting to enhance user experience. See the example for details.","title":"Operations and Management"},{"location":"usage/readme/#other-features","text":"Metrics Support for AMD64 and ARM64 architectures All features are compatible with ipv4-only, ipv6-only, and dual-stack scenarios. Refer to the example for use cases.","title":"Other Features"},{"location":"usage/reserved-ip-zh_CN/","text":"Reserved IP \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u901a\u8fc7 ReservedIP CR \u4e3a\u6574\u4e2a Kubernetes \u96c6\u7fa4\u4fdd\u7559\u4e00\u4e9b IP \u5730\u5740\uff0c\u8fd9\u4e9b IP \u5730\u5740\u5c06\u4e0d\u4f1a\u88ab IPAM \u5206\u914d\u3002 Reserved IP \u529f\u80fd \u5f53\u660e\u786e\u67d0\u4e2a IP \u5730\u5740\u5df2\u7ecf\u88ab\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528\u65f6\uff0c\u4e3a\u4e86\u907f\u514d IP \u51b2\u7a81\uff0c\u4ece\u5b58\u91cf\u7684 IPPool \u5b9e\u4f8b\u627e\u5230\u8be5 IP \u5730\u5740\u5e76\u5254\u9664\uff0c\u4e5f\u8bb8\u662f\u4e00\u4ef6\u8017\u65f6\u8017\u529b\u7684\u5de5\u4f5c\u3002\u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u5e0c\u671b\u5b58\u91cf\u6216\u8005\u672a\u6765\u7684\u6240\u6709 IPPool \u8d44\u6e90\u4e2d\uff0c\u90fd\u4e0d\u4f1a\u5206\u914d\u51fa\u8be5 IP \u5730\u5740\u3002\u56e0\u6b64\uff0c\u53ef\u5728 ReservedIP CR \u4e2d\u8bbe\u7f6e\u5e0c\u671b\u4e0d\u88ab\u96c6\u7fa4\u6240\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u8bbe\u7f6e\u540e\uff0c\u5373\u4f7f\u662f\u5728 IPPool \u5bf9\u8c61\u4e2d\u5305\u542b\u4e86\u8be5 IP \u5730\u5740\uff0cIPAM \u63d2\u4ef6\u4e5f\u4e0d\u4f1a\u628a\u8fd9\u4e9b IP \u5730\u5740\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u3002 ReservedIP \u4e2d\u7684 IP \u5730\u5740\u53ef\u4ee5\u662f\uff1a \u660e\u786e\u8be5 IP \u5730\u5740\u88ab\u96c6\u7fa4\u5916\u90e8\u4e3b\u673a\u4f7f\u7528 \u660e\u786e\u8be5 IP \u5730\u5740\u4e0d\u80fd\u88ab\u4f7f\u7528\u4e8e\u7f51\u7edc\u901a\u4fe1\uff0c\u4f8b\u5982\u5b50\u7f51 IP\u3001\u5e7f\u64ad IP \u7b49 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m \u521b\u5efa ReservedIP \u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u6307\u5b9a spec.ips \u4e3a 10.6.168.131-10.6.168.132 \uff0c\u5e76\u521b\u5efa ReservedIP\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF \u521b\u5efa IPPool \u521b\u5efa\u4e00\u4e2a spec.ips \u4e3a 10.6.168.131-10.6.168.133 \uff0c\u5171\u8ba1 3 \u4e2a IP \u5730\u5740\u7684 IPPool \u3002\u901a\u8fc7\u4e0e\u4e0a\u8ff0\u7684 ReservedIP \u5bf9\u6bd4\u53ef\u77e5\uff0c\u8be5 IP \u6c60\u53ea\u6709 1 \u4e2a IP \u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF \u4f7f\u7528\u5982\u4e0b\u7684 Yaml \uff0c\u521b\u5efa\u4e00\u4e2a\u5177\u6709 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5e76\u4ece\u4e0a\u9762\u7684 IPPool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\u7684 IP \u6c60\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u56e0\u4e3a IP \u6c60\u4e2d\u7684\u4e24\u4e2a IP \u88ab ReservedIP CR \u6240\u4fdd\u7559\uff0cIP \u6c60\u4e2d\u53ea\u6709\u4e00\u4e2a IP \u53ef\u7528\u3002\u5e94\u7528\u53ea\u4f1a\u6709\u4e00\u4e2a Pod \u53ef\u4ee5\u6210\u529f\u8fd0\u884c\uff0c\u53e6\u4e00\u4e2a Pod \u7531\u4e8e \"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\" \u800c\u521b\u5efa\u5931\u8d25\u3002 ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> \u5982\u679c\u5e94\u7528\u7684 Pod \u5df2\u7ecf\u5206\u914d\u4e86\u8981\u4fdd\u7559\u7684 IP \u5730\u5740\uff0c\u5c06\u8be5 IP \u5730\u5740\u6dfb\u52a0\u5230 ReservedIP CR \u4e2d\uff0c\u5f53\u5e94\u7528\u526f\u672c\u91cd\u542f\u540e\uff0c\u526f\u672c\u5c06\u65e0\u6cd5\u8fd0\u884c\u3002\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06 Pod \u6240\u5206\u914d\u7684 IP \u5730\u5740\u52a0\u5165\u5230 ReservedIP CR \u4e2d\uff0c\u7136\u540e\u91cd\u542f Pod \uff0cPod \u56e0\"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\"\u800c\u542f\u52a8\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> \u4fdd\u7559 IP \u88ab\u79fb\u9664\u540e\uff0cPod \u53ef\u4ee5\u83b7\u5f97 IP \u5730\u5740\u5e76\u8fd0\u884c\u3002 ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none> \u603b\u7ed3 SpiderReservedIP \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u66f4\u52a0\u5bb9\u6613\u7684\u8fdb\u884c\u7f51\u7edc\u89c4\u5212\u3002","title":"Reserved IP"},{"location":"usage/reserved-ip-zh_CN/#reserved-ip","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Reserved IP"},{"location":"usage/reserved-ip-zh_CN/#_1","text":"Spiderpool \u901a\u8fc7 ReservedIP CR \u4e3a\u6574\u4e2a Kubernetes \u96c6\u7fa4\u4fdd\u7559\u4e00\u4e9b IP \u5730\u5740\uff0c\u8fd9\u4e9b IP \u5730\u5740\u5c06\u4e0d\u4f1a\u88ab IPAM \u5206\u914d\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/reserved-ip-zh_CN/#reserved-ip_1","text":"\u5f53\u660e\u786e\u67d0\u4e2a IP \u5730\u5740\u5df2\u7ecf\u88ab\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528\u65f6\uff0c\u4e3a\u4e86\u907f\u514d IP \u51b2\u7a81\uff0c\u4ece\u5b58\u91cf\u7684 IPPool \u5b9e\u4f8b\u627e\u5230\u8be5 IP \u5730\u5740\u5e76\u5254\u9664\uff0c\u4e5f\u8bb8\u662f\u4e00\u4ef6\u8017\u65f6\u8017\u529b\u7684\u5de5\u4f5c\u3002\u5e76\u4e14\uff0c\u7f51\u7edc\u7ba1\u7406\u5458\u5e0c\u671b\u5b58\u91cf\u6216\u8005\u672a\u6765\u7684\u6240\u6709 IPPool \u8d44\u6e90\u4e2d\uff0c\u90fd\u4e0d\u4f1a\u5206\u914d\u51fa\u8be5 IP \u5730\u5740\u3002\u56e0\u6b64\uff0c\u53ef\u5728 ReservedIP CR \u4e2d\u8bbe\u7f6e\u5e0c\u671b\u4e0d\u88ab\u96c6\u7fa4\u6240\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u8bbe\u7f6e\u540e\uff0c\u5373\u4f7f\u662f\u5728 IPPool \u5bf9\u8c61\u4e2d\u5305\u542b\u4e86\u8be5 IP \u5730\u5740\uff0cIPAM \u63d2\u4ef6\u4e5f\u4e0d\u4f1a\u628a\u8fd9\u4e9b IP \u5730\u5740\u5206\u914d\u7ed9 Pod \u4f7f\u7528\u3002 ReservedIP \u4e2d\u7684 IP \u5730\u5740\u53ef\u4ee5\u662f\uff1a \u660e\u786e\u8be5 IP \u5730\u5740\u88ab\u96c6\u7fa4\u5916\u90e8\u4e3b\u673a\u4f7f\u7528 \u660e\u786e\u8be5 IP \u5730\u5740\u4e0d\u80fd\u88ab\u4f7f\u7528\u4e8e\u7f51\u7edc\u901a\u4fe1\uff0c\u4f8b\u5982\u5b50\u7f51 IP\u3001\u5e7f\u64ad IP \u7b49","title":"Reserved IP \u529f\u80fd"},{"location":"usage/reserved-ip-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/reserved-ip-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/reserved-ip-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool.","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/reserved-ip-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/reserved-ip-zh_CN/#reservedip","text":"\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u6307\u5b9a spec.ips \u4e3a 10.6.168.131-10.6.168.132 \uff0c\u5e76\u521b\u5efa ReservedIP\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF","title":"\u521b\u5efa ReservedIP"},{"location":"usage/reserved-ip-zh_CN/#ippool","text":"\u521b\u5efa\u4e00\u4e2a spec.ips \u4e3a 10.6.168.131-10.6.168.133 \uff0c\u5171\u8ba1 3 \u4e2a IP \u5730\u5740\u7684 IPPool \u3002\u901a\u8fc7\u4e0e\u4e0a\u8ff0\u7684 ReservedIP \u5bf9\u6bd4\u53ef\u77e5\uff0c\u8be5 IP \u6c60\u53ea\u6709 1 \u4e2a IP \u53ef\u7528\u3002 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF \u4f7f\u7528\u5982\u4e0b\u7684 Yaml \uff0c\u521b\u5efa\u4e00\u4e2a\u5177\u6709 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5e76\u4ece\u4e0a\u9762\u7684 IPPool \u4e2d\u5206\u914d IP \u5730\u5740\u3002 ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\u7684 IP \u6c60\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u56e0\u4e3a IP \u6c60\u4e2d\u7684\u4e24\u4e2a IP \u88ab ReservedIP CR \u6240\u4fdd\u7559\uff0cIP \u6c60\u4e2d\u53ea\u6709\u4e00\u4e2a IP \u53ef\u7528\u3002\u5e94\u7528\u53ea\u4f1a\u6709\u4e00\u4e2a Pod \u53ef\u4ee5\u6210\u529f\u8fd0\u884c\uff0c\u53e6\u4e00\u4e2a Pod \u7531\u4e8e \"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\" \u800c\u521b\u5efa\u5931\u8d25\u3002 ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> \u5982\u679c\u5e94\u7528\u7684 Pod \u5df2\u7ecf\u5206\u914d\u4e86\u8981\u4fdd\u7559\u7684 IP \u5730\u5740\uff0c\u5c06\u8be5 IP \u5730\u5740\u6dfb\u52a0\u5230 ReservedIP CR \u4e2d\uff0c\u5f53\u5e94\u7528\u526f\u672c\u91cd\u542f\u540e\uff0c\u526f\u672c\u5c06\u65e0\u6cd5\u8fd0\u884c\u3002\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5c06 Pod \u6240\u5206\u914d\u7684 IP \u5730\u5740\u52a0\u5165\u5230 ReservedIP CR \u4e2d\uff0c\u7136\u540e\u91cd\u542f Pod \uff0cPod \u56e0\"\u6240\u6709 IP \u90fd\u5df2\u7528\u5b8c\"\u800c\u542f\u52a8\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> \u4fdd\u7559 IP \u88ab\u79fb\u9664\u540e\uff0cPod \u53ef\u4ee5\u83b7\u5f97 IP \u5730\u5740\u5e76\u8fd0\u884c\u3002 ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none>","title":"\u521b\u5efa IPPool"},{"location":"usage/reserved-ip-zh_CN/#_4","text":"SpiderReservedIP \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u66f4\u52a0\u5bb9\u6613\u7684\u8fdb\u884c\u7f51\u7edc\u89c4\u5212\u3002","title":"\u603b\u7ed3"},{"location":"usage/reserved-ip/","text":"Reserved IP English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool reserve some IP addresses for the whole Kubernetes cluster through the ReservedIP CR, ensuring that these addresses are not allocated by IPAM. Features of Reserved IP To avoid IP conflicts when it is known that an IP address is being used externally to the cluster, it can be a time-consuming and labor-intensive task to remove that IP address from existing IPPool instances. Furthermore, network administrators want to ensure that this IP address is not allocated from any current or future IPPool resources. To address these concerns, the ReservedIP CR allows for the specification of IP addresses that should not be utilized by the cluster. Even if an IPPool instance includes those IP addresses, the IPAM plugin will refrain from assigning them to Pods. The IP addresses specified in the ReservedIP CR serve two purposes: Clearly identify those IP addresses already in use by hosts outside the cluster. Explicitly prevent the utilization of those IP addresses for network communication, such as subnet IPs or broadcast IPs. Prerequisites A ready Kubernetes kubernetes. Helm has been already installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master\uff1athe interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create a Macvlan SpiderMultusConfig that will automatically generate the corresponding Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m Create reserved IPs To create reserved IPs, use the following YAML to specify spec.ips as 10.6.168.131-10.6.168.132 : cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF Create an IP pool Create an IP pool with spec.ips set to 10.6.168.131-10.6.168.133 , containing a total of 3 IP addresses. However, given the previously created reserved IPs, only 1 IP address is available in this IP pool. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF To allocate IP addresses from this IP pool, use the following YAML to create a Deployment with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for assigning IP addresses to the application cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF Because both IP addresses in the IP pool are reserved by the ReservedIP CR, only one IP address is available in the pool. This means that only one Pod of the application can run successfully, while the other Pod fails to create due to the \"all IPs have been exhausted\" error. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> If a Pod of the application already has been assigned a reserved IP, adding that IP address to the ReservedIP CR will result in the replica failing to run after restarting. Use the following command to add the Pod's allocated IP address to the ReservedIP CR, and then restart the Pod. As expected, the Pod will fail to start due to the \"all IPs have been exhausted\" error. ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> Once the reserved IP is removed, the Pod can obtain an IP address and run successfully. ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none> Conclusion SpiderReservedIP simplifies network planning for infrastructure administrators.","title":"IPAM of Reserved IP"},{"location":"usage/reserved-ip/#reserved-ip","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"Reserved IP"},{"location":"usage/reserved-ip/#introduction","text":"Spiderpool reserve some IP addresses for the whole Kubernetes cluster through the ReservedIP CR, ensuring that these addresses are not allocated by IPAM.","title":"Introduction"},{"location":"usage/reserved-ip/#features-of-reserved-ip","text":"To avoid IP conflicts when it is known that an IP address is being used externally to the cluster, it can be a time-consuming and labor-intensive task to remove that IP address from existing IPPool instances. Furthermore, network administrators want to ensure that this IP address is not allocated from any current or future IPPool resources. To address these concerns, the ReservedIP CR allows for the specification of IP addresses that should not be utilized by the cluster. Even if an IPPool instance includes those IP addresses, the IPAM plugin will refrain from assigning them to Pods. The IP addresses specified in the ReservedIP CR serve two purposes: Clearly identify those IP addresses already in use by hosts outside the cluster. Explicitly prevent the utilization of those IP addresses for network communication, such as subnet IPs or broadcast IPs.","title":"Features of Reserved IP"},{"location":"usage/reserved-ip/#prerequisites","text":"A ready Kubernetes kubernetes. Helm has been already installed.","title":"Prerequisites"},{"location":"usage/reserved-ip/#steps","text":"","title":"Steps"},{"location":"usage/reserved-ip/#install-spiderpool","text":"Refer to Installation to install Spiderpool.","title":"Install Spiderpool"},{"location":"usage/reserved-ip/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master\uff1athe interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create a Macvlan SpiderMultusConfig that will automatically generate the corresponding Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"Install CNI"},{"location":"usage/reserved-ip/#create-reserved-ips","text":"To create reserved IPs, use the following YAML to specify spec.ips as 10.6.168.131-10.6.168.132 : cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderReservedIP metadata: name: test-reservedip spec: ips: - 10.6.168.131-10.6.168.132 EOF","title":"Create reserved IPs"},{"location":"usage/reserved-ip/#create-an-ip-pool","text":"Create an IP pool with spec.ips set to 10.6.168.131-10.6.168.133 , containing a total of 3 IP addresses. However, given the previously created reserved IPs, only 1 IP address is available in this IP pool. cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.131-10.6.168.133 EOF To allocate IP addresses from this IP pool, use the following YAML to create a Deployment with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for assigning IP addresses to the application cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF Because both IP addresses in the IP pool are reserved by the ReservedIP CR, only one IP address is available in the pool. This means that only one Pod of the application can run successfully, while the other Pod fails to create due to the \"all IPs have been exhausted\" error. ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-dv8xz 1 /1 Running 0 17s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 17s <none> node1 <none> <none> If a Pod of the application already has been assigned a reserved IP, adding that IP address to the ReservedIP CR will result in the replica failing to run after restarting. Use the following command to add the Pod's allocated IP address to the ReservedIP CR, and then restart the Pod. As expected, the Pod will fail to start due to the \"all IPs have been exhausted\" error. ~# kubectl patch spiderreservedip test-reservedip --patch '{\"spec\":{\"ips\":[\"10.6.168.131-10.6.168.133\"]}}' --type = merge \uff5e# kubectl delete po test-app-67dd9f645-dv8xz pod \"test-app-67dd9f645-dv8xz\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 0 /1 ContainerCreating 0 9s <none> node2 <none> <none> test-app-67dd9f645-lpjgs 0 /1 ContainerCreating 0 2m18s <none> node1 <none> <none> Once the reserved IP is removed, the Pod can obtain an IP address and run successfully. ~# kubectl delete sr test-reservedip spiderreservedip.spiderpool.spidernet.io \"test-reservedip\" deleted ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-67dd9f645-fvx4m 1 /1 Running 0 4m23s 10 .6.168.133 node2 <none> <none> test-app-67dd9f645-lpjgs 1 /1 Running 0 6m14s 10 .6.168.131 node1 <none> <none>","title":"Create an IP pool"},{"location":"usage/reserved-ip/#conclusion","text":"SpiderReservedIP simplifies network planning for infrastructure administrators.","title":"Conclusion"},{"location":"usage/route-zh_CN/","text":"\u8def\u7531\u652f\u6301 \u7b80\u4f53\u4e2d\u6587 \uff5c English \u4ecb\u7ecd Spiderpool \u63d0\u4f9b\u4e86\u4e3a Pod \u914d\u7f6e\u8def\u7531\u4fe1\u606f\u7684\u529f\u80fd\u3002 \u642d\u914d\u7f51\u5173\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531 \u4e3a SpiderIPPool \u8d44\u6e90\u8bbe\u7f6e**\u7f51\u5173\u5730\u5740**( spec.gateway )\u540e\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8be5\u7f51\u5173\u5730\u5740\u4e3a Pod \u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 \u7ee7\u627f IP \u6c60\u8def\u7531 \u6211\u4eec\u4e5f\u53ef\u4e3a SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u8def\u7531( spec.routes )\uff0c\u521b\u5efa Pod \u65f6\u4f1a\u7ee7\u627f\u8be5\u8def\u7531\uff1a \u5f53 SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u540e\uff0c\u8bf7\u52ff\u4e3a\u8def\u7531\u5b57\u6bb5\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1 \u81ea\u5b9a\u4e49\u8def\u7531 \u6211\u4eec\u4e5f\u652f\u6301\u4e3a\u5e94\u7528\u914d\u7f6e\u81ea\u5b9a\u4e49\u8def\u7531\u7684\u529f\u80fd\uff0c\u53ea\u9700\u4e3a Pod \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/routes : \u5f53 SpiderIPPool \u8d44\u6e90\u4e2d\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u3001\u6216\u914d\u7f6e\u4e86\u9ed8\u8ba4\u8def\u7531\u540e\uff0c\u8bf7\u52ff\u4e3a Pod \u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"\u8def\u7531\u652f\u6301"},{"location":"usage/route-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 \uff5c English","title":"\u8def\u7531\u652f\u6301"},{"location":"usage/route-zh_CN/#_2","text":"Spiderpool \u63d0\u4f9b\u4e86\u4e3a Pod \u914d\u7f6e\u8def\u7531\u4fe1\u606f\u7684\u529f\u80fd\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/route-zh_CN/#_3","text":"\u4e3a SpiderIPPool \u8d44\u6e90\u8bbe\u7f6e**\u7f51\u5173\u5730\u5740**( spec.gateway )\u540e\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8be5\u7f51\u5173\u5730\u5740\u4e3a Pod \u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0","title":"\u642d\u914d\u7f51\u5173\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531"},{"location":"usage/route-zh_CN/#ip","text":"\u6211\u4eec\u4e5f\u53ef\u4e3a SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u8def\u7531( spec.routes )\uff0c\u521b\u5efa Pod \u65f6\u4f1a\u7ee7\u627f\u8be5\u8def\u7531\uff1a \u5f53 SpiderIPPool \u8d44\u6e90\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u540e\uff0c\u8bf7\u52ff\u4e3a\u8def\u7531\u5b57\u6bb5\u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1","title":"\u7ee7\u627f IP \u6c60\u8def\u7531"},{"location":"usage/route-zh_CN/#_4","text":"\u6211\u4eec\u4e5f\u652f\u6301\u4e3a\u5e94\u7528\u914d\u7f6e\u81ea\u5b9a\u4e49\u8def\u7531\u7684\u529f\u80fd\uff0c\u53ea\u9700\u4e3a Pod \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/routes : \u5f53 SpiderIPPool \u8d44\u6e90\u4e2d\u914d\u7f6e\u4e86\u7f51\u5173\u5730\u5740\u3001\u6216\u914d\u7f6e\u4e86\u9ed8\u8ba4\u8def\u7531\u540e\uff0c\u8bf7\u52ff\u4e3a Pod \u914d\u7f6e\u9ed8\u8ba4\u8def\u7531\u3002 dst \u548c gw \u5b57\u6bb5\u90fd\u4e3a\u5fc5\u586b ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"\u81ea\u5b9a\u4e49\u8def\u7531"},{"location":"usage/route/","text":"Route support English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool supports the configuration of routing information for Pods. Configure Default Route with Gateway When setting the gateway address ( spec.gateway ) for a SpiderIPPool resource, a default route will be generated for Pods based on that gateway address: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 Inherit IP Pool Routes SpiderIPPool resources also support configuring routes ( spec.routes ), which will be inherited by Pods during their creation process: If a gateway address is configured for the SpiderIPPool resource, avoid setting default routes in the routes field. Both dst and gw fields are required. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1 Customize Routes You can customize routes for Pods by adding the annotation ipam.spidernet.io/routes : When a gateway address or default route is configured in the SpiderIPPool resource, avoid configuring default routes for Pods. Both dst and gw fields are required. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"Route Support"},{"location":"usage/route/#route-support","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"Route support"},{"location":"usage/route/#introduction","text":"Spiderpool supports the configuration of routing information for Pods.","title":"Introduction"},{"location":"usage/route/#configure-default-route-with-gateway","text":"When setting the gateway address ( spec.gateway ) for a SpiderIPPool resource, a default route will be generated for Pods based on that gateway address: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0","title":"Configure Default Route with Gateway"},{"location":"usage/route/#inherit-ip-pool-routes","text":"SpiderIPPool resources also support configuring routes ( spec.routes ), which will be inherited by Pods during their creation process: If a gateway address is configured for the SpiderIPPool resource, avoid setting default routes in the routes field. Both dst and gw fields are required. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : ipv4-ippool-route spec : subnet : 172.18.41.0/24 ips : - 172.18.41.51-172.18.41.60 gateway : 172.18.41.0 routes : - dst : 172.18.42.0/24 gw : 172.18.41.1","title":"Inherit IP Pool Routes"},{"location":"usage/route/#customize-routes","text":"You can customize routes for Pods by adding the annotation ipam.spidernet.io/routes : When a gateway address or default route is configured in the SpiderIPPool resource, avoid configuring default routes for Pods. Both dst and gw fields are required. ipam.spidernet.io/routes : |- [{ \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" },{ \"dst\": \"172.10.40.0/24\", \"gw\": \"172.18.40.1\" }]","title":"Customize Routes"},{"location":"usage/spider-affinity-zh_CN/","text":"SpiderIPPool Affinity \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderIPPool \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u56e0\u6b64\uff0c\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a\u5b50\u7f51\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002 \u5feb\u901f\u5165\u95e8 \u5728 SpiderIPPool CRD \u91cc\uff0c\u6211\u4eec\u6709\u5b9a\u4e49\u5f88\u591a\u7684\u5b57\u6bb5\u6765\u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528\uff0c\u5982: spec.podAffinity \u5b57\u6bb5\u53ef\u63a7\u5236\u8be5\u6c60\u662f\u5426\u53ef\u88ab Pod \u4f7f\u7528\u3002 spec.namespaceName \u548c spec.namespaceAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u7684Namespace\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( namespaceName \u4f18\u5148\u7ea7\u9ad8\u4e8e namespaceAffinity ) spec.nodeName \u548c spec.nodeAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u6240\u5728\u7684\u8282\u70b9\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( nodeName \u4f18\u5148\u7ea7\u9ad8\u4e8e nodeAffinity ) multusName \u5b57\u6bb5\u4f1a\u5224\u65ad\u5f53\u524d\u7f51\u5361\u662f\u5426\u4e0e multus \u7684 net-attach-def \u8d44\u6e90\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002 \u8fd9\u4e9b\u5b57\u6bb5\u4e0d\u4ec5\u8d77\u5230**\u8fc7\u6ee4**\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u4e5f\u4f1a\u8d77\u5230\u4e00\u4e2a**\u6392\u5e8f**\u7684\u6548\u679c\uff0c\u82e5\u5339\u914d\u7684\u5b57\u6bb5\u8d8a\u591a\uff0c\u8d8a\u4f18\u5148\u4f7f\u7528\u8be5 IP \u6c60\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-pod-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 podAffinity : matchLabels : app : test-app-3 nodeName : - master - worker1 namespaceName : - kube-system - default multusName : - kube-system/macvlan-vlan0 \u5e94\u7528\u4eb2\u548c\u6027 \u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u7531\u4e8e\u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff0c\u4e00\u7ec4 Deployment \u5b83\u7684\u6240\u6709 Pod \u671f\u671b\u80fd\u591f\u5728\u4e00\u4e2a\u56fa\u5b9a\u7684 IP \u5730\u5740\u8303\u56f4\u5185\u8f6e\u6eda\u5206\u914d IP \u5730\u5740\uff0c\u4ee5\u914d\u5408\u9632\u706b\u5899\u7684\u653e\u884c\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0 Underlay \u7f51\u7edc\u4e0b\u7684\u5357\u5317\u901a\u4fe1\u3002 \u5728\u793e\u533a\u73b0\u6709\u65b9\u6848\u4e2d\uff0c\u662f\u901a\u8fc7\u5728 Deployment \u4e0a\u5199\u5173\u4e8e IP \u5730\u5740\u7684\u6ce8\u89e3\u6765\u5b9e\u73b0\u3002\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\uff0c\u5982\uff1a \u968f\u7740\u5e94\u7528\u7684\u6269\u5bb9\uff0c\u9700\u8981\u4eba\u4e3a\u624b\u52a8\u7684\u4fee\u6539\u5e94\u7528\u7684 annotaiton \uff0c\u91cd\u65b0\u89c4\u5212 IP \u5730\u5740\u3002 annotaiton \u65b9\u5f0f\u7684 IP \u7ba1\u7406\uff0c\u8131\u94a9\u4e8e\u5b83\u4eec\u81ea\u8eab\u7684 IPPool CR \u673a\u5236\uff0c\u5f62\u6210\u7ba1\u7406\u4e0a\u7684\u7a7a\u767d\uff0c\u65e0\u6cd5\u83b7\u77e5\u54ea\u4e9b IP \u53ef\u7528\u3002 \u4e0d\u540c\u5e94\u7528\u95f4\u6781\u5176\u5bb9\u6613\u5206\u914d\u4e86\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u4ece\u800c\u5bfc\u81f4\u5e94\u7528\u521b\u5efa\u5931\u8d25\u3002 \u5bf9\u6b64\uff0cSpiderpool \u501f\u52a9\u4e8e IPPool \u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u7684\u7279\u70b9\uff0c\u5e76\u7ed3\u5408\u8bbe\u7f6e IPPool \u7684 podAffinity \uff0c\u53ef\u5b9e\u73b0\u540c\u4e00\u7ec4\u6216\u8005\u591a\u7ec4\u5e94\u7528\u7684\u4eb2\u548c\u7ed1\u5b9a\uff0c\u65e2\u4fdd\u8bc1\u4e86 IP \u7ba1\u7406\u65b9\u5f0f\u7684\u7edf\u4e00\uff0c\u53c8\u89e3\u8026 \"\u5e94\u7528\u6269\u5bb9\" \u548c \"IP \u5730\u5740\u6269\u5bb9\"\uff0c\u4e5f\u56fa\u5b9a\u4e86\u5e94\u7528\u7684 IP \u4f7f\u7528\u8303\u56f4\u3002 \u521b\u5efa\u5e94\u7528\u4eb2\u548c\u6027\u7684 IPPool SpiderIPPool \u63d0\u4f9b\u4e86 podAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u7684 selector.matchLabels \u7b26\u5408\u8be5 podAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u5e94\u7528\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a app: test-app-3 Pod \u7b26\u5408\u6761\u4ef6\u7684 selector.matchLabel \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF \u521b\u5efa\u6307\u5b9a matchLabels \u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa\u4e00\u7ec4 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u5e94\u7528\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8, \u521b\u5efa\u5e94\u7528\u540e\uff0cPod \u7684 matchLabels \u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u8bbe\u7f6e\uff0c\u6210\u529f\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> \u521b\u5efa\u53e6\u4e00\u4e2a\u5e94\u7528\uff0c\u5e76\u6307\u5b9a\u4e00\u4e2a\u4e0d\u7b26\u5408 IPPool \u5e94\u7528\u4eb2\u548c\u7684 matchLabels \uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\u3002 matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label \u4e3a test-unmatch-labels \uff0c\u4e0d\u5339\u914d IPPool \u4eb2\u548c\u6027\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF \u5f53 Pod \u7684 matchLabels \u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u65f6\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none> \u5e94\u7528\u5171\u4eab\u7684 IPPool \u521b\u5efa\u5e94\u7528\u5171\u4eab\u7684 IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 \u521b\u5efa\u4e24\u4e2a deployment\uff0c\u5176 Pod \u8bbe\u7f6e\u6ce8\u91ca \u201cipam.spidernet.io/ippool\u201d \u4ee5\u663e\u5f0f\u6307\u5b9a\u6c60\u9009\u62e9\u89c4\u5219\u3002\u5b83\u5c06\u6210\u529f\u83b7\u5f97IP\u5730\u5740 kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u786e\u8ba4\u6700\u7ec8\u72b6\u6001 kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker \u8282\u70b9\u4eb2\u548c\u6027 \u4e0d\u540c\u7684 node \u4e0a\uff0c\u53ef\u7528\u7684 IP \u8303\u56f4\u4e5f\u8bb8\u5e76\u4e0d\u76f8\u540c\uff0c\u4f8b\u5982\uff1a \u540c\u4e00\u6570\u636e\u4e2d\u5fc3\u5185\uff0c\u96c6\u7fa4\u63a5\u5165\u7684 node \u5206\u5c5e\u4e0d\u540c subnet \u3002 \u5355\u4e2a\u96c6\u7fa4\u4e2d\uff0cnode \u8de8\u8d8a\u4e86\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\u3002 \u5728\u4ee5\u4e0a\u573a\u666f\u4e2d\uff0c\u5f53\u540c\u4e00\u4e2a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u4e86\u4e0d\u540c\u7684 node \u4e0a\uff0c\u9700\u8981\u5206\u914d\u4e0d\u540c subnet \u4e0b\u7684 underlay IP \u5730\u5740\u3002\u5728\u5f53\u524d\u793e\u533a\u73b0\u6709\u65b9\u6848\uff0c\u5b83\u4eec\u5e76\u4e0d\u80fd\u6ee1\u8db3\u8fd9\u6837\u7684\u9700\u6c42\u3002 \u5bf9\u6b64\uff0cSpiderpool \u63d0\u4f9b\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u7684\u65b9\u5f0f\uff0c\u80fd\u5f88\u597d\u7684\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002Spiderpool \u7684 SpiderIPPool CR \u4e2d\uff0c\u63d0\u4f9b\u4e86 nodeAffinity \u4e0e nodeName \u5b57\u6bb5\uff0c\u7528\u4e8e\u8bbe\u7f6e node label selector\uff0c\u4ece\u800c\u5b9e\u73b0 IPPool \u548c node \u4e4b\u95f4\u4eb2\u548c\u6027\uff0c\u5f53 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e2a node \u4e0a\u540e\uff0cIPAM \u63d2\u4ef6\u80fd\u591f\u4ece\u4eb2\u548c\u7684 IPPool \u4e2d\u8fdb\u884c IP \u5730\u5740\u5206\u914d\u3002 \u521b\u5efa\u8282\u70b9\u4eb2\u548c\u7684 IPPool SpiderIPPool \u63d0\u4f9b\u4e86 nodeAffinity \u5b57\u6bb5\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u8282\u70b9\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a\u5728\u8fd0\u884c\u8be5\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a nodeName \uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 \u521b\u5efa\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u8282\u70b9\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 IP \u6c60\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF \u521b\u5efa\u5e94\u7528\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u53ea\u6709\u5f53 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 IPPool \u7684\u8282\u70b9\u4eb2\u548c\u8bbe\u7f6e\uff0c\u624d\u80fd\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none> \u79df\u6237\u4eb2\u548c\u6027 \u7ba1\u7406\u5458\u5f80\u5f80\u4f1a\u5728\u96c6\u7fa4\u5212\u5206\u591a\u79df\u6237\uff0c\u80fd\u66f4\u597d\u5730\u9694\u79bb\u3001\u7ba1\u7406\u548c\u534f\u4f5c\uff0c\u540c\u65f6\u4e5f\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u7075\u6d3b\u6027\u7b49\u3002\u9700\u8981\u4e0d\u540c\u529f\u80fd\u7684\u5e94\u7528\u90e8\u7f72\u5728\u4e0d\u540c\u79df\u6237\u4e0b\uff0c\u5bf9\u6b64\uff0c\u671f\u671b\u5b9e\u73b0\u4e00\u4e2a IPPool \u80fd\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a namespace \u4e0b\u7684\u5e94\u7528\u5b9e\u73b0\u4eb2\u548c\uff0c\u800c\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u521b\u5efa\uff0c\u80fd\u5e2e\u52a9\u7ba1\u7406\u5458\u51cf\u5c11\u8fd0\u7ef4\u8d1f\u62c5\u3002 \u5f53\u524d\u793e\u533a\u4e2d\u5e76\u6ca1\u6709\u89e3\u51b3\u4e0a\u8ff0\u573a\u666f\u7684\u6709\u6548\u65b9\u6848\uff0cSpiderpool \u901a\u8fc7\u8bbe\u7f6e SpiderIPPool CR \u4e2d\u7684 namespaceAffinity \u6216 namespaceName \u5b57\u6bb5\uff0c\u5b9e\u73b0\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u79df\u6237\u7684\u4eb2\u548c\u6027\uff0c\u4ece\u800c\u4f7f\u5f97\u6ee1\u8db3\u6761\u4ef6\u7684\u5e94\u7528\u624d\u80fd\u591f\u4ece IPPool \u4e2d\u5206\u914d\u5230 IP \u5730\u5740\u3002 \u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool \u521b\u5efa\u79df\u6237 ~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created \u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool\u3002 SpiderIPPool \u63d0\u4f9b\u4e86 namespaceAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u79df\u6237\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a namespaceName \uff0c\u5f53 namespaceName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u88ab\u521b\u5efa\u65f6\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u4e0d\u7b26\u5408 namespaceName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 namespaceName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 \u521b\u5efa\u6307\u5b9a\u79df\u6237\u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa\u4e00\u7ec4\u5728\u79df\u6237 test-ns1 \u4e0b\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u79df\u6237\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 namespace : \u6307\u5b9a\u5e94\u7528\u6240\u5728\u79df\u6237\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8, \u521b\u5efa\u5e94\u7528\u540e\uff0c\u5728\u79df\u6237\u5185\u7684\u5e94\u7528 Pod \u6210\u529f\u4ece\u6240\u4eb2\u548c\u7684 IPPool \u4e2d\u5206\u914d\u5230\u4e86 IP \u5730\u5740\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> \u521b\u5efa\u4e00\u4e2a\u4e0d\u5728\u4e0a\u8ff0 test-ns1 \u79df\u6237\u5185\u7684\u5e94\u7528\uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\uff0c\u81ea\u52a8\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u4f7f\u7528\u8be5 IPPool\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF \u5f53 Pod \u6240\u5c5e\u79df\u6237\u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u79df\u6237\u4eb2\u548c\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none> \u7f51\u5361\u914d\u7f6e\u4eb2\u548c\u6027 \u5f53\u4e3a\u5e94\u7528\u521b\u5efa\u591a\u7f51\u5361\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a**\u96c6\u7fa4\u7ea7\u522b\u7f3a\u7701\u6c60**\u6307\u5b9a multus \u7684 net-attach-def \u5b9e\u4f8b\u4eb2\u548c\u6027\u3002\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u901a\u8fc7\u6ce8\u89e3 ipam.spidernet.io/ippools \u663e\u5f0f\u6307\u5b9a\u7f51\u5361\u4e0e IPPool \u8d44\u6e90\u7684\u7ed1\u5b9a\u5173\u7cfb\u66f4\u4e3a\u7b80\u5355\u3002 \u9996\u5148\u4e3a IPPool \u8d44\u6e90\u914d\u7f6e\u597d\u5404\u7c7b\u5c5e\u6027\uff0c\u5176\u4e2d: spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true , \u4ee5\u6b64\u51cf\u5c11\u4e3a\u5e94\u7528\u6253\u4e0a ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6ce8\u89e3\uff0c\u8ba9\u4f53\u9a8c\u66f4\u4e3a\u7b80\u5355\u3002 spec.multusName \u5b57\u6bb5\u914d\u7f6e\u8be5 IPPool \u5bf9\u5e94\u7684 multus \u7f51\u5361\u914d\u7f6e\u3002(\u82e5\u60a8\u672a\u6307\u5b9a\u5bf9\u5e94 multus \u7684 net-attach-def \u5b9e\u4f8b\u7684 namespace\uff0c\u6211\u4eec\u4f1a\u9ed8\u8ba4\u5c06\u5176\u89c6\u4e3a\u5c5e\u4e8e spiderpool \u5b89\u88c5\u65f6\u7684\u547d\u540d\u7a7a\u95f4) apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 \u521b\u5efa\u591a\u7f51\u5361\u7684\u5e94\u7528\u3002\u6211\u4eec\u53ea\u9700\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa\u6709\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\u3002(\u82e5\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u800c\u76f4\u63a5\u4f7f\u7528 multus \u96c6\u7fa4\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\uff0c\u8bf7\u5728 helm \u5b89\u88c5 spiderpool \u65f6\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ) k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u989d\u5916\u7f51\u5361\u7684\u914d\u7f6e\u4fe1\u606f\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF \u603b\u7ed3 SpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002","title":"SpiderIPPool Affinity"},{"location":"usage/spider-affinity-zh_CN/#spiderippool-affinity","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderIPPool Affinity"},{"location":"usage/spider-affinity-zh_CN/#_1","text":"SpiderIPPool \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u4e00\u4e2a Subnet \u4e2d\u7684\u4e0d\u540c IP \u5730\u5740\uff0c\u53ef\u5206\u522b\u5b58\u50a8\u5230\u4e0d\u540c\u7684 IPPool \u5b9e\u4f8b\u4e2d\uff08Spiderpool \u4f1a\u6821\u9a8c IPPool \u4e4b\u95f4\u7684\u5730\u5740\u96c6\u5408\u4e0d\u91cd\u53e0\uff09\u3002\u56e0\u6b64\uff0c\u4f9d\u636e\u9700\u6c42\uff0cSpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a\u5b50\u7f51\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-affinity-zh_CN/#_2","text":"\u5728 SpiderIPPool CRD \u91cc\uff0c\u6211\u4eec\u6709\u5b9a\u4e49\u5f88\u591a\u7684\u5b57\u6bb5\u6765\u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528\uff0c\u5982: spec.podAffinity \u5b57\u6bb5\u53ef\u63a7\u5236\u8be5\u6c60\u662f\u5426\u53ef\u88ab Pod \u4f7f\u7528\u3002 spec.namespaceName \u548c spec.namespaceAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u7684Namespace\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( namespaceName \u4f18\u5148\u7ea7\u9ad8\u4e8e namespaceAffinity ) spec.nodeName \u548c spec.nodeAffinity \u5b57\u6bb5\u4f1a\u6821\u9a8c\u662f\u5426\u4e0e Pod \u6240\u5728\u7684\u8282\u70b9\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002( nodeName \u4f18\u5148\u7ea7\u9ad8\u4e8e nodeAffinity ) multusName \u5b57\u6bb5\u4f1a\u5224\u65ad\u5f53\u524d\u7f51\u5361\u662f\u5426\u4e0e multus \u7684 net-attach-def \u8d44\u6e90\u4f7f\u7528\u7684 CNI \u914d\u7f6e\u76f8\u5339\u914d\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u4e0d\u53ef\u4f7f\u7528\u3002 \u8fd9\u4e9b\u5b57\u6bb5\u4e0d\u4ec5\u8d77\u5230**\u8fc7\u6ee4**\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u4e5f\u4f1a\u8d77\u5230\u4e00\u4e2a**\u6392\u5e8f**\u7684\u6548\u679c\uff0c\u82e5\u5339\u914d\u7684\u5b57\u6bb5\u8d8a\u591a\uff0c\u8d8a\u4f18\u5148\u4f7f\u7528\u8be5 IP \u6c60\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-pod-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 podAffinity : matchLabels : app : test-app-3 nodeName : - master - worker1 namespaceName : - kube-system - default multusName : - kube-system/macvlan-vlan0","title":"\u5feb\u901f\u5165\u95e8"},{"location":"usage/spider-affinity-zh_CN/#_3","text":"\u5728\u96c6\u7fa4\u4e2d\uff0c\u9632\u706b\u5899\u901a\u5e38\u7528\u4e8e\u7ba1\u7406\u5357\u5317\u5411\u901a\u4fe1\uff0c\u5373\u96c6\u7fa4\u5185\u90e8\u548c\u5916\u90e8\u7f51\u7edc\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u7ba1\u63a7\uff0c\u9632\u706b\u5899\u9700\u8981\u5bf9\u901a\u4fe1\u6d41\u91cf\u8fdb\u884c\u68c0\u67e5\u548c\u8fc7\u6ee4\uff0c\u5e76\u5bf9\u51fa\u53e3\u901a\u4fe1\u8fdb\u884c\u9650\u5236\u3002\u7531\u4e8e\u9632\u706b\u5899\u5b89\u5168\u7ba1\u63a7\uff0c\u4e00\u7ec4 Deployment \u5b83\u7684\u6240\u6709 Pod \u671f\u671b\u80fd\u591f\u5728\u4e00\u4e2a\u56fa\u5b9a\u7684 IP \u5730\u5740\u8303\u56f4\u5185\u8f6e\u6eda\u5206\u914d IP \u5730\u5740\uff0c\u4ee5\u914d\u5408\u9632\u706b\u5899\u7684\u653e\u884c\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0 Underlay \u7f51\u7edc\u4e0b\u7684\u5357\u5317\u901a\u4fe1\u3002 \u5728\u793e\u533a\u73b0\u6709\u65b9\u6848\u4e2d\uff0c\u662f\u901a\u8fc7\u5728 Deployment \u4e0a\u5199\u5173\u4e8e IP \u5730\u5740\u7684\u6ce8\u89e3\u6765\u5b9e\u73b0\u3002\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\uff0c\u5982\uff1a \u968f\u7740\u5e94\u7528\u7684\u6269\u5bb9\uff0c\u9700\u8981\u4eba\u4e3a\u624b\u52a8\u7684\u4fee\u6539\u5e94\u7528\u7684 annotaiton \uff0c\u91cd\u65b0\u89c4\u5212 IP \u5730\u5740\u3002 annotaiton \u65b9\u5f0f\u7684 IP \u7ba1\u7406\uff0c\u8131\u94a9\u4e8e\u5b83\u4eec\u81ea\u8eab\u7684 IPPool CR \u673a\u5236\uff0c\u5f62\u6210\u7ba1\u7406\u4e0a\u7684\u7a7a\u767d\uff0c\u65e0\u6cd5\u83b7\u77e5\u54ea\u4e9b IP \u53ef\u7528\u3002 \u4e0d\u540c\u5e94\u7528\u95f4\u6781\u5176\u5bb9\u6613\u5206\u914d\u4e86\u51b2\u7a81\u7684 IP \u5730\u5740\uff0c\u4ece\u800c\u5bfc\u81f4\u5e94\u7528\u521b\u5efa\u5931\u8d25\u3002 \u5bf9\u6b64\uff0cSpiderpool \u501f\u52a9\u4e8e IPPool \u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u7684\u7279\u70b9\uff0c\u5e76\u7ed3\u5408\u8bbe\u7f6e IPPool \u7684 podAffinity \uff0c\u53ef\u5b9e\u73b0\u540c\u4e00\u7ec4\u6216\u8005\u591a\u7ec4\u5e94\u7528\u7684\u4eb2\u548c\u7ed1\u5b9a\uff0c\u65e2\u4fdd\u8bc1\u4e86 IP \u7ba1\u7406\u65b9\u5f0f\u7684\u7edf\u4e00\uff0c\u53c8\u89e3\u8026 \"\u5e94\u7528\u6269\u5bb9\" \u548c \"IP \u5730\u5740\u6269\u5bb9\"\uff0c\u4e5f\u56fa\u5b9a\u4e86\u5e94\u7528\u7684 IP \u4f7f\u7528\u8303\u56f4\u3002","title":"\u5e94\u7528\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#ippool","text":"SpiderIPPool \u63d0\u4f9b\u4e86 podAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u7684 selector.matchLabels \u7b26\u5408\u8be5 podAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u5e94\u7528\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a app: test-app-3 Pod \u7b26\u5408\u6761\u4ef6\u7684 selector.matchLabel \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF \u521b\u5efa\u6307\u5b9a matchLabels \u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa\u4e00\u7ec4 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u5e94\u7528\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8, \u521b\u5efa\u5e94\u7528\u540e\uff0cPod \u7684 matchLabels \u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u8bbe\u7f6e\uff0c\u6210\u529f\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> \u521b\u5efa\u53e6\u4e00\u4e2a\u5e94\u7528\uff0c\u5e76\u6307\u5b9a\u4e00\u4e2a\u4e0d\u7b26\u5408 IPPool \u5e94\u7528\u4eb2\u548c\u7684 matchLabels \uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\u3002 matchLabels : \u8bbe\u7f6e\u5e94\u7528\u7684 Label \u4e3a test-unmatch-labels \uff0c\u4e0d\u5339\u914d IPPool \u4eb2\u548c\u6027\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF \u5f53 Pod \u7684 matchLabels \u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u5e94\u7528\u4eb2\u548c\u65f6\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none>","title":"\u521b\u5efa\u5e94\u7528\u4eb2\u548c\u6027\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#ippool_1","text":"\u521b\u5efa\u5e94\u7528\u5171\u4eab\u7684 IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 \u521b\u5efa\u4e24\u4e2a deployment\uff0c\u5176 Pod \u8bbe\u7f6e\u6ce8\u91ca \u201cipam.spidernet.io/ippool\u201d \u4ee5\u663e\u5f0f\u6307\u5b9a\u6c60\u9009\u62e9\u89c4\u5219\u3002\u5b83\u5c06\u6210\u529f\u83b7\u5f97IP\u5730\u5740 kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u786e\u8ba4\u6700\u7ec8\u72b6\u6001 kubectl get po -l app = static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1 /1 Running 0 62s 172 .18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1 /1 Running 0 62s 172 .18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1 /1 Running 0 62s 172 .18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1 /1 Running 0 62s 172 .18.41.47 spider-worker","title":"\u5e94\u7528\u5171\u4eab\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#_4","text":"\u4e0d\u540c\u7684 node \u4e0a\uff0c\u53ef\u7528\u7684 IP \u8303\u56f4\u4e5f\u8bb8\u5e76\u4e0d\u76f8\u540c\uff0c\u4f8b\u5982\uff1a \u540c\u4e00\u6570\u636e\u4e2d\u5fc3\u5185\uff0c\u96c6\u7fa4\u63a5\u5165\u7684 node \u5206\u5c5e\u4e0d\u540c subnet \u3002 \u5355\u4e2a\u96c6\u7fa4\u4e2d\uff0cnode \u8de8\u8d8a\u4e86\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\u3002 \u5728\u4ee5\u4e0a\u573a\u666f\u4e2d\uff0c\u5f53\u540c\u4e00\u4e2a\u5e94\u7528\u7684\u4e0d\u540c\u526f\u672c\u88ab\u8c03\u5ea6\u5230\u4e86\u4e0d\u540c\u7684 node \u4e0a\uff0c\u9700\u8981\u5206\u914d\u4e0d\u540c subnet \u4e0b\u7684 underlay IP \u5730\u5740\u3002\u5728\u5f53\u524d\u793e\u533a\u73b0\u6709\u65b9\u6848\uff0c\u5b83\u4eec\u5e76\u4e0d\u80fd\u6ee1\u8db3\u8fd9\u6837\u7684\u9700\u6c42\u3002 \u5bf9\u6b64\uff0cSpiderpool \u63d0\u4f9b\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u7684\u65b9\u5f0f\uff0c\u80fd\u5f88\u597d\u7684\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002Spiderpool \u7684 SpiderIPPool CR \u4e2d\uff0c\u63d0\u4f9b\u4e86 nodeAffinity \u4e0e nodeName \u5b57\u6bb5\uff0c\u7528\u4e8e\u8bbe\u7f6e node label selector\uff0c\u4ece\u800c\u5b9e\u73b0 IPPool \u548c node \u4e4b\u95f4\u4eb2\u548c\u6027\uff0c\u5f53 Pod \u88ab\u8c03\u5ea6\u5230\u67d0\u4e2a node \u4e0a\u540e\uff0cIPAM \u63d2\u4ef6\u80fd\u591f\u4ece\u4eb2\u548c\u7684 IPPool \u4e2d\u8fdb\u884c IP \u5730\u5740\u5206\u914d\u3002","title":"\u8282\u70b9\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#ippool_2","text":"SpiderIPPool \u63d0\u4f9b\u4e86 nodeAffinity \u5b57\u6bb5\uff0c\u5f53 Pod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa\u5982\u4e0b\u5177\u5907\u8282\u70b9\u4eb2\u548c\u7684 SpiderIPPool\uff0c\u5b83\u5c06\u4e3a\u5728\u8fd0\u884c\u8be5\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u8282\u70b9\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a nodeName \uff0c\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 \u521b\u5efa\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u7ec4 DaemonSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u8282\u70b9\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684 IP \u6c60\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF \u521b\u5efa\u5e94\u7528\u540e\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u53ea\u6709\u5f53 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 IPPool \u7684\u8282\u70b9\u4eb2\u548c\u8bbe\u7f6e\uff0c\u624d\u80fd\u4ece\u8be5 IPPool \u4e2d\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u3002\u5e76\u4e14\u5e94\u7528\u7684 IP \u56fa\u5b9a\u5728\u8be5 IP \u6c60\u5185\u3002 \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none>","title":"\u521b\u5efa\u8282\u70b9\u4eb2\u548c\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#_5","text":"\u7ba1\u7406\u5458\u5f80\u5f80\u4f1a\u5728\u96c6\u7fa4\u5212\u5206\u591a\u79df\u6237\uff0c\u80fd\u66f4\u597d\u5730\u9694\u79bb\u3001\u7ba1\u7406\u548c\u534f\u4f5c\uff0c\u540c\u65f6\u4e5f\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u7075\u6d3b\u6027\u7b49\u3002\u9700\u8981\u4e0d\u540c\u529f\u80fd\u7684\u5e94\u7528\u90e8\u7f72\u5728\u4e0d\u540c\u79df\u6237\u4e0b\uff0c\u5bf9\u6b64\uff0c\u671f\u671b\u5b9e\u73b0\u4e00\u4e2a IPPool \u80fd\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a namespace \u4e0b\u7684\u5e94\u7528\u5b9e\u73b0\u4eb2\u548c\uff0c\u800c\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u521b\u5efa\uff0c\u80fd\u5e2e\u52a9\u7ba1\u7406\u5458\u51cf\u5c11\u8fd0\u7ef4\u8d1f\u62c5\u3002 \u5f53\u524d\u793e\u533a\u4e2d\u5e76\u6ca1\u6709\u89e3\u51b3\u4e0a\u8ff0\u573a\u666f\u7684\u6709\u6548\u65b9\u6848\uff0cSpiderpool \u901a\u8fc7\u8bbe\u7f6e SpiderIPPool CR \u4e2d\u7684 namespaceAffinity \u6216 namespaceName \u5b57\u6bb5\uff0c\u5b9e\u73b0\u540c\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u79df\u6237\u7684\u4eb2\u548c\u6027\uff0c\u4ece\u800c\u4f7f\u5f97\u6ee1\u8db3\u6761\u4ef6\u7684\u5e94\u7528\u624d\u80fd\u591f\u4ece IPPool \u4e2d\u5206\u914d\u5230 IP \u5730\u5740\u3002","title":"\u79df\u6237\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#ippool_3","text":"\u521b\u5efa\u79df\u6237 ~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created \u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool\u3002 SpiderIPPool \u63d0\u4f9b\u4e86 namespaceAffinity \u5b57\u6bb5\uff0c\u5f53\u5e94\u7528\u521b\u5efa\u65f6\uff0c\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u65f6\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceAffinity \u8bbe\u7f6e\uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u5426\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51faIP\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u79df\u6237\u4eb2\u548c\u6027\u65b9\u5f0f\u4f9b\u9009\u62e9\uff1a namespaceName \uff0c\u5f53 namespaceName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u88ab\u521b\u5efa\u65f6\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u79df\u6237\u7b26\u5408\u8be5 namespaceName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u79df\u6237\u4e0d\u7b26\u5408 namespaceName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 namespaceName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\uff0c\u53c2\u8003\u5982\u4e0b\uff1a apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 \u521b\u5efa\u6307\u5b9a\u79df\u6237\u7684\u5e94\u7528\u3002\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa\u4e00\u7ec4\u5728\u79df\u6237 test-ns1 \u4e0b\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1aSpiderpool \u7528\u4e8e\u6307\u5b9a\u8bbe\u7f6e\u4e86\u79df\u6237\u4eb2\u548c\u7684 IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 namespace : \u6307\u5b9a\u5e94\u7528\u6240\u5728\u79df\u6237\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8, \u521b\u5efa\u5e94\u7528\u540e\uff0c\u5728\u79df\u6237\u5185\u7684\u5e94\u7528 Pod \u6210\u529f\u4ece\u6240\u4eb2\u548c\u7684 IPPool \u4e2d\u5206\u914d\u5230\u4e86 IP \u5730\u5740\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> \u521b\u5efa\u4e00\u4e2a\u4e0d\u5728\u4e0a\u8ff0 test-ns1 \u79df\u6237\u5185\u7684\u5e94\u7528\uff0cSpiderpool \u5c06\u4f1a\u62d2\u7edd\u4e3a\u5176\u5206\u914d IP \u5730\u5740\uff0c\u81ea\u52a8\u62d2\u7edd\u4e0d\u76f8\u5e72\u79df\u6237\u7684\u5e94\u7528\u4f7f\u7528\u8be5 IPPool\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF \u5f53 Pod \u6240\u5c5e\u79df\u6237\u4e0d\u7b26\u5408\u8be5 IPPool \u7684\u79df\u6237\u4eb2\u548c\uff0c\u83b7\u5f97 IP \u5730\u5740\u5206\u914d\u5931\u8d25\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none>","title":"\u521b\u5efa\u79df\u6237\u4eb2\u548c\u7684 IPPool"},{"location":"usage/spider-affinity-zh_CN/#_6","text":"\u5f53\u4e3a\u5e94\u7528\u521b\u5efa\u591a\u7f51\u5361\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a**\u96c6\u7fa4\u7ea7\u522b\u7f3a\u7701\u6c60**\u6307\u5b9a multus \u7684 net-attach-def \u5b9e\u4f8b\u4eb2\u548c\u6027\u3002\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u901a\u8fc7\u6ce8\u89e3 ipam.spidernet.io/ippools \u663e\u5f0f\u6307\u5b9a\u7f51\u5361\u4e0e IPPool \u8d44\u6e90\u7684\u7ed1\u5b9a\u5173\u7cfb\u66f4\u4e3a\u7b80\u5355\u3002 \u9996\u5148\u4e3a IPPool \u8d44\u6e90\u914d\u7f6e\u597d\u5404\u7c7b\u5c5e\u6027\uff0c\u5176\u4e2d: spec.default \u5b57\u6bb5\u8bbe\u7f6e\u4e3a true , \u4ee5\u6b64\u51cf\u5c11\u4e3a\u5e94\u7528\u6253\u4e0a ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6ce8\u89e3\uff0c\u8ba9\u4f53\u9a8c\u66f4\u4e3a\u7b80\u5355\u3002 spec.multusName \u5b57\u6bb5\u914d\u7f6e\u8be5 IPPool \u5bf9\u5e94\u7684 multus \u7f51\u5361\u914d\u7f6e\u3002(\u82e5\u60a8\u672a\u6307\u5b9a\u5bf9\u5e94 multus \u7684 net-attach-def \u5b9e\u4f8b\u7684 namespace\uff0c\u6211\u4eec\u4f1a\u9ed8\u8ba4\u5c06\u5176\u89c6\u4e3a\u5c5e\u4e8e spiderpool \u5b89\u88c5\u65f6\u7684\u547d\u540d\u7a7a\u95f4) apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 \u521b\u5efa\u591a\u7f51\u5361\u7684\u5e94\u7528\u3002\u6211\u4eec\u53ea\u9700\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa\u6709\u4e24\u5f20\u7f51\u5361\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\u3002(\u82e5\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u800c\u76f4\u63a5\u4f7f\u7528 multus \u96c6\u7fa4\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f\uff0c\u8bf7\u5728 helm \u5b89\u88c5 spiderpool \u65f6\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u9ed8\u8ba4\u7f51\u5361\u914d\u7f6e\u4fe1\u606f --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ) k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u521b\u5efa\u7684\u5e94\u7528\u9009\u62e9\u989d\u5916\u7f51\u5361\u7684\u914d\u7f6e\u4fe1\u606f\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF","title":"\u7f51\u5361\u914d\u7f6e\u4eb2\u548c\u6027"},{"location":"usage/spider-affinity-zh_CN/#_7","text":"SpiderIPPool \u4e2d\u7684 IP \u96c6\u5408\u53ef\u5927\u53ef\u5c0f\u3002\u80fd\u5f88\u597d\u7684\u5e94\u5bf9 Underlay \u7f51\u7edc\u7684 IP \u5730\u5740\u8d44\u6e90\u6709\u9650\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u8bbe\u8ba1\u7279\u70b9\uff0c\u80fd\u591f\u901a\u8fc7\u5404\u79cd\u4eb2\u548c\u6027\u89c4\u5219\u8ba9\u4e0d\u540c\u7684\u5e94\u7528\u3001\u79df\u6237\u6765\u7ed1\u5b9a\u4e0d\u540c\u7684 SpiderIPPool\uff0c\u4e5f\u80fd\u5206\u4eab\u76f8\u540c\u7684 SpiderIPPool\uff0c\u65e2\u80fd\u591f\u8ba9\u6240\u6709\u5e94\u7528\u5171\u4eab\u4f7f\u7528\u540c\u4e00\u4e2a Subnet\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0 \"\u5fae\u9694\u79bb\"\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-affinity/","text":"SpiderIPPool Affinity English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction SpiderIPPool is a representation of a collection of IP addresses. It allows storing different IP addresses from the same subnet in separate IPPool instances, ensuring that there is no overlap between address sets. This design provides flexibility in managing IP resources within the underlay network, especially when faced with limited availability. SpiderIPPool offers the ability to assign different SpiderIPPool instances to various applications and tenants through affinity rules, allowing for both shared subnet usage and micro-isolation. Quick Start In SpiderIPPool CRD , we defined lots of properties to use with affinities: spec.podAffinity controls whether the pool can be used by the Pod. spec.namespaceName and spec.namespaceAffinity verify if they match the Namespace of the Pod. If there is no match, the pool cannot be used. ( namespaceName takes precedence over namespaceAffinity ). spec.nodeName and spec.nodeAffinity verify if they match the node where the Pod is located. If there is no match, the pool cannot be used. ( nodeName takes precedence over nodeAffinity ). multusName determines whether the current network card which using the pool matches the CNI configuration used by the multus net-attach-def resource. If there is no match, the pool cannot be used. These fields not only serve as filters but also have a sorting effect . The more matching fields there are, the higher priority the IP pool has for usage. Application Affinity Firewalls are commonly used in clusters to manage communication between internal and external networks (north-south communication). To enforce secure access control, firewalls inspect and filter communication traffic while restricting outbound communication. In order to align with firewall policies and enable north-south communication within the underlay network, certain Deployments require all Pods to be assigned IP addresses within a specific range. Existing community solutions rely on annotations to handle IP address allocation for such cases. However, this approach has limitations: Manual modification of annotations becomes necessary as the application scales, leading to potential errors. IP management through annotations is far apart from the IPPool CR mechanism, resulting in a lack of visibility into available IP addresses. Conflicting IP addresses can easily be assigned to different applications, causing deployment failures. Spiderpool addresses these challenges by leveraging the flexibility of IPPools, where IP address collections can be adjusted. By combining this with the podAffinity setting in the SpiderIPPool CR, Spiderpool enables the binding of specific applications or groups of applications to particular IPPools. This ensures a unified approach to IP management, decouples application scaling from IP address scaling, and provides a fixed IP usage range for each application. Create IPPool with Application Affinity SpiderIPPool provides the podAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the Pods' selector.matchLabels match the specified podAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. Based on the above, using the following Yaml, create the following SpiderIPPool with application affinity, which will provide the IP address for the app: test-app-3 Pod's eligible selector.matchLabel . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF Creating Applications with Specific matchLabels. In the example YAML provided, a group of Deployment applications is created. The configuration includes: ipam.spidernet.io/ippool : specify the IP pool with application affinity. v1.multus-cni.io/default-network : create a default network interface for the application. matchLabels : set the label for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF After creating the application, the Pods with matchLabels that match the IPPool's application affinity successfully obtain IP addresses from that SpiderIPPool. The assigned IP addresses remain within the IP pool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> However, when creating another application with different matchLabels that do not meet the IPPool's application affinity, Spiderpool will reject IP address allocation. matchLabels : set the label of the application to test-unmatch-labels , which does not match IPPool affinity. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF Getting an IP address assignment fails as expected when the Pod's matchLabels do not match the application affinity for that IPPool. kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none> Shared IPPool Create an IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 Create two Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. ```bash kubectl get po -l app=static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1/1 Running 0 62s 172.18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1/1 Running 0 62s 172.18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1/1 Running 0 62s 172.18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1/1 Running 0 62s 172.18.41.47 spider-worker ``` Node Affinity Nodes in a cluster might have access to different IP ranges. Some scenarios include: Nodes in the same data center belonging to different subnets. Nodes spanning multiple data centers within a single cluster. In such cases, replicas of an application are scheduled on different nodes require IP addresses from different subnets. Current community solutions are limited to satisfy this needs. To address this problem, Spiderpool support node affinity solution. By setting the nodeAffinity and nodeName fields in the SpiderIPPool CR, administrators can define a node label selector. This enables the IPAM plugin to allocate IP addresses from the specified IPPool when Pods are scheduled on nodes that match the affinity rules. IPPool with Node Affinity SpiderIPPool offers the nodeAffinity field. When a Pod is scheduled on a node and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the node satisfies the specified nodeAffinity condition. Otherwise, it will be unable to allocate an IP address from that SpiderIPPool. To create a SpiderIPPool with node affinity, use the following YAML configuration. This SpiderIPPool will provide IP addresses for Pods running on the designated node. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool provides an additional option for node affinity: nodeName . When nodeName is specified, a Pod is scheduled on a specific node and attempts to allocate an IP address from the SpiderIPPool. If the node matches the specified nodeName , the IP address can be successfully allocated from that SpiderIPPool. If not, it will be unable to allocate an IP address from that SpiderIPPool. When nodeName is left empty, Spiderpool does not impose any allocation restrictions on Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 create an Application ipam.spidernet.io/ippool : specify the IP pool with node affinity. v1.multus-cni.io/default-network : Identifies the IP pool used by the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF After creating an application, it can be observed that IP addresses are only allocated from the corresponding IPPool if the Pod's node matches the IPPool's node affinity. The IP address of the application remains within the assigned IPPool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none> IPPool with Namespace Affinity Cluster administrators often partition their clusters into multiple namespaces to improve isolation, management, collaboration, security, and resource utilization. When deploying applications under different namespaces, it becomes necessary to assign specific IPPools to each namespace, preventing applications from unrelated namespaces from using them. Spiderpool addresses this requirement by introducing the namespaceAffinity or namespaceName fields in the SpiderIPPool CR. This allows administrators to define affinity rules between IPPools and one or more namespaces, ensuring that only applications meeting the specified conditions can be allocated IP addresses from the respective IPPools. Create IPPool with Namespace Affinity ~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created To create an IPPool with namaspace affinity, use the following YAML: SpiderIPPool provides the namespaceAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it will only succeed if the Pod's namespace matches the specified namespaceAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool also offers another option for namespace affinity: namespaceName . When namespaceName is not empty, a Pod is created and attempts to allocate an IP address from the SpiderIPPool. If the namespace of the Pod matches the specified namespaceName , it can successfully obtain an IP from that SpiderIPPool. However, if the namespace does not match the namespaceName , it will be unable to allocate an IP address from that SpiderIPPool. When namespaceName is empty, Spiderpool does not impose any restrictions on IP allocation for Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 Create Applications in a Specified Namespace. In the provided YAML example, a group of Deployment applications is created under the test-ns1 namespace. The configuration includes: ipam.spidernet.io/ippool \uff1aspecify the IP pool with tenant affinity. v1.multus-cni.io/default-network : create a default network interface for the application. namespace : the namespace where the application resides. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF After creating the application, the Pods within the designated namespace successfully allocate IP addresses from the associated IPPool with namespace affinity. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> However, if an application is created outside the test-ns1 namespace, Spiderpool will reject IP address allocation, preventing unrelated namespace from using that IPPool. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF Getting an IP address assignment fails as expected when the Pod belongs to a namesapce that does not match the affinity of that IPPool. ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none> Multus affinity When creating multiple network interfaces for an application, we can specify the affinity of multus net-attach-def instance for the cluster-level default pool . This way is simpler compared to explicitly specifying the binding relationship between network interfaces and IPPool resources through the ipam.spidernet.io/ippools annotation. First, configure various properties for the IPPool resource, including: Set the spec.default field to true to simplify the experience by reducing the need to annotate the application with ipam.spidernet.io/ippool or ipam.spidernet.io/ippools . Configure the spec.multusName field to specify the multus net-attach-def instance. (If you do not specify the namespace of the corresponding multus net-attach-def instance, we will default to the namespace where Spiderpool is installed.) apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 Create an application with multiple network interfaces, you can use the following example YAML: v1.multus-cni.io/default-network : Choose the default network configuration for the created application. (If you don't specify this annotation and directly use the clusterNetwork configuration of the multus, please specify the default network configuration during the installation of Spiderpool via Helm using the parameter --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ). k8s.v1.cni.cncf.io/networks : Selects the additional network configuration for the created application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF","title":"IPAM of IPPool Affinity"},{"location":"usage/spider-affinity/#spiderippool-affinity","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderIPPool Affinity"},{"location":"usage/spider-affinity/#introduction","text":"SpiderIPPool is a representation of a collection of IP addresses. It allows storing different IP addresses from the same subnet in separate IPPool instances, ensuring that there is no overlap between address sets. This design provides flexibility in managing IP resources within the underlay network, especially when faced with limited availability. SpiderIPPool offers the ability to assign different SpiderIPPool instances to various applications and tenants through affinity rules, allowing for both shared subnet usage and micro-isolation.","title":"Introduction"},{"location":"usage/spider-affinity/#quick-start","text":"In SpiderIPPool CRD , we defined lots of properties to use with affinities: spec.podAffinity controls whether the pool can be used by the Pod. spec.namespaceName and spec.namespaceAffinity verify if they match the Namespace of the Pod. If there is no match, the pool cannot be used. ( namespaceName takes precedence over namespaceAffinity ). spec.nodeName and spec.nodeAffinity verify if they match the node where the Pod is located. If there is no match, the pool cannot be used. ( nodeName takes precedence over nodeAffinity ). multusName determines whether the current network card which using the pool matches the CNI configuration used by the multus net-attach-def resource. If there is no match, the pool cannot be used. These fields not only serve as filters but also have a sorting effect . The more matching fields there are, the higher priority the IP pool has for usage.","title":"Quick Start"},{"location":"usage/spider-affinity/#application-affinity","text":"Firewalls are commonly used in clusters to manage communication between internal and external networks (north-south communication). To enforce secure access control, firewalls inspect and filter communication traffic while restricting outbound communication. In order to align with firewall policies and enable north-south communication within the underlay network, certain Deployments require all Pods to be assigned IP addresses within a specific range. Existing community solutions rely on annotations to handle IP address allocation for such cases. However, this approach has limitations: Manual modification of annotations becomes necessary as the application scales, leading to potential errors. IP management through annotations is far apart from the IPPool CR mechanism, resulting in a lack of visibility into available IP addresses. Conflicting IP addresses can easily be assigned to different applications, causing deployment failures. Spiderpool addresses these challenges by leveraging the flexibility of IPPools, where IP address collections can be adjusted. By combining this with the podAffinity setting in the SpiderIPPool CR, Spiderpool enables the binding of specific applications or groups of applications to particular IPPools. This ensures a unified approach to IP management, decouples application scaling from IP address scaling, and provides a fixed IP usage range for each application.","title":"Application Affinity"},{"location":"usage/spider-affinity/#create-ippool-with-application-affinity","text":"SpiderIPPool provides the podAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the Pods' selector.matchLabels match the specified podAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. Based on the above, using the following Yaml, create the following SpiderIPPool with application affinity, which will provide the IP address for the app: test-app-3 Pod's eligible selector.matchLabel . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-pod-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.151-10.6.168.160 podAffinity: matchLabels: app: test-app-3 EOF Creating Applications with Specific matchLabels. In the example YAML provided, a group of Deployment applications is created. The configuration includes: ipam.spidernet.io/ippool : specify the IP pool with application affinity. v1.multus-cni.io/default-network : create a default network interface for the application. matchLabels : set the label for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-3 spec: replicas: 1 selector: matchLabels: app: test-app-3 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-3 spec: containers: - name: test-app-3 image: nginx imagePullPolicy: IfNotPresent EOF After creating the application, the Pods with matchLabels that match the IPPool's application affinity successfully obtain IP addresses from that SpiderIPPool. The assigned IP addresses remain within the IP pool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-pod-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-3 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-3-6994b9d5bb-qpf5p 1 /1 Running 0 52s 10 .6.168.154 node2 <none> <none> However, when creating another application with different matchLabels that do not meet the IPPool's application affinity, Spiderpool will reject IP address allocation. matchLabels : set the label of the application to test-unmatch-labels , which does not match IPPool affinity. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-unmatch-labels spec: replicas: 1 selector: matchLabels: app: test-unmatch-labels template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-pod-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-unmatch-labels spec: containers: - name: test-unmatch-labels image: nginx imagePullPolicy: IfNotPresent EOF Getting an IP address assignment fails as expected when the Pod's matchLabels do not match the application affinity for that IPPool. kubectl get po -l app = test-unmatch-labels -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-unmatch-labels-699755574-9ncp7 0 /1 ContainerCreating 0 16s <none> node1 <none> <none>","title":"Create IPPool with Application Affinity"},{"location":"usage/spider-affinity/#shared-ippool","text":"Create an IPPool kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : shared-static-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.44-172.18.41.47 Create two Deployment whose Pods are setting the Pod annotation ipam.spidernet.io/ippool to explicitly specify the pool selection rule. It will succeed to get IP address. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/ippool-affinity-pod/shared-static-ippool-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-1 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-1 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : shared-static-ippool-deploy-2 spec : replicas : 2 selector : matchLabels : app : static template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"shared-static-ipv4-ippool\"] } labels : app : static spec : containers : - name : shared-static-ippool-deploy-2 image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] The Pods are running. ```bash kubectl get po -l app=static -o wide NAME READY STATUS RESTARTS AGE IP NODE shared-static-ippool-deploy-1-8588c887cb-gcbjb 1/1 Running 0 62s 172.18.41.45 spider-control-plane shared-static-ippool-deploy-1-8588c887cb-wfdvt 1/1 Running 0 62s 172.18.41.46 spider-control-plane shared-static-ippool-deploy-2-797c8df6cf-6vllv 1/1 Running 0 62s 172.18.41.44 spider-worker shared-static-ippool-deploy-2-797c8df6cf-ftk2d 1/1 Running 0 62s 172.18.41.47 spider-worker ```","title":"Shared IPPool"},{"location":"usage/spider-affinity/#node-affinity","text":"Nodes in a cluster might have access to different IP ranges. Some scenarios include: Nodes in the same data center belonging to different subnets. Nodes spanning multiple data centers within a single cluster. In such cases, replicas of an application are scheduled on different nodes require IP addresses from different subnets. Current community solutions are limited to satisfy this needs. To address this problem, Spiderpool support node affinity solution. By setting the nodeAffinity and nodeName fields in the SpiderIPPool CR, administrators can define a node label selector. This enables the IPAM plugin to allocate IP addresses from the specified IPPool when Pods are scheduled on nodes that match the affinity rules.","title":"Node Affinity"},{"location":"usage/spider-affinity/#ippool-with-node-affinity","text":"SpiderIPPool offers the nodeAffinity field. When a Pod is scheduled on a node and attempts to allocate an IP address from the SpiderIPPool, it can successfully obtain an IP if the node satisfies the specified nodeAffinity condition. Otherwise, it will be unable to allocate an IP address from that SpiderIPPool. To create a SpiderIPPool with node affinity, use the following YAML configuration. This SpiderIPPool will provide IP addresses for Pods running on the designated node. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-node1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 nodeAffinity: matchExpressions: - {key: kubernetes.io/hostname, operator: In, values: [node1]} EOF SpiderIPPool provides an additional option for node affinity: nodeName . When nodeName is specified, a Pod is scheduled on a specific node and attempts to allocate an IP address from the SpiderIPPool. If the node matches the specified nodeName , the IP address can be successfully allocated from that SpiderIPPool. If not, it will be unable to allocate an IP address from that SpiderIPPool. When nodeName is left empty, Spiderpool does not impose any allocation restrictions on Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-node1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.101-10.6.168.110 nodeName : - node1 create an Application ipam.spidernet.io/ippool : specify the IP pool with node affinity. v1.multus-cni.io/default-network : Identifies the IP pool used by the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-app-1 labels: app: test-app-1 spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-node1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF After creating an application, it can be observed that IP addresses are only allocated from the corresponding IPPool if the Pod's node matches the IPPool's node affinity. The IP address of the application remains within the assigned IPPool. \uff5e# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-node1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-1 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-2cmnz 0 /1 ContainerCreating 0 115s <none> node2 <none> <none> test-app-1-br5gw 0 /1 ContainerCreating 0 115s <none> master <none> <none> test-app-1-dvhrx 1 /1 Running 0 115s 10 .6.168.108 node1 <none> <none>","title":"IPPool with Node Affinity"},{"location":"usage/spider-affinity/#ippool-with-namespace-affinity","text":"Cluster administrators often partition their clusters into multiple namespaces to improve isolation, management, collaboration, security, and resource utilization. When deploying applications under different namespaces, it becomes necessary to assign specific IPPools to each namespace, preventing applications from unrelated namespaces from using them. Spiderpool addresses this requirement by introducing the namespaceAffinity or namespaceName fields in the SpiderIPPool CR. This allows administrators to define affinity rules between IPPools and one or more namespaces, ensuring that only applications meeting the specified conditions can be allocated IP addresses from the respective IPPools.","title":"IPPool with Namespace Affinity"},{"location":"usage/spider-affinity/#create-ippool-with-namespace-affinity","text":"~# kubectl create ns test-ns1 namespace/test-ns1 created ~# kubectl create ns test-ns2 namespace/test-ns2 created To create an IPPool with namaspace affinity, use the following YAML: SpiderIPPool provides the namespaceAffinity field. When an application is created and attempts to allocate an IP address from the SpiderIPPool, it will only succeed if the Pod's namespace matches the specified namespaceAffinity. Otherwise, IP allocation from that SpiderIPPool will be denied. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ns1-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.111-10.6.168.120 namespaceAffinity: matchLabels: kubernetes.io/metadata.name: test-ns1 EOF SpiderIPPool also offers another option for namespace affinity: namespaceName . When namespaceName is not empty, a Pod is created and attempts to allocate an IP address from the SpiderIPPool. If the namespace of the Pod matches the specified namespaceName , it can successfully obtain an IP from that SpiderIPPool. However, if the namespace does not match the namespaceName , it will be unable to allocate an IP address from that SpiderIPPool. When namespaceName is empty, Spiderpool does not impose any restrictions on IP allocation for Pods. For example: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ns1-ippool spec : subnet : 10.6.0.0/16 ips : - 10.6.168.111-10.6.168.120 namespaceName : - test-ns1 Create Applications in a Specified Namespace. In the provided YAML example, a group of Deployment applications is created under the test-ns1 namespace. The configuration includes: ipam.spidernet.io/ippool \uff1aspecify the IP pool with tenant affinity. v1.multus-cni.io/default-network : create a default network interface for the application. namespace : the namespace where the application resides. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 namespace: test-ns1 spec: replicas: 1 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent EOF After creating the application, the Pods within the designated namespace successfully allocate IP addresses from the associated IPPool with namespace affinity. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ns1-ippool 4 10 .6.0.0/16 1 10 false ~# kubectl get po -l app = test-app-2 -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns1 test-app-2-975d9f-6bww2 1 /1 Running 0 44s 10 .6.168.111 node2 <none> <none> However, if an application is created outside the test-ns1 namespace, Spiderpool will reject IP address allocation, preventing unrelated namespace from using that IPPool. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-other-ns namespace: test-ns2 spec: replicas: 1 selector: matchLabels: app: test-other-ns template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ns1-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-other-ns spec: containers: - name: test-other-ns image: nginx imagePullPolicy: IfNotPresent EOF Getting an IP address assignment fails as expected when the Pod belongs to a namesapce that does not match the affinity of that IPPool. ~# kubectl get po -l app = test-other-ns -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-ns2 test-other-ns-56cc9b7d95-hx4b5 0 /1 ContainerCreating 0 6m3s <none> node2 <none> <none>","title":"Create IPPool with Namespace Affinity"},{"location":"usage/spider-affinity/#multus-affinity","text":"When creating multiple network interfaces for an application, we can specify the affinity of multus net-attach-def instance for the cluster-level default pool . This way is simpler compared to explicitly specifying the binding relationship between network interfaces and IPPool resources through the ipam.spidernet.io/ippools annotation. First, configure various properties for the IPPool resource, including: Set the spec.default field to true to simplify the experience by reducing the need to annotate the application with ipam.spidernet.io/ippool or ipam.spidernet.io/ippools . Configure the spec.multusName field to specify the multus net-attach-def instance. (If you do not specify the namespace of the corresponding multus net-attach-def instance, we will default to the namespace where Spiderpool is installed.) apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth0 spec : default : true subnet : 10.6.0.0/16 ips : - 10.6.168.151-10.6.168.160 multusName : - default/macvlan-vlan0-eth0 --- apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : test-ippool-eth1 spec : default : true subnet : 10.7.0.0/16 ips : - 10.7.168.151-10.7.168.160 multusName : - kube-system/macvlan-vlan0-eth1 Create an application with multiple network interfaces, you can use the following example YAML: v1.multus-cni.io/default-network : Choose the default network configuration for the created application. (If you don't specify this annotation and directly use the clusterNetwork configuration of the multus, please specify the default network configuration during the installation of Spiderpool via Helm using the parameter --set multus.multusCNI.defaultCniCRName=default/macvlan-vlan0-eth0 ). k8s.v1.cni.cncf.io/networks : Selects the additional network configuration for the created application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app namespace: default spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: annotations: v1.multus-cni.io/default-network: default/macvlan-vlan0-eth0 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-vlan0-eth1 labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent EOF","title":"Multus affinity"},{"location":"usage/spider-ippool-zh_CN/","text":"SpiderIPPool \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderIPPool \u8d44\u6e90\u4ee3\u8868 Spiderpool \u4e3a Pod \u5206\u914d IP \u7684 IP \u5730\u5740\u8303\u56f4\u3002 \u8bf7\u53c2\u7167 SpiderIPPool CRD \u4e3a\u4f60\u7684\u96c6\u7fa4\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002 SpiderIPPool \u529f\u80fd \u5355\u53cc\u6808\u4ee5\u53ca IPv6 \u652f\u6301 IP \u5730\u5740\u8303\u56f4\u63a7\u5236 \u7f51\u5173\u8def\u7531\u63a7\u5236 \u4ec5\u7528\u4ee5\u53ca\u5168\u5c40\u7f3a\u7701\u6c60\u63a7\u5236 \u642d\u914d\u5404\u79cd\u8d44\u6e90\u4eb2\u548c\u6027\u4f7f\u7528\u63a7\u5236 \u4f7f\u7528\u4ecb\u7ecd \u5355\u53cc\u6808\u63a7\u5236 Spiderpool \u652f\u6301 IPv4-only, IPv6-only, \u53cc\u6808\u8fd9\u4e09\u79cd IP \u5730\u5740\u5206\u914d\u65b9\u5f0f\uff0c\u53ef\u901a\u8fc7 configmap \u914d\u7f6e\u6765\u63a7\u5236\u3002 \u901a\u8fc7 Helm \u5b89\u88c5\u65f6\u53ef\u914d\u7f6e\u53c2\u6570\u6765\u6307\u5b9a\uff1a --set ipam.enableIPv4=true --set ipam.enableIPv6=true \u3002 \u5f53\u6211\u4eec Spiderpool \u73af\u5883\u5f00\u542f\u53cc\u6808\u914d\u7f6e\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u6307\u5b9a\u4f7f\u7528\u54ea\u4e9b IPv4 \u548c IPv6 \u6c60\u6765\u5206\u914d IP \u5730\u5740\uff1a \u5728\u53cc\u6808\u73af\u5883\u4e0b\uff0c\u4f60\u4e5f\u53ef\u4e3a Pod \u53ea\u5206\u914d IPv4/IPv6 \u7684IP\uff0c\u5982: ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] \u6307\u5b9a IPPool \u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740 \u5bf9\u4e8e\u4ee5\u4e0b\u6307\u5b9a\u4f7f\u7528 SpiderIPPool \u89c4\u5219\u7684\u4f18\u5148\u7ea7\uff0c\u8bf7\u53c2\u8003 IP \u5019\u9009\u6c60\u89c4\u5219 \u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60 \u6211\u4eec\u53ef\u501f\u52a9\u6ce8\u89e3 ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6807\u8bb0\u5728 Pod \u7684 Annotation\u4e0a\u6765\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b IP \u6c60, \u6ce8\u89e3 ipam.spidernet.io/ippools \u591a\u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u3002\u6b64\u5916\u6211\u4eec\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a IP \u6c60\u4ee5\u4f9b\u5907\u9009\uff0c\u5f53\u67d0\u4e2a\u6c60\u7684 IP \u88ab\u7528\u5b8c\u540e\uff0c\u53ef\u7ee7\u7eed\u4ece\u4f60\u6307\u5b9a\u7684\u5176\u4ed6\u6c60\u4e2d\u5206\u914d\u5730\u5740\u3002 ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\"] } \u5728\u4f7f\u7528\u6ce8\u89e3 ipam.spidernet.io/ippools \u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u65f6\uff0c\u4f60\u53ef\u663e\u5f0f\u7684\u901a\u8fc7\u6307\u5b9a interface \u5b57\u6bb5\u6807\u660e\u7f51\u5361\u540d\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7**\u6570\u7ec4\u987a\u5e8f\u6392\u5217**\u8ba9\u7b2c\u51e0\u5f20\u5361\u7528\u54ea\u4e9b IP \u6c60\u3002\u53e6\u5916\uff0c\u5b57\u6bb5 cleangateway \u6807\u660e\u662f\u5426\u9700\u8981\u6839\u636e IP \u6c60\u4e2d\u7684 gateway \u5b57\u6bb5\u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff0c\u5f53 cleangateway \u4e3a true \u6807\u660e\u65e0\u9700\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002(\u9ed8\u8ba4\u4e3afalse) \u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0c\u4e00\u822c\u65e0\u6cd5\u4e3a\u8def\u7531\u8868 main \u8868\u751f\u6210\u4e24\u6761\u53ca\u4ee5\u4e0a\u7684\u9ed8\u8ba4\u8def\u7531\u3002\u642d\u914d Coordinator \u63d2\u4ef6\u53ef\u4e3a\u4f60\u5904\u7406\u597d\u8be5\u95ee\u9898\uff0c\u56e0\u6b64\u4f60\u53ef\u4ee5\u5ffd\u7565 cleangateway \u5b57\u6bb5\u3002\u6216\u8005\u5728\u5355\u72ec\u4f7f\u7528 Spiderpool IPAM \u63d2\u4ef6\u65f6\uff0c\u53ef\u501f\u52a9 cleangateway: true \u6765\u6807\u660e\u4e0d\u6839\u636e IP \u6c60 gateway \u5b57\u6bb5\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002 ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], },{ \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] \u4f7f\u7528 Namespace \u6ce8\u89e3\u6307\u5b9a\u6c60 \u6211\u4eec\u53ef\u4ee5\u4e3a Namespace \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u548c ipam.spidernet.io/default-ipv6-ippool , \u5f53\u5e94\u7528\u90e8\u7f72\u65f6\uff0c\u53ef\u4ece\u5e94\u7528\u6240\u5728 Namespace \u7684\u6ce8\u89e3\u4e2d\u9009\u62e9 IP \u6c60\u4f7f\u7528\uff1a \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\u65f6\uff0c\u4f18\u5148\u4f7f\u7528\u6b64\u5904 Namespace \u6ce8\u89e3\u89c4\u5219\u3002 apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' name : kube-system ... \u4f7f\u7528 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60 \u6211\u4eec\u53ef\u4ee5\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6307\u5b9a\u7f3a\u7701\u7684 IPv4 \u548c IPv6 \u6c60\u4ee5\u4f9b\u5e94\u7528\u9009\u62e9\u8be5 CNI \u914d\u7f6e\u65f6\u4f7f\u7528\uff0c\u5177\u4f53\u53ef\u53c2\u7167 CNI\u914d\u7f6e \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\uff0c\u4e14\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u5c06\u4f18\u5148\u4f7f\u7528\u6b64\u5904 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60\u89c4\u5219\u3002 { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" ] } } \u4e3a SpiderIPPool \u8bbe\u7f6e\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b \u5728 SpiderIPPool CRD \u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230 spec.default \u5b57\u6bb5\u662f\u4e00\u4e2a bool \u7c7b\u578b\uff0c\u5f53\u6211\u4eec\u6ca1\u6709\u901a\u8fc7 Annotation \u6216 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a IPPool \u65f6\uff0c\u7cfb\u7edf\u4f1a\u6839\u636e\u8be5\u5b57\u6bb5\u6311\u9009\u51fa\u96c6\u7fa4\u9ed8\u8ba4\u6c60\u4f7f\u7528: \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528IP\u6c60\uff0c\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u4e14\u672a\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a IP \u6c60\u65f6\uff0c\u6b64\u5904\u4f1a\u751f\u6548\u3002 \u53ef\u4e3a\u591a\u4e2a IPPool \u8d44\u6e90\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ... SpiderIPPool \u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528 \u5177\u4f53\u8bf7\u53c2\u8003 IP \u6c60\u4eb2\u548c\u6027\u642d\u914d SpiderIPPool \u7f51\u5173\u4e0e\u8def\u7531\u914d\u7f6e \u5177\u4f53\u8bf7\u53c2\u8003 \u8def\u7531\u529f\u80fd \u56e0\u6b64 Pod \u4f1a\u62ff\u5230\u57fa\u4e8e\u7f51\u5173\u7684\u9ed8\u8ba4\u8def\u7531\uff0c\u4ee5\u53ca\u6b64 IP \u6c60\u4e0a\u7684\u81ea\u5b9a\u4e49\u8def\u7531\u3002(\u82e5 IP \u6c60\u4e0d\u8bbe\u7f6e\u7f51\u5173\uff0c\u5219\u4e0d\u4f1a\u751f\u6548\u9ed8\u8ba4\u8def\u7531) \u547d\u4ee4\u884c\u5de5\u4f5c (kubectl) \u67e5\u770b\u6269\u5c55\u5b57\u6bb5 \u4e3a\u4e86\u66f4\u7b80\u5355\u65b9\u4fbf\u7684\u67e5\u770b SpiderIPPool \u8d44\u6e90\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u6211\u4eec\u8865\u5145\u4e86\u4e00\u4e9b\u6269\u5c55\u5b57\u6bb5\u53ef\u8ba9\u7528\u6237\u901a\u8fc7 kubectl get sp -o wide \u67e5\u770b: ALLOCATED-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u5df2\u5206\u914d\u7684 IP \u6570\u91cf TOTAL-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u7684\u603b IP \u6570\u91cf DEFAULT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b DISABLE \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u88ab\u7981\u7528 NODENAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684\u8282\u70b9 MULTUSNAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684 multus \u5b9e\u4f8b APP-NAMESPACE \u5b57\u6bb5\u5c5e\u4e8e SpiderSubnet \u529f\u80fd\u72ec\u6709\uff0c\u8868\u660e\u8be5\u6c60\u662f\u4e00\u4e2a\u7cfb\u7edf\u81ea\u52a8\u521b\u5efa\u7684\u6c60\uff0c\u540c\u65f6\u8be5\u5b57\u6bb5\u8868\u660e\u5176\u5bf9\u5e94\u5e94\u7528\u7684\u547d\u540d\u7a7a\u95f4\u3002 ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ] \u6307\u6807(metric) \u6211\u4eec\u4e5f\u4e3a SpiderIPPool \u8d44\u6e90\u8865\u5145\u4e86\u76f8\u5173\u7684\u6307\u6807\u4fe1\u606f\uff0c\u8be6\u60c5\u8bf7\u770b metric","title":"SpiderIPPool"},{"location":"usage/spider-ippool-zh_CN/#spiderippool","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderIPPool"},{"location":"usage/spider-ippool-zh_CN/#_1","text":"SpiderIPPool \u8d44\u6e90\u4ee3\u8868 Spiderpool \u4e3a Pod \u5206\u914d IP \u7684 IP \u5730\u5740\u8303\u56f4\u3002 \u8bf7\u53c2\u7167 SpiderIPPool CRD \u4e3a\u4f60\u7684\u96c6\u7fa4\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_1","text":"\u5355\u53cc\u6808\u4ee5\u53ca IPv6 \u652f\u6301 IP \u5730\u5740\u8303\u56f4\u63a7\u5236 \u7f51\u5173\u8def\u7531\u63a7\u5236 \u4ec5\u7528\u4ee5\u53ca\u5168\u5c40\u7f3a\u7701\u6c60\u63a7\u5236 \u642d\u914d\u5404\u79cd\u8d44\u6e90\u4eb2\u548c\u6027\u4f7f\u7528\u63a7\u5236","title":"SpiderIPPool \u529f\u80fd"},{"location":"usage/spider-ippool-zh_CN/#_2","text":"","title":"\u4f7f\u7528\u4ecb\u7ecd"},{"location":"usage/spider-ippool-zh_CN/#_3","text":"Spiderpool \u652f\u6301 IPv4-only, IPv6-only, \u53cc\u6808\u8fd9\u4e09\u79cd IP \u5730\u5740\u5206\u914d\u65b9\u5f0f\uff0c\u53ef\u901a\u8fc7 configmap \u914d\u7f6e\u6765\u63a7\u5236\u3002 \u901a\u8fc7 Helm \u5b89\u88c5\u65f6\u53ef\u914d\u7f6e\u53c2\u6570\u6765\u6307\u5b9a\uff1a --set ipam.enableIPv4=true --set ipam.enableIPv6=true \u3002 \u5f53\u6211\u4eec Spiderpool \u73af\u5883\u5f00\u542f\u53cc\u6808\u914d\u7f6e\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u6307\u5b9a\u4f7f\u7528\u54ea\u4e9b IPv4 \u548c IPv6 \u6c60\u6765\u5206\u914d IP \u5730\u5740\uff1a \u5728\u53cc\u6808\u73af\u5883\u4e0b\uff0c\u4f60\u4e5f\u53ef\u4e3a Pod \u53ea\u5206\u914d IPv4/IPv6 \u7684IP\uff0c\u5982: ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ]","title":"\u5355\u53cc\u6808\u63a7\u5236"},{"location":"usage/spider-ippool-zh_CN/#ippool-ip","text":"\u5bf9\u4e8e\u4ee5\u4e0b\u6307\u5b9a\u4f7f\u7528 SpiderIPPool \u89c4\u5219\u7684\u4f18\u5148\u7ea7\uff0c\u8bf7\u53c2\u8003 IP \u5019\u9009\u6c60\u89c4\u5219","title":"\u6307\u5b9a IPPool \u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740"},{"location":"usage/spider-ippool-zh_CN/#pod-annotation-ip","text":"\u6211\u4eec\u53ef\u501f\u52a9\u6ce8\u89e3 ipam.spidernet.io/ippool \u6216 ipam.spidernet.io/ippools \u6807\u8bb0\u5728 Pod \u7684 Annotation\u4e0a\u6765\u6307\u5b9a Pod \u4f7f\u7528\u54ea\u4e9b IP \u6c60, \u6ce8\u89e3 ipam.spidernet.io/ippools \u591a\u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u3002\u6b64\u5916\u6211\u4eec\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a IP \u6c60\u4ee5\u4f9b\u5907\u9009\uff0c\u5f53\u67d0\u4e2a\u6c60\u7684 IP \u88ab\u7528\u5b8c\u540e\uff0c\u53ef\u7ee7\u7eed\u4ece\u4f60\u6307\u5b9a\u7684\u5176\u4ed6\u6c60\u4e2d\u5206\u914d\u5730\u5740\u3002 ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\"] } \u5728\u4f7f\u7528\u6ce8\u89e3 ipam.spidernet.io/ippools \u7528\u4e8e\u591a\u7f51\u5361\u6307\u5b9a\u65f6\uff0c\u4f60\u53ef\u663e\u5f0f\u7684\u901a\u8fc7\u6307\u5b9a interface \u5b57\u6bb5\u6807\u660e\u7f51\u5361\u540d\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7**\u6570\u7ec4\u987a\u5e8f\u6392\u5217**\u8ba9\u7b2c\u51e0\u5f20\u5361\u7528\u54ea\u4e9b IP \u6c60\u3002\u53e6\u5916\uff0c\u5b57\u6bb5 cleangateway \u6807\u660e\u662f\u5426\u9700\u8981\u6839\u636e IP \u6c60\u4e2d\u7684 gateway \u5b57\u6bb5\u751f\u6210\u4e00\u6761\u9ed8\u8ba4\u8def\u7531\uff0c\u5f53 cleangateway \u4e3a true \u6807\u660e\u65e0\u9700\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002(\u9ed8\u8ba4\u4e3afalse) \u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0c\u4e00\u822c\u65e0\u6cd5\u4e3a\u8def\u7531\u8868 main \u8868\u751f\u6210\u4e24\u6761\u53ca\u4ee5\u4e0a\u7684\u9ed8\u8ba4\u8def\u7531\u3002\u642d\u914d Coordinator \u63d2\u4ef6\u53ef\u4e3a\u4f60\u5904\u7406\u597d\u8be5\u95ee\u9898\uff0c\u56e0\u6b64\u4f60\u53ef\u4ee5\u5ffd\u7565 cleangateway \u5b57\u6bb5\u3002\u6216\u8005\u5728\u5355\u72ec\u4f7f\u7528 Spiderpool IPAM \u63d2\u4ef6\u65f6\uff0c\u53ef\u501f\u52a9 cleangateway: true \u6765\u6807\u660e\u4e0d\u6839\u636e IP \u6c60 gateway \u5b57\u6bb5\u751f\u6210\u9ed8\u8ba4\u8def\u7531\u3002 ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], },{ \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }]","title":"\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60"},{"location":"usage/spider-ippool-zh_CN/#namespace","text":"\u6211\u4eec\u53ef\u4ee5\u4e3a Namespace \u6253\u4e0a\u6ce8\u89e3 ipam.spidernet.io/default-ipv4-ippool \u548c ipam.spidernet.io/default-ipv6-ippool , \u5f53\u5e94\u7528\u90e8\u7f72\u65f6\uff0c\u53ef\u4ece\u5e94\u7528\u6240\u5728 Namespace \u7684\u6ce8\u89e3\u4e2d\u9009\u62e9 IP \u6c60\u4f7f\u7528\uff1a \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\u65f6\uff0c\u4f18\u5148\u4f7f\u7528\u6b64\u5904 Namespace \u6ce8\u89e3\u89c4\u5219\u3002 apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' name : kube-system ...","title":"\u4f7f\u7528 Namespace \u6ce8\u89e3\u6307\u5b9a\u6c60"},{"location":"usage/spider-ippool-zh_CN/#cni","text":"\u6211\u4eec\u53ef\u4ee5\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6307\u5b9a\u7f3a\u7701\u7684 IPv4 \u548c IPv6 \u6c60\u4ee5\u4f9b\u5e94\u7528\u9009\u62e9\u8be5 CNI \u914d\u7f6e\u65f6\u4f7f\u7528\uff0c\u5177\u4f53\u53ef\u53c2\u7167 CNI\u914d\u7f6e \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528 IP \u6c60\uff0c\u4e14\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u5c06\u4f18\u5148\u4f7f\u7528\u6b64\u5904 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60\u89c4\u5219\u3002 { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" ] } }","title":"\u4f7f\u7528 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a\u6c60"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_2","text":"\u5728 SpiderIPPool CRD \u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230 spec.default \u5b57\u6bb5\u662f\u4e00\u4e2a bool \u7c7b\u578b\uff0c\u5f53\u6211\u4eec\u6ca1\u6709\u901a\u8fc7 Annotation \u6216 CNI \u914d\u7f6e\u6587\u4ef6\u6307\u5b9a IPPool \u65f6\uff0c\u7cfb\u7edf\u4f1a\u6839\u636e\u8be5\u5b57\u6bb5\u6311\u9009\u51fa\u96c6\u7fa4\u9ed8\u8ba4\u6c60\u4f7f\u7528: \u672a\u4f7f\u7528 Pod Annotation \u6307\u5b9a\u4f7f\u7528IP\u6c60\uff0c\u6ca1\u6709\u901a\u8fc7 Namespace \u6ce8\u89e3\u6307\u5b9a IP \u6c60\u65f6\uff0c\u4e14\u672a\u5728 CNI \u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a IP \u6c60\u65f6\uff0c\u6b64\u5904\u4f1a\u751f\u6548\u3002 \u53ef\u4e3a\u591a\u4e2a IPPool \u8d44\u6e90\u8bbe\u7f6e\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b\u3002 apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ...","title":"\u4e3a SpiderIPPool \u8bbe\u7f6e\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_3","text":"\u5177\u4f53\u8bf7\u53c2\u8003 IP \u6c60\u4eb2\u548c\u6027\u642d\u914d","title":"SpiderIPPool \u642d\u914d\u4eb2\u548c\u6027\u4f7f\u7528"},{"location":"usage/spider-ippool-zh_CN/#spiderippool_4","text":"\u5177\u4f53\u8bf7\u53c2\u8003 \u8def\u7531\u529f\u80fd \u56e0\u6b64 Pod \u4f1a\u62ff\u5230\u57fa\u4e8e\u7f51\u5173\u7684\u9ed8\u8ba4\u8def\u7531\uff0c\u4ee5\u53ca\u6b64 IP \u6c60\u4e0a\u7684\u81ea\u5b9a\u4e49\u8def\u7531\u3002(\u82e5 IP \u6c60\u4e0d\u8bbe\u7f6e\u7f51\u5173\uff0c\u5219\u4e0d\u4f1a\u751f\u6548\u9ed8\u8ba4\u8def\u7531)","title":"SpiderIPPool \u7f51\u5173\u4e0e\u8def\u7531\u914d\u7f6e"},{"location":"usage/spider-ippool-zh_CN/#kubectl","text":"\u4e3a\u4e86\u66f4\u7b80\u5355\u65b9\u4fbf\u7684\u67e5\u770b SpiderIPPool \u8d44\u6e90\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u6211\u4eec\u8865\u5145\u4e86\u4e00\u4e9b\u6269\u5c55\u5b57\u6bb5\u53ef\u8ba9\u7528\u6237\u901a\u8fc7 kubectl get sp -o wide \u67e5\u770b: ALLOCATED-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u5df2\u5206\u914d\u7684 IP \u6570\u91cf TOTAL-IP-COUNT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u7684\u603b IP \u6570\u91cf DEFAULT \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u7ea7\u522b DISABLE \u5b57\u6bb5\u8868\u793a\u8be5\u6c60\u662f\u5426\u88ab\u7981\u7528 NODENAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684\u8282\u70b9 MULTUSNAME \u5b57\u6bb5\u8868\u793a\u4e0e\u8be5\u6c60\u4eb2\u548c\u7684 multus \u5b9e\u4f8b APP-NAMESPACE \u5b57\u6bb5\u5c5e\u4e8e SpiderSubnet \u529f\u80fd\u72ec\u6709\uff0c\u8868\u660e\u8be5\u6c60\u662f\u4e00\u4e2a\u7cfb\u7edf\u81ea\u52a8\u521b\u5efa\u7684\u6c60\uff0c\u540c\u65f6\u8be5\u5b57\u6bb5\u8868\u660e\u5176\u5bf9\u5e94\u5e94\u7528\u7684\u547d\u540d\u7a7a\u95f4\u3002 ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ]","title":"\u547d\u4ee4\u884c\u5de5\u4f5c (kubectl) \u67e5\u770b\u6269\u5c55\u5b57\u6bb5"},{"location":"usage/spider-ippool-zh_CN/#metric","text":"\u6211\u4eec\u4e5f\u4e3a SpiderIPPool \u8d44\u6e90\u8865\u5145\u4e86\u76f8\u5173\u7684\u6307\u6807\u4fe1\u606f\uff0c\u8be6\u60c5\u8bf7\u770b metric","title":"\u6307\u6807(metric)"},{"location":"usage/spider-ippool/","text":"SpiderIPPool English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction SpiderIPPool resources represent the IP address ranges allocated by Spiderpool for Pods. To create SpiderIPPool resources in your cluster, refer to the SpiderIPPool CRD . SpiderIPPool Features Single-stack, dual-stack, and IPv6 Support IP address range control Gateway route control Exclusive or global default pool control Compatible with various resource affinity settings Usage Single-stack and Dual-stack Control Spiderpool supports three modes of IP address allocation: IPv4-only, IPv6-only, and dual-stack. Refer to configmap for details. When installing Spiderpool via Helm, you can use configuration parameters to specify: --set ipam.enableIPv4=true --set ipam.enableIPv6=true . If dual-stack mode is enabled, you can manually specify which IPv4 and IPv6 pools should be used for IP address allocation: In a dual-stack environment, you can also configure Pods to only receive IPv4 or IPv6 addresses using the annotation ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' . apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] Specify IPPool to Allocate IP Addresses to Applications For the priority rules when specifying the SpiderIPPool, refer to the Candidate Pool Acquisition . Use Pod Annotation to Specify IP Pool You can use annotations like ipam.spidernet.io/ippool or ipam.spidernet.io/ippools on a Pod's annotation to indicate which IP pools should be used. The ipam.spidernet.io/ippools annotation is primarily used for specifying multiple network interfaces. Additionally, you can specify multiple pools as fallback options. If one pool's IP addresses are exhausted, addresses can be allocated from the other specified pools. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\"] } When using the annotation ipam.spidernet.io/ippools for specifying multiple network interfaces, you can explicitly indicate the interface name by specifying the interface field. Alternatively, you can use array ordering to determine which IP pools are assigned to which network interfaces. Additionally, the cleangateway field indicates whether a default route should be generated based on the gateway field of the IPPool. When cleangateway is set to true, it means that no default route needs to be generated (default is false). In scenarios with multiple network interfaces, it is generally not possible to generate two or more default routes in the main routing table. The plugin Coordinator already solved this problem and you can ignore clengateway field. If you want to use Spiderpool IPAM plugin alone, you can use cleangateway: true to indicate that a default route should not be generated based on the IPPool gateway field. ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], },{ \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }] Use Namespace Annotation to Specify IP Pool You can annotate the Namespace with ipam.spidernet.io/default-ipv4-ippool and ipam.spidernet.io/default-ipv6-ippool . When deploying applications, IP pools can be selected based on these annotations of the application's namespace: If IP pool is not explicitly specified, rules defined in the Namespace annotation take precedence. apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' name : kube-system ... Use CNI Configuration File to Specify IP Pool You can specify the default IPv4 and IPv6 pools for an application in the CNI configuration file. For more details, refer to CNI Configuration If IP pool is not explicitly specified using Pod Annotation and no IP pool is specified through Namespace annotation, the rules defined in the CNI configuration file take precedence. { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" ] } } Set Cluster Default Level for SpiderIPPool In the SpiderIPPool CRD , the spec.default field is a boolean type. It determines the cluster default pool when no specific IPPool is specified through annotations or the CNI configuration file: If no IP pool is specified using Pod annotations, and no IP pool is specified through Namespace annotations, and no IP pool is specified in the CNI configuration file, the system will use the pool defined by this field as the cluster default. Multiple IPPool resources can be set as the cluster default level. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ... Use SpiderIPPool with Affinity Refer to SpiderIPPool Affinity for details. SpiderIPPool Gateway and Route Configuration Refer to Route Support for details. As a result, Pods will receive the default route based on the gateway, as well as custom routes defined in the IP pool. If the IP pool does not have a gateway configured, the default route will not take effect. View Extended Fields with Command Line (kubectl) To simplify the viewing of properties related to SpiderIPPool resources, we have added some additional fields that can be displayed using the kubectl get sp -o wide command: ALLOCATED-IP-COUNT represents the number of allocated IP addresses in the pool. TOTAL-IP-COUNT represents the total number of IP addresses in the pool. DEFAULT indicates whether the pool is set as the cluster default level. DISABLE indicates whether the pool is disabled. NODENAME indicates the nodes have an affinity with the pool. MULTUSNAME indicates the Multus instances have an affinity with the pool. APP-NAMESPACE is specific to the SpiderSubnet feature. It signifies that the pool is automatically created by the system and corresponds to the namespace of the associated application. ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ] Metrics We have also supplemented SpiderIPPool resources with relevant metric information. For more details, refer to metric","title":"IPAM of SpiderIPPool"},{"location":"usage/spider-ippool/#spiderippool","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderIPPool"},{"location":"usage/spider-ippool/#introduction","text":"SpiderIPPool resources represent the IP address ranges allocated by Spiderpool for Pods. To create SpiderIPPool resources in your cluster, refer to the SpiderIPPool CRD .","title":"Introduction"},{"location":"usage/spider-ippool/#spiderippool-features","text":"Single-stack, dual-stack, and IPv6 Support IP address range control Gateway route control Exclusive or global default pool control Compatible with various resource affinity settings","title":"SpiderIPPool Features"},{"location":"usage/spider-ippool/#usage","text":"","title":"Usage"},{"location":"usage/spider-ippool/#single-stack-and-dual-stack-control","text":"Spiderpool supports three modes of IP address allocation: IPv4-only, IPv6-only, and dual-stack. Refer to configmap for details. When installing Spiderpool via Helm, you can use configuration parameters to specify: --set ipam.enableIPv4=true --set ipam.enableIPv6=true . If dual-stack mode is enabled, you can manually specify which IPv4 and IPv6 pools should be used for IP address allocation: In a dual-stack environment, you can also configure Pods to only receive IPv4 or IPv6 addresses using the annotation ipam.spidernet.io/ippool: '{\"ipv4\": [\"custom-ipv4-ippool\"]}' . apiVersion : apps/v1 kind : Deployment metadata : name : custom-dual-ippool-deploy spec : replicas : 3 selector : matchLabels : app : custom-dual-ippool-deploy template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"],\"ipv6\": [\"custom-ipv6-ippool\"] } labels : app : custom-dual-ippool-deploy spec : containers : - name : custom-dual-ippool-deploy image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ]","title":"Single-stack and Dual-stack Control"},{"location":"usage/spider-ippool/#specify-ippool-to-allocate-ip-addresses-to-applications","text":"For the priority rules when specifying the SpiderIPPool, refer to the Candidate Pool Acquisition .","title":"Specify IPPool to Allocate IP Addresses to Applications"},{"location":"usage/spider-ippool/#use-pod-annotation-to-specify-ip-pool","text":"You can use annotations like ipam.spidernet.io/ippool or ipam.spidernet.io/ippools on a Pod's annotation to indicate which IP pools should be used. The ipam.spidernet.io/ippools annotation is primarily used for specifying multiple network interfaces. Additionally, you can specify multiple pools as fallback options. If one pool's IP addresses are exhausted, addresses can be allocated from the other specified pools. ipam.spidernet.io/ippool : |- { \"ipv4\": [\"demo-v4-ippool1\", \"backup-ipv4-ippool\"], \"ipv6\": [\"demo-v6-ippool1\", \"backup-ipv6-ippool\"] } When using the annotation ipam.spidernet.io/ippools for specifying multiple network interfaces, you can explicitly indicate the interface name by specifying the interface field. Alternatively, you can use array ordering to determine which IP pools are assigned to which network interfaces. Additionally, the cleangateway field indicates whether a default route should be generated based on the gateway field of the IPPool. When cleangateway is set to true, it means that no default route needs to be generated (default is false). In scenarios with multiple network interfaces, it is generally not possible to generate two or more default routes in the main routing table. The plugin Coordinator already solved this problem and you can ignore clengateway field. If you want to use Spiderpool IPAM plugin alone, you can use cleangateway: true to indicate that a default route should not be generated based on the IPPool gateway field. ipam.spidernet.io/ippools : |- [{ \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], },{ \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": true }] ipam.spidernet.io/ippools : |- [{ \"interface\": \"eth0\", \"ipv4\": [\"demo-v4-ippool1\"], \"ipv6\": [\"demo-v6-ippool1\"], \"cleangateway\": true },{ \"interface\": \"net1\", \"ipv4\": [\"demo-v4-ippool2\"], \"ipv6\": [\"demo-v6-ippool2\"], \"cleangateway\": false }]","title":"Use Pod Annotation to Specify IP Pool"},{"location":"usage/spider-ippool/#use-namespace-annotation-to-specify-ip-pool","text":"You can annotate the Namespace with ipam.spidernet.io/default-ipv4-ippool and ipam.spidernet.io/default-ipv6-ippool . When deploying applications, IP pools can be selected based on these annotations of the application's namespace: If IP pool is not explicitly specified, rules defined in the Namespace annotation take precedence. apiVersion : v1 kind : Namespace metadata : annotations : ipam.spidernet.io/default-ipv4-ippool : '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' ipam.spidernet.io/default-ipv6-ippool : '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' name : kube-system ...","title":"Use Namespace Annotation to Specify IP Pool"},{"location":"usage/spider-ippool/#use-cni-configuration-file-to-specify-ip-pool","text":"You can specify the default IPv4 and IPv6 pools for an application in the CNI configuration file. For more details, refer to CNI Configuration If IP pool is not explicitly specified using Pod Annotation and no IP pool is specified through Namespace annotation, the rules defined in the CNI configuration file take precedence. { \"name\" : \"macvlan-vlan0\" , \"type\" : \"macvlan\" , \"master\" : \"eth0\" , \"ipam\" : { \"type\" : \"spiderpool\" , \"default_ipv4_ippool\" :[ \"default-v4-ippool\" , \"backup-ipv4-ippool\" ], \"default_ipv6_ippool\" :[ \"default-v6-ippool\" , \"backup-ipv6-ippool\" ] } }","title":"Use CNI Configuration File to Specify IP Pool"},{"location":"usage/spider-ippool/#set-cluster-default-level-for-spiderippool","text":"In the SpiderIPPool CRD , the spec.default field is a boolean type. It determines the cluster default pool when no specific IPPool is specified through annotations or the CNI configuration file: If no IP pool is specified using Pod annotations, and no IP pool is specified through Namespace annotations, and no IP pool is specified in the CNI configuration file, the system will use the pool defined by this field as the cluster default. Multiple IPPool resources can be set as the cluster default level. apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : master-172 spec : default : true ...","title":"Set Cluster Default Level for SpiderIPPool"},{"location":"usage/spider-ippool/#use-spiderippool-with-affinity","text":"Refer to SpiderIPPool Affinity for details.","title":"Use SpiderIPPool with Affinity"},{"location":"usage/spider-ippool/#spiderippool-gateway-and-route-configuration","text":"Refer to Route Support for details. As a result, Pods will receive the default route based on the gateway, as well as custom routes defined in the IP pool. If the IP pool does not have a gateway configured, the default route will not take effect.","title":"SpiderIPPool Gateway and Route Configuration"},{"location":"usage/spider-ippool/#view-extended-fields-with-command-line-kubectl","text":"To simplify the viewing of properties related to SpiderIPPool resources, we have added some additional fields that can be displayed using the kubectl get sp -o wide command: ALLOCATED-IP-COUNT represents the number of allocated IP addresses in the pool. TOTAL-IP-COUNT represents the total number of IP addresses in the pool. DEFAULT indicates whether the pool is set as the cluster default level. DISABLE indicates whether the pool is disabled. NODENAME indicates the nodes have an affinity with the pool. MULTUSNAME indicates the Multus instances have an affinity with the pool. APP-NAMESPACE is specific to the SpiderSubnet feature. It signifies that the pool is automatically created by the system and corresponds to the namespace of the associated application. ~# kubectl get sp -o wide NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE NODENAME MULTUSNAME APP-NAMESPACE auto4-demo-deploy-subnet-eth0-fcca4 4 172 .100.0.0/16 1 2 false false kube-system test-pod-ippool 4 10 .6.0.0/16 0 10 false false [ \"master\" , \"worker1\" ] [ \"kube-system/macvlan-vlan0\" ]","title":"View Extended Fields with Command Line (kubectl)"},{"location":"usage/spider-ippool/#metrics","text":"We have also supplemented SpiderIPPool resources with relevant metric information. For more details, refer to metric","title":"Metrics"},{"location":"usage/spider-multus-config-zh_CN/","text":"SpiderMultusConfig \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd Spiderpool \u63d0\u4f9b\u4e86 Spidermultusconfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f00\u6e90\u9879\u76ee Multus CNI \u914d\u7f6e\u7ba1\u7406\u7684\u6269\u5c55\u3002 SpiderMultusConfig \u529f\u80fd Multus \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u5b83\u901a\u8fc7\u8c03\u5ea6\u7b2c\u4e09\u65b9 CNI \u9879\u76ee\uff0c\u80fd\u591f\u5b9e\u73b0\u4e3a Pod \u63a5\u5165\u591a\u5f20\u7f51\u5361\u3002\u5e76\u4e14\uff0cMultus \u53ef\u4ee5\u901a\u8fc7 CRD \u65b9\u5f0f\u7ba1\u7406 CNI \u914d\u7f6e\uff0c\u907f\u514d\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u624b\u52a8\u7f16\u8f91 CNI \u914d\u7f6e\u6587\u4ef6\u3002\u4f46\u521b\u5efa Multus CR \u65f6\uff0c\u9700\u8981\u624b\u52a8\u4e66\u5199 JSON \u683c\u5f0f\u7684 CNI \u914d\u7f6e\u5b57\u7b26\u4e32\u3002\u5c06\u4f1a\u5bfc\u81f4\u5982\u4e0b\u95ee\u9898\u3002 \u4eba\u4e3a\u4e66\u5199\u5bb9\u6613\u51fa\u73b0 JSON \u683c\u5f0f\u9519\u8bef\uff0c\u589e\u52a0 Pod \u542f\u52a8\u5931\u8d25\u7684\u6392\u969c\u6210\u672c\u3002 CNI \u79cd\u7c7b\u591a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5404\u4e2a\u914d\u7f6e\u9879\u4e5f\u5f88\u591a\uff0c\u4e0d\u5bb9\u6613\u8bb0\u5fc6\uff0c\u7ecf\u5e38\u9700\u8981\u8fdb\u884c\u8d44\u6599\u67e5\u9605\uff0c\u7528\u6237\u4f53\u9a8c\u4e0d\u53cb\u597d\u3002 Spidermultusconfig CR \u57fa\u4e8e spec \u4e2d\u7684\u5b9a\u4e49\u81ea\u52a8\u751f\u6210 Multus CR\uff0c\u6539\u8fdb\u4e86\u5982\u4e0a\u95ee\u9898\uff0c\u5e76\u4e14\u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u70b9\uff1a \u8bef\u64cd\u4f5c\u5220\u9664 Multus CR\uff0cSpidermultusconfig \u5c06\u4f1a\u81ea\u52a8\u91cd\u5efa\uff1b\u63d0\u5347\u8fd0\u7ef4\u5bb9\u9519\u80fd\u529b\u3002 \u652f\u6301\u4f17\u591a CNI\uff0c\u5982 Macvlan\u3001IPvlan\u3001Ovs\u3001SR-IOV\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cr-name \u81ea\u5b9a\u4e49 Multus CR \u7684\u540d\u5b57\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cni-version \u81ea\u5b9a\u4e49\u8bbe\u7f6e CNI \u7684\u7248\u672c\u3002 \u5b8c\u5584\u7684 Webhook \u673a\u5236\uff0c\u63d0\u524d\u89c4\u907f\u4e00\u4e9b\u4eba\u4e3a\u9519\u8bef\uff0c\u964d\u4f4e\u540e\u7eed\u6392\u969c\u6210\u672c\u3002 \u652f\u6301 Spiderpool \u7684 CNI plugin\uff1a ifacer \u3001 coordinator \uff0c\u63d0\u9ad8\u4e86 Spiderpool \u7684 CNI plugin \u7684\u914d\u7f6e\u4f53\u9a8c\u3002 \u5728\u5df2\u5b58\u5728 Multus CR \u5b9e\u4f8b\u65f6\uff0c\u521b\u5efa\u4e0e\u5176\u540c\u540d Spidermultusconfig CR \uff0cMultus CR \u5b9e\u4f8b\u5c06\u4f1a\u88ab\u7eb3\u7ba1\uff0c\u5176\u914d\u7f6e\u5185\u5bb9\u5c06\u4f1a\u88ab\u8986\u76d6\u3002\u5982\u679c\u4e0d\u60f3\u53d1\u751f\u88ab\u8986\u76d6\u7684\u60c5\u51b5\uff0c\u8bf7\u907f\u514d\u521b\u5efa\u4e0e\u5b58\u91cf Multus CR \u5b9e\u4f8b\u540c\u540d\u7684 Spidermultusconfig CR \u5b9e\u4f8b\u6216\u8005\u5728 Spidermultusconfig CR \u4e2d\u6307\u5b9a multus.spidernet.io/cr-name \u4ee5\u66f4\u6539\u81ea\u52a8\u751f\u6210\u7684 Multus CR \u7684\u540d\u5b57\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u521b\u5efa CNI \u914d\u7f6e SpiderMultusConfig CR \u652f\u6301\u7684 CNI \u7c7b\u578b\u4f17\u591a\uff0c\u8ddf\u968f\u4e0b\u9762\u7ae0\u8282\u4e86\u89e3\uff0c\u8fdb\u884c\u521b\u5efa\u3002 \u521b\u5efa Macvlan \u914d\u7f6e \u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u521b\u5efa IPvlan \u914d\u7f6e \u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5355\u4e2a\u4e3b\u63a5\u53e3\u4e0d\u80fd\u540c\u65f6\u88ab Macvlan \u548c IPvlan \u6240\u5974\u5f79\u3002 IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 IPvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u521b\u5efa Sriov \u914d\u7f6e \u5982\u4e0b\u662f\u521b\u5efa Sriov SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u4e00\u4e2a\u57fa\u7840\u7684\u4f8b\u5b50 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u66f4\u591a\u4fe1\u606f\u53ef\u53c2\u8003 Sriov-cni \u4f7f\u7528 \u914d\u7f6e\u542f\u7528 RDMA \u529f\u80fd cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' \u66f4\u591a\u4fe1\u606f\u53ef\u53c2\u8003 Sriov-rdma \u4f7f\u7528 \u914d\u7f6e Sriov \u7f51\u7edc\u5e26\u5bbd \u6211\u4eec\u53ef\u901a\u8fc7 SpiderMultusConfig \u914d\u7f6e Sriov \u7684\u7f51\u7edc\u5e26\u5bbd: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps \u548c MaxTxRateMbps \u914d\u7f6e\u6b64 CNI \u914d\u7f6e\u6587\u4ef6\u7684\u7f51\u7edc\u4f20\u8f93\u5e26\u5bbd\u8303\u56f4\u4e3a: [100,1000] \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' Ifacer \u4f7f\u7528\u914d\u7f6e Ifacer \u63d2\u4ef6\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5728\u521b\u5efa Pod \u65f6\uff0c\u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361 \u6216\u8005 Vlan \u7f51\u5361\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u5e95\u5c42\u7f51\u7edc\u3002\u66f4\u591a\u4fe1\u606f\u53c2\u8003 Ifacer \u3002 \u81ea\u52a8\u521b\u5efa Vlan \u63a5\u53e3 \u5982\u679c\u6211\u4eec\u9700\u8981 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5\u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u6ce8\u5165 vlanID \u7684\u914d\u7f6e\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Vlan \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0cIPVLAN_MASTER_INTERFACE \u4e3a ens192\uff0cvlanID \u4e3a 100 \u4e3a\u914d\u7f6e\u4f8b\u5b50: ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e ens192 \u521b\u5efa\u4e00\u4e2a VLAN tag \u4e3a 100 \u7684\u5b50\u63a5\u53e3, \u540d\u4e3a ens192.100 main CNI: IPVlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: ens192.100 , \u4e5f\u5c31\u662f\u901a\u8fc7 ifacer \u521b\u5efa\u7684 VLAN \u5b50\u63a5\u53e3: ens192.100 \u6ce8\u610f: \u901a\u8fc7 ifacer \u521b\u5efa\u7684\u7f51\u5361\u4e0d\u662f\u6301\u4e45\u5316\u7684\uff0c\u91cd\u542f\u8282\u70b9\u6216\u8005\u4eba\u4e3a\u5220\u9664\u5c06\u4f1a\u88ab\u4e22\u5931\u3002\u91cd\u542f Pod \u4f1a\u81ea\u52a8\u6dfb\u52a0\u56de\u6765\u3002 \u6709\u65f6\u5019\u7f51\u7edc\u7ba1\u7406\u5458\u5df2\u7ecf\u521b\u5efa\u597d VLAN \u5b50\u63a5\u53e3\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u4f7f\u7528 ifacer \u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u3002\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u914d\u7f6e master \u5b57\u6bb5\u4e3a: ens192.100 \uff0c\u5e76\u4e14\u4e0d\u914d\u7f6e VLAN ID , \u5982\u4e0b: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100 \u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361 \u5982\u679c\u6211\u4eec\u9700\u8981 Bond \u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5 Bond \u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u914d\u7f6e\u591a\u4e2a master \u63a5\u53e3\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Bond \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0c\u4e3b\u673a\u63a5\u53e3 ens192, ens224 \u4e3a slave \u521b\u5efa Bond \u63a5\u53e3\u4e3a\u4f8b\u5b50: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u914d\u7f6e\u8bf4\u660e: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e [\"ens192\",\"ens224\"] \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 bond \u63a5\u53e3\uff0cmode \u4e3a 1(active-backup)\u3002 IPVlan \u4f5c\u4e3a main CNI\uff0c\u5176 master \u5b57\u6bb5\u7684\u503c\u4e3a: bond0 \uff0c bond0 \u627f\u63a5 Pod \u7684\u7f51\u7edc\u6d41\u91cf\u3002 \u521b\u5efa Bond \u5982\u679c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options \u5b9e\u73b0\u3002 \u8f93\u5165\u683c\u5f0f\u4e3a: \"primary=ens160;arp_interval=1\",\u591a\u4e2a\u53c2\u6570\u7528\";\"\u8fde\u63a5 \u5982\u679c\u6211\u4eec\u9700\u8981\u57fa\u4e8e\u5df2\u521b\u5efa\u7684 Bond \u7f51\u5361 bond0 \u521b\u5efa Vlan \u5b50\u63a5\u53e3\uff0c\u4ee5\u6b64 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u914d\u7f6e: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u4f7f\u7528\u4ee5\u4e0a\u914d\u7f6e\u521b\u5efa Pod\uff0c ifacer \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa\u4e00\u5f20 bond \u7f51\u5361 bond0 \u4ee5\u53ca\u4e00\u5f20 Vlan \u7f51\u5361 bond0.100 \u3002 \u5176\u4ed6 CNI \u914d\u7f6e \u521b\u5efa\u5176\u4ed6 CNI \u914d\u7f6e\uff0c\u5982 Ovs: \u53c2\u8003 \u521b\u5efa Ovs \u603b\u7ed3 SpiderMultusConfig CR \u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\uff0c\u63d0\u5347\u4e86\u521b\u5efa\u4f53\u9a8c\uff0c\u964d\u4f4e\u4e86\u8fd0\u7ef4\u6210\u672c\u3002","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config-zh_CN/#spidermultusconfig","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config-zh_CN/#_1","text":"Spiderpool \u63d0\u4f9b\u4e86 Spidermultusconfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR \uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f00\u6e90\u9879\u76ee Multus CNI \u914d\u7f6e\u7ba1\u7406\u7684\u6269\u5c55\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-multus-config-zh_CN/#spidermultusconfig_1","text":"Multus \u662f\u4e00\u4e2a CNI \u63d2\u4ef6\u9879\u76ee\uff0c\u5b83\u901a\u8fc7\u8c03\u5ea6\u7b2c\u4e09\u65b9 CNI \u9879\u76ee\uff0c\u80fd\u591f\u5b9e\u73b0\u4e3a Pod \u63a5\u5165\u591a\u5f20\u7f51\u5361\u3002\u5e76\u4e14\uff0cMultus \u53ef\u4ee5\u901a\u8fc7 CRD \u65b9\u5f0f\u7ba1\u7406 CNI \u914d\u7f6e\uff0c\u907f\u514d\u5728\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u624b\u52a8\u7f16\u8f91 CNI \u914d\u7f6e\u6587\u4ef6\u3002\u4f46\u521b\u5efa Multus CR \u65f6\uff0c\u9700\u8981\u624b\u52a8\u4e66\u5199 JSON \u683c\u5f0f\u7684 CNI \u914d\u7f6e\u5b57\u7b26\u4e32\u3002\u5c06\u4f1a\u5bfc\u81f4\u5982\u4e0b\u95ee\u9898\u3002 \u4eba\u4e3a\u4e66\u5199\u5bb9\u6613\u51fa\u73b0 JSON \u683c\u5f0f\u9519\u8bef\uff0c\u589e\u52a0 Pod \u542f\u52a8\u5931\u8d25\u7684\u6392\u969c\u6210\u672c\u3002 CNI \u79cd\u7c7b\u591a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5404\u4e2a\u914d\u7f6e\u9879\u4e5f\u5f88\u591a\uff0c\u4e0d\u5bb9\u6613\u8bb0\u5fc6\uff0c\u7ecf\u5e38\u9700\u8981\u8fdb\u884c\u8d44\u6599\u67e5\u9605\uff0c\u7528\u6237\u4f53\u9a8c\u4e0d\u53cb\u597d\u3002 Spidermultusconfig CR \u57fa\u4e8e spec \u4e2d\u7684\u5b9a\u4e49\u81ea\u52a8\u751f\u6210 Multus CR\uff0c\u6539\u8fdb\u4e86\u5982\u4e0a\u95ee\u9898\uff0c\u5e76\u4e14\u5177\u5907\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u70b9\uff1a \u8bef\u64cd\u4f5c\u5220\u9664 Multus CR\uff0cSpidermultusconfig \u5c06\u4f1a\u81ea\u52a8\u91cd\u5efa\uff1b\u63d0\u5347\u8fd0\u7ef4\u5bb9\u9519\u80fd\u529b\u3002 \u652f\u6301\u4f17\u591a CNI\uff0c\u5982 Macvlan\u3001IPvlan\u3001Ovs\u3001SR-IOV\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cr-name \u81ea\u5b9a\u4e49 Multus CR \u7684\u540d\u5b57\u3002 \u652f\u6301\u901a\u8fc7\u6ce8\u89e3 multus.spidernet.io/cni-version \u81ea\u5b9a\u4e49\u8bbe\u7f6e CNI \u7684\u7248\u672c\u3002 \u5b8c\u5584\u7684 Webhook \u673a\u5236\uff0c\u63d0\u524d\u89c4\u907f\u4e00\u4e9b\u4eba\u4e3a\u9519\u8bef\uff0c\u964d\u4f4e\u540e\u7eed\u6392\u969c\u6210\u672c\u3002 \u652f\u6301 Spiderpool \u7684 CNI plugin\uff1a ifacer \u3001 coordinator \uff0c\u63d0\u9ad8\u4e86 Spiderpool \u7684 CNI plugin \u7684\u914d\u7f6e\u4f53\u9a8c\u3002 \u5728\u5df2\u5b58\u5728 Multus CR \u5b9e\u4f8b\u65f6\uff0c\u521b\u5efa\u4e0e\u5176\u540c\u540d Spidermultusconfig CR \uff0cMultus CR \u5b9e\u4f8b\u5c06\u4f1a\u88ab\u7eb3\u7ba1\uff0c\u5176\u914d\u7f6e\u5185\u5bb9\u5c06\u4f1a\u88ab\u8986\u76d6\u3002\u5982\u679c\u4e0d\u60f3\u53d1\u751f\u88ab\u8986\u76d6\u7684\u60c5\u51b5\uff0c\u8bf7\u907f\u514d\u521b\u5efa\u4e0e\u5b58\u91cf Multus CR \u5b9e\u4f8b\u540c\u540d\u7684 Spidermultusconfig CR \u5b9e\u4f8b\u6216\u8005\u5728 Spidermultusconfig CR \u4e2d\u6307\u5b9a multus.spidernet.io/cr-name \u4ee5\u66f4\u6539\u81ea\u52a8\u751f\u6210\u7684 Multus CR \u7684\u540d\u5b57\u3002","title":"SpiderMultusConfig \u529f\u80fd"},{"location":"usage/spider-multus-config-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/spider-multus-config-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/spider-multus-config-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool.","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/spider-multus-config-zh_CN/#cni","text":"SpiderMultusConfig CR \u652f\u6301\u7684 CNI \u7c7b\u578b\u4f17\u591a\uff0c\u8ddf\u968f\u4e0b\u9762\u7ae0\u8282\u4e86\u89e3\uff0c\u8fdb\u884c\u521b\u5efa\u3002","title":"\u521b\u5efa CNI \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#macvlan","text":"\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"\u521b\u5efa Macvlan \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#ipvlan","text":"\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5355\u4e2a\u4e3b\u63a5\u53e3\u4e0d\u80fd\u540c\u65f6\u88ab Macvlan \u548c IPvlan \u6240\u5974\u5f79\u3002 IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF \u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 IPvlan SpiderMultusConfig\uff0c\u5e76\u4e14\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210 Multus NetworkAttachmentDefinition CR\uff0c\u5e76\u5c06\u7eb3\u7ba1\u5176\u751f\u547d\u5468\u671f\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"\u521b\u5efa IPvlan \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#sriov","text":"\u5982\u4e0b\u662f\u521b\u5efa Sriov SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u4e00\u4e2a\u57fa\u7840\u7684\u4f8b\u5b50 cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' \u66f4\u591a\u4fe1\u606f\u53ef\u53c2\u8003 Sriov-cni \u4f7f\u7528 \u914d\u7f6e\u542f\u7528 RDMA \u529f\u80fd cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' \u66f4\u591a\u4fe1\u606f\u53ef\u53c2\u8003 Sriov-rdma \u4f7f\u7528 \u914d\u7f6e Sriov \u7f51\u7edc\u5e26\u5bbd \u6211\u4eec\u53ef\u901a\u8fc7 SpiderMultusConfig \u914d\u7f6e Sriov \u7684\u7f51\u7edc\u5e26\u5bbd: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps \u548c MaxTxRateMbps \u914d\u7f6e\u6b64 CNI \u914d\u7f6e\u6587\u4ef6\u7684\u7f51\u7edc\u4f20\u8f93\u5e26\u5bbd\u8303\u56f4\u4e3a: [100,1000] \u521b\u5efa\u540e\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}'","title":"\u521b\u5efa Sriov \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#ifacer","text":"Ifacer \u63d2\u4ef6\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5728\u521b\u5efa Pod \u65f6\uff0c\u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361 \u6216\u8005 Vlan \u7f51\u5361\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u5e95\u5c42\u7f51\u7edc\u3002\u66f4\u591a\u4fe1\u606f\u53c2\u8003 Ifacer \u3002","title":"Ifacer \u4f7f\u7528\u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#vlan","text":"\u5982\u679c\u6211\u4eec\u9700\u8981 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5\u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u6ce8\u5165 vlanID \u7684\u914d\u7f6e\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Vlan \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0cIPVLAN_MASTER_INTERFACE \u4e3a ens192\uff0cvlanID \u4e3a 100 \u4e3a\u914d\u7f6e\u4f8b\u5b50: ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e ens192 \u521b\u5efa\u4e00\u4e2a VLAN tag \u4e3a 100 \u7684\u5b50\u63a5\u53e3, \u540d\u4e3a ens192.100 main CNI: IPVlan \u7684 master \u5b57\u6bb5\u7684\u503c\u4e3a: ens192.100 , \u4e5f\u5c31\u662f\u901a\u8fc7 ifacer \u521b\u5efa\u7684 VLAN \u5b50\u63a5\u53e3: ens192.100 \u6ce8\u610f: \u901a\u8fc7 ifacer \u521b\u5efa\u7684\u7f51\u5361\u4e0d\u662f\u6301\u4e45\u5316\u7684\uff0c\u91cd\u542f\u8282\u70b9\u6216\u8005\u4eba\u4e3a\u5220\u9664\u5c06\u4f1a\u88ab\u4e22\u5931\u3002\u91cd\u542f Pod \u4f1a\u81ea\u52a8\u6dfb\u52a0\u56de\u6765\u3002 \u6709\u65f6\u5019\u7f51\u7edc\u7ba1\u7406\u5458\u5df2\u7ecf\u521b\u5efa\u597d VLAN \u5b50\u63a5\u53e3\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u4f7f\u7528 ifacer \u521b\u5efa Vlan \u5b50\u63a5\u53e3 \u3002\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u914d\u7f6e master \u5b57\u6bb5\u4e3a: ens192.100 \uff0c\u5e76\u4e14\u4e0d\u914d\u7f6e VLAN ID , \u5982\u4e0b: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100","title":"\u81ea\u52a8\u521b\u5efa Vlan \u63a5\u53e3"},{"location":"usage/spider-multus-config-zh_CN/#bond","text":"\u5982\u679c\u6211\u4eec\u9700\u8981 Bond \u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u5e76\u4e14\u8be5 Bond \u63a5\u53e3\u5728\u8282\u70b9\u5c1a\u672a\u88ab\u521b\u5efa\u3002\u6211\u4eec\u53ef\u4ee5\u5728 Spidermultusconfig \u4e2d\u914d\u7f6e\u591a\u4e2a master \u63a5\u53e3\uff0c\u8fd9\u6837\u751f\u6210\u5bf9\u5e94\u7684 Multus NetworkAttachmentDefinition CR \u65f6\uff0c\u5c31\u4f1a\u6ce8\u5165 ifacer \u63d2\u4ef6\u5bf9\u5e94\u7684\u914d\u7f6e\uff0c\u8be5\u63d2\u4ef6\u5c06\u4f1a\u5728 Pod \u521b\u5efa\u65f6\uff0c\u52a8\u6001\u7684\u5728\u4e3b\u673a\u521b\u5efa Bond \u63a5\u53e3\uff0c\u7528\u4e8e\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5 CNI \u4e3a IPVlan\uff0c\u4e3b\u673a\u63a5\u53e3 ens192, ens224 \u4e3a slave \u521b\u5efa Bond \u63a5\u53e3\u4e3a\u4f8b\u5b50: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u521b\u5efa\u6210\u529f\uff0c\u67e5\u770b\u5bf9\u5e94\u7684 Multus network-attachment-definition \u5bf9\u8c61: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } \u914d\u7f6e\u8bf4\u660e: ifacer \u4f5c\u4e3a CNI \u94fe\u5f0f\u8c03\u7528\u987a\u5e8f\u7684\u7b2c\u4e00\u4e2a\uff0c\u6700\u5148\u88ab\u8c03\u7528\u3002 \u6839\u636e\u914d\u7f6e\uff0c ifacer \u5c06\u57fa\u4e8e [\"ens192\",\"ens224\"] \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a bond0 \u7684 bond \u63a5\u53e3\uff0cmode \u4e3a 1(active-backup)\u3002 IPVlan \u4f5c\u4e3a main CNI\uff0c\u5176 master \u5b57\u6bb5\u7684\u503c\u4e3a: bond0 \uff0c bond0 \u627f\u63a5 Pod \u7684\u7f51\u7edc\u6d41\u91cf\u3002 \u521b\u5efa Bond \u5982\u679c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options \u5b9e\u73b0\u3002 \u8f93\u5165\u683c\u5f0f\u4e3a: \"primary=ens160;arp_interval=1\",\u591a\u4e2a\u53c2\u6570\u7528\";\"\u8fde\u63a5 \u5982\u679c\u6211\u4eec\u9700\u8981\u57fa\u4e8e\u5df2\u521b\u5efa\u7684 Bond \u7f51\u5361 bond0 \u521b\u5efa Vlan \u5b50\u63a5\u53e3\uff0c\u4ee5\u6b64 Vlan \u5b50\u63a5\u53e3\u627f\u63a5 Pod \u7684\u5e95\u5c42\u7f51\u7edc\uff0c\u53ef\u53c2\u8003\u4ee5\u4e0b\u7684\u914d\u7f6e: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF \u5f53\u4f7f\u7528\u4ee5\u4e0a\u914d\u7f6e\u521b\u5efa Pod\uff0c ifacer \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa\u4e00\u5f20 bond \u7f51\u5361 bond0 \u4ee5\u53ca\u4e00\u5f20 Vlan \u7f51\u5361 bond0.100 \u3002","title":"\u81ea\u52a8\u521b\u5efa Bond \u7f51\u5361"},{"location":"usage/spider-multus-config-zh_CN/#cni_1","text":"\u521b\u5efa\u5176\u4ed6 CNI \u914d\u7f6e\uff0c\u5982 Ovs: \u53c2\u8003 \u521b\u5efa Ovs","title":"\u5176\u4ed6 CNI \u914d\u7f6e"},{"location":"usage/spider-multus-config-zh_CN/#_4","text":"SpiderMultusConfig CR \u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\uff0c\u63d0\u5347\u4e86\u521b\u5efa\u4f53\u9a8c\uff0c\u964d\u4f4e\u4e86\u8fd0\u7ef4\u6210\u672c\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-multus-config/","text":"SpiderMultusConfig English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Spiderpool introduces the SpiderMultusConfig CR to automate the management of Multus NetworkAttachmentDefinition CR and extend the capabilities of Multus CNI configurations. SpiderMultusConfig Features Multus is a CNI plugin project that enables Pods to access multiple network interfaces by leveraging third-party CNI plugins. While Multus allows management of CNI configurations through CRDs, manually writing JSON-formatted CNI configuration strings can lead to error: Human errors in writing JSON format may cause troubleshooting difficulties and Pod startup failures. There are numerous CNIs with various configuration options, making it difficult to remember them all, and users often need to refer to documentation, resulting in a poor user experience. To address these issues, SpiderMultusConfig automatically generates the Multus CR based on its spec . It offers several features: In case of accidental deletion of a Multus CR, SpiderMultusConfig will automatically recreate it, improving operational fault tolerance. Support for various CNIs, such as Macvlan, IPvlan, Ovs, and SR-IOV. The annotation multus.spidernet.io/cr-name allows users to define a custom name for Multus CR. The annotation multus.spidernet.io/cni-version enables specifying a specific CNI version. A robust webhook mechanism is involved to proactively detect and prevent human errors, reducing troubleshooting efforts. Spiderpool's CNI plugins, including ifacer and coordinator are integrated, enhancing the overall configuration experience. It is important to note that when creating a SpiderMultusConfig CR with the same name as an existing Multus CR, the Multus CR instance will be managed by SpiderMultusConfig, and its configuration will be overwritten. To avoid overwriting existing Multus CR instances, it is recommended to either refrain from creating SpiderMultusConfig CR instances with the same name or specify a different name for the generated Multus CR using the multus.spidernet.io/cr-name annotation in the SpiderMultusConfig CR. Prerequisites A ready Kubernetes cluster. Helm has been installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. Create CNI Configurations SpiderMultusConfig CR supports various types of CNIs. The following sections explain how to create these configurations. Create Macvlan Configurations Here is an example of creating Macvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF Create the Macvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' Create IPvlan Configurations Here is an example of creating IPvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. When using IPVlan as the cluster's CNI, the kernel version must be higher than 4.2. A single main interface cannot be used by both Macvlan and IPvlan simultaneously. IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF Create the IPvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' Create Sriov Configuration Here is an example of creating Sriov SpiderMultusConfig configuration: Basic example cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' For more information, refer to sriov-cni usage Enable RDMA feature cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' For more information, refer to Sriov-rdma usage Configure Sriov-CNI Network Bandwidth We can configure the network bandwidth of Sriov through SpiderMultusConfig: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps and MaxTxRateMbps configure the transmission bandwidth range for pods created with this configuration: [100,1000]. After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' Ifacer Configurations The Ifacer plug-in can help us automatically create a Bond NIC or VLAN NIC when creating a pod to undertake the pod's underlying network. For more information, refer to Ifacer . Ifacer create vlan interface If we need a VLAN sub-interface to take over the underlying network of the pod, and the interface has not yet been created on the node. We can inject the configuration of the vlanID in Spidermultusconfig so that when the corresponding Multus NetworkAttachmentDefinition CR is generated, it will be injected The ifacer plug-in will dynamically create a VLAN interface on the host when the pod is created, which is used to undertake the pod's underlay network. The following is an example of CNI as IPVlan, IPVLAN_MASTER_INTERFACE as ens192, and vlanID as 100. ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a sub-interface with a VLAN tag of 100 named ens192.100 based on ens192 . main CNI: The value of the master field of IPVlan is: ens192.100 , which is the VLAN sub-interface created by 'ifacer': ens192.100 . Note: The NIC created by ifacer is not persistent, and will be lost if the node is restarted or manually deleted. Restarting the pod is automatically added back. Sometimes the network administrator has already created the VLAN sub-interface, and we don't need to use ifacer to create the VLAN sub-interface. We can directly configure the master field as: ens192.100 and not configure the VLAN ID, as follows: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100 Ifacer create bond interface If we need a bond interface to take over the underlying network of the pod, and the bond interface has not yet been created on the node. We can configure multiple master interfaces in Spidermultusconfig so that the corresponding Multus NetworkAttachmentDefinition CR is generated and injected The ifacer' plug-in will dynamically create a bond interface on the host when the pod is created, which is used to undertake the underlying network of the pod. The following is an example of CNI as IPVlan, host interface ens192, and ens224 as slave to create a bond interface: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } Configuration description: ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a bond interface named 'bond0' based on [\"ens192\",\"ens224\"] with mode 1 (active-backup). main CNI: The value of the master field of IPvlan is: bond0 , bond0 takes over the network traffic of the pod. Create a Bond If you need a more advanced configuration, you can do so by configuring SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options. The input format is: \"primary=ens160; arp_interval=1\", use \";\" for multiple parameters. If we need to create a VLAN sub-interface based on the created BOND NIC: bond0, so that the VLAN sub-interface undertakes the underlying network of the pod, we can refer to the following configuration: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF When creating a pod with the above configuration, ifacer will create a bond NIC bond0 and a VLAN NIC bond0.100 on the host. Other CNI Configurationsi To create other types of CNI configurations, such OVS, refer to Ovs . Conclusion SpiderMultusConfig CR automates the management of Multus NetworkAttachmentDefinition CRs, improving the experience of creating configurations and reducing operational costs.","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config/#spidermultusconfig","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderMultusConfig"},{"location":"usage/spider-multus-config/#introduction","text":"Spiderpool introduces the SpiderMultusConfig CR to automate the management of Multus NetworkAttachmentDefinition CR and extend the capabilities of Multus CNI configurations.","title":"Introduction"},{"location":"usage/spider-multus-config/#spidermultusconfig-features","text":"Multus is a CNI plugin project that enables Pods to access multiple network interfaces by leveraging third-party CNI plugins. While Multus allows management of CNI configurations through CRDs, manually writing JSON-formatted CNI configuration strings can lead to error: Human errors in writing JSON format may cause troubleshooting difficulties and Pod startup failures. There are numerous CNIs with various configuration options, making it difficult to remember them all, and users often need to refer to documentation, resulting in a poor user experience. To address these issues, SpiderMultusConfig automatically generates the Multus CR based on its spec . It offers several features: In case of accidental deletion of a Multus CR, SpiderMultusConfig will automatically recreate it, improving operational fault tolerance. Support for various CNIs, such as Macvlan, IPvlan, Ovs, and SR-IOV. The annotation multus.spidernet.io/cr-name allows users to define a custom name for Multus CR. The annotation multus.spidernet.io/cni-version enables specifying a specific CNI version. A robust webhook mechanism is involved to proactively detect and prevent human errors, reducing troubleshooting efforts. Spiderpool's CNI plugins, including ifacer and coordinator are integrated, enhancing the overall configuration experience. It is important to note that when creating a SpiderMultusConfig CR with the same name as an existing Multus CR, the Multus CR instance will be managed by SpiderMultusConfig, and its configuration will be overwritten. To avoid overwriting existing Multus CR instances, it is recommended to either refrain from creating SpiderMultusConfig CR instances with the same name or specify a different name for the generated Multus CR using the multus.spidernet.io/cr-name annotation in the SpiderMultusConfig CR.","title":"SpiderMultusConfig Features"},{"location":"usage/spider-multus-config/#prerequisites","text":"A ready Kubernetes cluster. Helm has been installed.","title":"Prerequisites"},{"location":"usage/spider-multus-config/#steps","text":"","title":"Steps"},{"location":"usage/spider-multus-config/#install-spiderpool","text":"Refer to Installation to install Spiderpool.","title":"Install Spiderpool"},{"location":"usage/spider-multus-config/#create-cni-configurations","text":"SpiderMultusConfig CR supports various types of CNIs. The following sections explain how to create these configurations.","title":"Create CNI Configurations"},{"location":"usage/spider-multus-config/#create-macvlan-configurations","text":"Here is an example of creating Macvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF Create the Macvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-11T09:02:43Z\" generation: 1 name: macvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 94bbd704-ff9d-4318-8356-f4ae59856228 resourceVersion: \"5288986\" uid: d8fa48c8-0877-440d-9b66-88edd7af5808 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"master\":\"ens192\",\"mode\":\"bridge\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"Create Macvlan Configurations"},{"location":"usage/spider-multus-config/#create-ipvlan-configurations","text":"Here is an example of creating IPvlan SpiderMultusConfig configurations: master: ens192 is used as the master interface parameter. When using IPVlan as the cluster's CNI, the kernel version must be higher than 4.2. A single main interface cannot be used by both Macvlan and IPvlan simultaneously. IPVLAN_MASTER_INTERFACE = \"ens192\" IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} EOF Create the IPvlan SpiderMultusConfig using the provided configuration. This will automatically generate the corresponding Multus NetworkAttachmentDefinition CR and manage its lifecycle. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-ens192 12s ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-ens192 -oyaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"ipvlan-ens192\" , \"namespace\" : \"kube-system\" } , \"spec\" : { \"cniType\" : \"ipvlan\" , \"enableCoordinator\" :true, \"ipvlan\" : { \"master\" : [ \"ens192\" ]}}} creationTimestamp: \"2023-09-14T10:21:26Z\" generation: 1 name: ipvlan-ens192 namespace: kube-system ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: ipvlan-ens192 uid: accac945-9296-440e-abe8-6f6938fdb895 resourceVersion: \"5950921\" uid: e24afb76-e552-4f73-bab0-8fd345605c2a spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ipvlan-ens192\",\"plugins\":[{\"type\":\"ipvlan\",\"master\":\"ens192\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}'","title":"Create IPvlan Configurations"},{"location":"usage/spider-multus-config/#create-sriov-configuration","text":"Here is an example of creating Sriov SpiderMultusConfig configuration: Basic example cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-demo namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-demo -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-demo namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-demo uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-demo\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"coordinator\"}]}' For more information, refer to sriov-cni usage Enable RDMA feature cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-rdma namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: enableRdma: true resourceName: spidernet.io/sriov_netdeivce vlanID: 100 EOF After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-rdma namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-rdma uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e resourceVersion: \"153002297\" uid: 4413e1fa-ce15-4acf-bce8-48b5028c0568 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-rdma\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}' For more information, refer to Sriov-rdma usage Configure Sriov-CNI Network Bandwidth We can configure the network bandwidth of Sriov through SpiderMultusConfig: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-bandwidth namespace: kube-system spec: cniType: sriov enableCoordinator: true sriov: resourceName: spidernet.io/sriov_netdeivce vlanID: 100 minTxRateMbps: 100 MaxTxRateMbps: 1000 EOF minTxRateMbps and MaxTxRateMbps configure the transmission bandwidth range for pods created with this configuration: [100,1000]. After creation, check the corresponding Multus NetworkAttachmentDefinition CR: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-rdma -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: sriov-bandwidth namespace: kube-system annotations: k8s.v1.cni.cncf.io/resourceName: spidernet.io/sriov_netdeivce ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: sriov-bandwidth uid: b08ce054-1ae8-414a-b37c-7fd6988b1b8e spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"sriov-bandwidth\",\"plugins\":[{\"vlan\":100,\"type\":\"sriov\",\"minTxRate\": 100, \"maxTxRate\": 1000,\"ipam\":{\"type\":\"spiderpool\"}},{\"type\":\"rdma\"},{\"type\":\"coordinator\"}]}'","title":"Create Sriov Configuration"},{"location":"usage/spider-multus-config/#ifacer-configurations","text":"The Ifacer plug-in can help us automatically create a Bond NIC or VLAN NIC when creating a pod to undertake the pod's underlying network. For more information, refer to Ifacer .","title":"Ifacer Configurations"},{"location":"usage/spider-multus-config/#ifacer-create-vlan-interface","text":"If we need a VLAN sub-interface to take over the underlying network of the pod, and the interface has not yet been created on the node. We can inject the configuration of the vlanID in Spidermultusconfig so that when the corresponding Multus NetworkAttachmentDefinition CR is generated, it will be injected The ifacer plug-in will dynamically create a VLAN interface on the host when the pod is created, which is used to undertake the pod's underlay network. The following is an example of CNI as IPVlan, IPVLAN_MASTER_INTERFACE as ens192, and vlanID as 100. ~# IPVLAN_MASTER_INTERFACE = \"ens192\" ~# IPVLAN_MULTUS_NAME = \"ipvlan- $IPVLAN_MASTER_INTERFACE \" ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-ens192-vlan100 namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE} vlanID: 100 EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system macvlan-conf -o = jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-ens192-vlan100\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" ] , \"vlanID\" : 100 } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"ens192.100\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a sub-interface with a VLAN tag of 100 named ens192.100 based on ens192 . main CNI: The value of the master field of IPVlan is: ens192.100 , which is the VLAN sub-interface created by 'ifacer': ens192.100 . Note: The NIC created by ifacer is not persistent, and will be lost if the node is restarted or manually deleted. Restarting the pod is automatically added back. Sometimes the network administrator has already created the VLAN sub-interface, and we don't need to use ifacer to create the VLAN sub-interface. We can directly configure the master field as: ens192.100 and not configure the VLAN ID, as follows: apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderMultusConfig metadata : name : macvlan-conf namespace : kube-system spec : cniType : macvlan macvlan : master : - ens192.100 ippools : ipv4 : - vlan100","title":"Ifacer create vlan interface"},{"location":"usage/spider-multus-config/#ifacer-create-bond-interface","text":"If we need a bond interface to take over the underlying network of the pod, and the bond interface has not yet been created on the node. We can configure multiple master interfaces in Spidermultusconfig so that the corresponding Multus NetworkAttachmentDefinition CR is generated and injected The ifacer' plug-in will dynamically create a bond interface on the host when the pod is created, which is used to undertake the underlying network of the pod. The following is an example of CNI as IPVlan, host interface ens192, and ens224 as slave to create a bond interface: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 bond: name: bond0 mode: 1 options: \"\" EOF When the Spidermultuconfig object is created, view the corresponding Multus network-attachment-definition object: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system ipvlan-conf -o jsonpath = '{.spec.config}' | jq { \"cniVersion\" : \"0.3.1\" , \"name\" : \"ipvlan-conf\" , \"plugins\" : [ { \"type\" : \"ifacer\" , \"interfaces\" : [ \"ens192\" \"ens224\" ] , \"bond\" : { \"name\" : \"bond0\" , \"mode\" : 1 } } , { \"type\" : \"ipvlan\" , \"ipam\" : { \"type\" : \"spiderpool\" } , \"master\" : \"bond0\" , \"mode\" : \"bridge\" } , { \"type\" : \"coordinator\" , } ] } Configuration description: ifacer is called first in the CNI chaining sequence. Depending on the configuration, ifacer will create a bond interface named 'bond0' based on [\"ens192\",\"ens224\"] with mode 1 (active-backup). main CNI: The value of the master field of IPvlan is: bond0 , bond0 takes over the network traffic of the pod. Create a Bond If you need a more advanced configuration, you can do so by configuring SpiderMultusConfig: macvlan-conf.spec.macvlan.bond.options. The input format is: \"primary=ens160; arp_interval=1\", use \";\" for multiple parameters. If we need to create a VLAN sub-interface based on the created BOND NIC: bond0, so that the VLAN sub-interface undertakes the underlying network of the pod, we can refer to the following configuration: ~# cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ipvlan-conf namespace: kube-system spec: cniType: ipvlan macvlan: master: - ens192 - ens224 vlanID: 100 bond: name: bond0 mode: 1 options: \"\" EOF When creating a pod with the above configuration, ifacer will create a bond NIC bond0 and a VLAN NIC bond0.100 on the host.","title":"Ifacer create bond interface"},{"location":"usage/spider-multus-config/#other-cni-configurationsi","text":"To create other types of CNI configurations, such OVS, refer to Ovs .","title":"Other CNI Configurationsi"},{"location":"usage/spider-multus-config/#conclusion","text":"SpiderMultusConfig CR automates the management of Multus NetworkAttachmentDefinition CRs, improving the experience of creating configurations and reducing operational costs.","title":"Conclusion"},{"location":"usage/spider-subnet-zh_CN/","text":"SpiderSubnet \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 SpiderIPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002 SpiderSubnet \u529f\u80fd \u542f\u7528 Subnet \u529f\u80fd\u65f6\uff0c\u6bcf\u4e00\u4e2a IPPool \u5b9e\u4f8b\u90fd\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684 Subnet \u5b9e\u4f8b\uff0cIPPool \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u5fc5\u987b\u662f Subnet \u5b9e\u4f8b\u4e2d IP \u5730\u5740\u7684\u5b50\u96c6\uff0cIPPool \u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u51fa\u73b0\u91cd\u53e0 IP \u5730\u5740\uff0c\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\u7684\u5404\u79cd\u8def\u7531\u5c5e\u6027\uff0c\u9ed8\u8ba4\u7ee7\u627f Subnet \u5b9e\u4f8b\u4e2d\u7684\u8bbe\u7f6e\u3002 \u5728\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e26\u6765\u4e86\u5982\u4e0b\u4e24\u79cd\u5b9e\u8df5\u624b\u6bb5\uff0c\u4ece\u800c\u5b8c\u6210\u5e94\u7528\u7ba1\u7406\u5458\u548c\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u804c\u8d23\u89e3\u8026\uff1a \u624b\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\uff0c\u53ef\u57fa\u4e8e\u5bf9\u5e94\u7684 Subnet \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u7ea6\u675f\uff0c\u6765\u83b7\u77e5\u53ef\u4f7f\u7528\u54ea\u4e9b IP \u5730\u5740\u3002 \u81ea\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u53ef\u5728 Pod annotation \u4e2d\u6ce8\u660e\u4f7f\u7528\u7684 Subnet \u5b9e\u4f8b\u540d\uff0c\u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u81ea\u52a8\u6839\u636e Subnet \u5b9e\u4f8b\u4e2d\u7684\u53ef\u7528 IP \u5730\u5740\u6765\u521b\u5efa\u56fa\u5b9a IP \u7684 IPPool \u5b9e\u4f8b\uff0c\u4ece\u4e2d\u5206\u914d IP \u5730\u5740\u7ed9 Pod\u3002\u5e76\u4e14 Spiderpool \u80fd\u591f\u81ea\u52a8\u76d1\u63a7\u5e94\u7528\u7684\u6269\u7f29\u5bb9\u548c\u5220\u9664\u4e8b\u4ef6\uff0c\u81ea\u52a8\u5b8c\u6210 IPPool \u4e2d\u7684 IP \u5730\u5740\u6269\u7f29\u5bb9\u548c\u5220\u9664\u3002 SpiderSubnet \u529f\u80fd\u8fd8\u652f\u6301\u4f17\u591a\u7684\u63a7\u5236\u5668\uff0c\u5982\uff1aReplicaSet\u3001Deployment\u3001Statefulset\u3001Daemonset\u3001Job\u3001Cronjob\uff0c\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\u7b49\u3002\u5bf9\u4e8e\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 \u793a\u4f8b \u6ce8\u610f\uff1a\u5728 v0.7.0 \u7248\u672c\u4e4b\u524d\uff0c\u5728\u542f\u52a8 SpiderSubnet \u529f\u80fd\u4e0b\u4f60\u5fc5\u987b\u5f97\u5148\u521b\u5efa\u4e00\u4e2a SpiderSubnet \u8d44\u6e90\u624d\u53ef\u4ee5\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002\u5728v0.7.0\u7248\u672c\u5f00\u59cb\uff0c\u652f\u6301\u521b\u5efa\u4e00\u4e2a\u72ec\u7acb\u7684 SpiderIPPool \u8d44\u6e90\u800c\u4e0d\u4f9d\u8d56\u4e8e SpiderSubnet \u8d44\u6e90\u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableSpiderSubnet=true \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u548c ens224 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 ens192 \u4e0e ens224 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m \u521b\u5efa Subnet ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF \u4f7f\u7528\u5982\u4e0a\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderSubnet\uff0c\u5e76\u5206\u522b\u4e3a\u5176\u914d\u7f6e\u7f51\u5173\u4e0e\u8def\u7531\u4fe1\u606f\u3002 ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } \u81ea\u52a8\u56fa\u5b9a\u5355\u7f51\u5361 IP \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnet \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 1 \u4e2a\u5bf9\u5e94\u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 ipam.spidernet.io/ippool-ip-number \uff1a\u7528\u4e8e\u6307\u5b9a\u521b\u5efa IP \u6c60 \u4e2d \u7684 IP \u6570\u91cf\u3002\u8be5 annotation \u7684\u5199\u6cd5\u652f\u6301\u4e24\u79cd\u65b9\u5f0f\uff1a\u4e00\u79cd\u662f\u6570\u5b57\u7684\u65b9\u5f0f\u6307\u5b9a IP \u6c60\u7684\u56fa\u5b9a\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a1 \uff1b\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u52a0\u53f7\u548c\u6570\u5b57\u6307\u5b9a IP \u6c60\u7684\u76f8\u5bf9\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a+1 \uff0c\u5373\u8868\u793a IP \u6c60\u4e2d\u7684\u6570\u91cf\u4f1a\u81ea\u52a8\u5b9e\u65f6\u4fdd\u6301\u5728\u5e94\u7528\u7684\u526f\u672c\u6570\u7684\u57fa\u7840\u4e0a\u591a 1 \u4e2a IP\uff0c\u4ee5\u89e3\u51b3\u5e94\u7528\u5728\u5f39\u6027\u6269\u7f29\u5bb9\u7684\u65f6\u6709\u4e34\u65f6\u7684 IP \u53ef\u7528\u3002 ipam.spidernet.io/ippool-reclaim \uff1a \u5176\u8868\u793a\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u662f\u5426\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\u800c\u88ab\u56de\u6536\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u56fa\u5b9a IP \u6c60 \u4e0e Pod \u7684\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u81ea\u52a8\u6c60\u4f1a\u81ea\u52a8\u7ee7\u627f\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u4e3a\u5b9e\u73b0\u56fa\u5b9a IP \u6c60\u6548\u679c\uff0cSpiderpool \u4f1a\u7ed9\u81ea\u52a8\u6c60\u8865\u5145\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u5185\u5efa Label \u548c PodAffinity\uff0c\u5b83\u4eec\u7528\u4e8e\u6307\u5411\u5e94\u7528\uff0c\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\uff0c\u660e\u786e\u8be5\u6c60\u53ea\u670d\u52a1\u4e8e\u6307\u5b9a\u7684\u5e94\u7528\u3002\u5f53 ipam.spidernet.io/ippool-reclaim: false \u65f6\uff0c\u5220\u9664\u5e94\u7528\u540e\uff0cIP\u4f1a\u88ab\u56de\u6536\uff0c\u4f46\u81ea\u52a8\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002\u5982\u679c\u671f\u671b\u8be5\u6c60\u80fd\u88ab\u5176\u4ed6\u5e94\u7528\u6240\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u6458\u9664\u8fd9\u4e9b\u5185\u5efa Label \u548c PodAffinity\u3002 Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name \u7ecf\u8fc7\u591a\u6b21\u6d4b\u8bd5\uff0c\u4e0d\u65ad\u91cd\u542f Pod\uff0c\u5176 Pod \u7684 IP \u90fd\u88ab\u56fa\u5b9a\u5728 IP \u6c60\u8303\u56f4\u5185: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none> \u56fa\u5b9a IP \u6c60 IP \u6570\u91cf\u7684\u52a8\u6001\u6269\u7f29\u5bb9 \u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-ip-number : '+1'\uff0c\u5176\u8868\u793a\u5e94\u7528\u5206\u914d\u5230\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u7684\u526f\u672c\u6570\u591a 1 \u4e2a\uff0c\u5728\u5e94\u7528\u6eda\u52a8\u66f4\u65b0\u65f6\uff0c\u80fd\u591f\u907f\u514d\u65e7 Pod \u672a\u5220\u9664\uff0c\u65b0 Pod \u6ca1\u6709\u53ef\u7528 IP \u7684\u95ee\u9898\u3002 \u4ee5\u4e0b\u6f14\u793a\u4e86\u6269\u5bb9\u573a\u666f\uff0c\u5c06\u5e94\u7528\u7684\u526f\u672c\u6570\u4ece 2 \u6269\u5bb9\u5230 3\uff0c\u5e94\u7528\u5bf9\u5e94\u7684\u4e24\u4e2a\u56fa\u5b9a IP \u6c60\u4f1a\u81ea\u52a8\u4ece 3 \u4e2a IP \u6269\u5bb9\u5230 4 \u4e2a IP\uff0c\u4e00\u76f4\u4fdd\u6301\u4e00\u4e2a\u5197\u4f59 IP\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false \u901a\u8fc7\u4e0a\u8ff0\uff0cSpiderpool \u5bf9\u4e8e\u5e94\u7528\u6269\u7f29\u5bb9\u7684\u573a\u666f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u5373\u53ef\u3002 \u81ea\u52a8\u56de\u6536 IP \u6c60 \u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-reclaim \uff0c\u8be5\u6ce8\u89e3\u9ed8\u8ba4\u503c\u4e3a true \uff0c\u4e3a true \u65f6\uff0c\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\uff0c\u5c06\u81ea\u52a8\u5220\u9664\u5bf9\u5e94\u7684\u81ea\u52a8\u6c60\u3002\u5728\u672c\u6587\u4e2d\u8bbe\u7f6e\u4e3a false \uff0c\u5176\u8868\u793a\u5220\u9664\u5e94\u7528\u65f6\uff0c\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u4f1a\u56de\u6536\u5176\u4e2d\u88ab\u5206\u914d\u7684 IP \uff0c\u4f46\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\uff0c\u5e76\u4e14\u5f53\u4f7f\u7528\u76f8\u540c\u914d\u7f6e\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\u65f6\uff0c\u4f1a\u81ea\u52a8\u7ee7\u627f\u8be5 IP \u6c60\u3002 ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false \u4f7f\u7528\u4e0a\u8ff0\u6240\u793a\u7684\u5e94\u7528 Yaml\uff0c\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u4e0d\u4f1a\u518d\u6b21\u521b\u5efa\u65b0\u7684 IP \u6c60\uff0c\u5c06\u81ea\u52a8\u590d\u7528\u65e7 IP \u6c60\uff0c\u5e76\u4e14\u5176\u526f\u672c\u6570\u548c IP \u6c60\u7684 IP \u5206\u914d\u60c5\u51b5\u4e0e\u5b9e\u9645\u76f8\u540c\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false \u81ea\u52a8\u56fa\u5b9a\u591a\u7f51\u5361 IP \u5982\u679c\u60a8\u5e0c\u671b\u4e3a Pod \u5b9e\u73b0\u591a\u7f51\u5361 IP \u7684\u56fa\u5b9a\uff0c\u53c2\u8003\u672c\u7ae0\u8282\u3002\u5728\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u6bcf\u4e2a\u526f\u672c\u62e5\u6709\u591a\u5f20\u7f51\u5361\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnets \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 2 \u4e2a\u5c5e\u4e8e\u4e0d\u540c Underlay \u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u53e6\u4e00\u5f20\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a 2 \u4e2a Underlay \u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u5bf9\u5e94\u7684\u56fa\u5b9a IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528 Pod \u7684\u4e24\u5f20\u7f51\u5361\u5206\u522b\u5f62\u6210\u7ed1\u5b9a\u3002\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u7684\u56fa\u5b9a\u6c60\u90fd\u5c06\u4f1a\u81ea\u52a8\u7ee7\u627f\u5176\u6240\u5f52\u5c5e\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet \u4e5f\u652f\u6301\u591a\u7f51\u5361\u7684\u52a8\u6001 IP \u6269\u7f29\u5bb9\u3001\u81ea\u52a8\u56de\u6536 IP \u6c60\u7b49\u529f\u80fd\u3002 \u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u7ee7\u627f\u5b50\u7f51\u5c5e\u6027 \u5982\u4e0b\u662f\u4e00\u4e2a\u5f52\u5c5e\u4e8e\u5b50\u7f51 subnet-6 \uff0c\u5b50\u7f51\u53f7\u4e3a\uff1a 10.6.0.0/16 \u7684 IPPool \u5b9e\u4f8b\u793a\u4f8b\u3002\u8be5 IPPool \u5b9e\u4f8b\u7684\u53ef\u7528 IP \u8303\u56f4\u5fc5\u987b\u662f\u5b50\u7f51 subnet-6.spec.ips \u7684\u5b50\u96c6\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF \u4f7f\u7528\u4e0a\u8ff0 Yaml\uff0c\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u5230\u5b83\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684\u5b50\u7f51\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u5bf9\u5e94\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u603b\u7ed3 SpiderSubnet \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u5c06\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u5458\u7684\u8d23\u4efb\u5206\u5f00\uff0c\u652f\u6301\u81ea\u52a8\u521b\u5efa\u548c\u52a8\u6001\u6269\u5c55\u56fa\u5b9a IPPool \u5230\u6bcf\u4e2a\u9700\u8981\u9759\u6001 IP \u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"SpiderSubnet"},{"location":"usage/spider-subnet-zh_CN/#spidersubnet","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"SpiderSubnet"},{"location":"usage/spider-subnet-zh_CN/#_1","text":"SpiderSubnet \u8d44\u6e90\u4ee3\u8868 IP \u5730\u5740\u7684\u96c6\u5408\uff0c\u5f53\u9700\u8981\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u9700\u8981\u5e73\u53f0\u7ba1\u7406\u5458\u544a\u77e5\u53ef\u7528\u7684 IP \u5730\u5740\u548c\u8def\u7531\u5c5e\u6027\u7b49\uff0c\u4f46\u53cc\u65b9\u5206\u5c5e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fd0\u8425\u90e8\u95e8\uff0c\u8fd9\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u5e94\u7528\u521b\u5efa\u7684\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u501f\u52a9\u4e8e Spiderpool \u7684 SpiderSubnet \u529f\u80fd\uff0c\u5b83\u80fd\u81ea\u52a8\u4ece\u4e2d\u5b50\u7f51\u5206\u914d IP \u7ed9 SpiderIPPool\uff0c\u5e76\u4e14\u8fd8\u80fd\u4e3a\u5e94\u7528\u56fa\u5b9a IP \u5730\u5740\uff0c\u6781\u5927\u7684\u51cf\u5c11\u4e86\u8fd0\u7ef4\u7684\u6210\u672c\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/spider-subnet-zh_CN/#spidersubnet_1","text":"\u542f\u7528 Subnet \u529f\u80fd\u65f6\uff0c\u6bcf\u4e00\u4e2a IPPool \u5b9e\u4f8b\u90fd\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684 Subnet \u5b9e\u4f8b\uff0cIPPool \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u5fc5\u987b\u662f Subnet \u5b9e\u4f8b\u4e2d IP \u5730\u5740\u7684\u5b50\u96c6\uff0cIPPool \u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u51fa\u73b0\u91cd\u53e0 IP \u5730\u5740\uff0c\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\u7684\u5404\u79cd\u8def\u7531\u5c5e\u6027\uff0c\u9ed8\u8ba4\u7ee7\u627f Subnet \u5b9e\u4f8b\u4e2d\u7684\u8bbe\u7f6e\u3002 \u5728\u4e3a\u5e94\u7528\u5206\u914d\u56fa\u5b9a\u7684 IP \u5730\u5740\u65f6\uff0c\u5e26\u6765\u4e86\u5982\u4e0b\u4e24\u79cd\u5b9e\u8df5\u624b\u6bb5\uff0c\u4ece\u800c\u5b8c\u6210\u5e94\u7528\u7ba1\u7406\u5458\u548c\u7f51\u7edc\u7ba1\u7406\u5458\u7684\u804c\u8d23\u89e3\u8026\uff1a \u624b\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u65f6\uff0c\u53ef\u57fa\u4e8e\u5bf9\u5e94\u7684 Subnet \u5b9e\u4f8b\u4e2d\u7684 IP \u5730\u5740\u7ea6\u675f\uff0c\u6765\u83b7\u77e5\u53ef\u4f7f\u7528\u54ea\u4e9b IP \u5730\u5740\u3002 \u81ea\u52a8\u521b\u5efa IPPool : \u5e94\u7528\u7ba1\u7406\u5458\u53ef\u5728 Pod annotation \u4e2d\u6ce8\u660e\u4f7f\u7528\u7684 Subnet \u5b9e\u4f8b\u540d\uff0c\u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u81ea\u52a8\u6839\u636e Subnet \u5b9e\u4f8b\u4e2d\u7684\u53ef\u7528 IP \u5730\u5740\u6765\u521b\u5efa\u56fa\u5b9a IP \u7684 IPPool \u5b9e\u4f8b\uff0c\u4ece\u4e2d\u5206\u914d IP \u5730\u5740\u7ed9 Pod\u3002\u5e76\u4e14 Spiderpool \u80fd\u591f\u81ea\u52a8\u76d1\u63a7\u5e94\u7528\u7684\u6269\u7f29\u5bb9\u548c\u5220\u9664\u4e8b\u4ef6\uff0c\u81ea\u52a8\u5b8c\u6210 IPPool \u4e2d\u7684 IP \u5730\u5740\u6269\u7f29\u5bb9\u548c\u5220\u9664\u3002 SpiderSubnet \u529f\u80fd\u8fd8\u652f\u6301\u4f17\u591a\u7684\u63a7\u5236\u5668\uff0c\u5982\uff1aReplicaSet\u3001Deployment\u3001Statefulset\u3001Daemonset\u3001Job\u3001Cronjob\uff0c\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\u7b49\u3002\u5bf9\u4e8e\u7b2c\u4e09\u65b9\u63a7\u5236\u5668\uff0c\u60a8\u53ef\u4ee5\u53c2\u8003 \u793a\u4f8b \u6ce8\u610f\uff1a\u5728 v0.7.0 \u7248\u672c\u4e4b\u524d\uff0c\u5728\u542f\u52a8 SpiderSubnet \u529f\u80fd\u4e0b\u4f60\u5fc5\u987b\u5f97\u5148\u521b\u5efa\u4e00\u4e2a SpiderSubnet \u8d44\u6e90\u624d\u53ef\u4ee5\u521b\u5efa SpiderIPPool \u8d44\u6e90\u3002\u5728v0.7.0\u7248\u672c\u5f00\u59cb\uff0c\u652f\u6301\u521b\u5efa\u4e00\u4e2a\u72ec\u7acb\u7684 SpiderIPPool \u8d44\u6e90\u800c\u4e0d\u4f9d\u8d56\u4e8e SpiderSubnet \u8d44\u6e90\u3002","title":"SpiderSubnet \u529f\u80fd"},{"location":"usage/spider-subnet-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/spider-subnet-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/spider-subnet-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableSpiderSubnet=true","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/spider-subnet-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u548c ens224 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 ens192 \u4e0e ens224 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/spider-subnet-zh_CN/#subnet","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF \u4f7f\u7528\u5982\u4e0a\u7684 Yaml\uff0c\u521b\u5efa 2 \u4e2a SpiderSubnet\uff0c\u5e76\u5206\u522b\u4e3a\u5176\u914d\u7f6e\u7f51\u5173\u4e0e\u8def\u7531\u4fe1\u606f\u3002 ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 }","title":"\u521b\u5efa Subnet"},{"location":"usage/spider-subnet-zh_CN/#ip","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment \u5e94\u7528 \uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnet \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 1 \u4e2a\u5bf9\u5e94\u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 ipam.spidernet.io/ippool-ip-number \uff1a\u7528\u4e8e\u6307\u5b9a\u521b\u5efa IP \u6c60 \u4e2d \u7684 IP \u6570\u91cf\u3002\u8be5 annotation \u7684\u5199\u6cd5\u652f\u6301\u4e24\u79cd\u65b9\u5f0f\uff1a\u4e00\u79cd\u662f\u6570\u5b57\u7684\u65b9\u5f0f\u6307\u5b9a IP \u6c60\u7684\u56fa\u5b9a\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a1 \uff1b\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u4f7f\u7528\u52a0\u53f7\u548c\u6570\u5b57\u6307\u5b9a IP \u6c60\u7684\u76f8\u5bf9\u6570\u91cf\uff0c\u4f8b\u5982 ipam.spidernet.io/ippool-ip-number\uff1a+1 \uff0c\u5373\u8868\u793a IP \u6c60\u4e2d\u7684\u6570\u91cf\u4f1a\u81ea\u52a8\u5b9e\u65f6\u4fdd\u6301\u5728\u5e94\u7528\u7684\u526f\u672c\u6570\u7684\u57fa\u7840\u4e0a\u591a 1 \u4e2a IP\uff0c\u4ee5\u89e3\u51b3\u5e94\u7528\u5728\u5f39\u6027\u6269\u7f29\u5bb9\u7684\u65f6\u6709\u4e34\u65f6\u7684 IP \u53ef\u7528\u3002 ipam.spidernet.io/ippool-reclaim \uff1a \u5176\u8868\u793a\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u662f\u5426\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\u800c\u88ab\u56de\u6536\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u56fa\u5b9a IP \u6c60 \u4e0e Pod \u7684\u7f51\u5361\u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u81ea\u52a8\u6c60\u4f1a\u81ea\u52a8\u7ee7\u627f\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } \u4e3a\u5b9e\u73b0\u56fa\u5b9a IP \u6c60\u6548\u679c\uff0cSpiderpool \u4f1a\u7ed9\u81ea\u52a8\u6c60\u8865\u5145\u5982\u4e0b\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u5185\u5efa Label \u548c PodAffinity\uff0c\u5b83\u4eec\u7528\u4e8e\u6307\u5411\u5e94\u7528\uff0c\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\uff0c\u660e\u786e\u8be5\u6c60\u53ea\u670d\u52a1\u4e8e\u6307\u5b9a\u7684\u5e94\u7528\u3002\u5f53 ipam.spidernet.io/ippool-reclaim: false \u65f6\uff0c\u5220\u9664\u5e94\u7528\u540e\uff0cIP\u4f1a\u88ab\u56de\u6536\uff0c\u4f46\u81ea\u52a8\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\u3002\u5982\u679c\u671f\u671b\u8be5\u6c60\u80fd\u88ab\u5176\u4ed6\u5e94\u7528\u6240\u4f7f\u7528\uff0c\u9700\u8981\u624b\u52a8\u6458\u9664\u8fd9\u4e9b\u5185\u5efa Label \u548c PodAffinity\u3002 Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name \u7ecf\u8fc7\u591a\u6b21\u6d4b\u8bd5\uff0c\u4e0d\u65ad\u91cd\u542f Pod\uff0c\u5176 Pod \u7684 IP \u90fd\u88ab\u56fa\u5b9a\u5728 IP \u6c60\u8303\u56f4\u5185: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none>","title":"\u81ea\u52a8\u56fa\u5b9a\u5355\u7f51\u5361 IP"},{"location":"usage/spider-subnet-zh_CN/#ip-ip","text":"\u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-ip-number : '+1'\uff0c\u5176\u8868\u793a\u5e94\u7528\u5206\u914d\u5230\u7684\u56fa\u5b9a IP \u6570\u91cf\u6bd4\u5e94\u7528\u7684\u526f\u672c\u6570\u591a 1 \u4e2a\uff0c\u5728\u5e94\u7528\u6eda\u52a8\u66f4\u65b0\u65f6\uff0c\u80fd\u591f\u907f\u514d\u65e7 Pod \u672a\u5220\u9664\uff0c\u65b0 Pod \u6ca1\u6709\u53ef\u7528 IP \u7684\u95ee\u9898\u3002 \u4ee5\u4e0b\u6f14\u793a\u4e86\u6269\u5bb9\u573a\u666f\uff0c\u5c06\u5e94\u7528\u7684\u526f\u672c\u6570\u4ece 2 \u6269\u5bb9\u5230 3\uff0c\u5e94\u7528\u5bf9\u5e94\u7684\u4e24\u4e2a\u56fa\u5b9a IP \u6c60\u4f1a\u81ea\u52a8\u4ece 3 \u4e2a IP \u6269\u5bb9\u5230 4 \u4e2a IP\uff0c\u4e00\u76f4\u4fdd\u6301\u4e00\u4e2a\u5197\u4f59 IP\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false \u901a\u8fc7\u4e0a\u8ff0\uff0cSpiderpool \u5bf9\u4e8e\u5e94\u7528\u6269\u7f29\u5bb9\u7684\u573a\u666f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u5373\u53ef\u3002","title":"\u56fa\u5b9a IP \u6c60 IP \u6570\u91cf\u7684\u52a8\u6001\u6269\u7f29\u5bb9"},{"location":"usage/spider-subnet-zh_CN/#ip_1","text":"\u521b\u5efa\u5e94\u7528\u65f6\u6307\u5b9a\u4e86\u6ce8\u89e3 ipam.spidernet.io/ippool-reclaim \uff0c\u8be5\u6ce8\u89e3\u9ed8\u8ba4\u503c\u4e3a true \uff0c\u4e3a true \u65f6\uff0c\u968f\u7740\u5e94\u7528\u7684\u5220\u9664\uff0c\u5c06\u81ea\u52a8\u5220\u9664\u5bf9\u5e94\u7684\u81ea\u52a8\u6c60\u3002\u5728\u672c\u6587\u4e2d\u8bbe\u7f6e\u4e3a false \uff0c\u5176\u8868\u793a\u5220\u9664\u5e94\u7528\u65f6\uff0c\u81ea\u52a8\u521b\u5efa\u7684\u56fa\u5b9a IP \u6c60\u4f1a\u56de\u6536\u5176\u4e2d\u88ab\u5206\u914d\u7684 IP \uff0c\u4f46\u6c60\u4e0d\u4f1a\u88ab\u56de\u6536\uff0c\u5e76\u4e14\u5f53\u4f7f\u7528\u76f8\u540c\u914d\u7f6e\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\u65f6\uff0c\u4f1a\u81ea\u52a8\u7ee7\u627f\u8be5 IP \u6c60\u3002 ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false \u4f7f\u7528\u4e0a\u8ff0\u6240\u793a\u7684\u5e94\u7528 Yaml\uff0c\u518d\u6b21\u521b\u5efa\u540c\u540d\u5e94\u7528\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u4e0d\u4f1a\u518d\u6b21\u521b\u5efa\u65b0\u7684 IP \u6c60\uff0c\u5c06\u81ea\u52a8\u590d\u7528\u65e7 IP \u6c60\uff0c\u5e76\u4e14\u5176\u526f\u672c\u6570\u548c IP \u6c60\u7684 IP \u5206\u914d\u60c5\u51b5\u4e0e\u5b9e\u9645\u76f8\u540c\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false","title":"\u81ea\u52a8\u56de\u6536 IP \u6c60"},{"location":"usage/spider-subnet-zh_CN/#ip_2","text":"\u5982\u679c\u60a8\u5e0c\u671b\u4e3a Pod \u5b9e\u73b0\u591a\u7f51\u5361 IP \u7684\u56fa\u5b9a\uff0c\u53c2\u8003\u672c\u7ae0\u8282\u3002\u5728\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u6bcf\u4e2a\u526f\u672c\u62e5\u6709\u591a\u5f20\u7f51\u5361\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/subnets \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684\u5b50\u7f51\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u56fa\u5b9a IP \u6c60\uff0c\u4e0e\u672c\u5e94\u7528\u7ed1\u5b9a\uff0c\u5b9e\u73b0 IP \u56fa\u5b9a\u7684\u6548\u679c\u3002\u5728\u672c\u793a\u4f8b\u4e2d\u8be5\u6ce8\u89e3\u4f1a\u4e3a Pod \u521b\u5efa 2 \u4e2a\u5c5e\u4e8e\u4e0d\u540c Underlay \u5b50\u7f51\u7684\u56fa\u5b9a IP \u6c60\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 k8s.v1.cni.cncf.io/networks \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u53e6\u4e00\u5f20\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF \u6700\u7ec8, \u5728\u5e94\u7528\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a 2 \u4e2a Underlay \u5b50\u7f51\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u521b\u5efa\u51fa\u5bf9\u5e94\u7684\u56fa\u5b9a IP \u6c60\uff0c\u5e76\u4e0e\u5e94\u7528 Pod \u7684\u4e24\u5f20\u7f51\u5361\u5206\u522b\u5f62\u6210\u7ed1\u5b9a\u3002\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u7684\u56fa\u5b9a\u6c60\u90fd\u5c06\u4f1a\u81ea\u52a8\u7ee7\u627f\u5176\u6240\u5f52\u5c5e\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet \u4e5f\u652f\u6301\u591a\u7f51\u5361\u7684\u52a8\u6001 IP \u6269\u7f29\u5bb9\u3001\u81ea\u52a8\u56de\u6536 IP \u6c60\u7b49\u529f\u80fd\u3002","title":"\u81ea\u52a8\u56fa\u5b9a\u591a\u7f51\u5361 IP"},{"location":"usage/spider-subnet-zh_CN/#ippool","text":"\u5982\u4e0b\u662f\u4e00\u4e2a\u5f52\u5c5e\u4e8e\u5b50\u7f51 subnet-6 \uff0c\u5b50\u7f51\u53f7\u4e3a\uff1a 10.6.0.0/16 \u7684 IPPool \u5b9e\u4f8b\u793a\u4f8b\u3002\u8be5 IPPool \u5b9e\u4f8b\u7684\u53ef\u7528 IP \u8303\u56f4\u5fc5\u987b\u662f\u5b50\u7f51 subnet-6.spec.ips \u7684\u5b50\u96c6\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF \u4f7f\u7528\u4e0a\u8ff0 Yaml\uff0c\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\uff0c\u53ef\u4ee5\u770b\u5230\u5b83\u5f52\u5c5e\u4e8e\u5b50\u7f51\u53f7\u76f8\u540c\u7684\u5b50\u7f51\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u5bf9\u5e94\u5b50\u7f51\u7684\u7f51\u5173\u3001\u8def\u7531\u7b49\u5c5e\u6027\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 }","title":"\u624b\u52a8\u521b\u5efa IPPool \u5b9e\u4f8b\u7ee7\u627f\u5b50\u7f51\u5c5e\u6027"},{"location":"usage/spider-subnet-zh_CN/#_4","text":"SpiderSubnet \u529f\u80fd\u53ef\u4ee5\u5e2e\u52a9\u5c06\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u5458\u548c\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u5458\u7684\u8d23\u4efb\u5206\u5f00\uff0c\u652f\u6301\u81ea\u52a8\u521b\u5efa\u548c\u52a8\u6001\u6269\u5c55\u56fa\u5b9a IPPool \u5230\u6bcf\u4e2a\u9700\u8981\u9759\u6001 IP \u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u603b\u7ed3"},{"location":"usage/spider-subnet/","text":"SpiderSubnet English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction The SpiderSubnet resource represents a set of IP addresses. When application administrators need to allocate fixed IP addresses for their applications, they usually have to rely on platform administrators to provide the available IPs and routing information. However, this collaboration between different operational teams can lead to complex workflows for creating each application. With the Spiderpool's SpiderSubnet, this process is greatly simplified. SpiderSubnet can automatically allocate IP addresses from the subnet to SpiderIPPool, while also allowing applications to have fixed IP addresses. This automation significantly reduces operational costs and streamlines the workflow. SpiderSubnet features When the Subnet feature is enabled, each instance of IPPool belongs to the same subnet as the Subnet instance. The IP addresses in the IPPool instance must be a subset of those in the Subnet instance, and there should be no overlapping IP addresses among different IPPool instances. By default, the routing configuration of IPPool instances inherits the settings from the corresponding Subnet instance. To allocate fixed IP addresses for applications and decouple the roles of application administrators and their network counterparts, the following two practices can be adopted: Manually create IPPool: application administrators manually create IPPool instances, ensuring that the range of available IP addresses are defined in the corresponding Subnet instance. This allows them to have control over which specific IP addresses are used. Automatically create IPPool: application administrators can specify the name of the Subnet instance in the Pod annotation. Spiderpool automatically creates an IPPool instance with fixed IP addresses coming from the Subnet instance. The IP addresses in the instance are then allocated to Pods. Spiderpool also monitors application scaling and deletion events, automatically adjusting the IP pool size or removing IPs as needed. SpiderSubnet also supports several controllers, including ReplicaSet, Deployment, StatefulSet, DaemonSet, Job, CronJob, and third-party controllers. If you need to use a third-party controller, you can refer to the doc Spiderpool supports third-party controllers . Notice: Before v0.7.0 version, you have to create a SpiderSubnet resource before you create a SpiderIPPool resource with SpiderSubnet feature enabled. Since v0.7.0 version, you can create an orphan SpiderIPPool without a SpiderSubnet resource. Prerequisites A ready Kubernetes cluster. Helm has already been installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. And make sure that the helm installs the option ipam.enableSpiderSubnet=true . Install CNI To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF With the provided configuration, we create the following two Macvlan SpiderMultusConfigs that will automatically generate a Multus NetworkAttachmentDefinition CR corresponding to the host's ens192 and ens224 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m Create Subnets ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF Apply the above YAML configuration to create two SpiderSubnet instances and configure gateway and routing information for each of them. ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } Automatically fix IPs for single NIC The following YAML example creates two replicas of a Deployment application: ipam.spidernet.io/subnet : specifies the Spiderpool subnet. Spiderpool automatically selects IP addresses from this subnet to create a fixed IP pool associated with the application, ensuring fixed IP assignment. ipam.spidernet.io/ippool-ip-number : specifies the number of IP addresses in the IP pool. This annotation can be written in two ways: specifying a fixed quantity using a numeric value, such as ipam.spidernet.io/ippool-ip-number\uff1a1 , or specifying a relative quantity using a plus and a number, such as ipam.spidernet.io/ippool-ip-number\uff1a+1 . The latter means that the IP pool will dynamically maintain an additional IP address based on the number of replicas, ensuring temporary IPs are available during elastic scaling. ipam.spidernet.io/ippool-reclaim : indicate whether the automatically created fixed IP pool should be reclaimed upon application deletion. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF When creating the application, Spiderpool selects random IP addresses from the specified subnet to create a fixed IP pool that is bound to the Pod's network interface. The automatic pool automatically inherits the gateway and routing of the subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } To achieve the desired fixed IP pool, Spiderpool adds built-in labels and PodAffinity to bind the pool to the specific application. With the annotation of ipam.spidernet.io/ippool-reclaim: false , IP addresses are reclaimed upon application deletion, but the automatic pool itself remains intact. If you want the pool to be available for other applications, you need to manually remove these built-in labels and PodAffinity. Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name After multiple tests and Pod restarts, the Pod's IP remains fixed within the IP pool range: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none> Dynamically scale fixed IP pools \u3002When creating the application, the annotation ipam.spidernet.io/ippool-ip-number : '+1' is specified to allocate one extra fixed IP compared to the number of replicas. This configuration prevents any issues during rolling updates, ensuring that new Pods have available IPs while the old Pods are not deleted yet. Let's consider a scaling scenario where the replica count increases from 2 to 3. In this case, the two fixed IP pools associated with the application will automatically scale from 3 IPs to 4 IPs, maintaining one redundant IP address as expected: ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false With the information mentioned, scaling the application in Spiderpool is as simple as adjusting the replica count for the application. Automatically reclaim IP pools During application creation, the annotation ipam.spidernet.io/ippool-reclaim is specified. Its default value of true indicates that when the application is deleted, the corresponding automatic pool is also removed. However, false in this case means that upon application deletion, the assigned IPs within the automatically created fixed IP pool will be reclaimed, while retaining the pool itself. Applications created with the same configuration and name will automatically inherited the IP pool. ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false With the provided application YAML, creating an application with the same name again will automatically reuse the existing IP pools. Instead of creating new IP pools, the previously created ones will be utilized. This ensures consistency in the replica count and the IP allocation within the pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false Automatically fix IPs for multiple NICs To assign fixed IPs to the multiple NICs of Pods, follow the instructions in this section. In the example YAML below, a Deployment with two replicas is created, each having multiple network interfaces. The annotations therein include: ipam.spidernet.io/subnets : specify the subnet for Spiderpool. Spiderpool will randomly select IPs from this subnet to create fixed IP pools associated with the application, ensuring persistent IP assignment. In this example, this annotation creates two fixed IP pools belonging to two different underlay subnets for the Pods. v1.multus-cni.io/default-network : create a default network interface for the application. k8s.v1.cni.cncf.io/networks : create an additional network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF During application creation, Spiderpool randomly selects IPs from the specified two Underlay subnets to create fixed IP pools. These pools are then associated with the two network interfaces of the application's Pods. Each network interface's fixed pool automatically inherits the gateway, routing, and other properties of its respective subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet also supports dynamic IP scaling for multiple network interfaces and automatic reclamation of IP pools. Manually create IPPool instances inheriting the subnet's properties Below is an example of creating an IPPool instance that inherits the properties of subnet-6 with a subnet ID of 10.6.0.0/16 . The available IP range of this IPPool instance must be a subset of subnet-6.spec.ips . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF Using the provided YAML, you can manually create an IPPool instance that will inherit the attributes of the subnet having the specified subnet ID, such as gateway, routing, and other properties. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } Conclusion SpiderSubnet helps to separate the roles of infrastructure administrators and their application counterparts by enabling automatic creation and dynamic scaling of fixed IP pools for applications that require static IPs.","title":"IPAM of SpiderSubnet"},{"location":"usage/spider-subnet/#spidersubnet","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"SpiderSubnet"},{"location":"usage/spider-subnet/#introduction","text":"The SpiderSubnet resource represents a set of IP addresses. When application administrators need to allocate fixed IP addresses for their applications, they usually have to rely on platform administrators to provide the available IPs and routing information. However, this collaboration between different operational teams can lead to complex workflows for creating each application. With the Spiderpool's SpiderSubnet, this process is greatly simplified. SpiderSubnet can automatically allocate IP addresses from the subnet to SpiderIPPool, while also allowing applications to have fixed IP addresses. This automation significantly reduces operational costs and streamlines the workflow.","title":"Introduction"},{"location":"usage/spider-subnet/#spidersubnet-features","text":"When the Subnet feature is enabled, each instance of IPPool belongs to the same subnet as the Subnet instance. The IP addresses in the IPPool instance must be a subset of those in the Subnet instance, and there should be no overlapping IP addresses among different IPPool instances. By default, the routing configuration of IPPool instances inherits the settings from the corresponding Subnet instance. To allocate fixed IP addresses for applications and decouple the roles of application administrators and their network counterparts, the following two practices can be adopted: Manually create IPPool: application administrators manually create IPPool instances, ensuring that the range of available IP addresses are defined in the corresponding Subnet instance. This allows them to have control over which specific IP addresses are used. Automatically create IPPool: application administrators can specify the name of the Subnet instance in the Pod annotation. Spiderpool automatically creates an IPPool instance with fixed IP addresses coming from the Subnet instance. The IP addresses in the instance are then allocated to Pods. Spiderpool also monitors application scaling and deletion events, automatically adjusting the IP pool size or removing IPs as needed. SpiderSubnet also supports several controllers, including ReplicaSet, Deployment, StatefulSet, DaemonSet, Job, CronJob, and third-party controllers. If you need to use a third-party controller, you can refer to the doc Spiderpool supports third-party controllers . Notice: Before v0.7.0 version, you have to create a SpiderSubnet resource before you create a SpiderIPPool resource with SpiderSubnet feature enabled. Since v0.7.0 version, you can create an orphan SpiderIPPool without a SpiderSubnet resource.","title":"SpiderSubnet features"},{"location":"usage/spider-subnet/#prerequisites","text":"A ready Kubernetes cluster. Helm has already been installed.","title":"Prerequisites"},{"location":"usage/spider-subnet/#steps","text":"","title":"Steps"},{"location":"usage/spider-subnet/#install-spiderpool","text":"Refer to Installation to install Spiderpool. And make sure that the helm installs the option ipam.enableSpiderSubnet=true .","title":"Install Spiderpool"},{"location":"usage/spider-subnet/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE0 = \"ens192\" MACVLAN_MULTUS_NAME0 = \"macvlan- $MACVLAN_MASTER_INTERFACE0 \" MACVLAN_MASTER_INTERFACE1 = \"ens224\" MACVLAN_MULTUS_NAME1 = \"macvlan- $MACVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE1} EOF With the provided configuration, we create the following two Macvlan SpiderMultusConfigs that will automatically generate a Multus NetworkAttachmentDefinition CR corresponding to the host's ens192 and ens224 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m macvlan-ens224 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m macvlan-ens224 27m","title":"Install CNI"},{"location":"usage/spider-subnet/#create-subnets","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-6 spec: subnet: 10.6.0.0/16 gateway: 10.6.0.1 ips: - 10.6.168.101-10.6.168.110 routes: - dst: 10.7.0.0/16 gw: 10.6.0.1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderSubnet metadata: name: subnet-7 spec: subnet: 10.7.0.0/16 gateway: 10.7.0.1 ips: - 10.7.168.101-10.7.168.110 routes: - dst: 10.6.0.0/16 gw: 10.7.0.1 EOF Apply the above YAML configuration to create two SpiderSubnet instances and configure gateway and routing information for each of them. ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 0 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spidersubnet subnet-6 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spidersubnet subnet-7 -o jsonpath = '{.spec}' | jq { \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 }","title":"Create Subnets"},{"location":"usage/spider-subnet/#automatically-fix-ips-for-single-nic","text":"The following YAML example creates two replicas of a Deployment application: ipam.spidernet.io/subnet : specifies the Spiderpool subnet. Spiderpool automatically selects IP addresses from this subnet to create a fixed IP pool associated with the application, ensuring fixed IP assignment. ipam.spidernet.io/ippool-ip-number : specifies the number of IP addresses in the IP pool. This annotation can be written in two ways: specifying a fixed quantity using a numeric value, such as ipam.spidernet.io/ippool-ip-number\uff1a1 , or specifying a relative quantity using a plus and a number, such as ipam.spidernet.io/ippool-ip-number\uff1a+1 . The latter means that the IP pool will dynamically maintain an additional IP address based on the number of replicas, ensuring temporary IPs are available during elastic scaling. ipam.spidernet.io/ippool-reclaim : indicate whether the automatically created fixed IP pool should be reclaimed upon application deletion. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-1 spec: replicas: 2 selector: matchLabels: app: test-app-1 template: metadata: annotations: ipam.spidernet.io/subnet: |- { \"ipv4\": [\"subnet-6\"] } ipam.spidernet.io/ippool-ip-number: '+1' v1.multus-cni.io/default-network: kube-system/macvlan-ens192 ipam.spidernet.io/ippool-reclaim: \"false\" labels: app: test-app-1 spec: containers: - name: test-app-1 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF When creating the application, Spiderpool selects random IP addresses from the specified subnet to create a fixed IP pool that is bound to the Pod's network interface. The automatic pool automatically inherits the gateway and routing of the subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-2ndzl 1 /1 Running 0 46s 10 .6.168.101 controller-node-1 <none> <none> test-app-1-74cbbf654-4f2w2 1 /1 Running 0 46s 10 .6.168.103 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-1-eth0-a5bd3 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101-10.6.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-1\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } To achieve the desired fixed IP pool, Spiderpool adds built-in labels and PodAffinity to bind the pool to the specific application. With the annotation of ipam.spidernet.io/ippool-reclaim: false , IP addresses are reclaimed upon application deletion, but the automatic pool itself remains intact. If you want the pool to be available for other applications, you need to manually remove these built-in labels and PodAffinity. Additional Labels: ipam.spidernet.io/owner-application-gv ipam.spidernet.io/owner-application-kind ipam.spidernet.io/owner-application-namespace ipam.spidernet.io/owner-application-name ipam.spidernet.io/owner-application-uid Additional PodAffinity: ipam.spidernet.io/app-api-group ipam.spidernet.io/app-api-version ipam.spidernet.io/app-kind ipam.spidernet.io/app-namespace ipam.spidernet.io/app-name After multiple tests and Pod restarts, the Pod's IP remains fixed within the IP pool range: ~# kubectl delete po -l app = test-app-1 ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 7s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 7s 10 .6.168.102 controller-node-1 <none> <none>","title":"Automatically fix IPs for single NIC"},{"location":"usage/spider-subnet/#dynamically-scale-fixed-ip-pools","text":"\u3002When creating the application, the annotation ipam.spidernet.io/ippool-ip-number : '+1' is specified to allocate one extra fixed IP compared to the number of replicas. This configuration prevents any issues during rolling updates, ensuring that new Pods have available IPs while the old Pods are not deleted yet. Let's consider a scaling scenario where the replica count increases from 2 to 3. In this case, the two fixed IP pools associated with the application will automatically scale from 3 IPs to 4 IPs, maintaining one redundant IP address as expected: ~# kubectl scale deploy test-app-1 --replicas 3 deployment.apps/test-app-1 scaled ~# kubectl get po -l app = test-app-1 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-74cbbf654-7v54p 1 /1 Running 0 54s 10 .6.168.101 worker-node-1 <none> <none> test-app-1-74cbbf654-9w8gd 1 /1 Running 0 19s 10 .6.168.103 worker-node-1 <none> <none> test-app-1-74cbbf654-qzxp7 1 /1 Running 0 54s 10 .6.168.102 controller-node-1 <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 3 4 false With the information mentioned, scaling the application in Spiderpool is as simple as adjusting the replica count for the application.","title":"Dynamically scale fixed IP pools"},{"location":"usage/spider-subnet/#automatically-reclaim-ip-pools","text":"During application creation, the annotation ipam.spidernet.io/ippool-reclaim is specified. Its default value of true indicates that when the application is deleted, the corresponding automatic pool is also removed. However, false in this case means that upon application deletion, the assigned IPs within the automatically created fixed IP pool will be reclaimed, while retaining the pool itself. Applications created with the same configuration and name will automatically inherited the IP pool. ~# kubectl delete deploy test-app-1 deployment.apps \"test-app-1\" deleted ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 0 4 false With the provided application YAML, creating an application with the same name again will automatically reuse the existing IP pools. Instead of creating new IP pools, the previously created ones will be utilized. This ensures consistency in the replica count and the IP allocation within the pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-1-eth0-a5bd3 4 10 .6.0.0/16 2 3 false","title":"Automatically reclaim IP pools"},{"location":"usage/spider-subnet/#automatically-fix-ips-for-multiple-nics","text":"To assign fixed IPs to the multiple NICs of Pods, follow the instructions in this section. In the example YAML below, a Deployment with two replicas is created, each having multiple network interfaces. The annotations therein include: ipam.spidernet.io/subnets : specify the subnet for Spiderpool. Spiderpool will randomly select IPs from this subnet to create fixed IP pools associated with the application, ensuring persistent IP assignment. In this example, this annotation creates two fixed IP pools belonging to two different underlay subnets for the Pods. v1.multus-cni.io/default-network : create a default network interface for the application. k8s.v1.cni.cncf.io/networks : create an additional network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app-2 spec: replicas: 2 selector: matchLabels: app: test-app-2 template: metadata: annotations: ipam.spidernet.io/subnets: |- [ { \"interface\": \"eth0\", \"ipv4\": [\"subnet-6\"] },{ \"interface\": \"net1\", \"ipv4\": [\"subnet-7\"] } ] v1.multus-cni.io/default-network: kube-system/macvlan-ens192 k8s.v1.cni.cncf.io/networks: kube-system/macvlan-ens224 labels: app: test-app-2 spec: containers: - name: test-app-2 image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF During application creation, Spiderpool randomly selects IPs from the specified two Underlay subnets to create fixed IP pools. These pools are then associated with the two network interfaces of the application's Pods. Each network interface's fixed pool automatically inherits the gateway, routing, and other properties of its respective subnet. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT auto4-test-app-2-eth0-44037 4 10 .6.0.0/16 2 3 false auto4-test-app-2-net1-44037 4 10 .7.0.0/16 2 3 false ~# kubectl get po -l app = test-app-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-2-f5d6b8d6c-8hxvw 1 /1 Running 0 6m22s 10 .6.168.101 controller-node-1 <none> <none> test-app-2-f5d6b8d6c-rvx55 1 /1 Running 0 6m22s 10 .6.168.105 worker-node-1 <none> <none> ~# kubectl get spiderippool auto4-test-app-2-eth0-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.101\" , \"10.6.168.105-10.6.168.106\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 } ~# kubectl get spiderippool auto4-test-app-2-net1-44037 -ojsonpath ={ .spec } | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.7.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.7.168.101-10.7.168.103\" ] , \"podAffinity\" : { \"matchLabels\" : { \"ipam.spidernet.io/app-api-group\" : \"apps\" , \"ipam.spidernet.io/app-api-version\" : \"v1\" , \"ipam.spidernet.io/app-kind\" : \"Deployment\" , \"ipam.spidernet.io/app-name\" : \"test-app-2\" , \"ipam.spidernet.io/app-namespace\" : \"default\" } } , \"routes\" : [ { \"dst\" : \"10.6.0.0/16\" , \"gw\" : \"10.7.0.1\" } ] , \"subnet\" : \"10.7.0.0/16\" , \"vlan\" : 0 } SpiderSubnet also supports dynamic IP scaling for multiple network interfaces and automatic reclamation of IP pools.","title":"Automatically fix IPs for multiple NICs"},{"location":"usage/spider-subnet/#manually-create-ippool-instances-inheriting-the-subnets-properties","text":"Below is an example of creating an IPPool instance that inherits the properties of subnet-6 with a subnet ID of 10.6.0.0/16 . The available IP range of this IPPool instance must be a subset of subnet-6.spec.ips . ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - 10.6.168.108-10.6.168.110 subnet: 10.6.0.0/16 EOF Using the provided YAML, you can manually create an IPPool instance that will inherit the attributes of the subnet having the specified subnet ID, such as gateway, routing, and other properties. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 10 .6.0.0/16 0 3 false ~# kubectl get spidersubnet NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT subnet-6 4 10 .6.0.0/16 3 10 subnet-7 4 10 .7.0.0/16 0 10 ~# kubectl get spiderippool ippool-test -o jsonpath = '{.spec}' | jq { \"default\" : false, \"disable\" : false, \"gateway\" : \"10.6.0.1\" , \"ipVersion\" : 4 , \"ips\" : [ \"10.6.168.108-10.6.168.110\" ] , \"routes\" : [ { \"dst\" : \"10.7.0.0/16\" , \"gw\" : \"10.6.0.1\" } ] , \"subnet\" : \"10.6.0.0/16\" , \"vlan\" : 0 }","title":"Manually create IPPool instances inheriting the subnet's properties"},{"location":"usage/spider-subnet/#conclusion","text":"SpiderSubnet helps to separate the roles of infrastructure administrators and their application counterparts by enabling automatic creation and dynamic scaling of fixed IP pools for applications that require static IPs.","title":"Conclusion"},{"location":"usage/statefulset-zh_CN/","text":"StatefulSet \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u7531\u4e8e StatefulSet \u591a\u7528\u5728\u6709\u72b6\u6001\u7684\u670d\u52a1\u4e2d\uff0c\u56e0\u6b64\u5bf9\u7f51\u7edc\u7a33\u5b9a\u7684\u6807\u8bc6\u4fe1\u606f\u6709\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002Spiderpool \u80fd\u4fdd\u8bc1 StatefulSet \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 StatefulSet \u56fa\u5b9a\u5730\u5740 StatefulSet \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a StatefulSet \u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u7684\u6545\u969c\u91cd\u5efa\u7684\u60c5\u51b5 StatefulSet \u5728\u526f\u672c\u6570\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5220\u9664 Pod \u4f7f\u5176\u91cd\u542f\u7684\u60c5\u51b5 \u6b64\u5916\uff0cStatefulSet \u548c Deployment \u63a7\u5236\u5668\uff0c\u5bf9\u4e8e IP \u5730\u5740\u56fa\u5b9a\u7684\u9700\u6c42\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u662f Pod UUID \u53d1\u751f\u4e86\u53d8\u5316\uff0c\u5176\u662f\u6709\u72b6\u6001\u7684\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u5e0c\u671b\u8be5 Pod \u91cd\u542f\u524d\u540e\uff0c\u4ecd\u80fd\u5206\u914d\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u5bf9\u4e8e Deployment\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u5b57\u548c Pod UUID \u90fd\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u6240\u4ee5\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5e76\u4e0d\u8981\u65b0\u8001\u4ea4\u66ff\u7684 Pod \u4f7f\u7528\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u6211\u4eec\u53ef\u80fd\u53ea\u5e0c\u671b Deployment \u4e2d\u6240\u6709\u526f\u672c\u6240\u4f7f\u7528\u7684 IP \u662f\u56fa\u5b9a\u5728\u67d0\u4e2a IP \u8303\u56f4\u5185\u5373\u53ef\u3002 \u5f00\u6e90\u793e\u533a\u7684\u4f17\u591a CNI \u65b9\u6848\u5e76\u4e0d\u80fd\u5f88\u597d\u7684\u652f\u6301 StatefulSet \u7684\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002\u800c Spiderpool \u63d0\u4f9b\u7684 StatefulSet \u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u8bc1 StatefulSet Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cStatefulSet \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba StatefulSet \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u914d\u5230\u76f8\u540c IP\u3002\u82e5\u5173\u95ed\uff0cStatefulSet \u5e94\u7528\u5c06\u88ab\u5f53\u505a\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7 --set ipam.enableStatefulSet=false \u5173\u95ed\u3002 \u5728 StatefulSet \u526f\u672c\u7ecf\u7531 \u7f29\u5bb9 \u5230 \u6269\u5bb9 \u7684\u53d8\u5316\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u5e76\u4e0d\u4fdd\u8bc1\u65b0\u6269\u5bb9 Pod \u80fd\u591f\u83b7\u53d6\u5230\u4e4b\u524d\u7f29\u5bb9 Pod \u7684 IP \u5730\u5740\u3002 \u76ee\u524d\uff0c\u5f53 StatefulSet \u51c6\u5907\u5c31\u7eea\u5e76\u4e14\u5176 Pod \u6b63\u5728\u8fd0\u884c\u65f6\uff0c\u5373\u4f7f\u4fee\u6539 StatefulSet \u6ce8\u89e3\u6307\u5b9a\u4e86\u53e6\u4e00\u4e2a IP \u6c60\uff0c\u5e76\u91cd\u542f Pod \uff0cPod IP \u5730\u5740\u4e5f\u4e0d\u4f1a\u751f\u6548\u5230\u65b0\u7684 IP \u6c60\u8303\u56f4\u5185\uff0c\u800c\u662f\u7ee7\u7eed\u4f7f\u7528\u65e7\u7684\u56fa\u5b9a IP \u3002 \u5b9e\u65bd\u8981\u6c42 \u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u5b89\u88c5 Spiderpool \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableStatefulSet=true \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m \u521b\u5efa IPPool ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF \u521b\u5efa StatefulSet \u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 StatefulSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9\u4e00\u4e9b IP \u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\uff0c\u5b9e\u73b0 StatefulSet \u5e94\u7528\u7684 IP \u56fa\u5b9a\u6548\u679c\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8\uff0c\u5728 StatefulSet \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> \u91cd\u542f StatefulSet Pod\uff0c\u89c2\u5bdf\u5230\u6bcf\u4e2a Pod \u7684 IP \u5747\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> \u6269\u5bb9\u3001\u7f29\u5bb9 StatefulSet Pod \uff0c\u89c2\u5bdf\u6bcf\u4e2a Pod \u7684 IP \u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none> \u603b\u7ed3 Spiderpool \u80fd\u4fdd\u8bc1 Statefulset Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 Statefulset \u7c7b\u578b\u63a7\u5236\u5668\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002","title":"StatefulSet"},{"location":"usage/statefulset-zh_CN/#statefulset","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"StatefulSet"},{"location":"usage/statefulset-zh_CN/#_1","text":"\u7531\u4e8e StatefulSet \u591a\u7528\u5728\u6709\u72b6\u6001\u7684\u670d\u52a1\u4e2d\uff0c\u56e0\u6b64\u5bf9\u7f51\u7edc\u7a33\u5b9a\u7684\u6807\u8bc6\u4fe1\u606f\u6709\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002Spiderpool \u80fd\u4fdd\u8bc1 StatefulSet \u7684 Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/statefulset-zh_CN/#statefulset_1","text":"StatefulSet \u4f1a\u5728\u4ee5\u4e0b\u4e00\u4e9b\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u56fa\u5b9a\u5730\u5740\u7684\u4f7f\u7528\uff1a StatefulSet \u5bf9\u5e94\u7684 Pod \u51fa\u73b0\u7684\u6545\u969c\u91cd\u5efa\u7684\u60c5\u51b5 StatefulSet \u5728\u526f\u672c\u6570\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5220\u9664 Pod \u4f7f\u5176\u91cd\u542f\u7684\u60c5\u51b5 \u6b64\u5916\uff0cStatefulSet \u548c Deployment \u63a7\u5236\u5668\uff0c\u5bf9\u4e8e IP \u5730\u5740\u56fa\u5b9a\u7684\u9700\u6c42\u662f\u4e0d\u4e00\u6837\u7684\uff1a \u5bf9\u4e8e StatefulSet\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u662f Pod UUID \u53d1\u751f\u4e86\u53d8\u5316\uff0c\u5176\u662f\u6709\u72b6\u6001\u7684\uff0c\u5e94\u7528\u7ba1\u7406\u5458\u5e0c\u671b\u8be5 Pod \u91cd\u542f\u524d\u540e\uff0c\u4ecd\u80fd\u5206\u914d\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u5bf9\u4e8e Deployment\uff0cPod \u526f\u672c\u91cd\u542f\u524d\u540e\uff0c\u5176 Pod \u540d\u5b57\u548c Pod UUID \u90fd\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u6240\u4ee5\u662f\u65e0\u72b6\u6001\u7684\uff0c\u56e0\u6b64\u5e76\u4e0d\u8981\u65b0\u8001\u4ea4\u66ff\u7684 Pod \u4f7f\u7528\u76f8\u540c\u7684 IP \u5730\u5740\uff0c\u6211\u4eec\u53ef\u80fd\u53ea\u5e0c\u671b Deployment \u4e2d\u6240\u6709\u526f\u672c\u6240\u4f7f\u7528\u7684 IP \u662f\u56fa\u5b9a\u5728\u67d0\u4e2a IP \u8303\u56f4\u5185\u5373\u53ef\u3002 \u5f00\u6e90\u793e\u533a\u7684\u4f17\u591a CNI \u65b9\u6848\u5e76\u4e0d\u80fd\u5f88\u597d\u7684\u652f\u6301 StatefulSet \u7684\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002\u800c Spiderpool \u63d0\u4f9b\u7684 StatefulSet \u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u8bc1 StatefulSet Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002 \u8be5\u529f\u80fd\u9ed8\u8ba4\u5f00\u542f\u3002\u82e5\u5f00\u542f\uff0c\u65e0\u4efb\u4f55\u9650\u5236\uff0cStatefulSet \u53ef\u901a\u8fc7\u6709\u9650 IP \u5730\u5740\u96c6\u5408\u7684 IP \u6c60\u6765\u56fa\u5316 IP \u7684\u8303\u56f4\uff0c\u4f46\u662f\uff0c\u65e0\u8bba StatefulSet \u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u7684 IP \u6c60\uff0c\u5b83\u7684 Pod \u90fd\u53ef\u4ee5\u6301\u7eed\u5206\u914d\u5230\u76f8\u540c IP\u3002\u82e5\u5173\u95ed\uff0cStatefulSet \u5e94\u7528\u5c06\u88ab\u5f53\u505a\u65e0\u72b6\u6001\u5bf9\u5f85\uff0c\u4f7f\u7528 Helm \u5b89\u88c5 Spiderpool \u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7 --set ipam.enableStatefulSet=false \u5173\u95ed\u3002 \u5728 StatefulSet \u526f\u672c\u7ecf\u7531 \u7f29\u5bb9 \u5230 \u6269\u5bb9 \u7684\u53d8\u5316\u8fc7\u7a0b\u4e2d\uff0cSpiderpool \u5e76\u4e0d\u4fdd\u8bc1\u65b0\u6269\u5bb9 Pod \u80fd\u591f\u83b7\u53d6\u5230\u4e4b\u524d\u7f29\u5bb9 Pod \u7684 IP \u5730\u5740\u3002 \u76ee\u524d\uff0c\u5f53 StatefulSet \u51c6\u5907\u5c31\u7eea\u5e76\u4e14\u5176 Pod \u6b63\u5728\u8fd0\u884c\u65f6\uff0c\u5373\u4f7f\u4fee\u6539 StatefulSet \u6ce8\u89e3\u6307\u5b9a\u4e86\u53e6\u4e00\u4e2a IP \u6c60\uff0c\u5e76\u91cd\u542f Pod \uff0cPod IP \u5730\u5740\u4e5f\u4e0d\u4f1a\u751f\u6548\u5230\u65b0\u7684 IP \u6c60\u8303\u56f4\u5185\uff0c\u800c\u662f\u7ee7\u7eed\u4f7f\u7528\u65e7\u7684\u56fa\u5b9a IP \u3002","title":"StatefulSet \u56fa\u5b9a\u5730\u5740"},{"location":"usage/statefulset-zh_CN/#_2","text":"\u4e00\u5957 Kubernetes \u96c6\u7fa4\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/statefulset-zh_CN/#_3","text":"","title":"\u6b65\u9aa4"},{"location":"usage/statefulset-zh_CN/#spiderpool","text":"\u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool. \u5176\u4e2d\uff0c\u52a1\u5fc5\u786e\u4fdd helm \u5b89\u88c5\u9009\u9879 ipam.enableStatefulSet=true","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/statefulset-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a master\uff1a\u5728\u6b64\u793a\u4f8b\u7528\u63a5\u53e3 ens192 \u4f5c\u4e3a master \u7684\u53c2\u6570\u3002 MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/statefulset-zh_CN/#ippool","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF","title":"\u521b\u5efa IPPool"},{"location":"usage/statefulset-zh_CN/#statefulset_2","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 StatefulSet \u5e94\u7528\uff0c\u5176\u4e2d\uff1a ipam.spidernet.io/ippool \uff1a\u7528\u4e8e\u6307\u5b9a Spiderpool \u7684 IP \u6c60\uff0cSpiderpool \u4f1a\u81ea\u52a8\u5728\u8be5\u6c60\u4e2d\u9009\u62e9\u4e00\u4e9b IP \u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\uff0c\u5b9e\u73b0 StatefulSet \u5e94\u7528\u7684 IP \u56fa\u5b9a\u6548\u679c\u3002 v1.multus-cni.io/default-network \uff1a\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF \u6700\u7ec8\uff0c\u5728 StatefulSet \u5e94\u7528\u88ab\u521b\u5efa\u65f6\uff0cSpiderpool \u4f1a\u4ece\u6307\u5b9a IPPool \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e9b IP \u6765\u4e0e\u5e94\u7528\u5f62\u6210\u7ed1\u5b9a\u5173\u7cfb\u3002 ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> \u91cd\u542f StatefulSet Pod\uff0c\u89c2\u5bdf\u5230\u6bcf\u4e2a Pod \u7684 IP \u5747\u4e0d\u4f1a\u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> \u6269\u5bb9\u3001\u7f29\u5bb9 StatefulSet Pod \uff0c\u89c2\u5bdf\u6bcf\u4e2a Pod \u7684 IP \u53d8\u5316\uff0c\u7b26\u5408\u9884\u671f\u3002 ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none>","title":"\u521b\u5efa StatefulSet \u5e94\u7528"},{"location":"usage/statefulset-zh_CN/#_4","text":"Spiderpool \u80fd\u4fdd\u8bc1 Statefulset Pod \u5728\u91cd\u542f\u3001\u91cd\u5efa\u573a\u666f\u4e0b\uff0c\u6301\u7eed\u83b7\u53d6\u5230\u76f8\u540c\u7684 IP \u5730\u5740\u3002\u80fd\u5f88\u597d\u7684\u6ee1\u8db3 Statefulset \u7c7b\u578b\u63a7\u5236\u5668\u7684\u56fa\u5b9a IP \u9700\u6c42\u3002","title":"\u603b\u7ed3"},{"location":"usage/statefulset/","text":"StatefulSet English \uff5c \u7b80\u4f53\u4e2d\u6587 Introduction Due to StatefulSet being commonly used for stateful services, there is a higher demand for stable network identifiers. Spiderpool ensures that StatefulSet Pods consistently retain the same IP address, even in scenarios such as restarts or rebuilds. StatefulSet features StatefulSet utilizes fixed addresses in the following scenarios: When a StatefulSet Pod fails and needs to be reconstructed. Once a Pod is deleted and needs to be restarted and the replicas of the StatefulSet remains unchanged. The requirements for fixed IP address differ between StatefulSet and Deployment: For StatefulSet, the Pod's name remains the same throughout Pod restarts despite its changed UUID. As the Pod is stateful, application administrators hope that each Pod continues to be assigned the same IP address after restarts. For Deployment, both the Pod name and its UUID change after restarts. Deployment Pods are stateless, so there is no need to maintain the same IP address between Pod restarts. Instead, administrators often prefer IP addresses to be allocated within a specified range for all replicas in Deployment. Many open-source CNI solutions provide limited support for fixing IP addresses for StatefulSet. However, Spiderpool's StatefulSet solution guarantees consistent allocation of the same IP address to the Pods during restarts and rebuilds. This feature is enabled by default. When it is enabled, StatefulSet Pods can be assigned fixed IP addresses from a specified IP pool range. Whether or not using a fixed IP pool, StatefulSet Pods will consistently receive the same IP address. StatefulSet applications will be treated as stateless if the feature is disabled. You can disable it during the installation of Spiderpool using Helm via the flag --set ipam.enableStatefulSet=false . During the transition from scaling down to scaling up StatefulSet replicas, Spiderpool does not guarantee that new Pods will inherit the IP addresses previously used by the scaled-down Pods. Currently, when a StatefulSet Pod is running, modifying the StatefulSet annotation to specify a different IP pool and restarting the Pod will not cause the Pod IP addresses to be allocated from the new IP pool range. Instead, the Pod will continue using their existing fixed IP addresses. Prerequisites A ready Kubernetes cluster. Helm has already been installed. Steps Install Spiderpool Refer to Installation to install Spiderpool. And make sure that the helm installs the option ipam.enableStatefulSet=true . Install CNI To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create the following Macvlan SpiderMultusConfig that will automatically generate a Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m Create an IP pool ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF Create StatefulSet applications The following YAML example creates a StatefulSet application with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for Spiderpool. Spiderpool automatically selects some IP addresses from this pool and bind them to the application, ensuring that the IP addresses remain fixed for the StatefulSet application. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF When the StatefulSet application is created, Spiderpool will select a random set of IP addresses from the specified IP pool and bind them to the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> Upon restarting StatefulSet Pods, it is observed that each Pod retains its assigned IP address. ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> Upon scaling up or down the StatefulSet Pods, the IP addresses of each Pod change as expected. ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none> Conclusion Spiderpool ensures that StatefulSet Pods maintain a consistent IP address even during scenarios like restarts or rebuilds, satisfying the requirement for fixed IP addresses in StatefulSet.","title":"IPAM for StatefulSet"},{"location":"usage/statefulset/#statefulset","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"StatefulSet"},{"location":"usage/statefulset/#introduction","text":"Due to StatefulSet being commonly used for stateful services, there is a higher demand for stable network identifiers. Spiderpool ensures that StatefulSet Pods consistently retain the same IP address, even in scenarios such as restarts or rebuilds.","title":"Introduction"},{"location":"usage/statefulset/#statefulset-features","text":"StatefulSet utilizes fixed addresses in the following scenarios: When a StatefulSet Pod fails and needs to be reconstructed. Once a Pod is deleted and needs to be restarted and the replicas of the StatefulSet remains unchanged. The requirements for fixed IP address differ between StatefulSet and Deployment: For StatefulSet, the Pod's name remains the same throughout Pod restarts despite its changed UUID. As the Pod is stateful, application administrators hope that each Pod continues to be assigned the same IP address after restarts. For Deployment, both the Pod name and its UUID change after restarts. Deployment Pods are stateless, so there is no need to maintain the same IP address between Pod restarts. Instead, administrators often prefer IP addresses to be allocated within a specified range for all replicas in Deployment. Many open-source CNI solutions provide limited support for fixing IP addresses for StatefulSet. However, Spiderpool's StatefulSet solution guarantees consistent allocation of the same IP address to the Pods during restarts and rebuilds. This feature is enabled by default. When it is enabled, StatefulSet Pods can be assigned fixed IP addresses from a specified IP pool range. Whether or not using a fixed IP pool, StatefulSet Pods will consistently receive the same IP address. StatefulSet applications will be treated as stateless if the feature is disabled. You can disable it during the installation of Spiderpool using Helm via the flag --set ipam.enableStatefulSet=false . During the transition from scaling down to scaling up StatefulSet replicas, Spiderpool does not guarantee that new Pods will inherit the IP addresses previously used by the scaled-down Pods. Currently, when a StatefulSet Pod is running, modifying the StatefulSet annotation to specify a different IP pool and restarting the Pod will not cause the Pod IP addresses to be allocated from the new IP pool range. Instead, the Pod will continue using their existing fixed IP addresses.","title":"StatefulSet features"},{"location":"usage/statefulset/#prerequisites","text":"A ready Kubernetes cluster. Helm has already been installed.","title":"Prerequisites"},{"location":"usage/statefulset/#steps","text":"","title":"Steps"},{"location":"usage/statefulset/#install-spiderpool","text":"Refer to Installation to install Spiderpool. And make sure that the helm installs the option ipam.enableStatefulSet=true .","title":"Install Spiderpool"},{"location":"usage/statefulset/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configuration, Spiderpool introduces the SpiderMultusConfig CR, which automates the management of Multus NetworkAttachmentDefinition CRs. Here is an example of creating a Macvlan SpiderMultusConfig: master: the interface ens192 is used as the spec for master. MACVLAN_MASTER_INTERFACE = \"ens192\" MACVLAN_MULTUS_NAME = \"macvlan- $MACVLAN_MASTER_INTERFACE \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${MACVLAN_MULTUS_NAME} namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF With the provided configuration, we create the following Macvlan SpiderMultusConfig that will automatically generate a Multus NetworkAttachmentDefinition CR. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-ens192 26m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-ens192 27m","title":"Install CNI"},{"location":"usage/statefulset/#create-an-ip-pool","text":"~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: test-ippool spec: subnet: 10.6.0.0/16 ips: - 10.6.168.101-10.6.168.110 EOF","title":"Create an IP pool"},{"location":"usage/statefulset/#create-statefulset-applications","text":"The following YAML example creates a StatefulSet application with 2 replicas: ipam.spidernet.io/ippool : specify the IP pool for Spiderpool. Spiderpool automatically selects some IP addresses from this pool and bind them to the application, ensuring that the IP addresses remain fixed for the StatefulSet application. v1.multus-cni.io/default-network : create a default network interface for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: StatefulSet metadata: name: test-sts spec: replicas: 2 selector: matchLabels: app: test-sts template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"test-ippool\"] } v1.multus-cni.io/default-network: kube-system/macvlan-ens192 labels: app: test-sts spec: containers: - name: test-sts image: nginx imagePullPolicy: IfNotPresent EOF When the StatefulSet application is created, Spiderpool will select a random set of IP addresses from the specified IP pool and bind them to the application. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT test-ippool 4 10 .6.0.0/16 2 10 false ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 3m13s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 3m12s 10 .6.168.102 node1 <none> <none> Upon restarting StatefulSet Pods, it is observed that each Pod retains its assigned IP address. ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 18s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 17s 10 .6.168.102 node1 <none> <none> Upon scaling up or down the StatefulSet Pods, the IP addresses of each Pod change as expected. ~# kubectl scale deploy test-sts --replicas 3 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 4m58s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4m57s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 4s 10 .6.168.109 node2 <none> <none> ~# kubectl get pod | grep \"test-sts\" | awk '{print $1}' | xargs kubectl delete pod pod \"test-sts-0\" deleted pod \"test-sts-1\" deleted pod \"test-sts-2\" deleted ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 6s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 4s 10 .6.168.102 node1 <none> <none> test-sts-2 1 /1 Running 0 3s 10 .6.168.109 node2 <none> <none> ~# kubectl scale sts test-sts --replicas 2 statefulset.apps/test-sts scaled ~# kubectl get po -l app = test-sts -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-sts-0 1 /1 Running 0 88s 10 .6.168.105 node2 <none> <none> test-sts-1 1 /1 Running 0 86s 10 .6.168.102 node1 <none> <none>","title":"Create StatefulSet applications"},{"location":"usage/statefulset/#conclusion","text":"Spiderpool ensures that StatefulSet Pods maintain a consistent IP address even during scenarios like restarts or rebuilds, satisfying the requirement for fixed IP addresses in StatefulSet.","title":"Conclusion"},{"location":"usage/third-party-controller-zh_CN/","text":"IPAM for third-party controllers English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"IPAM for third-party controllers"},{"location":"usage/third-party-controller-zh_CN/#ipam-for-third-party-controllers","text":"English \uff5c \u7b80\u4f53\u4e2d\u6587","title":"IPAM for third-party controllers"},{"location":"usage/third-party-controller/","text":"IPAM for third-party controllers \u7b80\u4f53\u4e2d\u6587 | English Description Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to Pods created not by kubernetes-native controller. There are two ways to do this: Manual ippool The administrator could create ippool object and assign IP to Pods. Automatical ippool Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application. This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet. This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: does not support automatically scale up and down the IP does not support automatically delete the ippool In the future, spiderpool may support all operation of automatical ippool. Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as stateless Pod like Deployment , this means Pods created by none kubernetes-native controller is able to fix the IP range like Deployment , but not able to bind each Pod to a specific IP address like Statefulset . Get Started It will use OpenKruise to demonstrate how Spiderpool supports third-party controllers. Set up Spiderpool See installation for more details. Set up OpenKruise Please refer to OpenKruise Create Pod by Manual ippool way Create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations ipam.spidernet.io/ippool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2 Create Pod by Automatical ippool way Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations ipam.spidernet.io/subnet apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] NOTICE: You must specify a fixed IP number for auto-created IPPool like ipam.spidernet.io/ippool-ip-number: \"5\" . Because Spiderpool has no idea about the replica number, so it does not support annotation like ipam.spidernet.io/ippool-ip-number: \"+5\" . Check status As expected, Spiderpool will create auto-created IPPool from subnet-demo-v4 and subnet-demo-v6 objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"IPAM for custom controllers"},{"location":"usage/third-party-controller/#ipam-for-third-party-controllers","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"IPAM for third-party controllers"},{"location":"usage/third-party-controller/#description","text":"Operator is popularly used to implement customized controller. Spiderpool supports to assign IP to Pods created not by kubernetes-native controller. There are two ways to do this: Manual ippool The administrator could create ippool object and assign IP to Pods. Automatical ippool Spiderpool support to automatically manage ippool for application, it could create, delete, scale up and down a dedicated spiderippool object with static IP address just for one application. This feature uses informer technology to watch application, parses its replicas number and manage spiderippool object, it works well with kubernetes-native controller like Deployment, ReplicaSet, StatefulSet, Job, CronJob, DaemonSet. This feature also support none kubernetes-native controller, but Spiderpool could not parse the object yaml of none kubernetes-native controller, has some limitations: does not support automatically scale up and down the IP does not support automatically delete the ippool In the future, spiderpool may support all operation of automatical ippool. Another issue about none kubernetes-native controller is stateful or stateless. Because Spiderpool has no idea whether application created by none kubernetes-native controller is stateful or not. So Spiderpool treats them as stateless Pod like Deployment , this means Pods created by none kubernetes-native controller is able to fix the IP range like Deployment , but not able to bind each Pod to a specific IP address like Statefulset .","title":"Description"},{"location":"usage/third-party-controller/#get-started","text":"It will use OpenKruise to demonstrate how Spiderpool supports third-party controllers.","title":"Get Started"},{"location":"usage/third-party-controller/#set-up-spiderpool","text":"See installation for more details.","title":"Set up Spiderpool"},{"location":"usage/third-party-controller/#set-up-openkruise","text":"Please refer to OpenKruise","title":"Set up OpenKruise"},{"location":"usage/third-party-controller/#create-pod-by-manual-ippool-way","text":"Create a custom IPPool. kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/basic/custom-ipv4-ippool.yaml apiVersion : spiderpool.spidernet.io/v2beta1 kind : SpiderIPPool metadata : name : custom-ipv4-ippool spec : subnet : 172.18.41.0/24 ips : - 172.18.41.40-172.18.41.50 Create an OpenKruise CloneSet that has 3 replicas, and sepecify the ippool by annotations ipam.spidernet.io/ippool apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/ippool : |- { \"ipv4\": [\"custom-ipv4-ippool\"] } labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] As expected, Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from IPPool custom-ipv4-ippool . kubectl get po -l app = custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-8m9ls 1 /1 Running 0 96s 172 .18.41.44 spider-worker <none> 2 /2 custom-kruise-cloneset-c4z9f 1 /1 Running 0 96s 172 .18.41.50 spider-worker <none> 2 /2 custom-kruise-cloneset-w9kfm 1 /1 Running 0 96s 172 .18.41.46 spider-worker <none> 2 /2","title":"Create Pod by Manual ippool way"},{"location":"usage/third-party-controller/#create-pod-by-automatical-ippool-way","text":"Create an OpenKruise CloneSet that has 3 replicas, and specify the subnet by annotations ipam.spidernet.io/subnet apiVersion : apps.kruise.io/v1alpha1 kind : CloneSet metadata : name : custom-kruise-cloneset spec : replicas : 3 selector : matchLabels : app : custom-kruise-cloneset template : metadata : annotations : ipam.spidernet.io/subnet : |- {\"ipv4\": [\"subnet-demo-v4\"], \"ipv6\": [\"subnet-demo-v6\"]} ipam.spidernet.io/ippool-ip-number : \"5\" labels : app : custom-kruise-cloneset spec : containers : - name : custom-kruise-cloneset image : busybox imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"trap : TERM INT; sleep infinity & wait\" ] NOTICE: You must specify a fixed IP number for auto-created IPPool like ipam.spidernet.io/ippool-ip-number: \"5\" . Because Spiderpool has no idea about the replica number, so it does not support annotation like ipam.spidernet.io/ippool-ip-number: \"+5\" . Check status As expected, Spiderpool will create auto-created IPPool from subnet-demo-v4 and subnet-demo-v6 objects. And Pods of OpenKruise CloneSet custom-kruise-cloneset will be assigned with IP addresses from the created IPPools. $ kubectl get sp | grep kruise NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE APP-NAMESPACE auto4-custom-kruise-cloneset-eth0-028d6 4 172.16.0.0/16 3 5 false false default auto6-custom-kruise-cloneset-eth0-028d6 6 fc00:f853:ccd:e790::/64 3 5 false false default ------------------------------------------------------------------------------------------ $ kubectl get po -l app=custom-kruise-cloneset -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES custom-kruise-cloneset-f52dn 1/1 Running 0 61s 172.16.41.4 spider-worker <none> 2/2 custom-kruise-cloneset-mq67v 1/1 Running 0 61s 172.16.41.5 spider-worker <none> 2/2 custom-kruise-cloneset-nprpf 1/1 Running 0 61s 172.16.41.1 spider-worker <none> 2/2","title":"Create Pod by Automatical ippool way"},{"location":"usage/underlay_cni_service-zh_CN/","text":"Underlay CNI \u8bbf\u95ee Service \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI(\u5982 Macvlan\u3001IPVlan\u3001Sriov-CNI \u7b49)\u4e00\u822c\u5bf9\u63a5\u5e95\u5c42\u7f51\u7edc\uff0c\u5f80\u5f80\u5e76\u4e0d\u539f\u751f\u652f\u6301\u8bbf\u95ee\u96c6\u7fa4\u7684 Service \u3002\u8fd9\u5927\u591a\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee Service \u9700\u8981\u7ecf\u8fc7\u4ea4\u6362\u673a\u7684\u7f51\u5173\u8f6c\u53d1\uff0c \u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 Service \u7684\u8def\u7531\uff0c\u9020\u6210\u65e0\u6cd5\u6b63\u786e\u8def\u7531\u8bbf\u95ee Service \u7684\u62a5\u6587\uff0c\u4ece\u800c\u4e22\u5305\u3002Spiderpool \u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u7684\u65b9\u6848\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898: \u901a\u8fc7 Spiderpool coordinator + kube-proxy \u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service \u901a\u8fc7 Cilium Without Kube-proxy \u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service \u8fd9\u4e24\u79cd\u65b9\u6848\u90fd\u89e3\u51b3\u4e86 Underlay CNI \u65e0\u6cd5\u8bbf\u95ee Service \u7684\u95ee\u9898\uff0c\u4f46\u5b9e\u73b0\u539f\u7406\u6709\u4e9b\u4e0d\u540c\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u5f0f: \u57fa\u4e8e Spiderpool coordinator + kube-proxy Spiderpool \u5185\u7f6e coordinator \u63d2\u4ef6\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u65e0\u7f1d\u5bf9\u63a5 kube-proxy \u4ee5\u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service\u3002 \u6839\u636e\u4e0d\u540c\u7684\u573a\u666f\uff0c coordinator \u53ef\u4ee5\u8fd0\u884c\u5728 underlay \u6216 overlay \u6a21\u5f0f\uff0c\u867d\u7136\u5b9e\u73b0\u65b9\u5f0f\u7a0d\u663e\u4e0d\u540c\uff0c\u4f46 \u6838\u5fc3\u539f\u7406\u90fd\u662f\u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u7684\u4e3b\u673a\u7f51\u7edc\u534f\u8bae\u6808\u4e0a\uff0c\u518d\u7ecf\u8fc7 Kube-proxy \u521b\u5efa\u7684 IPtables \u89c4\u5219\u505a\u8f6c\u53d1\u3002 \u4e0b\u9762\u662f\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b\u56fe\u4ecb\u7ecd: \u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Underlay \u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u63d2\u4ef6\u5c06\u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u5c06\u4e00\u7aef\u653e\u7f6e\u4e8e\u4e3b\u673a\uff0c\u53e6\u4e00\u7aef\u653e\u7f6e\u4e0e Pod \u7684 network namespace \u4e2d\uff0c\u7136\u540e\u5728 Pod \u91cc\u9762\u8bbe\u7f6e\u4e00\u4e9b\u8def\u7531\u89c4\u5219\uff0c \u4f7f Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth \u8bbe\u5907\u8f6c\u53d1\u3002 coordinator \u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c \u5b83\u5c06\u81ea\u52a8\u5224\u65ad\u5e94\u8be5\u8fd0\u884c underlay \u6216 overlay \u6a21\u5f0f\u3002\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> \u5373\u53ef\u3002 \u5f53\u4ee5 Underlay \u6a21\u5f0f\u521b\u5efa Pod \u540e\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 \u662f\u8282\u70b9\u7684 IP,\u786e\u4fdd Pod \u8bbf\u95ee\u672c\u8282\u70b9\u65f6\u4ece veth0 \u8f6c\u53d1\u3002 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 \u662f\u96c6\u7fa4 Service \u7684 CIDR, \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth0 \u8f6c\u53d1\u3002 \u8fd9\u4e2a\u65b9\u6848\u5f3a\u70c8\u4f9d\u8d56\u4e0e kube-proxy \u7684 MASQUERADE , \u5426\u5219\u56de\u590d\u62a5\u6587\u5c06\u76f4\u63a5\u8f6c\u53d1\u7ed9\u6e90 Pod, \u5982\u679c\u7ecf\u8fc7\u4e00\u4e9b\u5b89\u5168\u8bbe\u5907\uff0c\u5c06\u4f1a\u4e22\u5f03\u6570\u636e\u5305\u3002\u6240\u4ee5\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e kube-proxy \u7684 masqueradeAll \u4e3a true\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u4e0d\u540c\uff0c \u65e0\u9700\u5f00\u542f masqueradeAll , \u5b83\u4eec\u4e4b\u95f4\u7684\u8bbf\u95ee\u5c06\u4f1a\u88ab SNAT\u3002 \u5982\u679c Pod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u76f8\u540c\uff0c\u90a3\u6211\u4eec\u5fc5\u987b\u8981\u8bbe\u7f6e masqueradeAll \u4e3a true\u3002 \u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Overlay \u6a21\u5f0f \u914d\u7f6e coordinator \u4e3a Overlay \u6a21\u5f0f\u540c\u6837\u4e5f\u80fd\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u4f20\u7edf\u7684 Overlay \u7c7b\u578b(\u5982 Calico \u548c Cilium \u7b49)\u7684 CNI \u5df2\u7ecf\u5b8c\u7f8e\u89e3\u51b3\u4e86\u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u5b83\uff0c\u5e2e\u52a9 Underlay Pod \u8bbf\u95ee Service\u3002 \u6211\u4eec\u53ef\u4ee5\u4e3a Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\uff0c eth0 \u4e3a Overlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002 net1 \u4e3a Underlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1 Pod \u5357\u5317\u5411\u6d41\u91cf\u3002 \u901a\u8fc7 coordinator \u8bbe\u7f6e\u7684\u7b56\u7565\u8def\u7531\u8868\u9879 \u786e\u4fdd Pod \u8bbf\u95ee Service \u65f6\u4ece eth0 \u8f6c\u53d1, \u56de\u590d\u62a5\u6587\u4e5f\u8f6c\u53d1\u7ed9 eth0\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u8c03\u7528\u7f51\u5361\u662f\u5426\u4e0d\u662f eth0 \u3002\u5982\u679c\u4e0d\u662f\uff0c\u786e\u8ba4 Pod \u4e2d\u4e0d\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a overlay \u6a21\u5f0f\u3002 \u5728 overlay \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u540c\u6b65\u96c6\u7fa4\u7f3a\u7701 CNI \u7684 Pod \u5b50\u7f51\uff0c\u8fd9\u4e9b\u5b50\u7f51\u7528\u4e8e\u5728\u591a\u7f51\u5361 Pod \u4e2d\u8bbe\u7f6e\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u5b83\u8bbf\u95ee\u7531\u7f3a\u7701 CNI \u521b\u5efa\u7684 Pod \u4e4b\u95f4\u7684\u6b63\u5e38\u901a\u4fe1\u65f6\uff0c\u4ece eth0 \u8f6c\u53d1\u3002\u8fd9\u4e2a\u914d\u7f6e\u5bf9\u5e94 spidercoordinator.spec.podCIDRType \uff0c\u9ed8\u8ba4\u4e3a auto , \u53ef\u9009\u503c: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] \u8fd9\u4e9b\u8def\u7531\u662f\u5728 Pod \u542f\u52a8\u65f6\u6ce8\u5165\u7684\uff0c\u5982\u679c\u76f8\u5173\u7684 CIDR \u53d1\u751f\u4e86\u53d8\u52a8\uff0c\u65e0\u6cd5\u81ea\u52a8\u751f\u6548\u5230\u5df2\u7ecf Running \u7684 Pod \u4e2d\uff0c\u8fd9\u9700\u8981\u91cd\u542f Pod \u624d\u80fd\u751f\u6548\u3002 \u66f4\u591a\u8be6\u60c5\u53c2\u8003 CRD-Spidercoordinator \u5f53\u4ee5 Overlay \u6a21\u5f0f\u521b\u5efa Pod \u5e76\u8fdb\u5165 Pod \u7f51\u7edc\u547d\u4ee4\u7a7a\u95f4\uff0c\u67e5\u770b\u8def\u7531\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip rule 0 : from all lookup local 32759 : from 10 .233.105.154 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32759: from 10.233.105.154 lookup 100 : \u786e\u4fdd\u4ece eth0 (calico \u7f51\u5361)\u53d1\u51fa\u7684\u6570\u636e\u5305\u8d70 table 100\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b: \u9664\u4e86\u9ed8\u8ba4\u8def\u7531\uff0c\u6240\u6709\u8def\u7531\u90fd\u4fdd\u7559\u5728 Main \u8868\uff0c\u4f46\u4f1a\u628a net1 \u7684\u9ed8\u8ba4\u8def\u7531\u79fb\u52a8\u5230 table 100\u3002 \u8fd9\u4e9b\u7b56\u7565\u8def\u7531\u786e\u4fdd\u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0cUnderlay Pod \u4e5f\u80fd\u591f\u6b63\u5e38\u8bbf\u95ee Service\u3002 \u901a\u8fc7 Cilium Without Kube-proxy \u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service \u4e0a\u9762\u6211\u4eec\u4ecb\u7ecd\u4e86\u5728 Spiderpool \u4e2d, \u6211\u4eec\u901a\u8fc7 coordinator \u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u5230\u4e3b\u673a\u8f6c\u53d1\uff0c \u518d\u7ecf\u8fc7\u4e3b\u673a\u4e0a Kube-proxy \u8bbe\u7f6e\u7684 iptables \u89c4\u5219 DNAT (\u5c06\u76ee\u6807\u5730\u5740\u6539\u4e3a\u76ee\u6807 Pod) \u4e4b\u540e\uff0c\u518d\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u8fd9\u53ef\u4ee5\u867d\u7136\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5ef6\u957f\u4e86\u6570\u636e\u8bbf\u95ee\u8def\u5f84\uff0c\u9020\u6210\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u5931\u3002 \u793e\u533a\u5f00\u6e90\u7684 CNI \u9879\u76ee: Cilium \u652f\u6301\u57fa\u4e8e eBPF \u6280\u672f\u5b8c\u5168\u66ff\u4ee3 kube-proxy \u7cfb\u7edf\u7ec4\u4ef6\u3002\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u89e3\u6790 Service\u3002\u5f53\u8bbf\u95ee Service \u65f6\uff0cService \u5730\u5740\u4f1a\u88ab Cilium \u6302\u8f7d\u7684 eBPF \u7a0b\u5e8f\u76f4\u63a5\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u7684 IP\uff0c\u8fd9\u6837\u6e90 Pod \u76f4\u63a5\u5bf9\u76ee\u6807 Pod \u53d1\u8d77\u8bbf\u95ee\uff0c\u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff0c\u6781\u5927\u7684\u7f29\u77ed\u4e86\u8bbf\u95ee\u8def\u5f84\uff0c\u4ece\u800c\u5b9e\u73b0\u52a0\u901f\u8bbf\u95ee Service\u3002\u501f\u52a9\u4e8e\u5f3a\u5927\u7684 Cilium\uff0c \u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b83\u5b9e\u73b0\u52a0\u901f Underlay CNI\u7684 Service \u8bbf\u95ee\u3002 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e Macvlan CNI + Cilium \u52a0\u901f\u8bbf\u95ee Service\uff1a \u6ce8\u610f: \u9700\u8981\u786e\u4fdd\u96c6\u7fa4\u8282\u70b9\u7684\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u63d0\u524d\u51c6\u5907\u597d\u4e00\u4e2a\u672a\u5b89\u88c5 kube-proxy \u7ec4\u4ef6\u7684\u96c6\u7fa4\uff0c\u5982\u679c\u5df2\u7ecf\u5b89\u88c5 kube-proxy \u53ef\u53c2\u8003\u4e00\u4e0b\u547d\u4ee4\u5220\u9664 kube-proxy \u7ec4\u4ef6 ~# kubectl delete ds -n kube-system kube-proxy ~# # \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c ~# iptables-save | grep -v KUBE | iptables-restore \u4e5f\u53ef\u4ee5\u4f7f\u7528 kubeadm \u5b89\u88c5\u4e00\u4e2a\u65b0\u96c6\u7fa4, \u6ce8\u610f\u4e0d\u8981\u5b89\u88c5 kube-proxy: ~# kubeadm init --skip-phases = addon/kube-proxy \u5b89\u88c5 Cilium \u7ec4\u4ef6\uff0c\u6ce8\u610f\u5f00\u542f kube-proxy replacement \u529f\u80fd ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } \u5b89\u88c5\u5b8c\u6210\uff0c\u68c0\u67e5\u5b89\u88c5\u72b6\u6001\uff1a ~# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m \u5b89\u88c5 Spiderpool, \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none \u8bbe\u7f6e coordinator.podCIDRType=none, spiderpool \u5c06\u4e0d\u4f1a\u83b7\u53d6\u96c6\u7fa4\u7684 ServiceCIDR\u3002\u5728\u521b\u5efa Pod \u65f6\u4e5f\u5c31\u4e0d\u4f1a\u6ce8\u5165 Service \u76f8\u5173\u8def\u7531 \u8fd9\u6837\u8bbf\u95ee Service \u5b8c\u5168\u4f9d\u8d56 Cilium kube-proxy Replacement\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u521b\u5efa macvlan \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF \u9700\u8981\u786e\u4fdd ens192 \u5b58\u5728\u4e8e\u96c6\u7fa4\u8282\u70b9 \u5efa\u8bae\u8bbe\u7f6e enableCoordinator \u4e3a true, \u8fd9\u53ef\u4ee5\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u6d4b\u7684\u95ee\u9898 \u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\u7528\u4e8e\u6d4b\u8bd5\uff1a ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network : kube-system/macvlan-ens192\" NAME=ipvlan cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : DaemonSet metadata : name : ${NAME} labels : app : $NAME spec : selector : matchLabels : app : $NAME template : metadata : name : $NAME labels : app : $NAME annotations : ${ANNOTATION_MULTUS} spec : containers : - name : test-app image : nginx imagePullPolicy : IfNotPresent ports : - name : http containerPort : 80 protocol : TCP --- apiVersion : v1 kind : Service metadata : name : ${NAME} spec : - ports : name : http port : 80 protocol : TCP targetPort : 80 selector : app : ${NAME} type : ClusterIP EOF \u9a8c\u8bc1\u8bbf\u95ee Service \u7684\u8054\u901a\u6027\uff0c\u5e76\u67e5\u770b\u6027\u80fd\u662f\u5426\u63d0\u5347 ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes \u53e6\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\uff0c\u901a\u8fc7 tcpdump \u5de5\u5177\u67e5\u770b\u5230\u8bbf\u95ee Service \u7684\u6570\u636e\u5305\u4ece Pod \u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u53d1\u51fa\u65f6\uff0c\u76ee\u6807\u5730\u5740\u5df2\u7ecf\u88ab\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u5730\u5740: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 \u662f\u6e90 Pod \u7684 IP, 10.6.185.210 \u662f\u76ee\u6807 Pod \u7684 IP\uff0c\u53ef\u4ee5\u786e\u8ba4 Cilium \u89e3\u6790\u4e86 Service \u7684 IP\u3002 \u4f7f\u7528 sockperf \u5de5\u5177\u6d4b\u8bd5\u4f7f\u7528 Cilium \u52a0\u901f\u524d\u540e\uff0c \u6d4b\u5f97 Pod \u8de8\u8282\u70b9\u8bbf\u95ee ClusterIP \u7684\u6570\u636e\u5bf9\u6bd4: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 \u6839\u636e\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7 Cilium kube-proxy replacement \u4e4b\u540e\uff0c\u8bbf\u95ee Service \u5927\u7ea6\u52a0\u901f 30%\u3002\u66f4\u591a\u6d4b\u8bd5\u6570\u636e\u53c2\u8003 \u7f51\u7edc IO \u6027\u80fd \u7ed3\u8bba Underlay CNI \u8bbf\u95ee Service \u6709\u4ee5\u4e0a\u4e24\u79cd\u65b9\u6848\u89e3\u51b3\u3002kube-proxy \u7684\u65b9\u5f0f\u66f4\u52a0\u5e38\u7528\u7a33\u5b9a\uff0c\u5927\u90e8\u5206\u73af\u5883\u90fd\u53ef\u4ee5\u7a33\u5b9a\u4f7f\u7528\u3002 Cilium Without Kube-Proxy \u4e3a Underlay CNI \u8bbf\u95ee Service \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u53ef\u9009\u65b9\u6848\uff0c\u5e76\u4e14\u52a0\u901f\u4e86 Service \u8bbf\u95ee\uff0c\u5c3d\u7ba1\u8fd9\u6709\u4e00\u5b9a\u4f7f\u7528\u9650\u5236\u53ca\u95e8\u69db\uff0c\u4f46\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u7684\u9700\u6c42\u3002","title":"Underlay CNI \u8bbf\u95ee Service"},{"location":"usage/underlay_cni_service-zh_CN/#underlay-cni-service","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"Underlay CNI \u8bbf\u95ee Service"},{"location":"usage/underlay_cni_service-zh_CN/#_1","text":"\u76ee\u524d\u793e\u533a\u4e2d\u5927\u591a\u6570 Underlay \u7c7b\u578b\u7684 CNI(\u5982 Macvlan\u3001IPVlan\u3001Sriov-CNI \u7b49)\u4e00\u822c\u5bf9\u63a5\u5e95\u5c42\u7f51\u7edc\uff0c\u5f80\u5f80\u5e76\u4e0d\u539f\u751f\u652f\u6301\u8bbf\u95ee\u96c6\u7fa4\u7684 Service \u3002\u8fd9\u5927\u591a\u662f\u56e0\u4e3a underlay Pod \u8bbf\u95ee Service \u9700\u8981\u7ecf\u8fc7\u4ea4\u6362\u673a\u7684\u7f51\u5173\u8f6c\u53d1\uff0c \u4f46\u7f51\u5173\u4e0a\u5e76\u6ca1\u6709\u53bb\u5f80 Service \u7684\u8def\u7531\uff0c\u9020\u6210\u65e0\u6cd5\u6b63\u786e\u8def\u7531\u8bbf\u95ee Service \u7684\u62a5\u6587\uff0c\u4ece\u800c\u4e22\u5305\u3002Spiderpool \u63d0\u4f9b\u4ee5\u4e0b\u4e24\u79cd\u7684\u65b9\u6848\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898: \u901a\u8fc7 Spiderpool coordinator + kube-proxy \u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service \u901a\u8fc7 Cilium Without Kube-proxy \u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service \u8fd9\u4e24\u79cd\u65b9\u6848\u90fd\u89e3\u51b3\u4e86 Underlay CNI \u65e0\u6cd5\u8bbf\u95ee Service \u7684\u95ee\u9898\uff0c\u4f46\u5b9e\u73b0\u539f\u7406\u6709\u4e9b\u4e0d\u540c\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u5f0f:","title":"\u4ecb\u7ecd"},{"location":"usage/underlay_cni_service-zh_CN/#spiderpool-coordinator-kube-proxy","text":"Spiderpool \u5185\u7f6e coordinator \u63d2\u4ef6\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u65e0\u7f1d\u5bf9\u63a5 kube-proxy \u4ee5\u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service\u3002 \u6839\u636e\u4e0d\u540c\u7684\u573a\u666f\uff0c coordinator \u53ef\u4ee5\u8fd0\u884c\u5728 underlay \u6216 overlay \u6a21\u5f0f\uff0c\u867d\u7136\u5b9e\u73b0\u65b9\u5f0f\u7a0d\u663e\u4e0d\u540c\uff0c\u4f46 \u6838\u5fc3\u539f\u7406\u90fd\u662f\u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u7684\u4e3b\u673a\u7f51\u7edc\u534f\u8bae\u6808\u4e0a\uff0c\u518d\u7ecf\u8fc7 Kube-proxy \u521b\u5efa\u7684 IPtables \u89c4\u5219\u505a\u8f6c\u53d1\u3002 \u4e0b\u9762\u662f\u6570\u636e\u8f6c\u53d1\u6d41\u7a0b\u56fe\u4ecb\u7ecd: \u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Underlay \u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c coordinator \u63d2\u4ef6\u5c06\u521b\u5efa\u4e00\u5bf9 Veth \u8bbe\u5907\uff0c\u5c06\u4e00\u7aef\u653e\u7f6e\u4e8e\u4e3b\u673a\uff0c\u53e6\u4e00\u7aef\u653e\u7f6e\u4e0e Pod \u7684 network namespace \u4e2d\uff0c\u7136\u540e\u5728 Pod \u91cc\u9762\u8bbe\u7f6e\u4e00\u4e9b\u8def\u7531\u89c4\u5219\uff0c \u4f7f Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth \u8bbe\u5907\u8f6c\u53d1\u3002 coordinator \u9ed8\u8ba4\u4e3a auto \u6a21\u5f0f\uff0c \u5b83\u5c06\u81ea\u52a8\u5224\u65ad\u5e94\u8be5\u8fd0\u884c underlay \u6216 overlay \u6a21\u5f0f\u3002\u60a8\u53ea\u9700\u8981\u5728 Pod \u6ce8\u5165\u6ce8\u89e3: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> \u5373\u53ef\u3002 \u5f53\u4ee5 Underlay \u6a21\u5f0f\u521b\u5efa Pod \u540e\uff0c\u6211\u4eec\u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c\u770b\u770b\u8def\u7531\u7b49\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 \u662f\u8282\u70b9\u7684 IP,\u786e\u4fdd Pod \u8bbf\u95ee\u672c\u8282\u70b9\u65f6\u4ece veth0 \u8f6c\u53d1\u3002 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 \u662f\u96c6\u7fa4 Service \u7684 CIDR, \u786e\u4fdd Pod \u8bbf\u95ee ClusterIP \u65f6\u4ece veth0 \u8f6c\u53d1\u3002 \u8fd9\u4e2a\u65b9\u6848\u5f3a\u70c8\u4f9d\u8d56\u4e0e kube-proxy \u7684 MASQUERADE , \u5426\u5219\u56de\u590d\u62a5\u6587\u5c06\u76f4\u63a5\u8f6c\u53d1\u7ed9\u6e90 Pod, \u5982\u679c\u7ecf\u8fc7\u4e00\u4e9b\u5b89\u5168\u8bbe\u5907\uff0c\u5c06\u4f1a\u4e22\u5f03\u6570\u636e\u5305\u3002\u6240\u4ee5\u5728\u4e00\u4e9b\u7279\u6b8a\u7684\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u7f6e kube-proxy \u7684 masqueradeAll \u4e3a true\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u4e0d\u540c\uff0c \u65e0\u9700\u5f00\u542f masqueradeAll , \u5b83\u4eec\u4e4b\u95f4\u7684\u8bbf\u95ee\u5c06\u4f1a\u88ab SNAT\u3002 \u5982\u679c Pod \u7684 underlay \u5b50\u7f51\u4e0e\u96c6\u7fa4\u7684 clusterCIDR \u76f8\u540c\uff0c\u90a3\u6211\u4eec\u5fc5\u987b\u8981\u8bbe\u7f6e masqueradeAll \u4e3a true\u3002","title":"\u57fa\u4e8e Spiderpool coordinator + kube-proxy"},{"location":"usage/underlay_cni_service-zh_CN/#coordinator-overlay","text":"\u914d\u7f6e coordinator \u4e3a Overlay \u6a21\u5f0f\u540c\u6837\u4e5f\u80fd\u89e3\u51b3 Underlay CNI \u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u4f20\u7edf\u7684 Overlay \u7c7b\u578b(\u5982 Calico \u548c Cilium \u7b49)\u7684 CNI \u5df2\u7ecf\u5b8c\u7f8e\u89e3\u51b3\u4e86\u8bbf\u95ee Service \u7684\u95ee\u9898\u3002 \u6211\u4eec\u53ef\u4ee5\u501f\u52a9\u5b83\uff0c\u5e2e\u52a9 Underlay Pod \u8bbf\u95ee Service\u3002 \u6211\u4eec\u53ef\u4ee5\u4e3a Pod \u9644\u52a0\u591a\u5f20\u7f51\u5361\uff0c eth0 \u4e3a Overlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u3002 net1 \u4e3a Underlay CNI \u521b\u5efa\uff0c\u7528\u4e8e\u8f6c\u53d1 Pod \u5357\u5317\u5411\u6d41\u91cf\u3002 \u901a\u8fc7 coordinator \u8bbe\u7f6e\u7684\u7b56\u7565\u8def\u7531\u8868\u9879 \u786e\u4fdd Pod \u8bbf\u95ee Service \u65f6\u4ece eth0 \u8f6c\u53d1, \u56de\u590d\u62a5\u6587\u4e5f\u8f6c\u53d1\u7ed9 eth0\u3002 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b mode \u7684\u503c\u4e3aauto(spidercoordinator CR \u4e2d spec.mode \u4e3a auto), coordinator \u5c06\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d CNI \u8c03\u7528\u7f51\u5361\u662f\u5426\u4e0d\u662f eth0 \u3002\u5982\u679c\u4e0d\u662f\uff0c\u786e\u8ba4 Pod \u4e2d\u4e0d\u5b58\u5728 veth0 \u7f51\u5361\uff0c\u5219\u81ea\u52a8\u5224\u65ad\u4e3a overlay \u6a21\u5f0f\u3002 \u5728 overlay \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u4f1a\u81ea\u52a8\u540c\u6b65\u96c6\u7fa4\u7f3a\u7701 CNI \u7684 Pod \u5b50\u7f51\uff0c\u8fd9\u4e9b\u5b50\u7f51\u7528\u4e8e\u5728\u591a\u7f51\u5361 Pod \u4e2d\u8bbe\u7f6e\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0\u5b83\u8bbf\u95ee\u7531\u7f3a\u7701 CNI \u521b\u5efa\u7684 Pod \u4e4b\u95f4\u7684\u6b63\u5e38\u901a\u4fe1\u65f6\uff0c\u4ece eth0 \u8f6c\u53d1\u3002\u8fd9\u4e2a\u914d\u7f6e\u5bf9\u5e94 spidercoordinator.spec.podCIDRType \uff0c\u9ed8\u8ba4\u4e3a auto , \u53ef\u9009\u503c: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] \u8fd9\u4e9b\u8def\u7531\u662f\u5728 Pod \u542f\u52a8\u65f6\u6ce8\u5165\u7684\uff0c\u5982\u679c\u76f8\u5173\u7684 CIDR \u53d1\u751f\u4e86\u53d8\u52a8\uff0c\u65e0\u6cd5\u81ea\u52a8\u751f\u6548\u5230\u5df2\u7ecf Running \u7684 Pod \u4e2d\uff0c\u8fd9\u9700\u8981\u91cd\u542f Pod \u624d\u80fd\u751f\u6548\u3002 \u66f4\u591a\u8be6\u60c5\u53c2\u8003 CRD-Spidercoordinator \u5f53\u4ee5 Overlay \u6a21\u5f0f\u521b\u5efa Pod \u5e76\u8fdb\u5165 Pod \u7f51\u7edc\u547d\u4ee4\u7a7a\u95f4\uff0c\u67e5\u770b\u8def\u7531\u4fe1\u606f: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip rule 0 : from all lookup local 32759 : from 10 .233.105.154 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32759: from 10.233.105.154 lookup 100 : \u786e\u4fdd\u4ece eth0 (calico \u7f51\u5361)\u53d1\u51fa\u7684\u6570\u636e\u5305\u8d70 table 100\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b: \u9664\u4e86\u9ed8\u8ba4\u8def\u7531\uff0c\u6240\u6709\u8def\u7531\u90fd\u4fdd\u7559\u5728 Main \u8868\uff0c\u4f46\u4f1a\u628a net1 \u7684\u9ed8\u8ba4\u8def\u7531\u79fb\u52a8\u5230 table 100\u3002 \u8fd9\u4e9b\u7b56\u7565\u8def\u7531\u786e\u4fdd\u591a\u7f51\u5361\u573a\u666f\u4e0b\uff0cUnderlay Pod \u4e5f\u80fd\u591f\u6b63\u5e38\u8bbf\u95ee Service\u3002","title":"\u5bf9\u4e8e coordinator \u8fd0\u884c\u5728 Overlay \u6a21\u5f0f"},{"location":"usage/underlay_cni_service-zh_CN/#cilium-without-kube-proxy-underlay-cni-service","text":"\u4e0a\u9762\u6211\u4eec\u4ecb\u7ecd\u4e86\u5728 Spiderpool \u4e2d, \u6211\u4eec\u901a\u8fc7 coordinator \u5c06 Pod \u8bbf\u95ee Service \u7684\u6d41\u91cf\u52ab\u6301\u5230\u4e3b\u673a\u8f6c\u53d1\uff0c \u518d\u7ecf\u8fc7\u4e3b\u673a\u4e0a Kube-proxy \u8bbe\u7f6e\u7684 iptables \u89c4\u5219 DNAT (\u5c06\u76ee\u6807\u5730\u5740\u6539\u4e3a\u76ee\u6807 Pod) \u4e4b\u540e\uff0c\u518d\u8f6c\u53d1\u81f3\u76ee\u6807 Pod\u3002 \u8fd9\u53ef\u4ee5\u867d\u7136\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5ef6\u957f\u4e86\u6570\u636e\u8bbf\u95ee\u8def\u5f84\uff0c\u9020\u6210\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u5931\u3002 \u793e\u533a\u5f00\u6e90\u7684 CNI \u9879\u76ee: Cilium \u652f\u6301\u57fa\u4e8e eBPF \u6280\u672f\u5b8c\u5168\u66ff\u4ee3 kube-proxy \u7cfb\u7edf\u7ec4\u4ef6\u3002\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u89e3\u6790 Service\u3002\u5f53\u8bbf\u95ee Service \u65f6\uff0cService \u5730\u5740\u4f1a\u88ab Cilium \u6302\u8f7d\u7684 eBPF \u7a0b\u5e8f\u76f4\u63a5\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u7684 IP\uff0c\u8fd9\u6837\u6e90 Pod \u76f4\u63a5\u5bf9\u76ee\u6807 Pod \u53d1\u8d77\u8bbf\u95ee\uff0c\u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u4e3b\u673a\u7684\u7f51\u7edc\u534f\u8bae\u6808\uff0c\u6781\u5927\u7684\u7f29\u77ed\u4e86\u8bbf\u95ee\u8def\u5f84\uff0c\u4ece\u800c\u5b9e\u73b0\u52a0\u901f\u8bbf\u95ee Service\u3002\u501f\u52a9\u4e8e\u5f3a\u5927\u7684 Cilium\uff0c \u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b83\u5b9e\u73b0\u52a0\u901f Underlay CNI\u7684 Service \u8bbf\u95ee\u3002 \u4ee5\u4e0b\u6b65\u9aa4\u6f14\u793a\u5728\u5177\u5907 2 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\u4e0a\uff0c\u5982\u4f55\u57fa\u4e8e Macvlan CNI + Cilium \u52a0\u901f\u8bbf\u95ee Service\uff1a \u6ce8\u610f: \u9700\u8981\u786e\u4fdd\u96c6\u7fa4\u8282\u70b9\u7684\u5185\u6838\u7248\u672c\u81f3\u5c11\u5927\u4e8e 4.19 \u63d0\u524d\u51c6\u5907\u597d\u4e00\u4e2a\u672a\u5b89\u88c5 kube-proxy \u7ec4\u4ef6\u7684\u96c6\u7fa4\uff0c\u5982\u679c\u5df2\u7ecf\u5b89\u88c5 kube-proxy \u53ef\u53c2\u8003\u4e00\u4e0b\u547d\u4ee4\u5220\u9664 kube-proxy \u7ec4\u4ef6 ~# kubectl delete ds -n kube-system kube-proxy ~# # \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c ~# iptables-save | grep -v KUBE | iptables-restore \u4e5f\u53ef\u4ee5\u4f7f\u7528 kubeadm \u5b89\u88c5\u4e00\u4e2a\u65b0\u96c6\u7fa4, \u6ce8\u610f\u4e0d\u8981\u5b89\u88c5 kube-proxy: ~# kubeadm init --skip-phases = addon/kube-proxy \u5b89\u88c5 Cilium \u7ec4\u4ef6\uff0c\u6ce8\u610f\u5f00\u542f kube-proxy replacement \u529f\u80fd ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } \u5b89\u88c5\u5b8c\u6210\uff0c\u68c0\u67e5\u5b89\u88c5\u72b6\u6001\uff1a ~# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m \u5b89\u88c5 Spiderpool, \u53ef\u53c2\u8003 \u5b89\u88c5 \u5b89\u88c5 Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none \u8bbe\u7f6e coordinator.podCIDRType=none, spiderpool \u5c06\u4e0d\u4f1a\u83b7\u53d6\u96c6\u7fa4\u7684 ServiceCIDR\u3002\u5728\u521b\u5efa Pod \u65f6\u4e5f\u5c31\u4e0d\u4f1a\u6ce8\u5165 Service \u76f8\u5173\u8def\u7531 \u8fd9\u6837\u8bbf\u95ee Service \u5b8c\u5168\u4f9d\u8d56 Cilium kube-proxy Replacement\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u7684\u7ec4\u4ef6\u5982\u4e0b: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u521b\u5efa macvlan \u76f8\u5173\u7684 multus \u914d\u7f6e\uff0c\u5e76\u521b\u5efa\u914d\u5957\u7684 ippool \u8d44\u6e90: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF \u9700\u8981\u786e\u4fdd ens192 \u5b58\u5728\u4e8e\u96c6\u7fa4\u8282\u70b9 \u5efa\u8bae\u8bbe\u7f6e enableCoordinator \u4e3a true, \u8fd9\u53ef\u4ee5\u89e3\u51b3 Pod \u5065\u5eb7\u68c0\u6d4b\u7684\u95ee\u9898 \u521b\u5efa\u4e00\u7ec4\u8de8\u8282\u70b9\u7684 DaemonSet \u5e94\u7528\u7528\u4e8e\u6d4b\u8bd5\uff1a ANNOTATION_MULTUS=\"v1.multus-cni.io/default-network : kube-system/macvlan-ens192\" NAME=ipvlan cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : DaemonSet metadata : name : ${NAME} labels : app : $NAME spec : selector : matchLabels : app : $NAME template : metadata : name : $NAME labels : app : $NAME annotations : ${ANNOTATION_MULTUS} spec : containers : - name : test-app image : nginx imagePullPolicy : IfNotPresent ports : - name : http containerPort : 80 protocol : TCP --- apiVersion : v1 kind : Service metadata : name : ${NAME} spec : - ports : name : http port : 80 protocol : TCP targetPort : 80 selector : app : ${NAME} type : ClusterIP EOF \u9a8c\u8bc1\u8bbf\u95ee Service \u7684\u8054\u901a\u6027\uff0c\u5e76\u67e5\u770b\u6027\u80fd\u662f\u5426\u63d0\u5347 ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes \u53e6\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fdb\u5165\u5230 Pod \u7684\u7f51\u7edc\u547d\u540d\u7a7a\u95f4\uff0c\u901a\u8fc7 tcpdump \u5de5\u5177\u67e5\u770b\u5230\u8bbf\u95ee Service \u7684\u6570\u636e\u5305\u4ece Pod \u7f51\u7edc\u547d\u540d\u7a7a\u95f4\u53d1\u51fa\u65f6\uff0c\u76ee\u6807\u5730\u5740\u5df2\u7ecf\u88ab\u89e3\u6790\u4e3a\u76ee\u6807 Pod \u5730\u5740: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 \u662f\u6e90 Pod \u7684 IP, 10.6.185.210 \u662f\u76ee\u6807 Pod \u7684 IP\uff0c\u53ef\u4ee5\u786e\u8ba4 Cilium \u89e3\u6790\u4e86 Service \u7684 IP\u3002 \u4f7f\u7528 sockperf \u5de5\u5177\u6d4b\u8bd5\u4f7f\u7528 Cilium \u52a0\u901f\u524d\u540e\uff0c \u6d4b\u5f97 Pod \u8de8\u8282\u70b9\u8bbf\u95ee ClusterIP \u7684\u6570\u636e\u5bf9\u6bd4: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 \u6839\u636e\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7 Cilium kube-proxy replacement \u4e4b\u540e\uff0c\u8bbf\u95ee Service \u5927\u7ea6\u52a0\u901f 30%\u3002\u66f4\u591a\u6d4b\u8bd5\u6570\u636e\u53c2\u8003 \u7f51\u7edc IO \u6027\u80fd","title":"\u901a\u8fc7 Cilium Without Kube-proxy \u5b9e\u73b0 Underlay CNI \u8bbf\u95ee Service"},{"location":"usage/underlay_cni_service-zh_CN/#_2","text":"Underlay CNI \u8bbf\u95ee Service \u6709\u4ee5\u4e0a\u4e24\u79cd\u65b9\u6848\u89e3\u51b3\u3002kube-proxy \u7684\u65b9\u5f0f\u66f4\u52a0\u5e38\u7528\u7a33\u5b9a\uff0c\u5927\u90e8\u5206\u73af\u5883\u90fd\u53ef\u4ee5\u7a33\u5b9a\u4f7f\u7528\u3002 Cilium Without Kube-Proxy \u4e3a Underlay CNI \u8bbf\u95ee Service \u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u53ef\u9009\u65b9\u6848\uff0c\u5e76\u4e14\u52a0\u901f\u4e86 Service \u8bbf\u95ee\uff0c\u5c3d\u7ba1\u8fd9\u6709\u4e00\u5b9a\u4f7f\u7528\u9650\u5236\u53ca\u95e8\u69db\uff0c\u4f46\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u7684\u9700\u6c42\u3002","title":"\u7ed3\u8bba"},{"location":"usage/underlay_cni_service/","text":"Access to service for underlay CNI English | \u7b80\u4f53\u4e2d\u6587 Introduction At present, most Underlay-type CNIs (such as Macvlan, IPVlan, Sriov-CNI, etc.) In the community are generally connected to the underlying network, and often do not natively support accessing the Service of the cluster. This is mostly because underlay Pod access to the Service needs to be forwarded through the gateway of the switch. However, there is no route to the Service on the gateway, so the packets accessing the Service cannot be routed correctly, resulting in packet loss. Spiderpool provides the following two solutions to solve the problem of Underlay CNI accessing Service: Underlay CNI access Service via Spiderpool coordinator + kube-proxy Use Cilium Without Kube-proxy to access Underlay CNI Service Both of these ways solve the problem that Underlay CNI cannot access Service, but the implementation principle is somewhat different. Below we will introduce these two ways: Underlay CNI access Service via Spiderpool coordinator + kube-proxy Spiderpool has a built-in plugin called coordinator , which helps us seamlessly integrate with kube-proxy to achieve Underlay CNI access to Service. Depending on different scenarios, the coordinator can run in either underlay or overlay mode. Although the implementation methods are slightly different, the core principle is to hijack the traffic of Pods accessing Services onto the host network protocol stack and then forward it through the IPtables rules created by Kube-proxy. The following is a brief introduction to the data forwarding process flowchart: coordinator run in underlay Under this mode, the coordinator plugin will create a pair of Veth devices, with one end placed in the host and the other end placed in the network namespace of the Pod. Then set some routing rules inside the Pod to forward access to ClusterIP from the veth device. The coordinator defaults to auto mode, which will automatically determine whether to run in underlay or overlay mode. You only need to inject an annotation into the Pod: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> . After creating a Pod in Underlay mode, we enter the Pod and check the routing information: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 is the node's IP, which ensure where the Pod access the same node via veth0 . 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 is cluster service subnet, which ensure the Pod access ClusterIP via veth0 . This solution heavily relies on the MASQUERADE of kube-proxy, otherwise the reply packets will be directly forwarded to the source Pod, and if they pass through some security devices, the packets will be dropped. Therefore, in some special scenarios, we need to set masqueradeAll of kube-proxy to true. By default, the underlay subnet of a Pod is different from the clusterCIDR of the cluster, so there is no need to enable masqueradeAll , and access between them will be SNATed. If the underlay subnet of a Pod is the same as the clusterCIDR of the cluster, then we must set masqueradeAll to true. coordinator run in overlay Configuring coordinator as Overlay mode can also solve the problem of Underlay CNI accessing Service. The traditional Overlay type (such as Calico and Cilium etc.) CNI has perfectly solved the access to Service problem. We can use it to help Underlay Pods access Service. We can attach multiple network cards to the Pod, eth0 for creating by Overlay CNI, net1 for creating by Underlay CNI, and set up policy routing table items through coordinator to ensure that when a Pod accesses Service, it forwards from eth0 , and replies are also forwarded to eth0 . By default, the value of mode is auto(spidercoordinator CR spec.mode is auto), coordinator will automatically determine whether the current CNI call is not eth0 . If it's not, confirm that there is no veth0 network card in the Pod, then automatically determine it as overlay mode. In overlay mode, Spiderpool will automatically synchronize the cluster default CNI Pod subnets, which are used to set routes in multi-network card Pods to enable it to communicate normally with Pods created by the default CNI when it accesses Service from eth0 . This configuration corresponds to spidercoordinator.spec.podCIDRType , the default is auto , optional values: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] These routes are injected at the start of the Pod, and if the related CIDR changes, it cannot automatically take effect on already running Pods, this requires restarting the Pod to take effect. For more details, please refer to CRD-Spidercoordinator When creating a Pod in Overlay mode and entering the Pod network command space, view the routing information: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # ip rule 0 : from all lookup local 32765 : from 10 .6.212.227 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32762: from all to 10.233.64.0/18 lookup 100 : Ensure that when Pods access ClusterIP, they go through table 100 and are forwarded out from eth0 . In the default configuration: Except for the default route, all routes are retained in the Main table, but the default route for 'net1' is moved to table 100. These policy routes ensure that Underlay Pods can also normally access Service in multi-network card scenarios. Accessing Service with Cilium Without Kube-proxy for Underlay CNI In Spiderpool, we hijack the traffic of Pods accessing Services through a coordinator that forwards it to the host and then through the iptables rules set up by the host's Kube-proxy. This can solve the problem but may extend the data access path and cause some performance loss. The open-source CNI project, Cilium, supports replacing the kube-proxy system component entirely with eBPF technology. It can help us resolve Service addresses. When pod accessing a Service, the Service address will be directly resolved by the eBPF program mounted by Cilium on the target Pod, so that the source Pod can directly initiate access to the target Pod without going through the host's network protocol stack. This greatly shortens the access path and achieves acceleration in accessing Service. With the power of Cilium, we can also implement acceleration in accessing Service under the Underlay CNI through it. The following steps demonstrate how to accelerate access to a Service on a cluster with 2 nodes based on Macvlan CNI + Cilium: NOTE: Please ensure that the kernel version of the cluster nodes is at least greater than 4.19 Prepare a cluster without the kube-proxy component installed in advance. If kube-proxy is already installed, you can refer to the following commands to remove the kube-proxy component: ~# kubectl delete ds -n kube-system kube-proxy ~# # run the command on every node ~# iptables-save | grep -v KUBE | iptables-restore To install the Cilium component, make sure to enable the kube-proxy replacement feature: ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } The installation is complete, check the pod's state: \uff5e# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m To install Spiderpool, see Install to install Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none set coordinator.podCIDRType=none, the spiderpool will not get the cluster's ServiceCIDR. Service-related routes are also not injected when pods are created. Access to the Service in this way is entirely dependent on Cilium kube-proxy Replacement. show the installation of Spiderpool: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Create a MacVLAN-related Multus configuration and create a companion IPPools resource: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF needs to ensure that ens192 exists on the cluster nodes recommends setting enableCoordinator to true, which can resolve issues with pod health detection Create a set of cross-node DaemonSet apps for testing: ANNOTATION_MULTUS = \"v1.multus-cni.io/default-network: kube-system/macvlan-ens192\" NAME = ipvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: ${NAME} spec: - ports: name: http port: 80 protocol: TCP targetPort: 80 selector: app: ${NAME} type: ClusterIP EOF Verify the connectivity of the access service and see if the performance is improved: ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes Open another terminal, enter the network space of the pod, and use the tcpdump tool to see that when the packet accessing the service is sent from the pod network namespace, the destination address has been resolved to the target pod address: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 is the IP of the source pod and 10.6.185.210 is the IP of the destination pod. Before and after using the sockperf tool to test the Cilium acceleration, the data comparison of pods accessing ClusterIP across nodes is obtained: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 According to the results, after Cilium kube-proxy replacement, access to the service is accelerated by about 30%. For more test data, please refer to Network I/O Performance Conclusion There are two solutions to the Underlay CNI Access Service. The kube-proxy method is more commonly used and stable, and can be used stably in most environments. Cilium Without Kube-Proxy provides an alternative option for Underlay CNI to access the Service and accelerates Service access. Although there are certain restrictions and thresholds for use, it can meet the needs of users in specific scenarios.","title":"Access Service for Underlay CNI"},{"location":"usage/underlay_cni_service/#access-to-service-for-underlay-cni","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Access to service for underlay CNI"},{"location":"usage/underlay_cni_service/#introduction","text":"At present, most Underlay-type CNIs (such as Macvlan, IPVlan, Sriov-CNI, etc.) In the community are generally connected to the underlying network, and often do not natively support accessing the Service of the cluster. This is mostly because underlay Pod access to the Service needs to be forwarded through the gateway of the switch. However, there is no route to the Service on the gateway, so the packets accessing the Service cannot be routed correctly, resulting in packet loss. Spiderpool provides the following two solutions to solve the problem of Underlay CNI accessing Service: Underlay CNI access Service via Spiderpool coordinator + kube-proxy Use Cilium Without Kube-proxy to access Underlay CNI Service Both of these ways solve the problem that Underlay CNI cannot access Service, but the implementation principle is somewhat different. Below we will introduce these two ways:","title":"Introduction"},{"location":"usage/underlay_cni_service/#underlay-cni-access-service-via-spiderpool-coordinator-kube-proxy","text":"Spiderpool has a built-in plugin called coordinator , which helps us seamlessly integrate with kube-proxy to achieve Underlay CNI access to Service. Depending on different scenarios, the coordinator can run in either underlay or overlay mode. Although the implementation methods are slightly different, the core principle is to hijack the traffic of Pods accessing Services onto the host network protocol stack and then forward it through the IPtables rules created by Kube-proxy. The following is a brief introduction to the data forwarding process flowchart:","title":"Underlay CNI access Service via Spiderpool coordinator + kube-proxy"},{"location":"usage/underlay_cni_service/#coordinator-run-in-underlay","text":"Under this mode, the coordinator plugin will create a pair of Veth devices, with one end placed in the host and the other end placed in the network namespace of the Pod. Then set some routing rules inside the Pod to forward access to ClusterIP from the veth device. The coordinator defaults to auto mode, which will automatically determine whether to run in underlay or overlay mode. You only need to inject an annotation into the Pod: v1.multus-cni.io/default-network: kube-system/<Multus_CR_NAME> . After creating a Pod in Underlay mode, we enter the Pod and check the routing information: root@controller:~# kubectl exec -it macvlan-underlay-5496bb9c9b-c7rnp sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # # ip a show veth0 5 : veth0@if428513: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 4a:fe:19:22:65:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::48fe:19ff:fe22:6505/64 scope link valid_lft forever preferred_lft forever # ip r default via 10 .6.0.1 dev eth0 10 .6.0.0/16 dev eth0 proto kernel scope link src 10 .6.212.241 10 .6.212.101 dev veth0 scope link 10 .233.64.0/18 via 10 .6.212.101 dev veth0 10.6.212.101 dev veth0 scope link : 10.6.212.101 is the node's IP, which ensure where the Pod access the same node via veth0 . 10.233.64.0/18 via 10.6.212.101 dev veth0 : 10.233.64.0/18 is cluster service subnet, which ensure the Pod access ClusterIP via veth0 . This solution heavily relies on the MASQUERADE of kube-proxy, otherwise the reply packets will be directly forwarded to the source Pod, and if they pass through some security devices, the packets will be dropped. Therefore, in some special scenarios, we need to set masqueradeAll of kube-proxy to true. By default, the underlay subnet of a Pod is different from the clusterCIDR of the cluster, so there is no need to enable masqueradeAll , and access between them will be SNATed. If the underlay subnet of a Pod is the same as the clusterCIDR of the cluster, then we must set masqueradeAll to true.","title":"coordinator run in underlay"},{"location":"usage/underlay_cni_service/#coordinator-run-in-overlay","text":"Configuring coordinator as Overlay mode can also solve the problem of Underlay CNI accessing Service. The traditional Overlay type (such as Calico and Cilium etc.) CNI has perfectly solved the access to Service problem. We can use it to help Underlay Pods access Service. We can attach multiple network cards to the Pod, eth0 for creating by Overlay CNI, net1 for creating by Underlay CNI, and set up policy routing table items through coordinator to ensure that when a Pod accesses Service, it forwards from eth0 , and replies are also forwarded to eth0 . By default, the value of mode is auto(spidercoordinator CR spec.mode is auto), coordinator will automatically determine whether the current CNI call is not eth0 . If it's not, confirm that there is no veth0 network card in the Pod, then automatically determine it as overlay mode. In overlay mode, Spiderpool will automatically synchronize the cluster default CNI Pod subnets, which are used to set routes in multi-network card Pods to enable it to communicate normally with Pods created by the default CNI when it accesses Service from eth0 . This configuration corresponds to spidercoordinator.spec.podCIDRType , the default is auto , optional values: [\"auto\",\"calico\",\"cilium\",\"cluster\",\"none\"] These routes are injected at the start of the Pod, and if the related CIDR changes, it cannot automatically take effect on already running Pods, this requires restarting the Pod to take effect. For more details, please refer to CRD-Spidercoordinator When creating a Pod in Overlay mode and entering the Pod network command space, view the routing information: root@controller:~# kubectl exec -it macvlan-overlay-97bf89fdd-kdgrb sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. # ip rule 0 : from all lookup local 32765 : from 10 .6.212.227 lookup 100 32766 : from all lookup main 32767 : from all lookup default # ip r default via 169 .254.1.1 dev eth0 10 .6.212.102 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.102 dev eth0 10 .233.64.0/18 via 10 .6.212.102 dev eth0 169 .254.1.1 dev eth0 scope link # ip r show table 100 default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 proto kernel scope link src 10 .6.212.227 32762: from all to 10.233.64.0/18 lookup 100 : Ensure that when Pods access ClusterIP, they go through table 100 and are forwarded out from eth0 . In the default configuration: Except for the default route, all routes are retained in the Main table, but the default route for 'net1' is moved to table 100. These policy routes ensure that Underlay Pods can also normally access Service in multi-network card scenarios.","title":"coordinator run in overlay"},{"location":"usage/underlay_cni_service/#accessing-service-with-cilium-without-kube-proxy-for-underlay-cni","text":"In Spiderpool, we hijack the traffic of Pods accessing Services through a coordinator that forwards it to the host and then through the iptables rules set up by the host's Kube-proxy. This can solve the problem but may extend the data access path and cause some performance loss. The open-source CNI project, Cilium, supports replacing the kube-proxy system component entirely with eBPF technology. It can help us resolve Service addresses. When pod accessing a Service, the Service address will be directly resolved by the eBPF program mounted by Cilium on the target Pod, so that the source Pod can directly initiate access to the target Pod without going through the host's network protocol stack. This greatly shortens the access path and achieves acceleration in accessing Service. With the power of Cilium, we can also implement acceleration in accessing Service under the Underlay CNI through it. The following steps demonstrate how to accelerate access to a Service on a cluster with 2 nodes based on Macvlan CNI + Cilium: NOTE: Please ensure that the kernel version of the cluster nodes is at least greater than 4.19 Prepare a cluster without the kube-proxy component installed in advance. If kube-proxy is already installed, you can refer to the following commands to remove the kube-proxy component: ~# kubectl delete ds -n kube-system kube-proxy ~# # run the command on every node ~# iptables-save | grep -v KUBE | iptables-restore To install the Cilium component, make sure to enable the kube-proxy replacement feature: ~# helm repo add cilium https://helm.cilium.io ~# helm repo update ~# API_SERVER_IP = <your_api_server_ip> ~# # Kubeadm default is 6443 ~# API_SERVER_PORT = <your_api_server_port> ~# helm install cilium cilium/cilium --version 1 .14.3 \\ --namespace kube-system \\ --set kubeProxyReplacement = true \\ --set k8sServiceHost = ${ API_SERVER_IP } \\ --set k8sServicePort = ${ API_SERVER_PORT } The installation is complete, check the pod's state: \uff5e# kubectl get po -n kube-system | grep cilium cilium-2r6s5 1 /1 Running 0 15m cilium-lr9lx 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-lrk6r 1 /1 Running 0 15m cilium-operator-5ff9f86dfd-sb695 1 /1 Running 0 15m To install Spiderpool, see Install to install Spiderpool: ~# helm install spiderpool spiderpool/spiderpool -n kube-system \\ --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \\ --set coordinator.podCIDRType = none set coordinator.podCIDRType=none, the spiderpool will not get the cluster's ServiceCIDR. Service-related routes are also not injected when pods are created. Access to the Service in this way is entirely dependent on Cilium kube-proxy Replacement. show the installation of Spiderpool: ~# kubectl get pod -n kube-system spiderpool-agent-9sllh 1 /1 Running 0 1m spiderpool-agent-h92bv 1 /1 Running 0 1m spiderpool-controller-7df784cdb7-bsfwv 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Create a MacVLAN-related Multus configuration and create a companion IPPools resource: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: v4-pool spec: gateway: 172.81.0.1 ips: - 172.81.0.100-172.81.0.120 subnet: 172.81.0.0/16 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 namespace: kube-system spec: cniType: macvlan enableCoordinator: true macvlan: master: - \"ens192\" ippools: ipv4: [\"v4-pool\"] EOF needs to ensure that ens192 exists on the cluster nodes recommends setting enableCoordinator to true, which can resolve issues with pod health detection Create a set of cross-node DaemonSet apps for testing: ANNOTATION_MULTUS = \"v1.multus-cni.io/default-network: kube-system/macvlan-ens192\" NAME = ipvlan cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: ${NAME} labels: app: $NAME spec: selector: matchLabels: app: $NAME template: metadata: name: $NAME labels: app: $NAME annotations: ${ANNOTATION_MULTUS} spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: ${NAME} spec: - ports: name: http port: 80 protocol: TCP targetPort: 80 selector: app: ${NAME} type: ClusterIP EOF Verify the connectivity of the access service and see if the performance is improved: ~# kubectl exec -it ipvlan-test-55c97ccfd8-kd4vj sh kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl exec [ POD ] -- [ COMMAND ] instead. / # curl 10.233.42.25 -I HTTP/1.1 200 OK Server: nginx Date: Fri, 20 Oct 2023 07 :52:13 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Thu, 02 Mar 2023 10 :57:12 GMT Connection: keep-alive ETag: \"64008108-fd7\" Accept-Ranges: bytes Open another terminal, enter the network space of the pod, and use the tcpdump tool to see that when the packet accessing the service is sent from the pod network namespace, the destination address has been resolved to the target pod address: ~# tcpdump -nnev -i eth0 tcp and port 80 tcpdump: listening on eth0, link-type EN10MB ( Ethernet ) , capture size 262144 bytes 10 .6.185.218.43550 > 10 .6.185.210.80: Flags [ S ] , cksum 0x87e7 ( incorrect -> 0xe534 ) , seq 1391704016 , win 64240 , options [ mss 1460 ,sackOK,TS val 2667940841 ecr 0 ,nop,wscale 7 ] , length 0 10 .6.185.210.80 > 10 .6.185.218.43550: Flags [ S. ] , cksum 0x9d1a ( correct ) , seq 2119742376 , ack 1391704017 , win 65160 , options [ mss 1460 ,sackOK,TS val 3827707465 ecr 2667940841 ,nop,wscale 7 ] , length 0 10.6.185.218 is the IP of the source pod and 10.6.185.210 is the IP of the destination pod. Before and after using the sockperf tool to test the Cilium acceleration, the data comparison of pods accessing ClusterIP across nodes is obtained: latency(usec) RPS with kube-proxy 36.763 72254.34 without kube-proxy 27.743 107066.38 According to the results, after Cilium kube-proxy replacement, access to the service is accelerated by about 30%. For more test data, please refer to Network I/O Performance","title":"Accessing Service with Cilium Without Kube-proxy for Underlay CNI"},{"location":"usage/underlay_cni_service/#conclusion","text":"There are two solutions to the Underlay CNI Access Service. The kube-proxy method is more commonly used and stable, and can be used stably in most environments. Cilium Without Kube-Proxy provides an alternative option for Underlay CNI to access the Service and accelerates Service access. Although there are certain restrictions and thresholds for use, it can meet the needs of users in specific scenarios.","title":"Conclusion"},{"location":"usage/install/certificate/","text":"Certificates Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways. Auto certificates Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = auto Note that the default value of parameter spiderpoolController.tls.method is auto . Provided certificates If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA = ` cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT = ` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY = ` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = provided \\ --set spiderpoolController.tls.provided.tlsCa = ${ CA } \\ --set spiderpoolController.tls.provided.tlsCert = ${ SERVER_CERT } \\ --set spiderpoolController.tls.provided.tlsKey = ${ SERVER_KEY } Cert-manager certificates It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = certmanager \\ --set spiderpoolController.tls.certmanager.issuerName = ${ CERT_MANAGER_ISSUER_NAME }","title":"Certificates"},{"location":"usage/install/certificate/#certificates","text":"Spiderpool-controller needs TLS certificates to run webhook server. You can configure it in several ways.","title":"Certificates"},{"location":"usage/install/certificate/#auto-certificates","text":"Use Helm's template function genSignedCert to generate TLS certificates. This is the simplest and most common way to configure: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = auto Note that the default value of parameter spiderpoolController.tls.method is auto .","title":"Auto certificates"},{"location":"usage/install/certificate/#provided-certificates","text":"If you want to run spiderpool-controller with a self-signed certificate, provided would be a good choice. You can use OpenSSL to generate certificates, or run the following script: wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/tools/cert/generateCert.sh Generate the certificates: chmod +x generateCert.sh && ./generateCert.sh \"/tmp/tls\" CA = ` cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT = ` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY = ` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` Then, deploy Spiderpool in the provided mode: helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = provided \\ --set spiderpoolController.tls.provided.tlsCa = ${ CA } \\ --set spiderpoolController.tls.provided.tlsCert = ${ SERVER_CERT } \\ --set spiderpoolController.tls.provided.tlsKey = ${ SERVER_KEY }","title":"Provided certificates"},{"location":"usage/install/certificate/#cert-manager-certificates","text":"It is not recommended to use this mode directly , because the Spiderpool requires the TLS certificates provided by cert-manager, while the cert-manager requires the IP address provided by Spiderpool (cycle reference). Therefore, if possible, you must first deploy cert-manager using other IPAM CNI in the Kubernetes cluster, and then deploy Spiderpool. helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method = certmanager \\ --set spiderpoolController.tls.certmanager.issuerName = ${ CERT_MANAGER_ISSUER_NAME }","title":"Cert-manager certificates"},{"location":"usage/install/upgrade/","text":"Upgrading Spiderpool Versions This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool. Upgrade to 0.3.6 from (<=0.3.5) Description There's a design flaw for SpiderSubnet feature in auto-created IPPool label. The previous label ipam.spidernet.io/owner-application corresponding value uses '-' as separative sign. For example, we have deployment ns398-174835790/deploy398-82311862 and the corresponding label value is Deployment-ns398-174835790-deploy398-82311862 . It's very hard to unpack it to trace back what the application namespace and name is. Now, we use '_' rather than '-' as slash for SpiderSubnet feature label ipam.spidernet.io/owner-application , and the upper case will be like Deployment_ns398-174835790_deploy398-82311862 . Reference PR: #1162 In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool. The key is ipam.spidernet.io/interface , and the value is the corresponding interface name. Operation steps Find all auto-created IPPools, their name format is auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid} such as auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41 . Replace their label, just like this: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}' Add one more label kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Update your Spiderpool components version and restart them all. Upgrade to 0.4.0 from (<0.4.0) Description Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod . Operation steps Find all SpiderEndpoint objects that their Status OwnerControllerType is None Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from None to Pod Upgrade This upgrade guide is intended for Spiderpool running on Kubernetes. If you have questions, feel free to ping us on the Slack channel . NOTE: Read the full upgrade guide to understand all the necessary steps before performing them. Upgrade steps The following steps will describe how to upgrade all of the components from one stable release to a later stable release. Setup the Helm repository and update: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ...Successfully got an update from the \"spiderpool\" chart repository Update Complete. \u2388Happy Helming!\u2388 If the spiderpool-init pod exists it needs to be removed Spiderpool-init Pod will help us initialize some environment information. It is very useful to delete it before upgrading. ~# kubectl get po -n kube-system spiderpool-init NAME READY STATUS RESTARTS AGE spiderpool-init 0 /1 Completed 0 49m ~# kubectl delete po -n kube-system spiderpool-init pod \"spiderpool-init\" deleted Update version via helm (Optional) Spiderpool chart version has changed, you can get the latest stable version with the following command, unzip the chart package and apply all crds. ~# helm search repo spiderpool --versions ... ~# helm fetch spiderpool/spiderpool --version <upgraded-version> ... ~# tar -xvf spiderpool-<upgraded-version>.tgz && cd spiderpool/crds ~# ls | grep '\\.yaml$' | xargs -I {} kubectl apply -f {} To upgrade spiderpool using Helm, you can change <upgraded-version> to any stable version. helm upgrade --install spiderpool spiderpool/spiderpool --wait --debug --version <upgraded-version> \\ -n kube-system \\ Running the previous command will overwrite the existing cluster's ConfigMap, so it is important to retain any existing options, either by setting them on the command line or storing them in a YAML file, similar to: ~# kubectl get cm -n kube-system spiderpool-conf -oyaml > my-config.yaml The --reuse-values flag may only be safely used if the Spiderpool chart version remains unchanged, for example when helm upgrade is used to apply configuration changes without upgrading Spiderpool. Instead, if you want to reuse the values from your existing installation, save the old values in a values file, check the file for any renamed or deprecated values, and then pass it to the helm upgrade command as described above. You can retrieve and save the values from an existing installation with the following command: helm get values spiderpool --namespace = kube-system -o yaml > old-values.yaml Helm Optional: Spider v0.7.0 brings some changes. Can be adapted through --set In version 0.7.0, spiderpool integrates multus. If you have installed multus, pass --set multus.multusCNI.install=false to disable the installation when upgrading. In version 0.7.0, the ipam.enableIPv6 value is changed to true by default. Determine the status of this value in your cluster configuration. If it is false, please change it through --set ipam.enableIPv6=false . In version 0.7.0, the default value of ipam.enableSpiderSubnet was changed to false. Determine the status of this value in your cluster configuration. If it is true, change it with --set ipam.enableSpiderSubnet=true . verify You can verify that the upgraded version matches by running the following command. ~# helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION spiderpool kube-system 3 2023 -08-31 23 :29:28.884386053 +0800 CST deployed spiderpool-<upgraded-version> <upgraded-version> ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-22z7f 1 /1 Running 0 3m spiderpool-agent-cjps5 1 /1 Running 0 3m spiderpool-agent-mwpsp 1 /1 Running 0 3m spiderpool-controller-647888ff-g5q4n 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m ~# kubectl get po -n kube-system spiderpool-controller-647888ff-g5q4n -oyaml | grep image ~# kubectl get po -n kube-system spiderpool-agent-cjps5 -oyaml | grep image","title":"Upgrading"},{"location":"usage/install/upgrade/#upgrading-spiderpool-versions","text":"This document describes breaking changes, as well as how to fix them, that have occurred at given releases. Please consult the segments from your current release until now before upgrading your spiderpool.","title":"Upgrading Spiderpool Versions"},{"location":"usage/install/upgrade/#upgrade-to-036-from-035","text":"","title":"Upgrade to 0.3.6 from (&lt;=0.3.5)"},{"location":"usage/install/upgrade/#description","text":"There's a design flaw for SpiderSubnet feature in auto-created IPPool label. The previous label ipam.spidernet.io/owner-application corresponding value uses '-' as separative sign. For example, we have deployment ns398-174835790/deploy398-82311862 and the corresponding label value is Deployment-ns398-174835790-deploy398-82311862 . It's very hard to unpack it to trace back what the application namespace and name is. Now, we use '_' rather than '-' as slash for SpiderSubnet feature label ipam.spidernet.io/owner-application , and the upper case will be like Deployment_ns398-174835790_deploy398-82311862 . Reference PR: #1162 In order to support multiple interfaces with SpiderSubnet feature, we also add one more label for auto-created IPPool. The key is ipam.spidernet.io/interface , and the value is the corresponding interface name.","title":"Description"},{"location":"usage/install/upgrade/#operation-steps","text":"Find all auto-created IPPools, their name format is auto-${appKind}-${appNS}-${appName}-v${ipVersion}-${uid} such as auto-deployment-default-demo-deploy-subnet-v4-69d041b98b41 . Replace their label, just like this: kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/owner-application\": ${AppLabelValue}}}}' Add one more label kubectl patch sp ${ auto -pool } --type merge --patch '{\"metadata\": {\"labels\": {\"ipam.spidernet.io/interface\": \"eth0\"}}}}' Update your Spiderpool components version and restart them all.","title":"Operation steps"},{"location":"usage/install/upgrade/#upgrade-to-040-from-040","text":"","title":"Upgrade to 0.4.0 from (&lt;0.4.0)"},{"location":"usage/install/upgrade/#description_1","text":"Due to the architecture adjustment, the SpiderEndpoint.Status.OwnerControllerType property is changed from None to Pod .","title":"Description"},{"location":"usage/install/upgrade/#operation-steps_1","text":"Find all SpiderEndpoint objects that their Status OwnerControllerType is None Replace the subresource SpiderEndpoint.Status.OwnerControllerType property from None to Pod","title":"Operation steps"},{"location":"usage/install/upgrade/#upgrade","text":"This upgrade guide is intended for Spiderpool running on Kubernetes. If you have questions, feel free to ping us on the Slack channel . NOTE: Read the full upgrade guide to understand all the necessary steps before performing them.","title":"Upgrade"},{"location":"usage/install/upgrade/#upgrade-steps","text":"The following steps will describe how to upgrade all of the components from one stable release to a later stable release. Setup the Helm repository and update: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ...Successfully got an update from the \"spiderpool\" chart repository Update Complete. \u2388Happy Helming!\u2388 If the spiderpool-init pod exists it needs to be removed Spiderpool-init Pod will help us initialize some environment information. It is very useful to delete it before upgrading. ~# kubectl get po -n kube-system spiderpool-init NAME READY STATUS RESTARTS AGE spiderpool-init 0 /1 Completed 0 49m ~# kubectl delete po -n kube-system spiderpool-init pod \"spiderpool-init\" deleted Update version via helm (Optional) Spiderpool chart version has changed, you can get the latest stable version with the following command, unzip the chart package and apply all crds. ~# helm search repo spiderpool --versions ... ~# helm fetch spiderpool/spiderpool --version <upgraded-version> ... ~# tar -xvf spiderpool-<upgraded-version>.tgz && cd spiderpool/crds ~# ls | grep '\\.yaml$' | xargs -I {} kubectl apply -f {} To upgrade spiderpool using Helm, you can change <upgraded-version> to any stable version. helm upgrade --install spiderpool spiderpool/spiderpool --wait --debug --version <upgraded-version> \\ -n kube-system \\ Running the previous command will overwrite the existing cluster's ConfigMap, so it is important to retain any existing options, either by setting them on the command line or storing them in a YAML file, similar to: ~# kubectl get cm -n kube-system spiderpool-conf -oyaml > my-config.yaml The --reuse-values flag may only be safely used if the Spiderpool chart version remains unchanged, for example when helm upgrade is used to apply configuration changes without upgrading Spiderpool. Instead, if you want to reuse the values from your existing installation, save the old values in a values file, check the file for any renamed or deprecated values, and then pass it to the helm upgrade command as described above. You can retrieve and save the values from an existing installation with the following command: helm get values spiderpool --namespace = kube-system -o yaml > old-values.yaml Helm Optional: Spider v0.7.0 brings some changes. Can be adapted through --set In version 0.7.0, spiderpool integrates multus. If you have installed multus, pass --set multus.multusCNI.install=false to disable the installation when upgrading. In version 0.7.0, the ipam.enableIPv6 value is changed to true by default. Determine the status of this value in your cluster configuration. If it is false, please change it through --set ipam.enableIPv6=false . In version 0.7.0, the default value of ipam.enableSpiderSubnet was changed to false. Determine the status of this value in your cluster configuration. If it is true, change it with --set ipam.enableSpiderSubnet=true . verify You can verify that the upgraded version matches by running the following command. ~# helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION spiderpool kube-system 3 2023 -08-31 23 :29:28.884386053 +0800 CST deployed spiderpool-<upgraded-version> <upgraded-version> ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-22z7f 1 /1 Running 0 3m spiderpool-agent-cjps5 1 /1 Running 0 3m spiderpool-agent-mwpsp 1 /1 Running 0 3m spiderpool-controller-647888ff-g5q4n 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m ~# kubectl get po -n kube-system spiderpool-controller-647888ff-g5q4n -oyaml | grep image ~# kubectl get po -n kube-system spiderpool-agent-cjps5 -oyaml | grep image","title":"Upgrade steps"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/","text":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u7684\u8282\u70b9\u62d3\u6251\u529f\u80fd\u53ef\u4ee5\u5c06 IPPool \u4e0e\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u4e2a\u7f51\u5361\u7684\u53ef\u7528 IP \u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u8fd8\u5177\u5907\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\u3002 Spiderpool \u80fd\u57fa\u4e8e IPVlan Underlay CNI \u5728\u963f\u91cc\u4e91\u73af\u5883\u4e0a\u8fd0\u884c\uff0c\u5e76\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001IPvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u5904\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d IPVlan CNI \u8fdb\u884c\u89e3\u51b3\u3002IPVlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 IPvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002 \u5b9e\u65bd\u8981\u6c42 \u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u6b65\u9aa4 \u963f\u91cc\u4e91\u73af\u5883 \u51c6\u5907\u4e00\u5957\u963f\u91cc\u4e91\u73af\u5883\uff0c\u7ed9\u865a\u62df\u673a\u5206\u914d 2 \u4e2a\u7f51\u5361\uff0c\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe\uff1a \u5b9e\u4f8b\uff08\u865a\u62df\u673a\uff09\u662f\u80fd\u591f\u4e3a\u60a8\u7684\u4e1a\u52a1\u63d0\u4f9b\u8ba1\u7b97\u670d\u52a1\u7684\u6700\u5c0f\u5355\u4f4d\uff0c\u4e0d\u540c\u7684\u5b9e\u4f8b\u89c4\u683c\u53ef\u521b\u5efa\u7f51\u5361\u6570\u548c\u6bcf\u5f20\u7f51\u5361\u53ef\u5206\u914d\u7684\u8f85\u52a9 IP \u6570\u5b58\u5728\u5dee\u5f02\uff0c\u6839\u636e\u4e1a\u52a1\u573a\u666f\u548c\u4f7f\u7528\u573a\u666f\uff0c\u53c2\u8003\u963f\u91cc\u4e91 \u5b9e\u4f8b\u89c4\u683c\u65cf \u9009\u62e9\u5bf9\u5e94\u89c4\u683c\u8fdb\u884c\u521b\u5efa\u5b9e\u4f8b\u3002 \u5982\u679c\u6709 IPv6 \u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u53c2\u8003\u963f\u91cc\u4e91 \u914d\u7f6e IPv6 \u5730\u5740 \u3002 \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 IPVlan, \u4f60\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 IPVlan\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a IPvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u4e0e eth1 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m \u521b\u5efa IPPools Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u8be5\u5b57\u6bb5\u7684\u503c\u5fc5\u987b\u8bbe\u7f6e\u3002\u7531\u4e8e\u963f\u91cc\u4e91\u9650\u5236\u4e86\u8282\u70b9\u53ef\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u60a8\u53ef\u4ee5\u4ece\u963f\u91cc\u4e91\u7684\u5f39\u6027\u7f51\u5361\u754c\u9762\u83b7\u53d6\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( eth0\u3001eth1 )\u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u7ec4 DaemonSet \u5e94\u7528\u548c 1 \u4e2a type \u4e3a ClusterIP \u7684 service \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u5206\u522b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\uff0c\u5e94\u7528\u7684 IP \u5747\u5728\u671f\u671b\u7684 IP \u6c60\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true \u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027 \u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes \u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027 \u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee \u963f\u91cc\u4e91\u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u73af\u5883\u4e0b\u6784\u5efa\u4e00\u4e2a\u516c\u7f51\u6216\u79c1\u7f51\u6d41\u91cf\u7684\u51fa\u5165\u53e3\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u6d4b\u8bd5\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 \u5982\u679c\u5e0c\u671b\u901a\u8fc7 IPv6 \u5730\u5740\u5b9e\u73b0\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\uff0c\u4f60\u9700\u8981\u901a\u8fc7 IPv6 \u7f51\u5173\uff0c\u4e3a Pod \u6240\u5206\u914d\u5230\u7684 IPv6 \u5730\u5740 \u5f00\u901a\u516c\u7f51\u5e26\u5bbd \uff0c\u5c06\u79c1\u7f51 IPv6 \u8f6c\u6362\u4e3a\u516c\u7f51 IPv6 \u5730\u5740\u3002\u914d\u7f6e\u5982\u4e0b\u3002 \u6d4b\u8bd5 IPv6 \u8bbf\u95ee\u5982\u4e0b\uff1a ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms \u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee \u90e8\u7f72 Cloud Controller Manager CCM\uff08Cloud Controller Manager\uff09\u662f\u963f\u91cc\u4e91\u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e\u963f\u91cc\u4e91\u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e CCM \u6587\u6863 \u5b8c\u6210 CCM \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u5206\u522b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4ece\u800c\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5404\u81ea\u7684 providerID \u3002 http://100.100.100.200/latest/meta-data \u662f\u963f\u91cc\u4e91 CLI \u63d0\u4f9b\u83b7\u53d6\u5b9e\u4f8b\u5143\u6570\u636e\u7684 API \u5165\u53e3\uff0c\u5728\u4e0b\u5217\u793a\u4f8b\u4e2d\u65e0\u9700\u4fee\u6539\u5b83\u3002\u66f4\u591a\u7528\u6cd5\u53ef\u53c2\u8003 \u5b9e\u4f8b\u5143\u6570\u636e ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* \u5728\u96c6\u7fa4\u7684 master \u8282\u70b9\u901a\u8fc7 kubectl patch \u547d\u4ee4\u4e3a\u96c6\u7fa4\u4e2d\u7684 \u6bcf\u4e2a\u8282\u70b9 \u8865\u5145\u5404\u81ea\u7684 providerID \uff0c\u8be5\u6b65\u9aa4\u5fc5\u987b\u88ab\u6267\u884c\uff0c\u5426\u5219\u5bf9\u5e94\u8282\u70b9\u7684 CCM Pod \u5c06\u65e0\u6cd5\u8fd0\u884c\u3002 ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # \u5c06 <NODE_NAME> \u4e0e <provider_id> \u66ff\u6362\u4e3a\u5bf9\u5e94\u503c\u3002 \u521b\u5efa\u963f\u91cc\u4e91\u7684 RAM \u7528\u6237\uff0c\u5e76\u6388\u6743\u3002 RAM \u7528\u6237\u662f RAM \u4e2d\u7684\u4e00\u79cd\u5b9e\u4f53\u8eab\u4efd\uff0c\u4ee3\u8868\u9700\u8981\u8bbf\u95ee\u963f\u91cc\u4e91\u7684\u4eba\u5458\u6216\u5e94\u7528\u7a0b\u5e8f\u3002\u901a\u8fc7\u53c2\u9605 RAM \u8bbf\u95ee\u63a7\u5236 \u521b\u5efa RAM \u7528\u6237\uff0c\u5e76\u6388\u4e8e\u9700\u8981\u8bbf\u95ee\u8d44\u6e90\u7684\u6743\u9650\u3002 \u4e3a\u786e\u4fdd\u540e\u7eed\u6b65\u9aa4\u4e2d\u6240\u4f7f\u7528\u7684 RAM \u7528\u6237\u5177\u5907\u8db3\u591f\u7684\u6743\u9650\uff0c\u8bf7\u4e0e\u672c\u6587\u4fdd\u6301\u4e00\u81f4\uff0c\u7ed9\u4e88 RAM \u7528\u6237 AdministratorAccess \u548c AliyunSLBFullAccess \u6743\u9650\u3002 \u83b7\u53d6 RAM \u7528\u6237\u7684 AccessKey & AccessKeySecret \u767b\u5f55 RAM \u7528\u6237\uff0c\u8bbf\u95ee \u7528\u6237\u4e2d\u5fc3 \uff0c\u83b7\u53d6\u5bf9\u5e94 RAM \u7528\u7684 AccessKey & AccessKeySecret\u3002 \u521b\u5efa CCM \u7684 Cloud ConfigMap\u3002 \u5c06\u6b65\u9aa4 3 \u83b7\u53d6\u7684 AccessKey & AccessKeySecret\uff0c\u53c2\u8003\u4e0b\u5217\u65b9\u5f0f\u5199\u5165\u73af\u5883\u53d8\u91cf\u3002 ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u5b8c\u6210\u521b\u5efa cloud-config\u3002 accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF \u83b7\u53d6 Yaml \uff0c\u5e76\u901a\u8fc7 kubectl apply -f cloud-controller-manager.yaml \u65b9\u5f0f\u5b89\u88c5 CCM\uff0c\u672c\u6587\u4e2d\u5b89\u88c5\u7684\u7248\u672c\u4e3a v2.5.0 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u83b7\u53d6 cloud-controller-manager.yaml\uff0c\u5e76\u66ff\u6362\u5176\u4e2d <<cluster_cidr>> \u4e3a\u60a8\u771f\u5b9e\u96c6\u7fa4\u7684 cluster CIDR \uff1b\u60a8\u53ef\u4ee5\u901a\u8fc7 kubectl cluster-info dump | grep -m1 cluster-cidr \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u7684 cluster CIDR \u3002 ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml \u68c0\u67e5 CCM \u5b89\u88c5\u5b8c\u6210\u3002 ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s \u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3 \u5982\u4e0b\u7684 Yaml \u5c06\u521b\u5efa spec.type \u4e3a LoadBalancer \u7684 2 \u7ec4 service\uff0c\u4e00\u7ec4\u4e3a tcp \uff08\u56db\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u4e00\u7ec4\u4e3a http \uff08\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\u3002 service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port \uff1aCCM \u63d0\u4f9b\u7684\u521b\u5efa\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\u6ce8\u89e3\u3002\u53ef\u4ee5\u901a\u8fc7\u5b83\u81ea\u5b9a\u4e49\u66b4\u9732\u7aef\u53e3\u3002\u66f4\u591a\u7528\u6cd5\u53c2\u8003 CCM \u4f7f\u7528\u6587\u6863 \u3002 .spec.externalTrafficPolicy \uff1a\u8868\u793a\u6b64 Service \u662f\u5426\u5e0c\u671b\u5c06\u5916\u90e8\u6d41\u91cf\u8def\u7531\u5230\u8282\u70b9\u672c\u5730\u6216\u96c6\u7fa4\u8303\u56f4\u7684\u7aef\u70b9\u3002\u5b83\u6709\u4e24\u4e2a\u53ef\u7528\u9009\u9879\uff1aCluster\uff08\u9ed8\u8ba4\uff09\u548c Local\u3002\u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP\uff0c\u4f46\u516c\u6709\u4e91\u81ea\u5efa\u96c6\u7fa4\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5e73\u53f0\u7684 Loadbalancer \u7ec4\u4ef6\u8fdb\u884c nodePort \u8f6c\u53d1\u65f6\uff0c\u4f1a\u51fa\u73b0\u8bbf\u95ee\u4e0d\u901a\u3002\u9488\u5bf9\u8be5\u95ee\u9898 Spiderpool \u63d0\u4f9b\u4e86 coordinator \u63d2\u4ef6\uff0c\u8be5\u63d2\u4ef6\u901a\u8fc7 iptables \u5728\u6570\u636e\u5305\u4e2d\u6253\u6807\u8bb0\uff0c\u786e\u8ba4\u4ece veth0 \u8fdb\u5165\u7684\u6570\u636e\u7684\u56de\u590d\u5305\u4ecd\u4ece veth0 \u8f6c\u53d1\uff0c\u8fdb\u800c\u89e3\u51b3\u5728\u8be5\u6a21\u5f0f\u4e0b nodeport \u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM \u5c06\u81ea\u52a8\u5728 IaaS \u5c42\u521b\u5efa\u56db\u5c42\u4e0e\u4e03\u5c42\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u53ef\u4ee5\u901a\u8fc7\u963f\u91cc\u4e91\u754c\u9762\u8fdb\u884c\u67e5\u770b\uff0c\u5982\u4e0b\uff1a \u9a8c\u8bc1\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee \u5728\u516c\u7f51\u7684\u673a\u5668\u4e0a\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u7684 \u516c\u7f51 IP + \u7aef\u53e3 \u5b9e\u73b0\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee # \u8bbf\u95ee\u56db\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # \u8bbf\u95ee\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes \u963f\u91cc\u4e91\u7684 CCM \u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u7684\u5165\u53e3\u8bbf\u95ee\u65f6\uff0c\u5176\u4e0d\u652f\u6301\u540e\u7aef service \u7684 spec.ipFamilies \u8bbe\u7f6e\u4e3a IPv6 \u3002 ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6. \u603b\u7ed3 Spiderpool \u80fd\u591f\u8fd0\u884c\u5728\u963f\u91cc\u4e91\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\u3002","title":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_1","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"\u963f\u91cc\u4e91\u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_2","text":"\u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_3","text":"Spiderpool \u7684\u8282\u70b9\u62d3\u6251\u529f\u80fd\u53ef\u4ee5\u5c06 IPPool \u4e0e\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u4e2a\u7f51\u5361\u7684\u53ef\u7528 IP \u5f62\u6210\u7ed1\u5b9a\uff0c\u540c\u65f6\u8fd8\u5177\u5907\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\u3002 Spiderpool \u80fd\u57fa\u4e8e IPVlan Underlay CNI \u5728\u963f\u91cc\u4e91\u73af\u5883\u4e0a\u8fd0\u884c\uff0c\u5e76\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001IPvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u5904\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d IPVlan CNI \u8fdb\u884c\u89e3\u51b3\u3002IPVlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 IPvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_4","text":"\u4f7f\u7528 IPVlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_5","text":"","title":"\u6b65\u9aa4"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_6","text":"\u51c6\u5907\u4e00\u5957\u963f\u91cc\u4e91\u73af\u5883\uff0c\u7ed9\u865a\u62df\u673a\u5206\u914d 2 \u4e2a\u7f51\u5361\uff0c\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe\uff1a \u5b9e\u4f8b\uff08\u865a\u62df\u673a\uff09\u662f\u80fd\u591f\u4e3a\u60a8\u7684\u4e1a\u52a1\u63d0\u4f9b\u8ba1\u7b97\u670d\u52a1\u7684\u6700\u5c0f\u5355\u4f4d\uff0c\u4e0d\u540c\u7684\u5b9e\u4f8b\u89c4\u683c\u53ef\u521b\u5efa\u7f51\u5361\u6570\u548c\u6bcf\u5f20\u7f51\u5361\u53ef\u5206\u914d\u7684\u8f85\u52a9 IP \u6570\u5b58\u5728\u5dee\u5f02\uff0c\u6839\u636e\u4e1a\u52a1\u573a\u666f\u548c\u4f7f\u7528\u573a\u666f\uff0c\u53c2\u8003\u963f\u91cc\u4e91 \u5b9e\u4f8b\u89c4\u683c\u65cf \u9009\u62e9\u5bf9\u5e94\u89c4\u683c\u8fdb\u884c\u521b\u5efa\u5b9e\u4f8b\u3002 \u5982\u679c\u6709 IPv6 \u7684\u9700\u6c42\uff0c\u53ef\u4ee5\u53c2\u8003\u963f\u91cc\u4e91 \u914d\u7f6e IPv6 \u5730\u5740 \u3002 \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a","title":"\u963f\u91cc\u4e91\u73af\u5883"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 IPVlan, \u4f60\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 IPVlan\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa IPvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a IPvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u4e0e eth1 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#ippools","text":"Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u5f53 nodeName \u4e0d\u4e3a\u7a7a\u65f6\uff0cPod \u5728\u67d0\u4e2a\u8282\u70b9\u4e0a\u542f\u52a8\uff0c\u5e76\u5c1d\u8bd5\u4ece SpiderIPPool \u5206\u914d IP \u5730\u5740, \u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53 nodeName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u4e0d\u5b9e\u65bd\u4efb\u4f55\u5206\u914d\u9650\u5236\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u8be5\u5b57\u6bb5\u7684\u503c\u5fc5\u987b\u8bbe\u7f6e\u3002\u7531\u4e8e\u963f\u91cc\u4e91\u9650\u5236\u4e86\u8282\u70b9\u53ef\u4f7f\u7528\u7684 IP \u5730\u5740\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u60a8\u53ef\u4ee5\u4ece\u963f\u91cc\u4e91\u7684\u5f39\u6027\u7f51\u5361\u754c\u9762\u83b7\u53d6\u3002 \u4f9d\u636e\u5982\u4e0a\u6240\u8ff0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( eth0\u3001eth1 )\u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF","title":"\u521b\u5efa IPPools"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_7","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 2 \u7ec4 DaemonSet \u5e94\u7528\u548c 1 \u4e2a type \u4e3a ClusterIP \u7684 service \uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5b50\u7f51\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u5206\u522b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool \u81ea\u52a8\u4e3a\u5e94\u7528\u5206\u914d IP \u5730\u5740\uff0c\u5e94\u7528\u7684 IP \u5747\u5728\u671f\u671b\u7684 IP \u6c60\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_8","text":"\u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes","title":"\u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_9","text":"","title":"\u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#pod","text":"\u963f\u91cc\u4e91\u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u73af\u5883\u4e0b\u6784\u5efa\u4e00\u4e2a\u516c\u7f51\u6216\u79c1\u7f51\u6d41\u91cf\u7684\u51fa\u5165\u53e3\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u6d4b\u8bd5\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 \u5982\u679c\u5e0c\u671b\u901a\u8fc7 IPv6 \u5730\u5740\u5b9e\u73b0\u96c6\u7fa4\u5185 Pod \u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\uff0c\u4f60\u9700\u8981\u901a\u8fc7 IPv6 \u7f51\u5173\uff0c\u4e3a Pod \u6240\u5206\u914d\u5230\u7684 IPv6 \u5730\u5740 \u5f00\u901a\u516c\u7f51\u5e26\u5bbd \uff0c\u5c06\u79c1\u7f51 IPv6 \u8f6c\u6362\u4e3a\u516c\u7f51 IPv6 \u5730\u5740\u3002\u914d\u7f6e\u5982\u4e0b\u3002 \u6d4b\u8bd5 IPv6 \u8bbf\u95ee\u5982\u4e0b\uff1a ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms","title":"\u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_10","text":"","title":"\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#cloud-controller-manager","text":"CCM\uff08Cloud Controller Manager\uff09\u662f\u963f\u91cc\u4e91\u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e\u963f\u91cc\u4e91\u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408\u963f\u91cc\u4e91\u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e CCM \u6587\u6863 \u5b8c\u6210 CCM \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u5206\u522b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4ece\u800c\u83b7\u53d6\u6bcf\u4e2a\u8282\u70b9\u5404\u81ea\u7684 providerID \u3002 http://100.100.100.200/latest/meta-data \u662f\u963f\u91cc\u4e91 CLI \u63d0\u4f9b\u83b7\u53d6\u5b9e\u4f8b\u5143\u6570\u636e\u7684 API \u5165\u53e3\uff0c\u5728\u4e0b\u5217\u793a\u4f8b\u4e2d\u65e0\u9700\u4fee\u6539\u5b83\u3002\u66f4\u591a\u7528\u6cd5\u53ef\u53c2\u8003 \u5b9e\u4f8b\u5143\u6570\u636e ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* \u5728\u96c6\u7fa4\u7684 master \u8282\u70b9\u901a\u8fc7 kubectl patch \u547d\u4ee4\u4e3a\u96c6\u7fa4\u4e2d\u7684 \u6bcf\u4e2a\u8282\u70b9 \u8865\u5145\u5404\u81ea\u7684 providerID \uff0c\u8be5\u6b65\u9aa4\u5fc5\u987b\u88ab\u6267\u884c\uff0c\u5426\u5219\u5bf9\u5e94\u8282\u70b9\u7684 CCM Pod \u5c06\u65e0\u6cd5\u8fd0\u884c\u3002 ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # \u5c06 <NODE_NAME> \u4e0e <provider_id> \u66ff\u6362\u4e3a\u5bf9\u5e94\u503c\u3002 \u521b\u5efa\u963f\u91cc\u4e91\u7684 RAM \u7528\u6237\uff0c\u5e76\u6388\u6743\u3002 RAM \u7528\u6237\u662f RAM \u4e2d\u7684\u4e00\u79cd\u5b9e\u4f53\u8eab\u4efd\uff0c\u4ee3\u8868\u9700\u8981\u8bbf\u95ee\u963f\u91cc\u4e91\u7684\u4eba\u5458\u6216\u5e94\u7528\u7a0b\u5e8f\u3002\u901a\u8fc7\u53c2\u9605 RAM \u8bbf\u95ee\u63a7\u5236 \u521b\u5efa RAM \u7528\u6237\uff0c\u5e76\u6388\u4e8e\u9700\u8981\u8bbf\u95ee\u8d44\u6e90\u7684\u6743\u9650\u3002 \u4e3a\u786e\u4fdd\u540e\u7eed\u6b65\u9aa4\u4e2d\u6240\u4f7f\u7528\u7684 RAM \u7528\u6237\u5177\u5907\u8db3\u591f\u7684\u6743\u9650\uff0c\u8bf7\u4e0e\u672c\u6587\u4fdd\u6301\u4e00\u81f4\uff0c\u7ed9\u4e88 RAM \u7528\u6237 AdministratorAccess \u548c AliyunSLBFullAccess \u6743\u9650\u3002 \u83b7\u53d6 RAM \u7528\u6237\u7684 AccessKey & AccessKeySecret \u767b\u5f55 RAM \u7528\u6237\uff0c\u8bbf\u95ee \u7528\u6237\u4e2d\u5fc3 \uff0c\u83b7\u53d6\u5bf9\u5e94 RAM \u7528\u7684 AccessKey & AccessKeySecret\u3002 \u521b\u5efa CCM \u7684 Cloud ConfigMap\u3002 \u5c06\u6b65\u9aa4 3 \u83b7\u53d6\u7684 AccessKey & AccessKeySecret\uff0c\u53c2\u8003\u4e0b\u5217\u65b9\u5f0f\u5199\u5165\u73af\u5883\u53d8\u91cf\u3002 ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u5b8c\u6210\u521b\u5efa cloud-config\u3002 accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF \u83b7\u53d6 Yaml \uff0c\u5e76\u901a\u8fc7 kubectl apply -f cloud-controller-manager.yaml \u65b9\u5f0f\u5b89\u88c5 CCM\uff0c\u672c\u6587\u4e2d\u5b89\u88c5\u7684\u7248\u672c\u4e3a v2.5.0 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff0c\u83b7\u53d6 cloud-controller-manager.yaml\uff0c\u5e76\u66ff\u6362\u5176\u4e2d <<cluster_cidr>> \u4e3a\u60a8\u771f\u5b9e\u96c6\u7fa4\u7684 cluster CIDR \uff1b\u60a8\u53ef\u4ee5\u901a\u8fc7 kubectl cluster-info dump | grep -m1 cluster-cidr \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u7684 cluster CIDR \u3002 ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml \u68c0\u67e5 CCM \u5b89\u88c5\u5b8c\u6210\u3002 ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s","title":"\u90e8\u7f72 Cloud Controller Manager"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#loadbalancer","text":"\u5982\u4e0b\u7684 Yaml \u5c06\u521b\u5efa spec.type \u4e3a LoadBalancer \u7684 2 \u7ec4 service\uff0c\u4e00\u7ec4\u4e3a tcp \uff08\u56db\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\uff0c\u4e00\u7ec4\u4e3a http \uff08\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\uff09\u3002 service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port \uff1aCCM \u63d0\u4f9b\u7684\u521b\u5efa\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861\u6ce8\u89e3\u3002\u53ef\u4ee5\u901a\u8fc7\u5b83\u81ea\u5b9a\u4e49\u66b4\u9732\u7aef\u53e3\u3002\u66f4\u591a\u7528\u6cd5\u53c2\u8003 CCM \u4f7f\u7528\u6587\u6863 \u3002 .spec.externalTrafficPolicy \uff1a\u8868\u793a\u6b64 Service \u662f\u5426\u5e0c\u671b\u5c06\u5916\u90e8\u6d41\u91cf\u8def\u7531\u5230\u8282\u70b9\u672c\u5730\u6216\u96c6\u7fa4\u8303\u56f4\u7684\u7aef\u70b9\u3002\u5b83\u6709\u4e24\u4e2a\u53ef\u7528\u9009\u9879\uff1aCluster\uff08\u9ed8\u8ba4\uff09\u548c Local\u3002\u5c06 .spec.externalTrafficPolicy \u8bbe\u7f6e\u4e3a Local \uff0c\u53ef\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u6e90 IP\uff0c\u4f46\u516c\u6709\u4e91\u81ea\u5efa\u96c6\u7fa4\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5e73\u53f0\u7684 Loadbalancer \u7ec4\u4ef6\u8fdb\u884c nodePort \u8f6c\u53d1\u65f6\uff0c\u4f1a\u51fa\u73b0\u8bbf\u95ee\u4e0d\u901a\u3002\u9488\u5bf9\u8be5\u95ee\u9898 Spiderpool \u63d0\u4f9b\u4e86 coordinator \u63d2\u4ef6\uff0c\u8be5\u63d2\u4ef6\u901a\u8fc7 iptables \u5728\u6570\u636e\u5305\u4e2d\u6253\u6807\u8bb0\uff0c\u786e\u8ba4\u4ece veth0 \u8fdb\u5165\u7684\u6570\u636e\u7684\u56de\u590d\u5305\u4ecd\u4ece veth0 \u8f6c\u53d1\uff0c\u8fdb\u800c\u89e3\u51b3\u5728\u8be5\u6a21\u5f0f\u4e0b nodeport \u8bbf\u95ee\u4e0d\u901a\u7684\u95ee\u9898\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF \u521b\u5efa\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u67e5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM \u5c06\u81ea\u52a8\u5728 IaaS \u5c42\u521b\u5efa\u56db\u5c42\u4e0e\u4e03\u5c42\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u53ef\u4ee5\u901a\u8fc7\u963f\u91cc\u4e91\u754c\u9762\u8fdb\u884c\u67e5\u770b\uff0c\u5982\u4e0b\uff1a","title":"\u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_11","text":"\u5728\u516c\u7f51\u7684\u673a\u5668\u4e0a\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u5668\u7684 \u516c\u7f51 IP + \u7aef\u53e3 \u5b9e\u73b0\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee # \u8bbf\u95ee\u56db\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # \u8bbf\u95ee\u4e03\u5c42\u8d1f\u8f7d\u5747\u8861 $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes \u963f\u91cc\u4e91\u7684 CCM \u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u7684\u5165\u53e3\u8bbf\u95ee\u65f6\uff0c\u5176\u4e0d\u652f\u6301\u540e\u7aef service \u7684 spec.ipFamilies \u8bbe\u7f6e\u4e3a IPv6 \u3002 ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6.","title":"\u9a8c\u8bc1\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-alibaba-zh_CN/#_12","text":"Spiderpool \u80fd\u591f\u8fd0\u884c\u5728\u963f\u91cc\u4e91\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u4fdd\u8bc1\u96c6\u7fa4\u7684\u4e1c\u897f\u5411\u4e0e\u5357\u5317\u5411\u6d41\u91cf\u5747\u6b63\u5e38\u3002","title":"\u603b\u7ed3"},{"location":"usage/install/cloud/get-started-alibaba/","text":"Running On Alibaba Cloud English | \u7b80\u4f53\u4e2d\u6587 Introduction With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using Underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized Underlay solutions for public clouds. This page introduces Spiderpool , an Underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios. Features Spiderpool's node topology function can bind IP pools to the available IPs of each network card on each node, and also achieve the validity of MAC addresses. Spiderpool can run on the Alibaba Cloud environment based on IPVlan Underlay CNI, and ensures that the east-west and north-south traffic of the cluster are normal. Its implementation principle is as follows: When using Underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and IPvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based Underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with IPVlan CNI . The IPVlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating IPVlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved. Prerequisites The system kernel version must be greater than 4.2 when using IPVlan as the cluster's CNI. Helm is installed. Steps Alibaba Cloud Environment Prepare an Alibaba Cloud environment with virtual machines that have 2 network interfaces. Assign a set of auxiliary private IP addresses to each network interface, as shown in the picture: An instance (virtual machine) is the smallest unit that can provide computing services for your business. Different instance specifications vary in the number of network cards that can be created and the number of auxiliary IPs that can be assigned to each network card. For more information on specific business and usage scenarios, refer to Alibaba Cloud Instance Specification Family to select the corresponding specification to create an instance. If you have IPv6 requirements, you can refer to Alibaba Cloud Configuring IPv6 Addresses . Utilize the configured VMs to build a Kubernetes cluster. The available IP addresses for the nodes and the network topology of the cluster are depicted below: Install Spiderpool Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" If IPVlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install IPVlan in your cluster. If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Here is an example of creating an IPvlan SpiderMultusConfig configuration: IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two IPvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth0 and eth1 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m Create IP Pools The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips \uff1athis field must not be empty. Due to Alibaba Cloud's limitations on available IP addresses for nodes, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . You can obtain this information from the Elastic Network Interface page in the Alibaba Cloud console. Based on the provided information, use the following YAML configuration to create a SpiderIPPool for each network interface (eth0 and eth1) on every node. These SpiderIPPools will assign IP addresses to Pods running on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF Create Applications In the following example YAML, there are 2 sets of DaemonSet applications and 1 service with a type of ClusterIP: v1.multus-cni.io/default-network : specify the subnet that each application will use. In the example, the applications are assigned different subnets. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF Check the status of the running Pods: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool automatically assigns IP addresses to the applications, ensuring that the assigned IPs are within the expected IP pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true Test East-West Connectivity Test communication between Pods and their hosts: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms Test communication between Pods across different nodes and subnets: ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms Test communication between Pods and ClusterIP services: ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes Test North-South Connectivity Test egress traffic from Pods to external destinations Alibaba Cloud's NAT Gateway provides an ingress and egress gateway for public or private network traffic within a VPC environment. By utilizing NAT Gateway, the cluster can have egress connectivity. Please refer to the NAT Gateway documentation for creating a NAT Gateway as depicted in the picture: Test egress traffic from Pods ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 If you want to access the traffic egress of Pods in the cluster through IPv6 addresses, you need to activate public network bandwidth for the IPv6 address assigned to the Pod through the IPv6 gateway and convert the private IPv6 to a public IPv6 address. The configuration is as follows. Test Pod egress traffic over IPv6: ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms Load Balancer Traffic Ingress Access Deploy Cloud Controller Manager Cloud Controller Manager (CCM) is an Alibaba Cloud's component that enables integration between Kubernetes and Alibaba Cloud services. We will use CCM along with Alibaba Cloud infrastructure to facilitate load balancer traffic ingress access. Follow the steps below and refer to the CCM documentation for deploying CCM. Configure providerID on Cluster Nodes On each node in the cluster, run the following command to obtain the providerID for each node. http://100.100.100.200/latest/meta-data is the API entry point provided by Alibaba Cloud CLI for retrieving instance metadata. You don't need to modify it in the provided example. For more information, please refer to ECS instance metadata . ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* On the master node of the cluster, use the kubectl patch command to add the providerID for each node in the cluster. This step is necessary to ensure the proper functioning of the CCM Pod on each corresponding node. Failure to run this step will result in the CCM Pod being unable to run correctly. ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # Replace <NODE_NAME> and <provider_id> with corresponding values. Create an Alibaba Cloud RAM user and grant authorization. A RAM user is an entity within Alibaba Cloud's Resource Access Management (RAM) that represents individuals or applications requiring access to Alibaba Cloud resources. Refer to Overview of RAM users to create a RAM user and assign the necessary permissions for accessing resources. To ensure that the RAM user used in the subsequent steps has sufficient privileges, grant the AdministratorAccess and AliyunSLBFullAccess permissions to the RAM user, following the instructions provided here. Obtain the AccessKey & AccessKeySecret for the RAM user. Log in to the RAM User account and go to User Center to retrieve the corresponding AccessKey & AccessKeySecret for the RAM User. Create the Cloud ConfigMap for CCM. Use the following method to write the AccessKey & AccessKeySecret obtained in step 3 as environment variables. ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** Run the following command to create cloud-config: accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF Retrieve the YAML file and install CCM by running the command kubectl apply -f cloud-controller-manager.yaml . The version of CCM being installed here is v2.5.0. Use the following command to obtain the cloud-controller-manager.yaml file and replace <<cluster_cidr>> with the actual cluster CIDR; You can view the cluster CIDR of the cluster through the kubectl cluster-info dump | grep -m1 cluster-cidr command. ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml Verify if CCM is installed. ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s Create Load Balancer Ingress for Applications The following YAML will create two sets of services, one for TCP (layer 4 load balancing) and one for HTTP (layer 7 load balancing), with spec.type set to LoadBalancer . service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port : this annotation provided by CCM allows you to customize the exposed ports for layer 7 load balancing. For more information, refer to the CCM Usage Documentation . .spec.externalTrafficPolicy : indicates whether the service prefers to route external traffic to local or cluster-wide endpoints. It has two options: Cluster (default) and Local. Setting .spec.externalTrafficPolicy to Local preserves the client source IP. However, when a self-built public cloud cluster uses the platform's Loadbalancer component for nodePort forwarding in this mode, access will be blocked. In response to this problem, Spiderpool provides the coordinator plug-in, which uses iptables to mark the data packets to confirm that the reply packets of data entering from veth0 are still forwarded from veth0, thus solving the problem of nodeport being unable to access in this mode. ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF After the creation is complete, you can view the following: ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM will automatically create layer 4 and layer 7 load balancers at its IaaS services. You can easily access and manage them through the Alibaba Cloud console, as shown below: Verify Load Balancer Traffic Ingress Access On a public machine, access the load balancer's public IP + port to test the traffic ingress: # Access layer 4 load balancing $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # Access layer 7 load balancing $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes Alibaba Cloud's CCM implements ingress access for load balancing traffic, and it does not support setting the spec.ipFamilies of the backend service to IPv6. ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6. Summary Spiderpool is successfully running in an Alibaba Cloud cluster, ensuring normal east-west and north-south traffic.","title":"Alibaba Cloud"},{"location":"usage/install/cloud/get-started-alibaba/#running-on-alibaba-cloud","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Alibaba Cloud"},{"location":"usage/install/cloud/get-started-alibaba/#introduction","text":"With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using Underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized Underlay solutions for public clouds. This page introduces Spiderpool , an Underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios.","title":"Introduction"},{"location":"usage/install/cloud/get-started-alibaba/#features","text":"Spiderpool's node topology function can bind IP pools to the available IPs of each network card on each node, and also achieve the validity of MAC addresses. Spiderpool can run on the Alibaba Cloud environment based on IPVlan Underlay CNI, and ensures that the east-west and north-south traffic of the cluster are normal. Its implementation principle is as follows: When using Underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and IPvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based Underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with IPVlan CNI . The IPVlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating IPVlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved.","title":"Features"},{"location":"usage/install/cloud/get-started-alibaba/#prerequisites","text":"The system kernel version must be greater than 4.2 when using IPVlan as the cluster's CNI. Helm is installed.","title":"Prerequisites"},{"location":"usage/install/cloud/get-started-alibaba/#steps","text":"","title":"Steps"},{"location":"usage/install/cloud/get-started-alibaba/#alibaba-cloud-environment","text":"Prepare an Alibaba Cloud environment with virtual machines that have 2 network interfaces. Assign a set of auxiliary private IP addresses to each network interface, as shown in the picture: An instance (virtual machine) is the smallest unit that can provide computing services for your business. Different instance specifications vary in the number of network cards that can be created and the number of auxiliary IPs that can be assigned to each network card. For more information on specific business and usage scenarios, refer to Alibaba Cloud Instance Specification Family to select the corresponding specification to create an instance. If you have IPv6 requirements, you can refer to Alibaba Cloud Configuring IPv6 Addresses . Utilize the configured VMs to build a Kubernetes cluster. The available IP addresses for the nodes and the network topology of the cluster are depicted below:","title":"Alibaba Cloud Environment"},{"location":"usage/install/cloud/get-started-alibaba/#install-spiderpool","text":"Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"ipvlan-eth0\" If IPVlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install IPVlan in your cluster. If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus.","title":"Install Spiderpool"},{"location":"usage/install/cloud/get-started-alibaba/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Here is an example of creating an IPvlan SpiderMultusConfig configuration: IPVLAN_MASTER_INTERFACE0 = \"eth0\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"eth1\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: kube-system spec: cniType: ipvlan enableCoordinator: true ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two IPvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth0 and eth1 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE ipvlan-eth0 10m ipvlan-eth1 10m","title":"Install CNI"},{"location":"usage/install/cloud/get-started-alibaba/#create-ip-pools","text":"The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips \uff1athis field must not be empty. Due to Alibaba Cloud's limitations on available IP addresses for nodes, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . You can obtain this information from the Elastic Network Interface page in the Alibaba Cloud console. Based on the provided information, use the following YAML configuration to create a SpiderIPPool for each network interface (eth0 and eth1) on every node. These SpiderIPPools will assign IP addresses to Pods running on different nodes. ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-172 spec: default: true ips: - 172.31.199.185-172.31.199.189 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - master multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-192 spec: default: true ips: - 192.168.0.156-192.168.0.160 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - master multusName: - kube-system/ipvlan-eth1 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-172 spec: default: true ips: - 172.31.199.190-172.31.199.194 subnet: 172.31.192.0/20 gateway: 172.31.207.253 nodeName: - worker multusName: - kube-system/ipvlan-eth0 --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker-192 spec: default: true ips: - 192.168.0.161-192.168.0.165 subnet: 192.168.0.0/24 gateway: 192.168.0.253 nodeName: - worker multusName: - kube-system/ipvlan-eth1 EOF","title":"Create IP Pools"},{"location":"usage/install/cloud/get-started-alibaba/#create-applications","text":"In the following example YAML, there are 2 sets of DaemonSet applications and 1 service with a type of ClusterIP: v1.multus-cni.io/default-network : specify the subnet that each application will use. In the example, the applications are assigned different subnets. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-1 name: test-app-1 namespace: default spec: selector: matchLabels: app: test-app-1 template: metadata: labels: app: test-app-1 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth0 spec: containers: - image: busybox command: [\"sleep\", \"3600\"] imagePullPolicy: IfNotPresent name: test-app-1 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: test-app-2 name: test-app-2 namespace: default spec: selector: matchLabels: app: test-app-2 template: metadata: labels: app: test-app-2 annotations: v1.multus-cni.io/default-network: kube-system/ipvlan-eth1 spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-app-2 ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-svc labels: app: test-app-2 spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 EOF Check the status of the running Pods: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-1-ddlx7 1 /1 Running 0 16s 172 .31.199.187 master <none> <none> test-app-1-jpfkj 1 /1 Running 0 16s 172 .31.199.193 worker <none> <none> test-app-2-qbhwx 1 /1 Running 0 12s 192 .168.0.160 master <none> <none> test-app-2-r6gwx 1 /1 Running 0 12s 192 .168.0.161 worker <none> <none> Spiderpool automatically assigns IP addresses to the applications, ensuring that the assigned IPs are within the expected IP pool. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT master-172 4 172 .31.192.0/20 1 5 true master-192 4 192 .168.0.0/24 1 5 true worker-172 4 172 .31.192.0/20 1 5 true worker-192 4 192 .168.0.0/24 1 5 true","title":"Create Applications"},{"location":"usage/install/cloud/get-started-alibaba/#test-east-west-connectivity","text":"Test communication between Pods and their hosts: ~# kubectl get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready control-plane 2d12h v1.27.3 172 .31.199.183 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 worker Ready <none> 2d12h v1.27.3 172 .31.199.184 <none> CentOS Linux 7 ( Core ) 6 .4.0-1.el7.elrepo.x86_64 containerd://1.7.1 ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.183 -c 2 PING 172 .31.199.183 ( 172 .31.199.183 ) : 56 data bytes 64 bytes from 172 .31.199.183: seq = 0 ttl = 64 time = 0 .088 ms 64 bytes from 172 .31.199.183: seq = 1 ttl = 64 time = 0 .054 ms --- 172 .31.199.183 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.071/0.088 ms Test communication between Pods across different nodes and subnets: ~# kubectl exec -ti test-app-1-ddlx7 -- ping 172 .31.199.193 -c 2 PING 172 .31.199.193 ( 172 .31.199.193 ) : 56 data bytes 64 bytes from 172 .31.199.193: seq = 0 ttl = 64 time = 0 .460 ms 64 bytes from 172 .31.199.193: seq = 1 ttl = 64 time = 0 .210 ms --- 172 .31.199.193 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .210/0.335/0.460 ms ~# kubectl exec -ti test-app-1-ddlx7 -- ping 192 .168.0.161 -c 2 PING 192 .168.0.161 ( 192 .168.0.161 ) : 56 data bytes 64 bytes from 192 .168.0.161: seq = 0 ttl = 64 time = 0 .408 ms 64 bytes from 192 .168.0.161: seq = 1 ttl = 64 time = 0 .194 ms --- 192 .168.0.161 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.301/0.408 ms Test communication between Pods and ClusterIP services: ~# kubectl get svc test-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-svc ClusterIP 10 .233.23.194 <none> 80 /TCP 26s ~# kubectl exec -ti test-app-2-qbhwx -- curl 10 .233.23.194 -I HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Fri, 21 Jul 2023 06 :45:56 GMT Content-Type: text/html Content-Length: 4086 Last-Modified: Fri, 21 Jul 2023 06 :38:41 GMT Connection: keep-alive ETag: \"64ba27f1-ff6\" Accept-Ranges: bytes","title":"Test East-West Connectivity"},{"location":"usage/install/cloud/get-started-alibaba/#test-north-south-connectivity","text":"","title":"Test North-South Connectivity"},{"location":"usage/install/cloud/get-started-alibaba/#test-egress-traffic-from-pods-to-external-destinations","text":"Alibaba Cloud's NAT Gateway provides an ingress and egress gateway for public or private network traffic within a VPC environment. By utilizing NAT Gateway, the cluster can have egress connectivity. Please refer to the NAT Gateway documentation for creating a NAT Gateway as depicted in the picture: Test egress traffic from Pods ~# kubectl exec -ti test-app-2-qbhwx -- curl www.baidu.com -I HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Connection: keep-alive Content-Length: 277 Content-Type: text/html Date: Fri, 21 Jul 2023 08 :42:17 GMT Etag: \"575e1f60-115\" Last-Modified: Mon, 13 Jun 2016 02 :50:08 GMT Pragma: no-cache Server: bfe/1.0.8.18 If you want to access the traffic egress of Pods in the cluster through IPv6 addresses, you need to activate public network bandwidth for the IPv6 address assigned to the Pod through the IPv6 gateway and convert the private IPv6 to a public IPv6 address. The configuration is as follows. Test Pod egress traffic over IPv6: ~# kubectl exec -ti test-app-2-qbhwx -- ping -6 aliyun.com -c 2 PING aliyun.com ( 2401 :b180:1:60::6 ) : 56 data bytes 64 bytes from 2401 :b180:1:60::6: seq = 0 ttl = 96 time = 6 .058 ms 64 bytes from 2401 :b180:1:60::6: seq = 1 ttl = 96 time = 6 .079 ms --- aliyun.com ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 6 .058/6.068/6.079 ms","title":"Test egress traffic from Pods to external destinations"},{"location":"usage/install/cloud/get-started-alibaba/#load-balancer-traffic-ingress-access","text":"","title":"Load Balancer Traffic Ingress Access"},{"location":"usage/install/cloud/get-started-alibaba/#deploy-cloud-controller-manager","text":"Cloud Controller Manager (CCM) is an Alibaba Cloud's component that enables integration between Kubernetes and Alibaba Cloud services. We will use CCM along with Alibaba Cloud infrastructure to facilitate load balancer traffic ingress access. Follow the steps below and refer to the CCM documentation for deploying CCM. Configure providerID on Cluster Nodes On each node in the cluster, run the following command to obtain the providerID for each node. http://100.100.100.200/latest/meta-data is the API entry point provided by Alibaba Cloud CLI for retrieving instance metadata. You don't need to modify it in the provided example. For more information, please refer to ECS instance metadata . ~# META_EP = http://100.100.100.200/latest/meta-data ~# provider_id = ` curl -s $META_EP /region-id ` . ` curl -s $META_EP /instance-id ` ~# echo $provider_id cn-hangzhou.i-bp17345hor9******* On the master node of the cluster, use the kubectl patch command to add the providerID for each node in the cluster. This step is necessary to ensure the proper functioning of the CCM Pod on each corresponding node. Failure to run this step will result in the CCM Pod being unable to run correctly. ~# kubectl get nodes ~# kubectl patch node <NODE_NAME> -p '{\"spec\":{\"providerID\": \"<provider_id>\"}}' # Replace <NODE_NAME> and <provider_id> with corresponding values. Create an Alibaba Cloud RAM user and grant authorization. A RAM user is an entity within Alibaba Cloud's Resource Access Management (RAM) that represents individuals or applications requiring access to Alibaba Cloud resources. Refer to Overview of RAM users to create a RAM user and assign the necessary permissions for accessing resources. To ensure that the RAM user used in the subsequent steps has sufficient privileges, grant the AdministratorAccess and AliyunSLBFullAccess permissions to the RAM user, following the instructions provided here. Obtain the AccessKey & AccessKeySecret for the RAM user. Log in to the RAM User account and go to User Center to retrieve the corresponding AccessKey & AccessKeySecret for the RAM User. Create the Cloud ConfigMap for CCM. Use the following method to write the AccessKey & AccessKeySecret obtained in step 3 as environment variables. ~# export ACCESS_KEY_ID = LTAI******************** ~# export ACCESS_KEY_SECRET = HAeS************************** Run the following command to create cloud-config: accessKeyIDBase64 = ` echo -n \" $ACCESS_KEY_ID \" | base64 -w 0 ` accessKeySecretBase64 = ` echo -n \" $ACCESS_KEY_SECRET \" | base64 -w 0 ` cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cloud-config namespace: kube-system data: cloud-config.conf: |- { \"Global\": { \"accessKeyID\": \"$accessKeyIDBase64\", \"accessKeySecret\": \"$accessKeySecretBase64\" } } EOF Retrieve the YAML file and install CCM by running the command kubectl apply -f cloud-controller-manager.yaml . The version of CCM being installed here is v2.5.0. Use the following command to obtain the cloud-controller-manager.yaml file and replace <<cluster_cidr>> with the actual cluster CIDR; You can view the cluster CIDR of the cluster through the kubectl cluster-info dump | grep -m1 cluster-cidr command. ~# wget https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/alicloud-ccm/cloud-controller-manager.yaml ~# kubectl apply -f cloud-controller-manager.yaml Verify if CCM is installed. ~# kubectl get po -n kube-system | grep cloud-controller-manager NAME READY STATUS RESTARTS AGE cloud-controller-manager-72vzr 1 /1 Running 0 27s cloud-controller-manager-k7jpn 1 /1 Running 0 27s","title":"Deploy Cloud Controller Manager"},{"location":"usage/install/cloud/get-started-alibaba/#create-load-balancer-ingress-for-applications","text":"The following YAML will create two sets of services, one for TCP (layer 4 load balancing) and one for HTTP (layer 7 load balancing), with spec.type set to LoadBalancer . service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port : this annotation provided by CCM allows you to customize the exposed ports for layer 7 load balancing. For more information, refer to the CCM Usage Documentation . .spec.externalTrafficPolicy : indicates whether the service prefers to route external traffic to local or cluster-wide endpoints. It has two options: Cluster (default) and Local. Setting .spec.externalTrafficPolicy to Local preserves the client source IP. However, when a self-built public cloud cluster uses the platform's Loadbalancer component for nodePort forwarding in this mode, access will be blocked. In response to this problem, Spiderpool provides the coordinator plug-in, which uses iptables to mark the data packets to confirm that the reply packets of data entering from veth0 are still forwarded from veth0, thus solving the problem of nodeport being unable to access in this mode. ~# cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-service namespace: default spec: externalTrafficPolicy: Local ports: - name: tcp port: 999 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer --- apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-protocol-port: \"http:80\" name: http-service namespace: default spec: externalTrafficPolicy: Local ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app-2 type: LoadBalancer EOF After the creation is complete, you can view the following: ~# kubectl get svc | grep service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE http-service LoadBalancer 10 .233.1.108 121 .41.165.119 80 :30698/TCP 11s tcp-service LoadBalancer 10 .233.4.245 47 .98.137.75 999 :32635/TCP 15s CCM will automatically create layer 4 and layer 7 load balancers at its IaaS services. You can easily access and manage them through the Alibaba Cloud console, as shown below:","title":"Create Load Balancer Ingress for Applications"},{"location":"usage/install/cloud/get-started-alibaba/#verify-load-balancer-traffic-ingress-access","text":"On a public machine, access the load balancer's public IP + port to test the traffic ingress: # Access layer 4 load balancing $ curl 47 .98.137.75:999 -I HTTP/1.1 200 OK Server: nginx/1.25.1 Date: Sun, 30 Jul 2023 09 :12:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT Connection: keep-alive ETag: \"6488865a-267\" Accept-Ranges: bytes # Access layer 7 load balancing $ curl 121 .41.165.119:80 -I HTTP/1.1 200 OK Date: Sun, 30 Jul 2023 09 :13:17 GMT Content-Type: text/html Content-Length: 615 Connection: keep-alive Last-Modified: Tue, 13 Jun 2023 15 :08:10 GMT ETag: \"6488865a-267\" Accept-Ranges: bytes Alibaba Cloud's CCM implements ingress access for load balancing traffic, and it does not support setting the spec.ipFamilies of the backend service to IPv6. ~# kubectl describe svc lb-ipv6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SyncLoadBalancerFailed 3m5s ( x37 over 159m ) nlb-controller Error syncing load balancer [ nlb-rddqbe6gnp9jil4i15 ] : Message: code: 400 , The operation is not allowed because of ServerGroupNotSupportIpv6.","title":"Verify Load Balancer Traffic Ingress Access"},{"location":"usage/install/cloud/get-started-alibaba/#summary","text":"Spiderpool is successfully running in an Alibaba Cloud cluster, ensuring normal east-west and north-south traffic.","title":"Summary"},{"location":"usage/install/cloud/get-started-aws-zh_CN/","text":"AWS \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English \u4ecb\u7ecd \u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002 \u9879\u76ee\u529f\u80fd Spiderpool \u80fd\u57fa\u4e8e ipvlan Underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u4e0a\uff0c\u5e76\u5b9e\u73b0\u6709\u8282\u70b9\u62d3\u6251\u3001\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001ipvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d ipvlan CNI \u8fdb\u884c\u89e3\u51b3\u3002ipvlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 ipvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002 \u5b9e\u65bd\u8981\u6c42 \u4f7f\u7528 ipvlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u4e86\u89e3 AWS VPC \u516c\u6709 & \u79c1\u6709\u5b50\u7f51 \u57fa\u7840\u77e5\u8bc6\u3002 \u5728 AWS VPC \u4e0b\u521b\u5efa\u7684\u5b50\u7f51\uff0c\u5982\u679c\u8bbe\u7f6e\u4e86\u51fa\u53e3\u8def\u7531 0.0.0.0/0, ::/0 \u7684\u4e0b\u4e00\u8df3\u4e3a Internet Gateway\uff0c\u5219\u8be5\u5b50\u7f51\u5c31\u96b6\u5c5e\u4e8e \u516c\u6709\u5b50\u7f51 \uff0c\u5426\u5219\u5c31\u662f \u79c1\u6709\u5b50\u7f51 \u3002 \u6b65\u9aa4 AWS \u73af\u5883 \u5728 VPC \u4e0b\u521b\u5efa\u516c\u6709\u5b50\u7f51\u4ee5\u53ca\u79c1\u6709\u5b50\u7f51\uff0c\u5e76\u5728\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u865a\u62df\u673a\uff0c\u5982\u56fe\uff1a \u672c\u4f8b\u4f1a\u5728\u540c\u4e00\u4e2a VPC \u4e0b\u5148\u521b\u5efa 1 \u4e2a\u516c\u6709\u5b50\u7f51\u4ee5\u53ca 2 \u4e2a\u79c1\u6709\u5b50\u7f51(\u8bf7\u5c06\u5b50\u7f51\u90e8\u7f72\u5728\u4e0d\u540c\u7684\u53ef\u7528\u533a)\uff0c\u63a5\u7740\u4f1a\u5728\u516c\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u4e00\u4e2a AWS EC2 \u5b9e\u4f8b\u4f5c\u4e3a\u8df3\u677f\u673a\uff0c\u7136\u540e\u4f1a\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u5bf9\u5e94\u7684 AWS EC2 \u5b9e\u4f8b\u7528\u4e8e\u90e8\u7f72 Kubernetes \u96c6\u7fa4\u3002 \u989d\u5916\u521b\u5efa\u4e24\u4e2a\u79c1\u6709\u5b50\u7f51\u7528\u4e8e\u7ed9\u5b9e\u4f8b\u8865\u5145\u7b2c\u4e8c\u5f20\u7f51\u5361(\u8bf7\u5c06\u5b50\u7f51\u90e8\u7f72\u5728\u4e0e\u5b9e\u4f8b\u76f8\u540c\u7684\u53ef\u7528\u533a)\uff0c\u5982\u56fe\uff1a \u7ed9\u5b9e\u4f8b\u4eec\u7684\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe: \u56e0\u4e3a\u6839\u636e AWS EC2 \u5b9e\u4f8b\u89c4\u683c \uff0c\u5b9e\u4f8b\u7684\u7f51\u5361\u6570\u91cf\u4ee5\u53ca\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u53ef\u7ed1\u5b9a\u7684\u8f85\u52a9 IP \u6709\u9650\u5236\uff0c\u4e3a\u4e86\u80fd\u591f\u5c3d\u53ef\u80fd\u7684\u5145\u5206\u5229\u7528\u5b9e\u4f8b\u8d44\u6e90\u6765\u90e8\u7f72\u5e94\u7528\uff0c\u6211\u4eec\u56e0\u6b64\u9009\u62e9\u7ed9\u5b9e\u4f8b\u7ed1\u5b9a2\u5f20\u7f51\u5361\u4ee5\u53ca\u5bf9\u5e94\u7684\u8f85\u52a9 IP\u3002 | Node | ens5 primary IP | ens5 secondary IPs | ens6 primary IP | ens6 secondary IPs | | --------- | ----------------- | --------------------------- | ----------------- | --------------------------- | | master | 172 .31.22.228 | 172 .31.16.4-172.31.16.8 | 210 .22.16.10 | 210 .22.16.11-210.22.16.15 | | worker1 | 180 .17.16.17 | 180 .17.16.11-180.17.16.15 | 210 .22.32.10 | 210 .22.32.11-210.22.32.15 | \u521b\u5efa AWS NAT \u7f51\u5173\uff0cAWS \u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u79c1\u6709\u5b50\u7f51\u4e2d\u7684\u5b9e\u4f8b\u8fde\u63a5\u5230 VPC \u5916\u90e8\u7684\u670d\u52a1\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u5728\u4e0a\u8ff0\u7684\u516c\u6709\u5b50\u7f51 public-172-31-0-0 \u4e0b\u521b\u5efa NAT \u7f51\u5173\uff0c\u5e76\u4e3a\u79c1\u6709\u5b50\u7f51\u7684\u8def\u7531\u8868\u914d\u7f6e 0.0.0.0/0 \u51fa\u53e3\u8def\u7531\u7684\u4e0b\u4e00\u8df3\u4e3a\u8be5 NAT \u7f51\u5173\u3002(\u6ce8\u610f IPv6 \u662f\u7531 AWS \u5206\u914d\u7684\u5168\u5c40\u552f\u4e00\u7684\u5730\u5740\uff0c\u53ef\u76f4\u63a5\u501f\u52a9 Internet Gateway \u8bbf\u95ee\u4e92\u8054\u7f51) \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u5b89\u88c5 Spiderpool \u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"default/ipvlan-ens5\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5b89\u88c5 CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u6839\u636e\u524d\u9762\u521b\u5efa AWS EC2 \u5b9e\u4f8b\u865a\u62df\u673a\u8fc7\u7a0b\u4e2d\u521b\u5efa\u7684\u7f51\u5361\u60c5\u51b5\uff0c\u4e3a\u865a\u62df\u673a\u7684\u6bcf\u4e2a\u7528\u4e8e\u8fd0\u884c ipvlan CNI \u7684\u7f51\u5361\u521b\u5efa\u5982\u4e0b SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"ens5\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"ens6\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a ipvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth5 \u4e0e eth6 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d \u521b\u5efa IP \u6c60 Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u8be5\u5b57\u6bb5\u9650\u5236\u5f53\u524d SpiderIPPool\u8d44\u6e90\u4ec5\u9002\u7528\u4e8e\u54ea\u4e9b\u8282\u70b9\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53\u8be5\u5b57\u6bb5\u4e3a\u7a7a\u65f6\uff0c\u8868\u660e\u5f53\u524d Spiderpool \u8d44\u6e90\u9002\u7528\u4e8e\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u6839\u636e\u4e0a\u6587 AWS EC2 \u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca\u8f85\u52a9 IP \u5730\u5740\u7b49\u4fe1\u606f\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u4e14\u5bf9\u5e94\u552f\u4e00\u7684\u4e00\u5f20\u5b9e\u4f8b\u7f51\u5361\u3002 \u7ed3\u5408\u4e0a\u6587 AWS \u73af\u5883 \u6bcf\u53f0\u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca\u5bf9\u5e94\u7684\u8f85\u52a9 IP \u4fe1\u606f\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( ens5\u3001ens6) \u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens5 spec: subnet: 172.31.16.0/20 ips: - 172.31.16.4-172.31.16.8 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens6 spec: subnet: 210.22.16.0/24 ips: - 210.22.16.11-210.22.16.15 gateway: 210.22.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens6\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens5 spec: subnet: 180.17.16.0/24 ips: - 180.17.16.11-180.17.16.15 gateway: 180.17.16.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens6 spec: subnet: 210.22.32.0/24 ips: - 210.22.32.11-210.22.32.15 gateway: 210.22.32.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens6\"] EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u7684 CNI \u914d\u7f6e\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u9009\u62e9\u4f7f\u7528\u5bf9\u5e94\u4e8e\u5bbf\u4e3b\u673a ens5 \u7684 ipvlan \u914d\u7f6e\uff0c\u5e76\u6839\u636e\u6211\u4eec\u7684\u7f3a\u7701 SpiderIPPool \u8d44\u6e90\u9ed8\u8ba4\u6311\u9009\u5176\u5bf9\u5e94\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb-1 spec: selector: matchLabels: run: nginx-lb-1 replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens5\" labels: run: nginx-lb-1 spec: containers: - name: nginx-lb-1 image: nginx ports: - containerPort: 80 EOF \u67e5\u770b Pod \u7684\u8fd0\u884c\u72b6\u6001\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0c\u6211\u4eec\u4e24\u4e2a\u8282\u70b9\u4e0a\u90fd\u8fd0\u884c\u4e86 1 \u4e2a Pod \u4e14\u4f7f\u7528\u7684 IP \u90fd\u5bf9\u5e94\u5bbf\u4e3b\u673a\u7684\u7b2c\u4e00\u5f20\u7f51\u5361\u7684\u8f85\u52a9 IP: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-1-55d4c48fc8-skrxh 1 /1 Running 0 5s 172 .31.16.5 master <none> <none> nginx-lb-1-55d4c48fc8-jl8b9 1 /1 Running 0 5s 180 .17.16.14 worker1 <none> <none> \u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027 \u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a export NODE_MASTER_IP=172.31.22.228 export NODE_WORKER1_IP= 180.17.16.17 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${NODE_MASTER_IP} -c 1 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-jl8b9 -- ping ${NODE_WORKER1_IP} -c 1 \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping 180.17.16.14 -c 1 \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${CLUSTER_IP} -c 1 \u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027 \u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee \u501f\u52a9\u4e0a\u6587\u6211\u4eec\u521b\u5efa\u7684 AWS NAT \u7f51\u5173 \uff0c\u6211\u4eec\u7684 VPC \u79c1\u7f51\u5df2\u53ef\u5b9e\u73b0\u8bbf\u95ee\u4e92\u8054\u7f51\u3002 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- curl www.baidu.com -I \u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee(\u53ef\u9009) \u90e8\u7f72 AWS Load Balancer Controller AWS \u57fa\u7840\u4ea7\u54c1 \u8d1f\u8f7d\u5747\u8861 \u62e5\u6709 NLB (Network Load Balancer) \u548c ALB(Application Load Balancer) \u4e24\u79cd\u6a21\u5f0f\u5206\u522b\u5bf9\u5e94 Layer4 \u4e0e Layer7\u3002 aws-load-balancer-controller \u662f AWS \u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e AWS \u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u53ef\u5b9e\u73b0 kubernetes Service LoadBalancer \u548c Ingress \u529f\u80fd\u3002\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408 AWS \u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5\u6f14\u793a\uff0c \u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e aws-load-balancer-controller \u6587\u6863 \u5b8c\u6210 aws-load-balancer-controller \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u4e3a Kubernetes \u4e0a\u7684\u6bcf\u4e2a Node \u8bbe\u7f6e\u4e0a providerID \uff0c\u60a8\u53ef\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0: - \u53ef\u76f4\u63a5\u5728 AWS EC2 dashboard \u4e2d\u627e\u5230\u5b9e\u4f8b\u7684Instance ID. - \u4f7f\u7528 AWS CLI \u6765\u67e5\u8be2Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . \u4e3a AWS EC2 \u5b9e\u4f8b\u6240\u4f7f\u7528\u7684 IAM role \u8865\u5145 policy \u4ecb\u4e8e aws-load-balancer-controller \u8fd0\u884c\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u4e14\u9700\u8981\u8bbf\u95ee AWS \u7684 NLB/ALB APIs\uff0c\u56e0\u6b64\u9700\u8981 AWS IAM \u5173\u4e8e NLB/ALB \u76f8\u5173\u8bf7\u6c42\u7684\u6388\u6743\u3002\u53c8\u56e0\u6211\u4eec\u662f\u81ea\u5efa\u96c6\u7fa4\uff0c\u6211\u4eec\u9700\u8981\u501f\u7528\u8282\u70b9\u81ea\u8eab\u7684 IAM Role \u6765\u5b9e\u73b0\u6388\u6743\uff0c\u8be6\u60c5\u53ef\u770b aws-load-balancer-controller IAM \u3002 curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json \u4f7f\u7528\u5982\u4e0a\u83b7\u53d6\u7684 json \u5185\u5bb9\uff0c\u5728 AWS IAM Dashboard \u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684policy\uff0c\u5e76\u5c06\u8be5 policy \u4e0e\u60a8\u5f53\u524d\u865a\u62df\u673a\u5b9e\u4f8b\u7684 IAM Role \u8fdb\u884c\u5173\u8054\u3002 \u4e3a\u60a8 AWS EC2 \u5b9e\u4f8b\u6240\u5728\u7684\u53ef\u7528\u533a\u521b\u5efa\u4e00\u4e2a public subnet \u5e76\u6253\u4e0a\u53ef\u81ea\u52a8\u53d1\u73b0\u7684 tag. ALB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 2 \u4e2a\u8de8\u53ef\u7528\u533a\u7684\u5b50\u7f51\uff0c\u5bf9\u4e8e NLB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 1 \u4e2a\u5b50\u7f51\u3002\u8be6\u60c5\u8bf7\u770b \u5b50\u7f51\u81ea\u52a8\u53d1\u73b0 \u3002 \u5bf9\u4e8e\u516c\u7f51\u8bbf\u95ee\u7684 LB\uff0c\u60a8\u9700\u8981\u4e3a\u5b9e\u4f8b\u6240\u5728\u53ef\u7528\u533a\u7684 public subnet \u6253\u4e0a tag: kubernetes.io/role/elb:1 \uff0c\u5bf9\u4e8e VPC \u95f4\u8bbf\u95ee\u7684 LB\uff0c\u8bf7\u521b\u5efa private subnet \u5e76\u6253\u4e0a tag: kubernetes.io/role/internal-elb:1 \uff0c\u8bf7\u7ed3\u5408 AWS \u73af\u5883 \u6765\u521b\u5efa\u6240\u9700\u7684\u5b50\u7f51\uff1a \u9488\u5bf9\u56e0\u7279\u7f51\u66b4\u9732\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa public subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u6211\u4eec\u7684 Main \u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002(\u6ce8\u610f Main \u8def\u7531\u8868\u7684 0.0.0.0/0 \u8def\u7531\u7684\u4e0b\u4e00\u8df3\u9ed8\u8ba4\u4e3a Internet \u7f51\u5173\uff0c\u82e5\u4e22\u5931\u8bf7\u81ea\u884c\u521b\u5efa\u8be5\u8def\u7531\u89c4\u5219)\u3002 \u5728 AWS VPC Dashboard Route tables \u680f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8def\u7531\u8868\u5e76\u914d\u7f6e 0.0.0.0/0 \u7684\u8def\u7531\u4e0b\u4e00\u8df3\u4e3a NAT \u7f51\u5173\uff0c::/0 \u8def\u7531\u4e0b\u4e00\u8df3\u4e3a Internet \u7f51\u5173\u3002 \u9488\u5bf9 VPC \u95f4\u8bbf\u95ee\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa private subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002 \u4f7f\u7528 helm \u5b89\u88c5aws-load-balancer-controller(\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5) helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> \u68c0\u67e5 aws-load-balancer-controller \u5b89\u88c5\u5b8c\u6210 ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s \u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3 \u4e0a\u6587\u4e2d\u5df2\u521b\u5efa \u5e94\u7528 , \u73b0\u5728\u6211\u4eec\u4e3a\u5b83\u521b\u5efa\u4e00\u4e2a kubernetes Service LoadBalancer \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack \u6ce8\u89e3): cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb-1 labels: run: nginx-lb-1 annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb-1 EOF \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a NLB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 NLB \u8fd8\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/load-balancer-source-ranges \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90 IP\u3002\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 \u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-scheme \u9009\u62e9\u6b64 NLB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true \u63d0\u4f9b\u4e86\u5ba2\u6237\u7aef\u6e90 IP \u4fdd\u7559\u529f\u80fd\u3002 \u4e3a\u5e94\u7528\u521b\u5efa Ingress \u8bbf\u95ee\u5165\u53e3 \u63a5\u4e0b\u6765\u6211\u4eec\u901a\u8fc7 AWS EC2 \u4e2d\u7ed1\u5b9a\u7684\u7b2c\u4e8c\u5f20\u7f51\u5361\u6765\u521b\u5efa\u4e00\u4e2akubernetes Ingress \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 alb.ingress.kubernetes.io/ip-address-type: dualstack \u6ce8\u89e3): apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a ALB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 ALB \u4e5f\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 alb.ingress.kubernetes.io/target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u4f7f\u7528 ALB \u7684 instance \u6a21\u5f0f\u9700\u8981\u6307\u5b9a service \u4e3a NodePort \u6a21\u5f0f\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/inbound-cidrs \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90IP\u3002(\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 alb.ingress.kubernetes.io/ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 )\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/scheme \u9009\u62e9\u6b64 ALB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u82e5\u60f3\u6574\u5408\u591a\u4e2a Ingress \u8d44\u6e90\u5171\u4eab\u540c\u4e00\u4e2a\u5165\u53e3\uff0c\u53ef\u914d\u7f6e\u6ce8\u89e3 alb.ingress.kubernetes.io/group.name \u6765\u663e\u793a\u6307\u5b9a\u4e00\u4e2a\u540d\u5b57\u3002\uff08\u6ce8\u610f\uff0c\u9ed8\u8ba4\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u7684 Ingresses \u8d44\u6e90\u5e76\u4e0d\u5c5e\u4e8e\u4efb\u4f55 IngressGroup\uff0c\u7cfb\u7edf\u4f1a\u5c06\u5176\u89c6\u4e3a\u7531 Ingress \u672c\u8eab\u7ec4\u6210\u7684 \"\u9690\u5f0f IngressGroup\"\uff09 \u5982\u679c\u60f3\u6307\u5b9a Ingress \u7684 host\uff0c\u9700\u8981\u642d\u914d externalDNS \u4f7f\u7528\u3002\u8be6\u60c5\u8bf7\u67e5\u770b \u914d\u7f6e externalDNS \u3002","title":"AWS \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#aws","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"AWS \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_1","text":"\u5f53\u524d\u516c\u6709\u4e91\u5382\u5546\u4f17\u591a\uff0c\u5982\uff1a\u963f\u91cc\u4e91\u3001\u534e\u4e3a\u4e91\u3001\u817e\u8baf\u4e91\u3001AWS \u7b49\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u793e\u533a\u7684\u4e3b\u6d41 CNI \u63d2\u4ef6\u96be\u4ee5\u4ee5 Underlay \u7f51\u7edc\u65b9\u5f0f\u8fd0\u884c\u5176\u4e0a\uff0c\u53ea\u80fd\u4f7f\u7528\u6bcf\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u4e13\u6709 CNI \u63d2\u4ef6\uff0c\u6ca1\u6709\u7edf\u4e00\u7684\u516c\u6709\u4e91 Underlay \u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u5c06\u4ecb\u7ecd\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u7684\u516c\u6709\u4e91\u73af\u5883\u4e2d\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff1a Spiderpool \uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4e91\u573a\u666f\u4e0b\uff0c\u7edf\u4e00\u7684 CNI \u65b9\u6848\u80fd\u591f\u4fbf\u4e8e\u591a\u4e91\u7ba1\u7406\u3002","title":"\u4ecb\u7ecd"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_2","text":"Spiderpool \u80fd\u57fa\u4e8e ipvlan Underlay CNI \u8fd0\u884c\u5728\u516c\u6709\u4e91\u73af\u5883\u4e0a\uff0c\u5e76\u5b9e\u73b0\u6709\u8282\u70b9\u62d3\u6251\u3001\u89e3\u51b3 MAC \u5730\u5740\u5408\u6cd5\u6027\u7b49\u529f\u80fd\uff0c\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\uff1a \u516c\u6709\u4e91\u4e0b\u4f7f\u7528 Underlay \u7f51\u7edc\uff0c\u4f46\u516c\u6709\u4e91\u7684\u6bcf\u4e2a\u4e91\u670d\u52a1\u5668\u7684\u6bcf\u5f20\u7f51\u5361\u53ea\u80fd\u5206\u914d\u6709\u9650\u7684 IP \u5730\u5740\uff0c\u5f53\u5e94\u7528\u8fd0\u884c\u5728\u67d0\u4e2a\u4e91\u670d\u52a1\u5668\u4e0a\u65f6\uff0c\u9700\u8981\u540c\u6b65\u83b7\u53d6\u5230 VPC \u7f51\u7edc\u4e2d\u5206\u914d\u7ed9\u8be5\u4e91\u670d\u52a1\u5668\u4e0d\u540c\u7f51\u5361\u7684\u5408\u6cd5 IP \u5730\u5740\uff0c\u624d\u80fd\u5b9e\u73b0\u901a\u4fe1\u3002\u6839\u636e\u4e0a\u8ff0\u5206\u914d IP \u7684\u7279\u70b9\uff0cSpiderpool \u7684 CRD\uff1a SpiderIPPool \u53ef\u4ee5\u8bbe\u7f6e nodeName\uff0cmultusName \u5b9e\u73b0\u8282\u70b9\u62d3\u6251\u7684\u529f\u80fd\uff0c\u901a\u8fc7 IP \u6c60\u4e0e\u8282\u70b9\u3001ipvlan Multus \u914d\u7f6e\u7684\u4eb2\u548c\u6027\uff0c\u80fd\u6700\u5927\u5316\u7684\u5229\u7528\u4e0e\u7ba1\u7406\u8282\u70b9\u53ef\u7528\u7684 IP \u5730\u5740\uff0c\u7ed9\u5e94\u7528\u5206\u914d\u5230\u5408\u6cd5\u7684 IP \u5730\u5740\uff0c\u8ba9\u5e94\u7528\u5728 VPC \u7f51\u7edc\u5185\u81ea\u7531\u901a\u4fe1\uff0c\u5305\u62ec Pod \u4e0e Pod \u901a\u4fe1\uff0cPod \u4e0e\u4e91\u670d\u52a1\u5668\u901a\u4fe1\u7b49\u3002 \u516c\u6709\u4e91\u7684 VPC \u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u7f51\u7edc\u5b89\u5168\u7ba1\u63a7\u548c\u6570\u636e\u5305\u8f6c\u53d1\u7684\u539f\u7406\uff0c\u5f53\u7f51\u7edc\u6570\u636e\u62a5\u6587\u4e2d\u51fa\u73b0 VPC \u7f51\u7edc\u672a\u77e5\u7684 MAC \u548c IP \u5730\u5740\u65f6\uff0c\u5b83\u65e0\u6cd5\u5f97\u5230\u6b63\u786e\u7684\u8f6c\u53d1\u3002\u4f8b\u5982\uff0c\u57fa\u4e8e Macvlan \u548c OVS \u539f\u7406\u7684 Underlay CNI \u63d2\u4ef6\uff0cPod \u7f51\u5361\u4e2d\u7684 MAC \u5730\u5740\u662f\u65b0\u751f\u6210\u7684\uff0c\u4f1a\u5bfc\u81f4 Pod \u65e0\u6cd5\u901a\u4fe1\u3002\u9488\u5bf9\u8be5\u95ee\u9898\uff0cSpiderpool \u53ef\u642d\u914d ipvlan CNI \u8fdb\u884c\u89e3\u51b3\u3002ipvlan \u57fa\u4e8e\u4e09\u5c42\u7f51\u7edc\uff0c\u65e0\u9700\u4f9d\u8d56\u4e8c\u5c42\u5e7f\u64ad\uff0c\u5e76\u4e14\u4e0d\u4f1a\u91cd\u65b0\u751f\u6210 Mac \u5730\u5740\uff0c\u4e0e\u7236\u63a5\u53e3\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u901a\u8fc7 ipvlan \u53ef\u4ee5\u89e3\u51b3\u516c\u6709\u4e91\u4e2d\u5173\u4e8e MAC \u5730\u5740\u5408\u6cd5\u6027\u7684\u95ee\u9898\u3002","title":"\u9879\u76ee\u529f\u80fd"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_3","text":"\u4f7f\u7528 ipvlan \u505a\u96c6\u7fa4 CNI \u65f6\uff0c\u7cfb\u7edf\u5185\u6838\u7248\u672c\u5fc5\u987b\u5927\u4e8e 4.2\u3002 \u5df2\u5b89\u88c5 Helm \u3002 \u4e86\u89e3 AWS VPC \u516c\u6709 & \u79c1\u6709\u5b50\u7f51 \u57fa\u7840\u77e5\u8bc6\u3002 \u5728 AWS VPC \u4e0b\u521b\u5efa\u7684\u5b50\u7f51\uff0c\u5982\u679c\u8bbe\u7f6e\u4e86\u51fa\u53e3\u8def\u7531 0.0.0.0/0, ::/0 \u7684\u4e0b\u4e00\u8df3\u4e3a Internet Gateway\uff0c\u5219\u8be5\u5b50\u7f51\u5c31\u96b6\u5c5e\u4e8e \u516c\u6709\u5b50\u7f51 \uff0c\u5426\u5219\u5c31\u662f \u79c1\u6709\u5b50\u7f51 \u3002","title":"\u5b9e\u65bd\u8981\u6c42"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_4","text":"","title":"\u6b65\u9aa4"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#aws_1","text":"\u5728 VPC \u4e0b\u521b\u5efa\u516c\u6709\u5b50\u7f51\u4ee5\u53ca\u79c1\u6709\u5b50\u7f51\uff0c\u5e76\u5728\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u865a\u62df\u673a\uff0c\u5982\u56fe\uff1a \u672c\u4f8b\u4f1a\u5728\u540c\u4e00\u4e2a VPC \u4e0b\u5148\u521b\u5efa 1 \u4e2a\u516c\u6709\u5b50\u7f51\u4ee5\u53ca 2 \u4e2a\u79c1\u6709\u5b50\u7f51(\u8bf7\u5c06\u5b50\u7f51\u90e8\u7f72\u5728\u4e0d\u540c\u7684\u53ef\u7528\u533a)\uff0c\u63a5\u7740\u4f1a\u5728\u516c\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u4e00\u4e2a AWS EC2 \u5b9e\u4f8b\u4f5c\u4e3a\u8df3\u677f\u673a\uff0c\u7136\u540e\u4f1a\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u79c1\u6709\u5b50\u7f51\u4e0b\u521b\u5efa\u5bf9\u5e94\u7684 AWS EC2 \u5b9e\u4f8b\u7528\u4e8e\u90e8\u7f72 Kubernetes \u96c6\u7fa4\u3002 \u989d\u5916\u521b\u5efa\u4e24\u4e2a\u79c1\u6709\u5b50\u7f51\u7528\u4e8e\u7ed9\u5b9e\u4f8b\u8865\u5145\u7b2c\u4e8c\u5f20\u7f51\u5361(\u8bf7\u5c06\u5b50\u7f51\u90e8\u7f72\u5728\u4e0e\u5b9e\u4f8b\u76f8\u540c\u7684\u53ef\u7528\u533a)\uff0c\u5982\u56fe\uff1a \u7ed9\u5b9e\u4f8b\u4eec\u7684\u6bcf\u5f20\u7f51\u5361\u5747\u5206\u914d\u4e00\u4e9b\u8f85\u52a9\u79c1\u7f51 IP\uff0c\u5982\u56fe: \u56e0\u4e3a\u6839\u636e AWS EC2 \u5b9e\u4f8b\u89c4\u683c \uff0c\u5b9e\u4f8b\u7684\u7f51\u5361\u6570\u91cf\u4ee5\u53ca\u6bcf\u5f20\u7f51\u5361\u5bf9\u5e94\u53ef\u7ed1\u5b9a\u7684\u8f85\u52a9 IP \u6709\u9650\u5236\uff0c\u4e3a\u4e86\u80fd\u591f\u5c3d\u53ef\u80fd\u7684\u5145\u5206\u5229\u7528\u5b9e\u4f8b\u8d44\u6e90\u6765\u90e8\u7f72\u5e94\u7528\uff0c\u6211\u4eec\u56e0\u6b64\u9009\u62e9\u7ed9\u5b9e\u4f8b\u7ed1\u5b9a2\u5f20\u7f51\u5361\u4ee5\u53ca\u5bf9\u5e94\u7684\u8f85\u52a9 IP\u3002 | Node | ens5 primary IP | ens5 secondary IPs | ens6 primary IP | ens6 secondary IPs | | --------- | ----------------- | --------------------------- | ----------------- | --------------------------- | | master | 172 .31.22.228 | 172 .31.16.4-172.31.16.8 | 210 .22.16.10 | 210 .22.16.11-210.22.16.15 | | worker1 | 180 .17.16.17 | 180 .17.16.11-180.17.16.15 | 210 .22.32.10 | 210 .22.32.11-210.22.32.15 | \u521b\u5efa AWS NAT \u7f51\u5173\uff0cAWS \u7684 NAT \u7f51\u5173\u80fd\u5b9e\u73b0\u4e3a VPC \u79c1\u6709\u5b50\u7f51\u4e2d\u7684\u5b9e\u4f8b\u8fde\u63a5\u5230 VPC \u5916\u90e8\u7684\u670d\u52a1\u3002\u901a\u8fc7 NAT \u7f51\u5173\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7684\u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee\u3002\u53c2\u8003 NAT \u7f51\u5173\u6587\u6863 \u521b\u5efa NAT \u7f51\u5173\uff0c\u5982\u56fe\uff1a \u5728\u4e0a\u8ff0\u7684\u516c\u6709\u5b50\u7f51 public-172-31-0-0 \u4e0b\u521b\u5efa NAT \u7f51\u5173\uff0c\u5e76\u4e3a\u79c1\u6709\u5b50\u7f51\u7684\u8def\u7531\u8868\u914d\u7f6e 0.0.0.0/0 \u51fa\u53e3\u8def\u7531\u7684\u4e0b\u4e00\u8df3\u4e3a\u8be5 NAT \u7f51\u5173\u3002(\u6ce8\u610f IPv6 \u662f\u7531 AWS \u5206\u914d\u7684\u5168\u5c40\u552f\u4e00\u7684\u5730\u5740\uff0c\u53ef\u76f4\u63a5\u501f\u52a9 Internet Gateway \u8bbf\u95ee\u4e92\u8054\u7f51) \u4f7f\u7528\u4e0a\u8ff0\u914d\u7f6e\u7684\u865a\u62df\u673a\uff0c\u642d\u5efa\u4e00\u5957 Kubernetes \u96c6\u7fa4\uff0c\u8282\u70b9\u7684\u7684\u53ef\u7528 IP \u53ca\u96c6\u7fa4\u7f51\u7edc\u62d3\u6251\u56fe\u5982\u4e0b\uff1a","title":"AWS \u73af\u5883"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#spiderpool","text":"\u901a\u8fc7 helm \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"default/ipvlan-ens5\" \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u4e2d\u56fd\u5927\u9646\u7684\u4e91\u5382\u5546\u670d\u52a1\u5668\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 Spiderpool \u53ef\u4ee5\u4e3a\u63a7\u5236\u5668\u7c7b\u578b\u4e3a\uff1a Statefulset \u7684\u5e94\u7528\u526f\u672c\u56fa\u5b9a IP \u5730\u5740\u3002\u5728\u516c\u6709\u4e91\u7684 Underlay \u7f51\u7edc\u573a\u666f\u4e2d\uff0c\u4e91\u4e3b\u673a\u53ea\u80fd\u4f7f\u7528\u9650\u5b9a\u7684 IP \u5730\u5740\uff0c\u5f53 StatefulSet \u7c7b\u578b\u7684\u5e94\u7528\u526f\u672c\u6f02\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u4f46\u7531\u4e8e\u539f\u56fa\u5b9a\u7684 IP \u5728\u5176\u4ed6\u8282\u70b9\u662f\u975e\u6cd5\u4e0d\u53ef\u7528\u7684\uff0c\u65b0\u7684 Pod \u5c06\u51fa\u73b0\u7f51\u7edc\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002\u5bf9\u6b64\u573a\u666f\uff0c\u5c06 ipam.enableStatefulSet \u8bbe\u7f6e\u4e3a false \uff0c\u7981\u7528\u8be5\u529f\u80fd\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u6839\u636e\u524d\u9762\u521b\u5efa AWS EC2 \u5b9e\u4f8b\u865a\u62df\u673a\u8fc7\u7a0b\u4e2d\u521b\u5efa\u7684\u7f51\u5361\u60c5\u51b5\uff0c\u4e3a\u865a\u62df\u673a\u7684\u6bcf\u4e2a\u7528\u4e8e\u8fd0\u884c ipvlan CNI \u7684\u7f51\u5361\u521b\u5efa\u5982\u4e0b SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a IPVLAN_MASTER_INTERFACE0 = \"ens5\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"ens6\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684\u4e24\u4e2a ipvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u4eec\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth5 \u4e0e eth6 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d","title":"\u5b89\u88c5 CNI \u914d\u7f6e"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#ip","text":"Spiderpool \u7684 CRD\uff1a SpiderIPPool \u63d0\u4f9b\u4e86 nodeName \u3001 multusName \u4e0e ips \u5b57\u6bb5\uff1a nodeName \uff1a\u8be5\u5b57\u6bb5\u9650\u5236\u5f53\u524d SpiderIPPool\u8d44\u6e90\u4ec5\u9002\u7528\u4e8e\u54ea\u4e9b\u8282\u70b9\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u7b26\u5408\u8be5 nodeName \uff0c\u5219\u80fd\u4ece\u8be5 SpiderIPPool \u4e2d\u6210\u529f\u5206\u914d\u51fa IP\uff0c\u82e5 Pod \u6240\u5728\u8282\u70b9\u4e0d\u7b26\u5408 nodeName \uff0c\u5219\u65e0\u6cd5\u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d\u51fa IP\u3002\u5f53\u8be5\u5b57\u6bb5\u4e3a\u7a7a\u65f6\uff0c\u8868\u660e\u5f53\u524d Spiderpool \u8d44\u6e90\u9002\u7528\u4e8e\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\u3002 multusName \uff1aSpiderpool \u901a\u8fc7\u8be5\u5b57\u6bb5\u4e0e Multus CNI \u6df1\u5ea6\u7ed3\u5408\u4ee5\u5e94\u5bf9\u591a\u7f51\u5361\u573a\u666f\u3002\u5f53 multusName \u4e0d\u4e3a\u7a7a\u65f6\uff0cSpiderIPPool \u4f1a\u4f7f\u7528\u5bf9\u5e94\u7684 Multus CR \u5b9e\u4f8b\u4e3a Pod \u914d\u7f6e\u7f51\u7edc\uff0c\u82e5 multusName \u5bf9\u5e94\u7684 Multus CR \u4e0d\u5b58\u5728\uff0c\u90a3\u4e48 Spiderpool \u5c06\u65e0\u6cd5\u4e3a Pod \u6307\u5b9a Multus CR\u3002\u5f53 multusName \u4e3a\u7a7a\u65f6\uff0cSpiderpool \u5bf9 Pod \u6240\u4f7f\u7528\u7684 Multus CR \u4e0d\u4f5c\u9650\u5236\u3002 spec.ips \uff1a\u6839\u636e\u4e0a\u6587 AWS EC2 \u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca\u8f85\u52a9 IP \u5730\u5740\u7b49\u4fe1\u606f\uff0c\u6545\u8be5\u503c\u7684\u8303\u56f4\u5fc5\u987b\u5728 nodeName \u5bf9\u5e94\u4e3b\u673a\u7684\u8f85\u52a9\u79c1\u7f51 IP \u8303\u56f4\u5185\uff0c\u4e14\u5bf9\u5e94\u552f\u4e00\u7684\u4e00\u5f20\u5b9e\u4f8b\u7f51\u5361\u3002 \u7ed3\u5408\u4e0a\u6587 AWS \u73af\u5883 \u6bcf\u53f0\u5b9e\u4f8b\u7684\u7f51\u5361\u4ee5\u53ca\u5bf9\u5e94\u7684\u8f85\u52a9 IP \u4fe1\u606f\uff0c\u4f7f\u7528\u5982\u4e0b\u7684 Yaml\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u6bcf\u5f20\u7f51\u5361( ens5\u3001ens6) \u5206\u522b\u521b\u5efa\u4e86\u4e00\u4e2a SpiderIPPool\uff0c\u5b83\u4eec\u5c06\u4e3a\u4e0d\u540c\u8282\u70b9\u4e0a\u7684 Pod \u63d0\u4f9b IP \u5730\u5740\u3002 ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens5 spec: subnet: 172.31.16.0/20 ips: - 172.31.16.4-172.31.16.8 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens6 spec: subnet: 210.22.16.0/24 ips: - 210.22.16.11-210.22.16.15 gateway: 210.22.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens6\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens5 spec: subnet: 180.17.16.0/24 ips: - 180.17.16.11-180.17.16.15 gateway: 180.17.16.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens6 spec: subnet: 210.22.32.0/24 ips: - 210.22.32.11-210.22.32.15 gateway: 210.22.32.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens6\"] EOF","title":"\u521b\u5efa IP \u6c60"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_5","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c\u4f1a\u521b\u5efa 1 \u4e2a Deployment \u5e94\u7528\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a\u5e94\u7528\u7684 CNI \u914d\u7f6e\uff0c\u793a\u4f8b\u4e2d\u7684\u5e94\u7528\u9009\u62e9\u4f7f\u7528\u5bf9\u5e94\u4e8e\u5bbf\u4e3b\u673a ens5 \u7684 ipvlan \u914d\u7f6e\uff0c\u5e76\u6839\u636e\u6211\u4eec\u7684\u7f3a\u7701 SpiderIPPool \u8d44\u6e90\u9ed8\u8ba4\u6311\u9009\u5176\u5bf9\u5e94\u7684\u5b50\u7f51\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb-1 spec: selector: matchLabels: run: nginx-lb-1 replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens5\" labels: run: nginx-lb-1 spec: containers: - name: nginx-lb-1 image: nginx ports: - containerPort: 80 EOF \u67e5\u770b Pod \u7684\u8fd0\u884c\u72b6\u6001\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0c\u6211\u4eec\u4e24\u4e2a\u8282\u70b9\u4e0a\u90fd\u8fd0\u884c\u4e86 1 \u4e2a Pod \u4e14\u4f7f\u7528\u7684 IP \u90fd\u5bf9\u5e94\u5bbf\u4e3b\u673a\u7684\u7b2c\u4e00\u5f20\u7f51\u5361\u7684\u8f85\u52a9 IP: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-1-55d4c48fc8-skrxh 1 /1 Running 0 5s 172 .31.16.5 master <none> <none> nginx-lb-1-55d4c48fc8-jl8b9 1 /1 Running 0 5s 180 .17.16.14 worker1 <none> <none>","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_6","text":"\u6d4b\u8bd5 Pod \u4e0e\u5bbf\u4e3b\u673a\u7684\u901a\u8baf\u60c5\u51b5\uff1a export NODE_MASTER_IP=172.31.22.228 export NODE_WORKER1_IP= 180.17.16.17 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${NODE_MASTER_IP} -c 1 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-jl8b9 -- ping ${NODE_WORKER1_IP} -c 1 \u6d4b\u8bd5 Pod \u4e0e\u8de8\u8282\u70b9\u3001\u8de8\u5b50\u7f51 Pod \u7684\u901a\u8baf\u60c5\u51b5 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping 180.17.16.14 -c 1 \u6d4b\u8bd5 Pod \u4e0e ClusterIP \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${CLUSTER_IP} -c 1","title":"\u6d4b\u8bd5\u96c6\u7fa4\u4e1c\u897f\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_7","text":"","title":"\u6d4b\u8bd5\u96c6\u7fa4\u5357\u5317\u5411\u8fde\u901a\u6027"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#pod","text":"\u501f\u52a9\u4e0a\u6587\u6211\u4eec\u521b\u5efa\u7684 AWS NAT \u7f51\u5173 \uff0c\u6211\u4eec\u7684 VPC \u79c1\u7f51\u5df2\u53ef\u5b9e\u73b0\u8bbf\u95ee\u4e92\u8054\u7f51\u3002 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- curl www.baidu.com -I","title":"\u96c6\u7fa4\u5185\u7684 Pod \u6d41\u91cf\u51fa\u53e3\u8bbf\u95ee"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#_8","text":"","title":"\u8d1f\u8f7d\u5747\u8861\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee(\u53ef\u9009)"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#aws-load-balancer-controller","text":"AWS \u57fa\u7840\u4ea7\u54c1 \u8d1f\u8f7d\u5747\u8861 \u62e5\u6709 NLB (Network Load Balancer) \u548c ALB(Application Load Balancer) \u4e24\u79cd\u6a21\u5f0f\u5206\u522b\u5bf9\u5e94 Layer4 \u4e0e Layer7\u3002 aws-load-balancer-controller \u662f AWS \u63d0\u4f9b\u7684\u4e00\u4e2a\u7528\u4e8e Kubernetes \u4e0e AWS \u57fa\u7840\u4ea7\u54c1\u8fdb\u884c\u5bf9\u63a5\u7684\u7ec4\u4ef6\uff0c\u53ef\u5b9e\u73b0 kubernetes Service LoadBalancer \u548c Ingress \u529f\u80fd\u3002\u672c\u6587\u4e2d\u901a\u8fc7\u8be5\u7ec4\u4ef6\u7ed3\u5408 AWS \u57fa\u7840\u8bbe\u65bd\u5b8c\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6d41\u91cf\u5165\u53e3\u8bbf\u95ee\u3002\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5\u6f14\u793a\uff0c \u53c2\u8003\u4e0b\u5217\u6b65\u9aa4\u4e0e aws-load-balancer-controller \u6587\u6863 \u5b8c\u6210 aws-load-balancer-controller \u7684\u90e8\u7f72\u3002 \u96c6\u7fa4\u8282\u70b9\u914d\u7f6e providerID \u52a1\u5fc5\u4e3a Kubernetes \u4e0a\u7684\u6bcf\u4e2a Node \u8bbe\u7f6e\u4e0a providerID \uff0c\u60a8\u53ef\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0: - \u53ef\u76f4\u63a5\u5728 AWS EC2 dashboard \u4e2d\u627e\u5230\u5b9e\u4f8b\u7684Instance ID. - \u4f7f\u7528 AWS CLI \u6765\u67e5\u8be2Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . \u4e3a AWS EC2 \u5b9e\u4f8b\u6240\u4f7f\u7528\u7684 IAM role \u8865\u5145 policy \u4ecb\u4e8e aws-load-balancer-controller \u8fd0\u884c\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u4e14\u9700\u8981\u8bbf\u95ee AWS \u7684 NLB/ALB APIs\uff0c\u56e0\u6b64\u9700\u8981 AWS IAM \u5173\u4e8e NLB/ALB \u76f8\u5173\u8bf7\u6c42\u7684\u6388\u6743\u3002\u53c8\u56e0\u6211\u4eec\u662f\u81ea\u5efa\u96c6\u7fa4\uff0c\u6211\u4eec\u9700\u8981\u501f\u7528\u8282\u70b9\u81ea\u8eab\u7684 IAM Role \u6765\u5b9e\u73b0\u6388\u6743\uff0c\u8be6\u60c5\u53ef\u770b aws-load-balancer-controller IAM \u3002 curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json \u4f7f\u7528\u5982\u4e0a\u83b7\u53d6\u7684 json \u5185\u5bb9\uff0c\u5728 AWS IAM Dashboard \u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684policy\uff0c\u5e76\u5c06\u8be5 policy \u4e0e\u60a8\u5f53\u524d\u865a\u62df\u673a\u5b9e\u4f8b\u7684 IAM Role \u8fdb\u884c\u5173\u8054\u3002 \u4e3a\u60a8 AWS EC2 \u5b9e\u4f8b\u6240\u5728\u7684\u53ef\u7528\u533a\u521b\u5efa\u4e00\u4e2a public subnet \u5e76\u6253\u4e0a\u53ef\u81ea\u52a8\u53d1\u73b0\u7684 tag. ALB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 2 \u4e2a\u8de8\u53ef\u7528\u533a\u7684\u5b50\u7f51\uff0c\u5bf9\u4e8e NLB \u7684\u4f7f\u7528\u9700\u8981\u81f3\u5c11 1 \u4e2a\u5b50\u7f51\u3002\u8be6\u60c5\u8bf7\u770b \u5b50\u7f51\u81ea\u52a8\u53d1\u73b0 \u3002 \u5bf9\u4e8e\u516c\u7f51\u8bbf\u95ee\u7684 LB\uff0c\u60a8\u9700\u8981\u4e3a\u5b9e\u4f8b\u6240\u5728\u53ef\u7528\u533a\u7684 public subnet \u6253\u4e0a tag: kubernetes.io/role/elb:1 \uff0c\u5bf9\u4e8e VPC \u95f4\u8bbf\u95ee\u7684 LB\uff0c\u8bf7\u521b\u5efa private subnet \u5e76\u6253\u4e0a tag: kubernetes.io/role/internal-elb:1 \uff0c\u8bf7\u7ed3\u5408 AWS \u73af\u5883 \u6765\u521b\u5efa\u6240\u9700\u7684\u5b50\u7f51\uff1a \u9488\u5bf9\u56e0\u7279\u7f51\u66b4\u9732\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa public subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u6211\u4eec\u7684 Main \u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002(\u6ce8\u610f Main \u8def\u7531\u8868\u7684 0.0.0.0/0 \u8def\u7531\u7684\u4e0b\u4e00\u8df3\u9ed8\u8ba4\u4e3a Internet \u7f51\u5173\uff0c\u82e5\u4e22\u5931\u8bf7\u81ea\u884c\u521b\u5efa\u8be5\u8def\u7531\u89c4\u5219)\u3002 \u5728 AWS VPC Dashboard Route tables \u680f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8def\u7531\u8868\u5e76\u914d\u7f6e 0.0.0.0/0 \u7684\u8def\u7531\u4e0b\u4e00\u8df3\u4e3a NAT \u7f51\u5173\uff0c::/0 \u8def\u7531\u4e0b\u4e00\u8df3\u4e3a Internet \u7f51\u5173\u3002 \u9488\u5bf9 VPC \u95f4\u8bbf\u95ee\u7684\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u521b\u5efa private subnet: \u5728 AWS VPC Dashboard Subnets \u680f\u9009\u62e9\u521b\u5efa\u5b50\u7f51\uff0c\u5e76\u9009\u62e9\u4e0e EC2 \u76f8\u540c\u7684\u53ef\u7528\u533a\u3002\u968f\u540e\u5728 Route tables \u680f\u9009\u4e2d\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u8def\u7531\u8868\u5e76\u9009\u62e9\u5b50\u7f51\u5173\u8054\u3002 \u4f7f\u7528 helm \u5b89\u88c5aws-load-balancer-controller(\u672c\u4f8b\u57fa\u4e8e v2.6 \u7248\u672c\u8fdb\u884c\u5b89\u88c5) helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> \u68c0\u67e5 aws-load-balancer-controller \u5b89\u88c5\u5b8c\u6210 ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s","title":"\u90e8\u7f72 AWS Load Balancer Controller"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#loadbalancer","text":"\u4e0a\u6587\u4e2d\u5df2\u521b\u5efa \u5e94\u7528 , \u73b0\u5728\u6211\u4eec\u4e3a\u5b83\u521b\u5efa\u4e00\u4e2a kubernetes Service LoadBalancer \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack \u6ce8\u89e3): cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb-1 labels: run: nginx-lb-1 annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb-1 EOF \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a NLB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 NLB \u8fd8\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/load-balancer-source-ranges \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90 IP\u3002\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 \u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-scheme \u9009\u62e9\u6b64 NLB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u6ce8\u89e3 service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true \u63d0\u4f9b\u4e86\u5ba2\u6237\u7aef\u6e90 IP \u4fdd\u7559\u529f\u80fd\u3002","title":"\u4e3a\u5e94\u7528\u521b\u5efa Loadbalancer \u8d1f\u8f7d\u5747\u8861\u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-aws-zh_CN/#ingress","text":"\u63a5\u4e0b\u6765\u6211\u4eec\u901a\u8fc7 AWS EC2 \u4e2d\u7ed1\u5b9a\u7684\u7b2c\u4e8c\u5f20\u7f51\u5361\u6765\u521b\u5efa\u4e00\u4e2akubernetes Ingress \u8d44\u6e90(\u82e5\u6709\u53cc\u6808\u9700\u6c42\u8bf7\u653e\u5f00 alb.ingress.kubernetes.io/ip-address-type: dualstack \u6ce8\u89e3): apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 \u6211\u4eec\u53ef\u4ee5\u5728 AWS Dashboard EC2 Load Balancing \u680f\u4e2d\u770b\u5230\u5df2\u7ecf\u6709\u4e00\u4e2a ALB \u5df2\u88ab\u521b\u5efa\u51fa\u6765\u4e14\u53ef\u88ab\u8bbf\u95ee\u3002 ALB \u4e5f\u53ef\u652f\u6301 instance \u6a21\u5f0f\u521b\u5efa LB\uff0c\u53ea\u9700\u4fee\u6539\u6ce8\u89e3 alb.ingress.kubernetes.io/target-type \u5373\u53ef\uff0c\u4f46\u56e0\u914d\u5408 service.spec.externalTraffic=Local \u6a21\u5f0f\u4e0d\u652f\u6301\u76d1\u542c\u8282\u70b9\u6f02\u79fb\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002 \u4f7f\u7528 ALB \u7684 instance \u6a21\u5f0f\u9700\u8981\u6307\u5b9a service \u4e3a NodePort \u6a21\u5f0f\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/inbound-cidrs \u6765\u9650\u5236\u53ef\u8bbf\u95ee\u6e90IP\u3002(\u6ce8\u610f\uff0c\u8be5\u529f\u80fd\u4e0e\u6ce8\u89e3 alb.ingress.kubernetes.io/ip-address-type \u5173\u8054\uff0c\u82e5\u9ed8\u8ba4 ipv4 \u5219\u8be5\u503c\u9ed8\u8ba4\u4e3a 0.0.0.0/0 , \u82e5\u662f dualstack \u5219\u9ed8\u8ba4\u4e3a 0.0.0.0/0, ::/0 )\u3002 \u53ef\u901a\u8fc7\u6ce8\u89e3 alb.ingress.kubernetes.io/scheme \u9009\u62e9\u6b64 ALB \u662f\u66b4\u9732\u7ed9\u516c\u7f51\u8bbf\u95ee\u8fd8\u662f\u7559\u7ed9 VPC \u95f4\u8bbf\u95ee\uff0c\u9ed8\u8ba4\u503c\u4e3a internal \u4f9b VPC \u95f4\u8bbf\u95ee\u3002 \u82e5\u60f3\u6574\u5408\u591a\u4e2a Ingress \u8d44\u6e90\u5171\u4eab\u540c\u4e00\u4e2a\u5165\u53e3\uff0c\u53ef\u914d\u7f6e\u6ce8\u89e3 alb.ingress.kubernetes.io/group.name \u6765\u663e\u793a\u6307\u5b9a\u4e00\u4e2a\u540d\u5b57\u3002\uff08\u6ce8\u610f\uff0c\u9ed8\u8ba4\u4e0d\u6307\u5b9a\u8be5\u6ce8\u89e3\u7684 Ingresses \u8d44\u6e90\u5e76\u4e0d\u5c5e\u4e8e\u4efb\u4f55 IngressGroup\uff0c\u7cfb\u7edf\u4f1a\u5c06\u5176\u89c6\u4e3a\u7531 Ingress \u672c\u8eab\u7ec4\u6210\u7684 \"\u9690\u5f0f IngressGroup\"\uff09 \u5982\u679c\u60f3\u6307\u5b9a Ingress \u7684 host\uff0c\u9700\u8981\u642d\u914d externalDNS \u4f7f\u7528\u3002\u8be6\u60c5\u8bf7\u67e5\u770b \u914d\u7f6e externalDNS \u3002","title":"\u4e3a\u5e94\u7528\u521b\u5efa Ingress \u8bbf\u95ee\u5165\u53e3"},{"location":"usage/install/cloud/get-started-aws/","text":"Running On AWS English | \u7b80\u4f53\u4e2d\u6587 Introduction With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized underlay solutions for public clouds. This page introduces Spiderpool , an underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios. Features Spiderpool can operate in public cloud environments using the ipvlan underlay CNI and provide features such as node topology and MAC address validity resolution. Here is how it works: When using underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and ipvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with ipvlan CNI . The ipvlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating ipvlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved. Prerequisites The system kernel version must be greater than 4.2 when using ipvlan as the cluster's CNI. Helm is installed. Understand the basics of AWS VPC Public and Private Subnets . In an AWS VPC, a subnet is categorized as a public subnet if it has an outbound route configured with the Internet Gateway as the next hop for destinations 0.0.0.0/0 or ::/0. Otherwise, a subnet is considered a private subnet if it lacks this specific outbound routing configuration. Steps AWS Environment Create a public subnet and multiple private subnets within a VPC, and deploy virtual machines in the private subnets as shown in the following picture: We will create one public subnet and two private subnets within the same VPC. Each private subnet should be deployed in a different availability zone. A EC2 instance as a jump server will be created in the public subnet for secure access. Additionally, two AWS EC2 instances will be created in the respective different private subnets to set up the Kubernetes cluster. Create two additional private subnets in the same availability zones as the instances to provide secondary network interfaces for the instances, as the picture below: Allocate some secondary private IP addresses to each network interface of the instances: To make the most efficient use of instance resources for application deployment, it is recommended to attach two network interfaces with their corresponding secondary IP addresses to each instance. This approach helps overcome limitations on the number of network interfaces and secondary IP addresses per instance, as specified in the AWS EC2 instance specifications . | Node | ens5 primary IP | ens5 secondary IPs | ens6 primary IP | ens6 secondary IPs | | --------- | ----------------- | --------------------------- | ----------------- | --------------------------- | | master | 172 .31.22.228 | 172 .31.16.4-172.31.16.8 | 210 .22.16.10 | 210 .22.16.11-210.22.16.15 | | worker1 | 180 .17.16.17 | 180 .17.16.11-180.17.16.15 | 210 .22.32.10 | 210 .22.32.11-210.22.32.15 | Create an AWS NAT gateway to allow instances in the VPC's private subnets to connect to external services. The NAT gateway serves as an outbound traffic gateway for the cluster. Follow the NAT gateway documentation to create a NAT gateway: Create a NAT gateway in the public subnet, public-172-31-0-0 , and configure the route table of the private subnets to set the next-hop of the outbound route 0.0.0.0/0 to NAT gateway. (IPv6 addresses provided by AWS are globally unique and can access the internet directly via the Internet Gateway). Use the configured virtual machines to establish a Kubernetes cluster. The available IP addresses for the nodes and the network topology diagram of the cluster are shown below: Install Spiderpool Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"default/ipvlan-ens5\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Install CNI To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Based on the network interface configuration created during the process of setting up the AWS EC2 instances, here is an example configuration of SpiderMultusConfig for each network interface used to run ipvlan CNI: IPVLAN_MASTER_INTERFACE0 = \"ens5\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"ens6\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two ipvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth5 and eth6 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d Create IP Pools The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips : based on the information provided about the network interfaces and secondary IP addresses of the AWS EC2 instances, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . Each value should correspond to a unique instance network interface. Taking into account the network interfaces and associated secondary IP information for each instance in the AWS environment , the following YAML is used to create a separate SpiderIPPool for each network interface (ens5, ens6) on each node. These pools will provide IP addresses for Pods on different nodes: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens5 spec: subnet: 172.31.16.0/20 ips: - 172.31.16.4-172.31.16.8 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens6 spec: subnet: 210.22.16.0/24 ips: - 210.22.16.11-210.22.16.15 gateway: 210.22.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens6\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens5 spec: subnet: 180.17.16.0/24 ips: - 180.17.16.11-180.17.16.15 gateway: 180.17.16.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens6 spec: subnet: 210.22.32.0/24 ips: - 210.22.32.11-210.22.32.15 gateway: 210.22.32.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens6\"] EOF Create Applications The following YAML example creates a Deployment application with the following configuration: v1.multus-cni.io/default-network : specify the CNI configuration for the application. In this example, the application is configured to use the ipvlan configuration associated with the ens5 interface of the host machine. The subnet is selected automatically according to the default SpiderIPPool resource. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb-1 spec: selector: matchLabels: run: nginx-lb-1 replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens5\" labels: run: nginx-lb-1 spec: containers: - name: nginx-lb-1 image: nginx ports: - containerPort: 80 EOF By checking the running status of the Pods, you can observe that one Pod is running on each node, and the Pods are assigned the secondary IP address of the first network interface of their respective host machines: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-1-55d4c48fc8-skrxh 1 /1 Running 0 5s 172 .31.16.5 master <none> <none> nginx-lb-1-55d4c48fc8-jl8b9 1 /1 Running 0 5s 180 .17.16.14 worker1 <none> <none> Test East-West Connectivity Test communication between Pods and their hosts: export NODE_MASTER_IP=172.31.22.228 export NODE_WORKER1_IP= 180.17.16.17 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${NODE_MASTER_IP} -c 1 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-jl8b9 -- ping ${NODE_WORKER1_IP} -c 1 Test communication between Pods across different nodes and subnets: ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping 180.17.16.14 -c 1 Test communication between Pods and ClusterIP: ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${CLUSTER_IP} -c 1 Test North-South Connectivity Test egress traffic from Pods to external destinations With the AWS NAT gateway created in the previous section, our VPC's private network can now be accessed from the internet. ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- curl www.baidu.com -I Load Balancer Ingress Access (Optional) Deploy AWS Load Balancer Controller The AWS Load Balancer product offers two modes: NLB (Network Load Balancer) and ALB (Application Load Balancer), corresponding to Layer 4 and Layer 7, respectively. The aws-load-balancer-controller is an AWS-provided component that integrates Kubernetes with AWS Load Balancer, enabling Kubernetes Service LoadBalancer and Ingress functionality. We will use this component to facilitate load balancing ingress access with AWS infrastructure. The installation demo is based on version v2.6 . You can follow the steps below and refer to the aws-load-balancer-controller documentation for aws-load-balancer-controller deployment: Configure providerID for cluster nodes. It is necessary to set the providerID for each Node in Kubernetes. You can achieve this in either of the following ways: Find the Instance ID for each instance directly in the AWS EC2 dashboard. Use the AWS CLI to query the Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . Add necessary IAM role policy for AWS EC2 instances The aws-load-balancer-controller runs on each node and requires access to AWS NLB/ALB APIs. Therefore, it needs authorization to make requests related to NLB/ALB through AWS IAM. As we are deploying a self-managed cluster, we need to leverage the IAM Role of the nodes themselves to grant this authorization. For more details, refer to the aws-load-balancer-controller IAM . curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json Create a new policy in the AWS IAM Dashboard by using the obtained JSON content and associate it with the IAM Role of your virtual machine instance. Create a public subnet for the availability zone where your AWS EC2 instances are located and apply an auto-discoverable tag. For ALB, you need at least two subnets across different availability zones. For NLB, at least one subnet is required. Refer to the Subnet Discovery for more details. To enable LB with public access, add the kubernetes.io/role/elb:1 tag to the public subnet in the availability zone where the instances reside. Regarding cross-VPC access for LB, create a private subnet and apply the kubernetes.io/role/internal-elb:1 tag. Use the AWS environment to create the necessary subnets: To create a public subnet for an internet-exposed load balancer, go to the AWS VPC Dashboard, select \"Create subnet\" in the Subnets section, and choose the same availability zone as the EC2 instance, and associate the subnet with the Main route table (make sure the default 0.0.0.0/0 route in the Main route table has the Internet Gateway as the next hop; if not, create this route rule). Create a new route table in the AWS VPC Dashboard and configure the 0.0.0.0/0 route with the NAT Gateway as the next hop, and the ::/0 route with the Internet Gateway as the next hop. To create a private subnet for LB with cross-VPC access, go to the AWS VPC Dashboard Subnets section, select \"Create subnet,\" choose the same availability zone as the EC2 instance, and associate it with the route table created in the previous step. Install aws-load-balancer-controller v2.6 using Helm. helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> Check if aws-load-balancer-controller has been installed already ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s Create a LoadBalancer for Application Access To provide access to the application created in the previous section Create Applications , we will create a Kubernetes Service of type LoadBalancer. If you have a dual-stack requirement, add the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack : cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb-1 labels: run: nginx-lb-1 annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb-1 EOF As shown in the AWS EC2 Load Balancing dashboard, an NLB has been created and is accessible. NLB also supports creating LB in instance mode by just modifying the annotation service.beta.kubernetes.io/aws-load-balancer-nlb-target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. Use the annotation service.beta.kubernetes.io/load-balancer-source-ranges to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation service.beta.kubernetes.io/aws-load-balancer-scheme to specify whether the NLB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. The annotation service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true enables client source IP preservation ability. Create an Ingress for Application Access Next, we will create a Kubernetes Ingress resource using the second network interface bound in AWS EC2. If you have a dual-stack requirement, add the annotation alb.ingress.kubernetes.io/ip-address-type: dualstack : apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 As shown in the AWS EC2 Load Balancing dashboard, an ALB has been created and is accessible. ALB also supports creating LB in instance mode by just modifying the annotation alb.ingress.kubernetes.io/target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. When using ALB in instance mode, specify the service as NodePort mode. Use the annotation alb.ingress.kubernetes.io/inbound-cidrs to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation alb.ingress.kubernetes.io/ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation alb.ingress.kubernetes.io/scheme to specify whether the ALB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. To integrate multiple Ingress resources and share the same entry point, configure the annotation alb.ingress.kubernetes.io/group.name to specify a name. Ingress resources without this annotation are treated as an \"implicit IngressGroup\" composed by the Ingress itself. To specify the host for the Ingress, refer to Configuring externalDNS to enable it.","title":"AWS Cloud"},{"location":"usage/install/cloud/get-started-aws/#running-on-aws","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On AWS"},{"location":"usage/install/cloud/get-started-aws/#introduction","text":"With a multitude of public cloud providers available, such as Alibaba Cloud, Huawei Cloud, Tencent Cloud, AWS, and more, it can be challenging to use mainstream open-source CNI plugins to operate on these platforms using underlay networks. Instead, one has to rely on proprietary CNI plugins provided by each cloud vendor, leading to a lack of standardized underlay solutions for public clouds. This page introduces Spiderpool , an underlay networking solution designed to work seamlessly in any public cloud environment. A unified CNI solution offers easier management across multiple clouds, particularly in hybrid cloud scenarios.","title":"Introduction"},{"location":"usage/install/cloud/get-started-aws/#features","text":"Spiderpool can operate in public cloud environments using the ipvlan underlay CNI and provide features such as node topology and MAC address validity resolution. Here is how it works: When using underlay networks in a public cloud environment, each network interface of a cloud server can only be assigned a limited number of IP addresses. To enable communication when an application runs on a specific cloud server, it needs to obtain the valid IP addresses allocated to different network interfaces within the VPC network. To address this IP allocation requirement, Spiderpool introduces a CRD named SpiderIPPool . By configuring the nodeName and multusName fields in SpiderIPPool , it enables node topology functionality. Spiderpool leverages the affinity between the IP pool and nodes, as well as the affinity between the IP pool and ipvlan Multus, facilitating the utilization and management of available IP addresses on the nodes. This ensures that applications are assigned valid IP addresses, enabling seamless communication within the VPC network, including communication between Pods and also between Pods and cloud servers. In a public cloud VPC network, network security controls and packet forwarding principles dictate that when network data packets contain MAC and IP addresses unknown to the VPC network, correct forwarding becomes unattainable. This issue arises in scenarios where Macvlan or OVS based underlay CNI plugins generate new MAC addresses for Pod NICs, resulting in communication failures among Pods. To address this challenge, Spiderpool offers a solution in conjunction with ipvlan CNI . The ipvlan CNI operates at the L3 of the network, eliminating the reliance on L2 broadcasts and avoiding the generation of new MAC addresses. Instead, it maintains consistency with the parent interface. By incorporating ipvlan, the legitimacy of MAC addresses in a public cloud environment can be effectively resolved.","title":"Features"},{"location":"usage/install/cloud/get-started-aws/#prerequisites","text":"The system kernel version must be greater than 4.2 when using ipvlan as the cluster's CNI. Helm is installed. Understand the basics of AWS VPC Public and Private Subnets . In an AWS VPC, a subnet is categorized as a public subnet if it has an outbound route configured with the Internet Gateway as the next hop for destinations 0.0.0.0/0 or ::/0. Otherwise, a subnet is considered a private subnet if it lacks this specific outbound routing configuration.","title":"Prerequisites"},{"location":"usage/install/cloud/get-started-aws/#steps","text":"","title":"Steps"},{"location":"usage/install/cloud/get-started-aws/#aws-environment","text":"Create a public subnet and multiple private subnets within a VPC, and deploy virtual machines in the private subnets as shown in the following picture: We will create one public subnet and two private subnets within the same VPC. Each private subnet should be deployed in a different availability zone. A EC2 instance as a jump server will be created in the public subnet for secure access. Additionally, two AWS EC2 instances will be created in the respective different private subnets to set up the Kubernetes cluster. Create two additional private subnets in the same availability zones as the instances to provide secondary network interfaces for the instances, as the picture below: Allocate some secondary private IP addresses to each network interface of the instances: To make the most efficient use of instance resources for application deployment, it is recommended to attach two network interfaces with their corresponding secondary IP addresses to each instance. This approach helps overcome limitations on the number of network interfaces and secondary IP addresses per instance, as specified in the AWS EC2 instance specifications . | Node | ens5 primary IP | ens5 secondary IPs | ens6 primary IP | ens6 secondary IPs | | --------- | ----------------- | --------------------------- | ----------------- | --------------------------- | | master | 172 .31.22.228 | 172 .31.16.4-172.31.16.8 | 210 .22.16.10 | 210 .22.16.11-210.22.16.15 | | worker1 | 180 .17.16.17 | 180 .17.16.11-180.17.16.15 | 210 .22.32.10 | 210 .22.32.11-210.22.32.15 | Create an AWS NAT gateway to allow instances in the VPC's private subnets to connect to external services. The NAT gateway serves as an outbound traffic gateway for the cluster. Follow the NAT gateway documentation to create a NAT gateway: Create a NAT gateway in the public subnet, public-172-31-0-0 , and configure the route table of the private subnets to set the next-hop of the outbound route 0.0.0.0/0 to NAT gateway. (IPv6 addresses provided by AWS are globally unique and can access the internet directly via the Internet Gateway). Use the configured virtual machines to establish a Kubernetes cluster. The available IP addresses for the nodes and the network topology diagram of the cluster are shown below:","title":"AWS Environment"},{"location":"usage/install/cloud/get-started-aws/#install-spiderpool","text":"Install Spiderpool via helm: helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set ipam.enableStatefulSet = false --set multus.multusCNI.defaultCniCRName = \"default/ipvlan-ens5\" If you are using a cloud server from a Chinese mainland cloud provider, you can enhance image pulling speed by specifying the parameter --set global.imageRegistryOverride=ghcr.m.daocloud.io . Spiderpool allows for fixed IP addresses for application replicas with a controller type of StatefulSet . However, in the underlay network scenario of public clouds, cloud instances are limited to using specific IP addresses. When StatefulSet replicas migrate to different nodes, the original fixed IP becomes invalid and unavailable on the new node, causing network unavailability for the new Pods. To address this issue, set ipam.enableStatefulSet to false to disable this feature. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus.","title":"Install Spiderpool"},{"location":"usage/install/cloud/get-started-aws/#install-cni","text":"To simplify the creation of JSON-formatted Multus CNI configurations, Spiderpool offers the SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CRs. Based on the network interface configuration created during the process of setting up the AWS EC2 instances, here is an example configuration of SpiderMultusConfig for each network interface used to run ipvlan CNI: IPVLAN_MASTER_INTERFACE0 = \"ens5\" IPVLAN_MULTUS_NAME0 = \"ipvlan- $IPVLAN_MASTER_INTERFACE0 \" IPVLAN_MASTER_INTERFACE1 = \"ens6\" IPVLAN_MULTUS_NAME1 = \"ipvlan- $IPVLAN_MASTER_INTERFACE1 \" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME0} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE0} --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ${IPVLAN_MULTUS_NAME1} namespace: default spec: cniType: ipvlan ipvlan: master: - ${IPVLAN_MASTER_INTERFACE1} EOF This case uses the given configuration to create two ipvlan SpiderMultusConfig instances. These instances will automatically generate corresponding Multus NetworkAttachmentDefinition CRs for the host's eth5 and eth6 network interfaces. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -A NAMESPACE NAME AGE default ipvlan-ens5 8d default ipvlan-ens6 8d","title":"Install CNI"},{"location":"usage/install/cloud/get-started-aws/#create-ip-pools","text":"The Spiderpool's CRD, SpiderIPPool , introduces the following fields: nodeName , multusName , and ips : nodeName : when nodeName is not empty, Pods are scheduled on a specific node and attempt to acquire an IP address from the corresponding SpiderIPPool. If the Pod's node matches the specified nodeName , it successfully obtains an IP. Otherwise, it cannot obtain an IP from that SpiderIPPool. When nodeName is empty, Spiderpool does not impose any allocation restrictions on the Pod. multusName \uff1aSpiderpool integrates with Multus CNI to cope with cases involving multiple network interface cards. When multusName is not empty, SpiderIPPool utilizes the corresponding Multus CR instance to configure the network for the Pod. If the Multus CR specified by multusName does not exist, Spiderpool cannot assign a Multus CR to the Pod. When multusName is empty, Spiderpool does not impose any restrictions on the Multus CR used by the Pod. spec.ips : based on the information provided about the network interfaces and secondary IP addresses of the AWS EC2 instances, the specified range of values must fall within the auxiliary private IP range of the host associated with the specified nodeName . Each value should correspond to a unique instance network interface. Taking into account the network interfaces and associated secondary IP information for each instance in the AWS environment , the following YAML is used to create a separate SpiderIPPool for each network interface (ens5, ens6) on each node. These pools will provide IP addresses for Pods on different nodes: ~# cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens5 spec: subnet: 172.31.16.0/20 ips: - 172.31.16.4-172.31.16.8 gateway: 172.31.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: master-v4-ens6 spec: subnet: 210.22.16.0/24 ips: - 210.22.16.11-210.22.16.15 gateway: 210.22.16.1 default: true nodeName: [\"master\"] multusName: [\"default/ipvlan-ens6\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens5 spec: subnet: 180.17.16.0/24 ips: - 180.17.16.11-180.17.16.15 gateway: 180.17.16.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens5\"] --- apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: worker1-v4-ens6 spec: subnet: 210.22.32.0/24 ips: - 210.22.32.11-210.22.32.15 gateway: 210.22.32.1 default: true nodeName: [\"worker1\"] multusName: [\"default/ipvlan-ens6\"] EOF","title":"Create IP Pools"},{"location":"usage/install/cloud/get-started-aws/#create-applications","text":"The following YAML example creates a Deployment application with the following configuration: v1.multus-cni.io/default-network : specify the CNI configuration for the application. In this example, the application is configured to use the ipvlan configuration associated with the ens5 interface of the host machine. The subnet is selected automatically according to the default SpiderIPPool resource. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-lb-1 spec: selector: matchLabels: run: nginx-lb-1 replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens5\" labels: run: nginx-lb-1 spec: containers: - name: nginx-lb-1 image: nginx ports: - containerPort: 80 EOF By checking the running status of the Pods, you can observe that one Pod is running on each node, and the Pods are assigned the secondary IP address of the first network interface of their respective host machines: ~# kubectl get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-lb-1-55d4c48fc8-skrxh 1 /1 Running 0 5s 172 .31.16.5 master <none> <none> nginx-lb-1-55d4c48fc8-jl8b9 1 /1 Running 0 5s 180 .17.16.14 worker1 <none> <none>","title":"Create Applications"},{"location":"usage/install/cloud/get-started-aws/#test-east-west-connectivity","text":"Test communication between Pods and their hosts: export NODE_MASTER_IP=172.31.22.228 export NODE_WORKER1_IP= 180.17.16.17 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${NODE_MASTER_IP} -c 1 ~# kubectl exec -it nginx-lb-1-55d4c48fc8-jl8b9 -- ping ${NODE_WORKER1_IP} -c 1 Test communication between Pods across different nodes and subnets: ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping 180.17.16.14 -c 1 Test communication between Pods and ClusterIP: ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- ping ${CLUSTER_IP} -c 1","title":"Test East-West Connectivity"},{"location":"usage/install/cloud/get-started-aws/#test-north-south-connectivity","text":"","title":"Test North-South Connectivity"},{"location":"usage/install/cloud/get-started-aws/#test-egress-traffic-from-pods-to-external-destinations","text":"With the AWS NAT gateway created in the previous section, our VPC's private network can now be accessed from the internet. ~# kubectl exec -it nginx-lb-1-55d4c48fc8-skrxh -- curl www.baidu.com -I","title":"Test egress traffic from Pods to external destinations"},{"location":"usage/install/cloud/get-started-aws/#load-balancer-ingress-access-optional","text":"","title":"Load Balancer Ingress Access (Optional)"},{"location":"usage/install/cloud/get-started-aws/#deploy-aws-load-balancer-controller","text":"The AWS Load Balancer product offers two modes: NLB (Network Load Balancer) and ALB (Application Load Balancer), corresponding to Layer 4 and Layer 7, respectively. The aws-load-balancer-controller is an AWS-provided component that integrates Kubernetes with AWS Load Balancer, enabling Kubernetes Service LoadBalancer and Ingress functionality. We will use this component to facilitate load balancing ingress access with AWS infrastructure. The installation demo is based on version v2.6 . You can follow the steps below and refer to the aws-load-balancer-controller documentation for aws-load-balancer-controller deployment: Configure providerID for cluster nodes. It is necessary to set the providerID for each Node in Kubernetes. You can achieve this in either of the following ways: Find the Instance ID for each instance directly in the AWS EC2 dashboard. Use the AWS CLI to query the Instance ID: aws ec2 describe-instances --query 'Reservations[*].Instances[*].{Instance:InstanceId}' . Add necessary IAM role policy for AWS EC2 instances The aws-load-balancer-controller runs on each node and requires access to AWS NLB/ALB APIs. Therefore, it needs authorization to make requests related to NLB/ALB through AWS IAM. As we are deploying a self-managed cluster, we need to leverage the IAM Role of the nodes themselves to grant this authorization. For more details, refer to the aws-load-balancer-controller IAM . curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json Create a new policy in the AWS IAM Dashboard by using the obtained JSON content and associate it with the IAM Role of your virtual machine instance. Create a public subnet for the availability zone where your AWS EC2 instances are located and apply an auto-discoverable tag. For ALB, you need at least two subnets across different availability zones. For NLB, at least one subnet is required. Refer to the Subnet Discovery for more details. To enable LB with public access, add the kubernetes.io/role/elb:1 tag to the public subnet in the availability zone where the instances reside. Regarding cross-VPC access for LB, create a private subnet and apply the kubernetes.io/role/internal-elb:1 tag. Use the AWS environment to create the necessary subnets: To create a public subnet for an internet-exposed load balancer, go to the AWS VPC Dashboard, select \"Create subnet\" in the Subnets section, and choose the same availability zone as the EC2 instance, and associate the subnet with the Main route table (make sure the default 0.0.0.0/0 route in the Main route table has the Internet Gateway as the next hop; if not, create this route rule). Create a new route table in the AWS VPC Dashboard and configure the 0.0.0.0/0 route with the NAT Gateway as the next hop, and the ::/0 route with the Internet Gateway as the next hop. To create a private subnet for LB with cross-VPC access, go to the AWS VPC Dashboard Subnets section, select \"Create subnet,\" choose the same availability zone as the EC2 instance, and associate it with the route table created in the previous step. Install aws-load-balancer-controller v2.6 using Helm. helm repo add eks https://aws.github.io/eks-charts kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName = <cluster-name> Check if aws-load-balancer-controller has been installed already ~# kubectl get po -n kube-system | grep aws-load-balancer-controller NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-5984487f57-q6qcq 1 /1 Running 0 30s aws-load-balancer-controller-5984487f57-wdkxl 1 /1 Running 0 30s","title":"Deploy AWS Load Balancer Controller"},{"location":"usage/install/cloud/get-started-aws/#create-a-loadbalancer-for-application-access","text":"To provide access to the application created in the previous section Create Applications , we will create a Kubernetes Service of type LoadBalancer. If you have a dual-stack requirement, add the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack : cat <<EOF | kubectl create -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb-1 labels: run: nginx-lb-1 annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true # service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: run: nginx-lb-1 EOF As shown in the AWS EC2 Load Balancing dashboard, an NLB has been created and is accessible. NLB also supports creating LB in instance mode by just modifying the annotation service.beta.kubernetes.io/aws-load-balancer-nlb-target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. Use the annotation service.beta.kubernetes.io/load-balancer-source-ranges to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation service.beta.kubernetes.io/aws-load-balancer-ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation service.beta.kubernetes.io/aws-load-balancer-scheme to specify whether the NLB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. The annotation service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true enables client source IP preservation ability.","title":"Create a LoadBalancer for Application Access"},{"location":"usage/install/cloud/get-started-aws/#create-an-ingress-for-application-access","text":"Next, we will create a Kubernetes Ingress resource using the second network interface bound in AWS EC2. If you have a dual-stack requirement, add the annotation alb.ingress.kubernetes.io/ip-address-type: dualstack : apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress spec: selector: matchLabels: run: nginx-ingress replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: run: nginx-ingress spec: containers: - name: nginx-ingress image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-ingress labels: run: nginx-ingress spec: type: NodePort ports: - port: 80 protocol: TCP selector: run: nginx-ingress --- apiVersion: apps/v1 kind: Deployment metadata: name: echoserver spec: selector: matchLabels: app: echoserver replicas: 2 template: metadata: annotations: v1.multus-cni.io/default-network: \"default/ipvlan-ens6\" labels: app: echoserver spec: containers: - image: k8s.gcr.io/e2e-test-images/echoserver:2.5 imagePullPolicy: Always name: echoserver ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: echoserver spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: NodePort selector: app: echoserver --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-app-ingress annotations: alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/ip-address-type: dualstack spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Exact backend: service: name: nginx-svc-ingress port: number: 80 - http: paths: - path: /echo pathType: Exact backend: service: name: echoserver port: number: 80 As shown in the AWS EC2 Load Balancing dashboard, an ALB has been created and is accessible. ALB also supports creating LB in instance mode by just modifying the annotation alb.ingress.kubernetes.io/target-type . However, instance mode does not support node drift when using service.spec.externalTraffic=Local , so it is not recommended. When using ALB in instance mode, specify the service as NodePort mode. Use the annotation alb.ingress.kubernetes.io/inbound-cidrs to restrict the source IP addresses that can access the NLB. This feature is associated with the annotation alb.ingress.kubernetes.io/ip-address-type . If the default mode is IPv4, the value is 0.0.0.0/0 . For dualstack, the default is 0.0.0.0/0, ::/0 . Use the annotation alb.ingress.kubernetes.io/scheme to specify whether the ALB should be exposed for public access or restricted to cross-VPC communication. The default value is internal for cross-VPC communication. To integrate multiple Ingress resources and share the same entry point, configure the annotation alb.ingress.kubernetes.io/group.name to specify a name. Ingress resources without this annotation are treated as an \"implicit IngressGroup\" composed by the Ingress itself. To specify the host for the Ingress, refer to Configuring externalDNS to enable it.","title":"Create an Ingress for Application Access"},{"location":"usage/install/cloud/get-started-openstack-zh_CN/","text":"openstack \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English","title":"openstack \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-openstack-zh_CN/#openstack","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"openstack \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-openstack/","text":"Running On Openstack English | \u7b80\u4f53\u4e2d\u6587","title":"OpenStack"},{"location":"usage/install/cloud/get-started-openstack/#running-on-openstack","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Openstack"},{"location":"usage/install/cloud/get-started-vmware-zh_CN/","text":"vmware \u73af\u5883\u8fd0\u884c \u7b80\u4f53\u4e2d\u6587 | English","title":"vmware \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-vmware-zh_CN/#vmware","text":"\u7b80\u4f53\u4e2d\u6587 | English","title":"vmware \u73af\u5883\u8fd0\u884c"},{"location":"usage/install/cloud/get-started-vmware/","text":"Running On Vmware English | \u7b80\u4f53\u4e2d\u6587","title":"VWware vSphere"},{"location":"usage/install/cloud/get-started-vmware/#running-on-vmware","text":"English | \u7b80\u4f53\u4e2d\u6587","title":"Running On Vmware"},{"location":"usage/install/overlay/get-started-calico-zh_cn/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Calico \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Calico \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0c \u5b83\u7684\u597d\u5904\u662f\uff1a \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u5e2e\u52a9\u89e3\u51b3 Macvlan \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u96c6\u7fa4\u5916\u90e8\u8bbf\u95ee NodePort \u65f6\uff0c\u53ef\u501f\u52a9 Calico \u6570\u636e\u8def\u5f84\u8fdb\u884c\u8f6c\u53d1\uff0c\u65e0\u9700\u5916\u90e8\u8def\u7531\u3002\u5426\u5219 Macvlan \u4f5c\u4e3a CNI \u65f6\uff0c\u53ea\u80fd\u501f\u52a9\u5916\u90e8\u8def\u7531\u8f6c\u53d1\u624d\u80fd\u5b9e\u73b0\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u4fdd\u8bc1 Pod \u8bbf\u95ee\u65f6\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u8054\u901a\u6027\u6b63\u5e38\u3002 \u6ce8: \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR\u7684\u7b80\u5199\u3002 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5 Spiderpool \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Calico \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# calicoctl get ippools NAME CIDR SELECTOR default-ipv4-ippool 10 .222.64.0/18 all () default-ipv4-ippool1 10 .223.64.0/18 all () ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.\u5982\u679c phase \u4e0d\u4e3a Synced, \u90a3\u4e48\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa 2.\u5982\u679c overlayPodCIDR \u4e0d\u6b63\u5e38, \u53ef\u80fd\u4f1a\u5bfc\u81f4\u901a\u4fe1\u95ee\u9898 \u521b\u5efa SpiderIPPool \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 ips \u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u521b\u5efa SpiderMultusConfig \u672c\u6587\u901a\u8fc7 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.spiderpoolConfigPools.IPv4IPPool \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d IP\u3001\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link \u4ee5\u4e0a\u8868\u9879\u89e3\u91ca: Pod \u4e2d\u5206\u914d\u4e86 Calico(eth0) \u548c Macvlan(net1) \u4e24\u5f20\u7f51\u5361, IPv4 \u5730\u5740\u5206\u522b\u662f: 10.233.73.210 \u548c 10.6.212.145 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod\u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.132 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u4e0b\u9762\u6d4b\u8bd5 Pod \u57fa\u672c\u7f51\u7edc\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 Pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Calico \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Calico \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0c \u5b83\u7684\u597d\u5904\u662f\uff1a \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u5e2e\u52a9\u89e3\u51b3 Macvlan \u8bbf\u95ee ClusterIP \u7684\u95ee\u9898 \u96c6\u7fa4\u5916\u90e8\u8bbf\u95ee NodePort \u65f6\uff0c\u53ef\u501f\u52a9 Calico \u6570\u636e\u8def\u5f84\u8fdb\u884c\u8f6c\u53d1\uff0c\u65e0\u9700\u5916\u90e8\u8def\u7531\u3002\u5426\u5219 Macvlan \u4f5c\u4e3a CNI \u65f6\uff0c\u53ea\u80fd\u501f\u52a9\u5916\u90e8\u8def\u7531\u8f6c\u53d1\u624d\u80fd\u5b9e\u73b0\u3002 \u5f53 Pod \u9644\u52a0\u4e86 Calico \u548c Macvlan \u591a\u5f20\u7f51\u5361\u65f6\uff0c\u8c03\u8c10 Pod \u7684\u5b50\u7f51\u8def\u7531\uff0c\u4fdd\u8bc1 Pod \u8bbf\u95ee\u65f6\u6765\u56de\u8def\u5f84\u4e00\u81f4\uff0c\u786e\u4fdd\u7f51\u7edc\u8054\u901a\u6027\u6b63\u5e38\u3002 \u6ce8: \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR\u7684\u7b80\u5199\u3002","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#_1","text":"\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u53c2\u8003\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm \u4e8c\u8fdb\u5236","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spiderpool","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Calico \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# calicoctl get ippools NAME CIDR SELECTOR default-ipv4-ippool 10 .222.64.0/18 all () default-ipv4-ippool1 10 .223.64.0/18 all () ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: creationTimestamp: \"2023-10-18T08:31:09Z\" finalizers: - spiderpool.spidernet.io generation: 7 name: default resourceVersion: \"195405\" uid: 8bdceced-15db-497b-be07-81cbcba7caac spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .222.64.0/18 - 10 .223.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.\u5982\u679c phase \u4e0d\u4e3a Synced, \u90a3\u4e48\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa 2.\u5982\u679c overlayPodCIDR \u4e0d\u6b63\u5e38, \u53ef\u80fd\u4f1a\u5bfc\u81f4\u901a\u4fe1\u95ee\u9898","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spiderippool","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 ips \u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002","title":"\u521b\u5efa SpiderIPPool"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#spidermultusconfig","text":"\u672c\u6587\u901a\u8fc7 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.spiderpoolConfigPools.IPv4IPPool \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"\u521b\u5efa SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-calico-zh_cn/#_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d IP\u3001\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link \u4ee5\u4e0a\u8868\u9879\u89e3\u91ca: Pod \u4e2d\u5206\u914d\u4e86 Calico(eth0) \u548c Macvlan(net1) \u4e24\u5f20\u7f51\u5361, IPv4 \u5730\u5740\u5206\u522b\u662f: 10.233.73.210 \u548c 10.6.212.145 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod\u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.132 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u4e0b\u9762\u6d4b\u8bd5 Pod \u57fa\u672c\u7f51\u7edc\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 Pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/overlay/get-started-calico/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Calico serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . This setup enables Pod's east-west traffic to be forwarded through the Calico-created NIC (eth0). The advantages offered by Spiderpool's solution are: Solve Macvlan's problem for accessing ClusterIP when Pods have both Calico and Macvlan NICs attached. Facilitate the forwarding of external access to NodePort through Calico's data path, eliminating the need for external routing. Whereas, external routing is typically required for forwarding when Macvlan is used as the CNI. Coordinate subnet routing for Pods with multiple Calico and Macvlan NICs, guaranteeing consistent traffic forwarding path for Pods and uninterrupted network connectivity. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR. Prerequisites A ready Kubernetes cluster. Calico has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm binary Install Spiderpool Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.If the phase is not synced, the pod will be prevented from being created. 2.If the overlayPodCIDR does not meet expectations, it may cause pod communication issue. Create SpiderIPPool The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. Create SpiderMultusConfig The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 \u3002 Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' Create an application Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller Enter the Pod and use the command ip to view information such as IP addresses and routes within the Pod: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link Explanation of the above: The Pod is allocated two interfaces: eth0 (Calico) and net1 (Macvlan), having IPv4 addresses of 10.233.73.210 and 10.6.212.145, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, it will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the basic network connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Calico"},{"location":"usage/install/overlay/get-started-calico/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Calico serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . This setup enables Pod's east-west traffic to be forwarded through the Calico-created NIC (eth0). The advantages offered by Spiderpool's solution are: Solve Macvlan's problem for accessing ClusterIP when Pods have both Calico and Macvlan NICs attached. Facilitate the forwarding of external access to NodePort through Calico's data path, eliminating the need for external routing. Whereas, external routing is typically required for forwarding when Macvlan is used as the CNI. Coordinate subnet routing for Pods with multiple Calico and Macvlan NICs, guaranteeing consistent traffic forwarding path for Pods and uninterrupted network connectivity. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR.","title":"Calico Quick Start"},{"location":"usage/install/overlay/get-started-calico/#prerequisites","text":"A ready Kubernetes cluster. Calico has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# kubectl apply -f https://github.com/projectcalico/calico/blob/master/manifests/calico.yaml ~# kubectl wait --for = condition = ready -l k8s-app = calico-node pod -n kube-system Helm binary","title":"Prerequisites"},{"location":"usage/install/overlay/get-started-calico/#install-spiderpool","text":"Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-htcnc 1 /1 Running 0 1m spiderpool-agent-pjqr9 1 /1 Running 0 1m spiderpool-controller-7b7f8dd9cc-xdj95 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.If the phase is not synced, the pod will be prevented from being created. 2.If the overlayPodCIDR does not meet expectations, it may cause pod communication issue.","title":"Install Spiderpool"},{"location":"usage/install/overlay/get-started-calico/#create-spiderippool","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.100-10.6.212.200 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones.","title":"Create SpiderIPPool"},{"location":"usage/install/overlay/get-started-calico/#create-spidermultusconfig","text":"The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 \u3002 Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"Create SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-calico/#create-an-application","text":"Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-4653bc4f24-aswpm 1 /1 Running 0 2m 10 .233.105.167 controller <none> <none> nginx-4653bc4f24-rswak 1 /1 Running 0 2m 10 .233.73.210 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-rswak net1 10 -6-v4 10 .6.212.145/16 worker01 nginx-4653bc4f24-aswpm net1 10 -6-v4 10 .6.212.148/16 controller Enter the Pod and use the command ip to view information such as IP addresses and routes within the Pod: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-rswak sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2 : tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0 .0.0.0 brd 0 .0.0.0 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.73.210/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:eb84/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:180/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.145/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::2e5/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever /# ip rule 0 : from all lookup local 32760 : from 10 .233.73.210 lookup 100 32762 : from all to 169 .254.1.1 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.132 lookup 100 32766 : from all lookup main 32767 : from all lookup default /# ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.145 / # ip route show table 100 default via 169 .254.1.1 dev eth0 10 .6.212.132 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 169 .254.1.1 dev eth0 scope link Explanation of the above: The Pod is allocated two interfaces: eth0 (Calico) and net1 (Macvlan), having IPv4 addresses of 10.233.73.210 and 10.6.212.145, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, it will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the basic network connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Create an application"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/","text":"Cilium Quick Start English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Cilium \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u5f20\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0\u4ee5\u4e0b\u6548\u679c: Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u4e24\u5f20\u7f51\u5361 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Cilium \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0cPod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u4ece Macvlan \u521b\u5efa\u7684\u7f51\u5361(net1)\u8f6c\u53d1\u3002 Pod \u591a\u7f51\u5361\u7684\u8def\u7531\u8c03\u534f\uff0c\u4f7f Pod \u5bf9\u5185\u5bf9\u5916\u8bbf\u95ee\u6b63\u5e38 \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR \u7684\u7b80\u5199\u3002 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Cilium \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm \u4e8c\u8fdb\u5236 \u5b89\u88c5 Spiderpool \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Cilium \u914d\u7f6e\u7684 Pod \u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.\u5982\u679c phase \u4e0d\u4e3a Synced, \u90a3\u4e48\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa 2.\u5982\u679c overlayPodCIDR \u4e0d\u6b63\u5e38, \u53ef\u80fd\u4f1a\u5bfc\u81f4\u901a\u4fe1\u95ee\u9898 \u521b\u5efa SpiderIPPool \u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF Note: subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002 \u521b\u5efa SpiderMultusConfig \u672c\u6587\u4f7f\u7528 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Note: spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.ippools.ipv4 \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 \u4ee5\u4e0a\u4fe1\u606f\u89e3\u91ca: Pod \u5206\u914d\u4e86\u4e24\u5f20\u7f51\u5361: eth0(cilium)\u3001net1(macvlan),\u5bf9\u5e94\u7684 IPv4 \u5730\u5740\u5206\u522b\u4e3a: 10.233.120.101 \u548c 10.6.212.202 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod \u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.131 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u7684\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#cilium-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728\u4e00\u4e2a Cilium \u4f5c\u4e3a\u7f3a\u7701 CNI \u7684\u96c6\u7fa4\uff0c\u901a\u8fc7 Spiderpool \u8fd9\u4e00\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7 Multus \u4e3a Pod \u989d\u5916\u9644\u52a0\u4e00\u5f20\u7531 Macvlan \u521b\u5efa\u7684\u7f51\u5361\uff0c\u5e76\u901a\u8fc7 coordinator \u89e3\u51b3 Pod \u591a\u5f20\u7f51\u5361\u4e4b\u95f4\u8def\u7531\u8c03\u534f\u95ee\u9898\u3002\u8be5\u65b9\u6848\u53ef\u5b9e\u73b0\u4ee5\u4e0b\u6548\u679c: Pod \u9644\u52a0\u4e86 Cilium \u548c Macvlan \u4e24\u5f20\u7f51\u5361 Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u4e1c\u897f\u5411\u6d41\u91cf\u4ece Cilium \u521b\u5efa\u7684\u7f51\u5361\u8f6c\u53d1(eth0)\uff0cPod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u4ece Macvlan \u521b\u5efa\u7684\u7f51\u5361(net1)\u8f6c\u53d1\u3002 Pod \u591a\u7f51\u5361\u7684\u8def\u7531\u8c03\u534f\uff0c\u4f7f Pod \u5bf9\u5185\u5bf9\u5916\u8bbf\u95ee\u6b63\u5e38 \u672c\u6587\u4e2d NAD \u4e3a Multus **N**etwork-**A**ttachment-**D**efinition CR \u7684\u7b80\u5199\u3002","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#_1","text":"\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5b89\u88c5 Cilium \u4f5c\u4e3a\u96c6\u7fa4\u7684\u7f3a\u7701 CNI\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u6216\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm \u4e8c\u8fdb\u5236","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spiderpool","text":"\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\uff0c\u67e5\u770b Spiderpool \u7ec4\u4ef6\u72b6\u6001: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m \u8bf7\u68c0\u67e5 Spidercoordinator.status \u4e2d\u7684 Phase \u662f\u5426\u4e3a Synced, \u5e76\u4e14 overlayPodCIDR \u662f\u5426\u4e0e\u96c6\u7fa4\u4e2d Cilium \u914d\u7f6e\u7684 Pod \u5b50\u7f51\u4fdd\u6301\u4e00\u81f4: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.\u5982\u679c phase \u4e0d\u4e3a Synced, \u90a3\u4e48\u5c06\u4f1a\u963b\u6b62 Pod \u88ab\u521b\u5efa 2.\u5982\u679c overlayPodCIDR \u4e0d\u6b63\u5e38, \u53ef\u80fd\u4f1a\u5bfc\u81f4\u901a\u4fe1\u95ee\u9898","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spiderippool","text":"\u672c\u6587\u96c6\u7fa4\u8282\u70b9\u7f51\u5361: ens192 \u6240\u5728\u5b50\u7f51\u4e3a 10.6.0.0/16 , \u4ee5\u8be5\u5b50\u7f51\u521b\u5efa SpiderIPPool: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF Note: subnet \u5e94\u8be5\u4e0e\u8282\u70b9\u7f51\u5361 ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u4e0d\u4e0e\u73b0\u6709\u4efb\u4f55 IP \u51b2\u7a81\u3002","title":"\u521b\u5efa SpiderIPPool"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#spidermultusconfig","text":"\u672c\u6587\u4f7f\u7528 Spidermultusconfig \u521b\u5efa Multus \u7684 NAD \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Note: spec.macvlan.master \u8bbe\u7f6e\u4e3a ens192 , ens192 \u5fc5\u987b\u5b58\u5728\u4e8e\u4e3b\u673a\u4e0a\u3002\u5e76\u4e14 spec.macvlan.ippools.ipv4 \u8bbe\u7f6e\u7684\u5b50\u7f51\u548c ens192 \u7684\u5b50\u7f51\u4fdd\u6301\u4e00\u81f4\u3002 \u521b\u5efa\u6210\u529f\u540e, \u67e5\u770b Multus NAD \u662f\u5426\u6210\u529f\u521b\u5efa: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"\u521b\u5efa SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-cilium-zh_cn/#_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5\u5e94\u7528 nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : \u8be5\u5b57\u6bb5\u6307\u5b9a Multus \u4f7f\u7528 macvlan-ens192 \u4e3a Pod \u9644\u52a0\u4e00\u5f20\u7f51\u5361\u3002 \u7b49\u5f85 Pod ready, \u67e5\u770b IP \u5206\u914d\u60c5\u51b5: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller \u8fdb\u5165\u5230 Pod \u5185\u90e8\uff0c \u901a\u8fc7 ip \u547d\u4ee4\u67e5\u770b Pod \u4e2d\u8def\u7531\u7b49\u4fe1\u606f: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 \u4ee5\u4e0a\u4fe1\u606f\u89e3\u91ca: Pod \u5206\u914d\u4e86\u4e24\u5f20\u7f51\u5361: eth0(cilium)\u3001net1(macvlan),\u5bf9\u5e94\u7684 IPv4 \u5730\u5740\u5206\u522b\u4e3a: 10.233.120.101 \u548c 10.6.212.202 10.233.0.0/18 \u548c 10.233.64.0/18 \u662f\u96c6\u7fa4\u7684 CIDR, Pod \u8bbf\u95ee\u8be5\u5b50\u7f51\u65f6\u4ece eth0 \u8f6c\u53d1, \u6bcf\u4e2a route table \u90fd\u4f1a\u63d2\u5165\u6b64\u8def\u7531 10.6.212.131 \u662f Pod \u6240\u5728\u8282\u70b9\u7684\u5730\u5740\uff0c\u6b64\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u8be5\u4e3b\u673a\u65f6\u4ece eth0 \u8f6c\u53d1 \u8fd9\u4e00\u7cfb\u5217\u7684\u8def\u7531\u786e\u4fdd Pod \u8bbf\u95ee\u96c6\u7fa4\u5185\u76ee\u6807\u65f6\u4ece eth0 \u8f6c\u53d1\uff0c\u8bbf\u95ee\u5916\u90e8\u76ee\u6807\u65f6\u4ece net1 \u8f6c\u53d1 \u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPod \u7684\u9ed8\u8ba4\u8def\u7531\u4fdd\u7559\u5728 net1\u3002\u5982\u679c\u60f3\u8981\u4fdd\u7559\u5728 eth0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 Pod \u7684 annotations \u4e2d\u6ce8\u5165: \"ipam.spidernet.io/default-route-nic: eth0\" \u5b9e\u73b0\u3002 \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u4e1c\u897f\u5411\u6d41\u91cf\u7684\u8fde\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee CoreDNS \u7684 Pod \u548c Service \u4e3a\u4f8b: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# \u8de8\u8282\u70b9\u8bbf\u95ee CoreDNS \u7684 pod ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# \u8bbf\u95ee CoreDNS \u7684 service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server \u6d4b\u8bd5 Pod \u8bbf\u95ee\u96c6\u7fa4\u5357\u5317\u5411\u6d41\u91cf\u7684\u8054\u901a\u6027\uff0c\u4ee5\u8bbf\u95ee\u5176\u4ed6\u7f51\u6bb5\u76ee\u6807(10.7.212.101)\u4e3a\u4f8b: [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/overlay/get-started-cilium/","text":"Cilium Quick Start English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Cilium serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . The advantages offered by Spiderpool's solution are: Pods have both Cilium and Macvlan NICs. East-west traffic is routed through the Cilium NIC (eth0), while north-south traffic is routed through the Macvlan NIC (net1). The routing coordination among multiple NICs of the Pod ensures seamless connectivity for both internal and external access. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR. Prerequisites A ready Kubernetes cluster. Cilium has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm binary Install Spiderpool Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.If the phase is not synced, the pod will be prevented from being created. 2.If the overlayPodCIDR does not meet expectations, it may cause pod communication issue. Create SpiderIPPool The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones. Create SpiderMultusConfig The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 . Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}' Create an application Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller Use the command ip to view the Pod's information such as routes: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (cilium) and net1 (macvlan), having IPv4 addresses of 10.233.120.101 and 10.6.212.202, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, traffic will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the east-west connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Cilium"},{"location":"usage/install/overlay/get-started-cilium/#cilium-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 This page showcases the utilization of Spiderpool , a comprehensive Underlay network solution, in a cluster where Cilium serves as the default CNI. Spiderpool leverages Multus to attach an additional NIC created with Macvlan to Pods and coordinates routes among multiple NICs using coordinator . The advantages offered by Spiderpool's solution are: Pods have both Cilium and Macvlan NICs. East-west traffic is routed through the Cilium NIC (eth0), while north-south traffic is routed through the Macvlan NIC (net1). The routing coordination among multiple NICs of the Pod ensures seamless connectivity for both internal and external access. NAD is an abbreviation for Multus **N**etwork-**A**ttachment-**D**efinition CR.","title":"Cilium Quick Start"},{"location":"usage/install/overlay/get-started-cilium/#prerequisites","text":"A ready Kubernetes cluster. Cilium has been already installed as the default CNI for your cluster. If it is not installed, please refer to the official documentation or follow the commands below for installation: ~# helm repo add cilium https://helm.cilium.io/ ~# helm install cilium cilium/cilium -namespace kube-system ~# kubectl wait --for = condition = ready -l k8s-app = cilium pod -n kube-system Helm binary","title":"Prerequisites"},{"location":"usage/install/overlay/get-started-cilium/#install-spiderpool","text":"Follow the command below to install Spiderpool: ~# helm repo add spiderpool https://spidernet-io.github.io/spiderpool ~# helm repo update spiderpool ~# helm install spiderpool spiderpool/spiderpool --namespace kube-system --set coordinator.mode = overlay --wait If Macvlan CNI is not installed in your cluster, you can install it on each node by using the Helm parameter --set plugins.installCNI=true . Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Check the status of Spiderpool after the installation is complete: ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-bcwqk 1 /1 Running 0 1m spiderpool-agent-udgi4 1 /1 Running 0 1m spiderpool-controller-bgnh3rkcb-k7sc9 1 /1 Running 0 1m spiderpool-init 0 /1 Completed 0 1m Please check if Spidercoordinator.status.phase is Synced , and if the overlayPodCIDR is consistent with the pod subnet configured by Cilium in the cluster: ~# kubectl get configmaps -n kube-system cilium-config -o yaml | grep cluster-pool cluster-pool-ipv4-cidr: 10 .244.64.0/18 cluster-pool-ipv4-mask-size: \"24\" ipam: cluster-pool ~# kubectl get spidercoordinators.spiderpool.spidernet.io default -o yaml apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderCoordinator metadata: finalizers: - spiderpool.spidernet.io name: default spec: detectGateway: false detectIPConflict: false hijackCIDR: - 169 .254.0.0/16 hostRPFilter: 0 hostRuleTable: 500 mode: auto podCIDRType: calico podDefaultRouteNIC: \"\" podMACPrefix: \"\" tunePodRoutes: true status: overlayPodCIDR: - 10 .244.64.0/18 phase: Synced serviceCIDR: - 10 .233.0.0/18 1.If the phase is not synced, the pod will be prevented from being created. 2.If the overlayPodCIDR does not meet expectations, it may cause pod communication issue.","title":"Install Spiderpool"},{"location":"usage/install/overlay/get-started-cilium/#create-spiderippool","text":"The subnet for the interface ens192 on the cluster nodes here is 10.6.0.0/16 . Create a SpiderIPPool using this subnet: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: 10-6-v4 spec: disable: false gateway: 10.6.0.1 ips: - 10.6.212.200-10.6.212.240 subnet: 10.6.0.0/16 EOF The subnet should be consistent with the subnet of ens192 on the nodes, and ensure that the IP addresses do not conflict with any existing ones.","title":"Create SpiderIPPool"},{"location":"usage/install/overlay/get-started-cilium/#create-spidermultusconfig","text":"The Multus NAD instance is created using Spidermultusconfig: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-ens192 spec: cniType: macvlan macvlan: master: - ens192 ippools: ipv4: - 10-6-v4 vlanID: 0 EOF Set spec.macvlan.master to ens192 which must be present on the host. The subnet specified in spec.macvlan.spiderpoolConfigPools.IPv4IPPool should match that of ens192 . Check if the Multus NAD has been created successfully: ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io macvlan-ens192 -o yaml apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"spiderpool.spidernet.io/v2beta1\" , \"kind\" : \"SpiderMultusConfig\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"macvlan-ens192\" , \"namespace\" : \"default\" } , \"spec\" : { \"cniType\" : \"macvlan\" , \"coordinator\" : { \"podCIDRType\" : \"cluster\" , \"tuneMode\" : \"overlay\" } , \"enableCoordinator\" :true, \"macvlan\" : { \"master\" : [ \"ens192\" ] , \"spiderpoolConfigPools\" : { \"IPv4IPPool\" : [ \"10-6-v4\" ]} , \"vlanID\" :0 }}} creationTimestamp: \"2023-06-30T07:12:21Z\" generation: 1 name: macvlan-ens192 namespace: default ownerReferences: - apiVersion: spiderpool.spidernet.io/v2beta1 blockOwnerDeletion: true controller: true kind: SpiderMultusConfig name: macvlan-ens192 uid: 3f902f46-d9d4-4c62-a7c3-98d4a9aa26e4 resourceVersion: \"24713635\" uid: 712d1e58-ab57-49a7-9189-0fffc64aa9c3 spec: config: '{\"cniVersion\":\"0.3.1\",\"name\":\"macvlan-ens192\",\"plugins\":[{\"type\":\"macvlan\",\"ipam\":{\"type\":\"spiderpool\",\"default_ipv4_ippool\":[\"10-6-v4\"]},\"master\":\"ens192\",\"mode\":\"bridge\"},{\"type\":\"coordinattor\",\"ipam\":{},\"dns\":{},\"detectGateway\":false,\"tunePodRoutes\":true,\"mode\":\"overlay\",\"hostRuleTable\":500,\"detectIPConflict\":false}]}'","title":"Create SpiderMultusConfig"},{"location":"usage/install/overlay/get-started-cilium/#create-an-application","text":"Run the following command to create the demo application nginx: ~# cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: k8s.v1.cni.cncf.io/networks: macvlan-ens192 labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF k8s.v1.cni.cncf.io/networks : specifies that Multus uses macvlan-ens192 to attach an additional interface to the Pod. Check the Pod's IP allocation after it is ready: ~# kubectl get po -l app = nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-x34abcsf74-xngkm 1 /1 Running 0 2m 10 .233.120.101 controller <none> <none> nginx-x34abcsf74-ougjk 1 /1 Running 0 2m 10 .233.84.230 worker01 <none> <none> ~# kubectl get se NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE nginx-4653bc4f24-xngkm net1 10 -6-v4 10 .6.212.202/16 worker01 nginx-4653bc4f24-ougjk net1 10 -6-v4 10 .6.212.230/16 controller Use the command ip to view the Pod's information such as routes: [ root@controller1 ~ ] # kubectl exec it nginx-4653bc4f24-xngkm sh # ip a 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 4 : eth0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1430 qdisc noqueue state UP group default link/ether a2:99:9d:04:01:80 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .233.120.101/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fd85:ee78:d8a6:8607::1:f2d5/128 scope global valid_lft forever preferred_lft forever inet6 fe80::a099:9dff:fe04:131/64 scope link valid_lft forever preferred_lft forever 5 : net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 2a:1e:a1:db:2a:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10 .6.212.202/16 brd 10 .6.255.255 scope global net1 valid_lft forever preferred_lft forever inet6 fd00:10:6::df3/64 scope global valid_lft forever preferred_lft forever inet6 fe80::281e:a1ff:fedb:2a9a/64 scope link valid_lft forever preferred_lft forever / # ip rule 0 : from all lookup local 32760 : from 10 .233.120.101 lookup 100 32762 : from all to 10 .233.65.96 lookup 100 32763 : from all to 10 .233.64.0/18 lookup 100 32764 : from all to 10 .233.0.0/18 lookup 100 32765 : from all to 10 .6.212.131 lookup 100 32766 : from all lookup main 32767 : from all lookup default / # ip route default via 10 .6.0.1 dev net1 10 .6.0.0/16 dev net1 scope link src 10 .6.212.202 / # ip route show table 100 default via 10 .233.65.96 dev eth0 10 .233.65.96 dev eth0 scope link 10 .6.212.131 dev eth0 scope link 10 .233.0.0/18 via 10 .6.212.132 dev eth0 10 .233.64.0/18 via 10 .6.212.132 dev eth0 Explanation of the above: The Pod is allocated two interfaces: eth0 (cilium) and net1 (macvlan), having IPv4 addresses of 10.233.120.101 and 10.6.212.202, respectively. 10.233.0.0/18 and 10.233.64.0/18 represent the cluster's CIDR. When the Pod accesses this subnet, traffic will be forwarded through eth0. Each route table will include this route. 10.6.212.132 is the IP address of the node where the Pod has been scheduled. This route ensures that when the Pod accesses the host, traffic will be forwarded through eth0. This series of routing rules guarantees that the Pod will forward traffic through eth0 when accessing targets within the cluster and through net1 for external targets. By default, the Pod's default route is reserved in net1. To reserve it in eth0, add the following annotation to the Pod's metadata: \"ipam.spidernet.io/default-route-nic: eth0\". To test the east-west connectivity of the Pod, we will use the example of accessing the CoreDNS Pod and Service: ~# kubectl get all -n kube-system -l k8s-app = kube-dns -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/coredns-57fbf68cf6-2z65h 1 /1 Running 1 ( 91d ago ) 91d 10 .233.105.131 worker1 <none> <none> pod/coredns-57fbf68cf6-kvcwl 1 /1 Running 3 ( 91d ago ) 91d 10 .233.73.195 controller <none> <none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR service/coredns ClusterIP 10 .233.0.3 <none> 53 /UDP,53/TCP,9153/TCP 91d k8s-app = kube-dns ~# Access the CoreDNS Pod across nodes ~# kubectl exec nginx-4653bc4f24-rswak -- ping 10 .233.73.195 -c 2 PING 10 .233.73.195 ( 10 .233.73.195 ) : 56 data bytes 64 bytes from 10 .233.73.195: seq = 0 ttl = 62 time = 2 .348 ms 64 bytes from 10 .233.73.195: seq = 1 ttl = 62 time = 0 .586 ms --- 10 .233.73.195 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .586/1.467/2.348 ms ~# Access the CoreDNS Service ~# kubectl exec nginx-4653bc4f24-rswak -- curl 10 .233.0.3:53 -I % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0 :00:02 --:--:-- 0 curl: ( 52 ) Empty reply from server Test the Pod's connectivity for north-south traffic, specifically accessing targets in another subnet (10.7.212.101): [ root@controller1 cyclinder ] # kubectl exec nginx-4653bc4f24-rswak -- ping 10.7.212.101 -c 2 PING 10 .7.212.101 ( 10 .7.212.101 ) : 56 data bytes 64 bytes from 10 .7.212.101: seq = 0 ttl = 61 time = 4 .349 ms 64 bytes from 10 .7.212.101: seq = 1 ttl = 61 time = 0 .877 ms --- 10 .7.212.101 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .877/2.613/4.349 ms","title":"Create an application"},{"location":"usage/install/underlay/get-started-calico-zh_CN/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3: \u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740 IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9 \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528 \u5148\u51b3\u6761\u4ef6 \u4e00\u4e2a Kubernetes \u96c6\u7fa4(\u63a8\u8350 k8s version > 1.22), \u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002 \u786e\u8ba4 calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002 \u786e\u8ba4 calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002 Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177 \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u521b\u5efa Pod \u4f7f\u7528\u7684 SpiderIPPool \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602 \u914d\u7f6e Calico BGP [\u53ef\u9009] \u672c\u4f8b\u5e0c\u671b calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u7684 IP \u6c60\u6240\u5728\u7684\u5b50\u7f51( 10.244.0.0/16 )\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 Pod IP\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002 \u7f51\u7edc\u62d3\u6251\u5982\u4e0b: \u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router \u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR: root@router:~# apt install -y frr FRR \u5f00\u542f BGP \u529f\u80fd: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr \u914d\u7f6e FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy \u914d\u7f6e\u89e3\u91ca: Router \u4fa7\u7684 AS \u4e3a 23000 , \u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a 64512 \u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a ebgp , \u8282\u70b9\u4e4b\u95f4\u4e3a ibgp \u9700\u8981\u5173\u95ed ebgp-requires-policy , \u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb 172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP \u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting \u3002 \u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45 Calico \u9700\u8981\u914d\u7f6e calico_backend: bird , \u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 \u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh : [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' \u521b\u5efa BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740 asNumber \u4e3a BGP Router \u7684 AS \u53f7 \u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. \u66f4\u591a Calico BGP \u914d\u7f6e, \u8bf7\u53c2\u8003 Calico BGP \u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60 \u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60, \u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-ippool spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51: 10.244.0.0/16 \u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a: Never \u5207\u6362 Calico \u7684 IPAM \u4e3a Spiderpool \u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a Calico \u7684 CNI \u914d\u7f6e\u6587\u4ef6: /etc/cni/net.d/10-calico.conflist , \u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" }, \u521b\u5efa\u5e94\u7528 \u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : \u4ece \"nginx-ippool-v4\" SpiderIPPool \u4e2d\u5206\u914d\u56fa\u5b9a IP \u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annnotations \u6307\u5b9a\u7684 ippool: nginx-ippool-v4 \u4e2d\u7ed9 Pod \u5206\u914d IP\u3002 [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false \u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 nginx-ippool-v4 \u7684 IP \u6c60\u8303\u56f4\u5185: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> \u6269\u5bb9\u526f\u672c\u6570\u5230 3 , \u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece IP \u6c60: nginx-ippool-v4 \u4e2d\u5206\u914d: [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> \u67e5\u770b IP \u6c60: nginx-ippool-v4 \u7684 ALLOCATED-IP-COUNT \u65b0\u589e 1 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 25602 false false \u7ed3\u8bba \u7ecf\u8fc7\u6d4b\u8bd5: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38(\u5305\u62ec\u8de8 Calico \u5b50\u7f51)\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\uff0c\u4e3a Deployment\u3001StatefulSet \u7b49\u7c7b\u578b\u5e94\u7528\u63d0\u4f9b\u56fa\u5b9a IP \u529f\u80fd\u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd\u5728 Calico + BGP \u6a21\u5f0f\u4e0b: \u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u73af\u5883\uff0c\u642d\u914d Spiderpool \u5b9e\u73b0\u5e94\u7528\u7684\u56fa\u5b9a IP \u529f\u80fd\uff0c\u8be5\u65b9\u6848\u53ef\u6ee1\u8db3: \u5e94\u7528\u5206\u914d\u5230\u56fa\u5b9a\u7684 IP \u5730\u5740 IP \u6c60\u80fd\u968f\u7740\u5e94\u7528\u526f\u672c\u81ea\u52a8\u6269\u7f29\u5bb9 \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u8df3\u8fc7\u5e94\u7528 IP \u8bbf\u95ee\u5e94\u7528","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_1","text":"\u4e00\u4e2a Kubernetes \u96c6\u7fa4(\u63a8\u8350 k8s version > 1.22), \u5e76\u5b89\u88c5 Calico \u4f5c\u4e3a\u96c6\u7fa4\u7684\u9ed8\u8ba4 CNI\u3002 \u786e\u8ba4 calico \u4e0d\u914d\u7f6e\u4f7f\u7528 IPIP \u6216\u8005 vxlan \u96a7\u9053\uff0c\u56e0\u4e3a\u672c\u4f8b\u5c06\u6f14\u793a\u5982\u4f55\u4f7f\u7528 calico \u5bf9\u63a5 underlay \u7f51\u7edc\u3002 \u786e\u8ba4 calico \u5f00\u542f\u4e86 fullmesh \u65b9\u5f0f\u7684 BGP \u914d\u7f6e\u3002 Helm\u3001Calicoctl \u4e8c\u8fdb\u5236\u5de5\u5177","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#spiderpool","text":"helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u521b\u5efa Pod \u4f7f\u7528\u7684 SpiderIPPool \u5b9e\u4f8b: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-bgp","text":"\u672c\u4f8b\u5e0c\u671b calico \u4ee5 underlay \u65b9\u5f0f\u5de5\u4f5c\uff0c\u5c06 Spiderpool \u7684 IP \u6c60\u6240\u5728\u7684\u5b50\u7f51( 10.244.0.0/16 )\u901a\u8fc7 BGP \u534f\u8bae\u5ba3\u544a\u81f3 BGP Router\uff0c\u786e\u4fdd\u96c6\u7fa4\u5916\u7684\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7 BGP Router \u76f4\u63a5\u8bbf\u95ee Pod \u771f\u5b9e\u7684 IP \u5730\u5740\u3002 \u5982\u679c\u60a8\u5e76\u4e0d\u9700\u8981\u96c6\u7fa4\u5916\u90e8\u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5230 Pod IP\uff0c\u53ef\u5ffd\u7565\u672c\u6b65\u9aa4\u3002 \u7f51\u7edc\u62d3\u6251\u5982\u4e0b: \u914d\u7f6e\u673a\u5668\u5916\u7684\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3a BGP Router \u672c\u6b21\u793a\u4f8b\u5c06\u4e00\u53f0 Ubuntu \u670d\u52a1\u5668\u4f5c\u4e3a BGP Router\u3002\u9700\u8981\u524d\u7f6e\u5b89\u88c5 FRR: root@router:~# apt install -y frr FRR \u5f00\u542f BGP \u529f\u80fd: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr \u914d\u7f6e FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy \u914d\u7f6e\u89e3\u91ca: Router \u4fa7\u7684 AS \u4e3a 23000 , \u96c6\u7fa4\u8282\u70b9\u4fa7 AS \u4e3a 64512 \u3002Router \u4e0e\u8282\u70b9\u4e4b\u95f4\u4e3a ebgp , \u8282\u70b9\u4e4b\u95f4\u4e3a ibgp \u9700\u8981\u5173\u95ed ebgp-requires-policy , \u5426\u5219 BGP \u4f1a\u8bdd\u65e0\u6cd5\u5efa\u7acb 172.16.13.11/21 \u4e3a\u96c6\u7fa4\u8282\u70b9 IP \u66f4\u591a\u914d\u7f6e\u53c2\u8003 frrouting \u3002 \u914d\u7f6e Calico \u7684 BGP \u90bb\u5c45 Calico \u9700\u8981\u914d\u7f6e calico_backend: bird , \u5426\u5219\u65e0\u6cd5\u5efa\u7acb BGP \u4f1a\u8bdd: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 \u672c\u4f8b\u8282\u70b9\u7684\u9ed8\u8ba4\u8def\u7531\u5728 BGP Router, \u8282\u70b9\u4e4b\u95f4\u4e0d\u9700\u8981\u76f8\u4e92\u540c\u6b65\u8def\u7531\uff0c\u53ea\u9700\u8981\u5c06\u5176\u81ea\u8eab\u8def\u7531\u540c\u6b65\u7ed9 BGP Router\uff0c\u6240\u4ee5\u5173\u95ed Calico BGP Full-Mesh : [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' \u521b\u5efa BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP \u4e3a BGP Router \u7684 IP \u5730\u5740 asNumber \u4e3a BGP Router \u7684 AS \u53f7 \u67e5\u770b BGP \u4f1a\u8bdd\u662f\u5426\u6210\u529f\u5efa\u7acb: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. \u66f4\u591a Calico BGP \u914d\u7f6e, \u8bf7\u53c2\u8003 Calico BGP","title":"\u914d\u7f6e Calico BGP [\u53ef\u9009]"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-ip","text":"\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e0e Spiderpool \u5b50\u7f51 CIDR \u76f8\u540c\u7684 Calico IP \u6c60, \u5426\u5219 Calico \u4e0d\u4f1a\u5ba3\u544a Spiderpool \u5b50\u7f51\u7684\u8def\u7531: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-ippool spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF cidr \u9700\u8981\u5bf9\u5e94 Spiderpool \u7684\u5b50\u7f51: 10.244.0.0/16 \u8bbe\u7f6e ipipMode \u548c vxlanMode \u4e3a: Never","title":"\u521b\u5efa\u540c\u5b50\u7f51\u7684 Calico IP \u6c60"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#calico-ipam-spiderpool","text":"\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a Calico \u7684 CNI \u914d\u7f6e\u6587\u4ef6: /etc/cni/net.d/10-calico.conflist , \u5c06 ipam \u5b57\u6bb5\u5207\u6362\u4e3a Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" },","title":"\u5207\u6362 Calico \u7684 IPAM \u4e3a Spiderpool"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_2","text":"\u4ee5 Nginx \u5e94\u7528\u4e3a\u4f8b: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : \u4ece \"nginx-ippool-v4\" SpiderIPPool \u4e2d\u5206\u914d\u56fa\u5b9a IP \u5f53\u5e94\u7528 Pod \u88ab\u521b\u5efa\uff0cSpiderpool \u4ece annnotations \u6307\u5b9a\u7684 ippool: nginx-ippool-v4 \u4e2d\u7ed9 Pod \u5206\u914d IP\u3002 [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false \u5f53\u526f\u672c\u91cd\u542f\uff0c\u5176 IP \u90fd\u88ab\u56fa\u5b9a\u5728 nginx-ippool-v4 \u7684 IP \u6c60\u8303\u56f4\u5185: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> \u6269\u5bb9\u526f\u672c\u6570\u5230 3 , \u65b0\u526f\u672c\u7684 IP \u5730\u5740\u4ecd\u7136\u4ece IP \u6c60: nginx-ippool-v4 \u4e2d\u5206\u914d: [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> \u67e5\u770b IP \u6c60: nginx-ippool-v4 \u7684 ALLOCATED-IP-COUNT \u65b0\u589e 1 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 25602 false false","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-calico-zh_CN/#_3","text":"\u7ecf\u8fc7\u6d4b\u8bd5: \u96c6\u7fa4\u5916\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u901a\u8fc7 Nginx Pod \u7684 IP \u6b63\u5e38\u8bbf\u95ee\uff0c\u96c6\u7fa4\u5185\u90e8\u901a\u8baf Nginx Pod \u8de8\u8282\u70b9\u4e5f\u90fd\u901a\u4fe1\u6b63\u5e38(\u5305\u62ec\u8de8 Calico \u5b50\u7f51)\u3002\u5728 Calico BGP \u6a21\u5f0f\u4e0b\uff0cSpiderpool \u53ef\u642d\u914d Calico \u5b9e\u73b0 Deployment \u7b49\u7c7b\u578b\u5e94\u7528\u56fa\u5b9a IP \u7684\u9700\u6c42\u3002","title":"\u7ed3\u8bba"},{"location":"usage/install/underlay/get-started-calico/","text":"Calico Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements: Assign static IP addresses to applications Scale IP pools dynamically based on replica counts Enable external clients outside the cluster to access applications without their IPs Prerequisites An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI. Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks. Confirm that Calico has enabled BGP configuration in full-mesh mode. Helm and Calicoctl Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Create the SpiderIPPool instance used by the Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF Verify the installation\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602 Configure Calico BGP [optional] In this example, we want Calico to work in underlay mode and announce the subnet where Spiderpool's IPPool resides ( 10.244.0.0/16 ) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router. If you don't need external clients to access pod IPs directly, skip this step. The network topology is as follows: Configure a host outside the cluster as BGP Router We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand: root@router:~# apt install -y frr FRR enable BGP: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr Configure FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy Configuration descriptions: The AS on the router side is 23000 , and the AS on the cluster node side is 64512 . The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp. ebgp-requires-policy needs to be disabled, otherwise the BGP session cannot be established. 172.16.13.11/21 is the IP address of the cluster node. For more information, refer to frrouting . Configure BGP neighbor for Calico calico_backend: bird needs to be configured to establish a BGP session: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled: [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' Create BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP is the IP address of BGP Router asNumber is the AS number of BGP Router Check if the BGP session is established: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. For more information on Calico BGP configuration, refer to Calico BGP . Create a Calico IP pool in the same subnet Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-subnet spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF The CIDR needs to correspond to the subnet of Spiderpool: 10.244.0.0/16 Set ipipMode and vxlanMode to: Never Switch Calico's IPAM to Spiderpool Change the Calico CNI configuration file /etc/cni/net.d/10-calico.conflist on each node to switch the ipam field to Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" }, Create applications Take the Nginx application as an example: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : Assign static IPs from \"nginx-ippool-v4\" SpiderIPPool When the application Pod is created, Spiderpool assigns the IP to the Pod from the ippool: nginx-ippool-v4 specified in the annnotations. [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false When replicas are restarted, their IPs are fixed within the range of the nginx-ippool-v4 IPPool: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> Expand the number of replicas to 3 , the IP address of the new replica is still allocated from the IPPool: nginx-ippool-v4 : [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> View IP pool: Added 1 to ALLOCATED-IP-COUNT of nginx-ippool-v4 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 5 false false Conclusion The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.","title":"Calico"},{"location":"usage/install/underlay/get-started-calico/#calico-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool is able to provide static IPs to Deployments, StatefulSets, and other types of applications in underlay networks. In this page, we'll introduce how to build a complete Underlay network environment in Calico + BGP mode, and integrate it with Spiderpool to enable fixed IP addresses for applications. This solution meets the following requirements: Assign static IP addresses to applications Scale IP pools dynamically based on replica counts Enable external clients outside the cluster to access applications without their IPs","title":"Calico Quick Start"},{"location":"usage/install/underlay/get-started-calico/#prerequisites","text":"An available Kubernetes cluster with a recommended version higher than 1.22, where Calico is installed as the default CNI. Make sure that Calico is not configured to use IPIP or VXLAN tunneling as we'll demonstrate how to use Calico for underlay networks. Confirm that Calico has enabled BGP configuration in full-mesh mode. Helm and Calicoctl","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-calico/#install-spiderpool","text":"helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Create the SpiderIPPool instance used by the Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: nginx-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-244-0-0-16 spec: ips: - 10.244.100.0-10.244.200.1 subnet: 10.244.0.0/16 EOF Verify the installation\uff1a [ root@master ~ ] # kubectl get po -n kube-system |grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@master ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT nginx-ippool-v4 4 10 .244.0.0/16 0 25602","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-calico/#configure-calico-bgp-optional","text":"In this example, we want Calico to work in underlay mode and announce the subnet where Spiderpool's IPPool resides ( 10.244.0.0/16 ) to the BGP router via the BGP protocol, ensuring that clients outside the cluster can directly access the real IP addresses of the Pods through BGP router. If you don't need external clients to access pod IPs directly, skip this step. The network topology is as follows: Configure a host outside the cluster as BGP Router We will use an Ubuntu server as BGP Router. FRR needs to be installed beforehand: root@router:~# apt install -y frr FRR enable BGP: root@router:~# sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons root@router:~# systemctl restart frr Configure FRR: root@router:~# vtysh router# config router ( config ) # router bgp 23000 router ( config ) # bgp router-id 172.16.13.1 router ( config ) # neighbor 172.16.13.11 remote-as 64512 router ( config ) # neighbor 172.16.13.21 remote-as 64512 router ( config ) # no bgp ebgp-requires-policy Configuration descriptions: The AS on the router side is 23000 , and the AS on the cluster node side is 64512 . The BGP neighbor relationship between the router and the node is ebgp, while the relationship between the nodes is ibgp. ebgp-requires-policy needs to be disabled, otherwise the BGP session cannot be established. 172.16.13.11/21 is the IP address of the cluster node. For more information, refer to frrouting . Configure BGP neighbor for Calico calico_backend: bird needs to be configured to establish a BGP session: [ root@master1 ~ ] # kubectl get cm -n kube-system calico-config -o yaml apiVersion: v1 data: calico_backend: bird cluster_type: kubespray,bgp kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"calico_backend\" : \"bird\" , \"cluster_type\" : \"kubespray,bgp\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"name\" : \"calico-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2023-02-26T15:16:35Z\" name: calico-config namespace: kube-system resourceVersion: \"2056\" uid: 001bbd09-9e6f-42c6-9339-39f71f81d363 In this example, the default route for the node is on BGP router. As a result, nodes simply need to synchronize their local routes with BGP Router without synchronizing them with each other. Consequently, Calico BGP Full-Mesh needs to be disabled: [ root@master1 ~ ] # calicoctl patch bgpconfiguration default -p '{\"spec\": {\"nodeToNodeMeshEnabled\": false}}' Create BGPPeer: [ root@master1 ~ ] # cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: my-global-peer spec: peerIP: 172 .16.13.1 asNumber: 23000 EOF peerIP is the IP address of BGP Router asNumber is the AS number of BGP Router Check if the BGP session is established: [ root@master1 ~ ] # calicoctl node status Calico process is running. IPv4 BGP status +--------------+-----------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-----------+-------+------------+-------------+ | 172 .16.13.1 | global | up | 2023 -03-15 | Established | +--------------+-----------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. For more information on Calico BGP configuration, refer to Calico BGP .","title":"Configure Calico BGP [optional]"},{"location":"usage/install/underlay/get-started-calico/#create-a-calico-ip-pool-in-the-same-subnet","text":"Create a Calico IP pool with the same CIDR as the Spiderpool subnet, otherwise Calico won't advertise the route of the Spiderpool subnet: cat << EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: spiderpool-subnet spec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Never natOutgoing: false nodeSelector: all() vxlanMode: Never EOF The CIDR needs to correspond to the subnet of Spiderpool: 10.244.0.0/16 Set ipipMode and vxlanMode to: Never","title":"Create a Calico IP pool in the same subnet"},{"location":"usage/install/underlay/get-started-calico/#switch-calicos-ipam-to-spiderpool","text":"Change the Calico CNI configuration file /etc/cni/net.d/10-calico.conflist on each node to switch the ipam field to Spiderpool: \"ipam\" : { \"type\" : \"spiderpool\" },","title":"Switch Calico's IPAM to Spiderpool"},{"location":"usage/install/underlay/get-started-calico/#create-applications","text":"Take the Nginx application as an example: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"nginx-ippool-v4\"]}' labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ipam.spidernet.io/ippool : Assign static IPs from \"nginx-ippool-v4\" SpiderIPPool When the application Pod is created, Spiderpool assigns the IP to the Pod from the ippool: nginx-ippool-v4 specified in the annnotations. [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 2 25602 false false When replicas are restarted, their IPs are fixed within the range of the nginx-ippool-v4 IPPool: [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 23s 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 23s 10 .244.100.92 master1 <none> <none> Expand the number of replicas to 3 , the IP address of the new replica is still allocated from the IPPool: nginx-ippool-v4 : [ root@master1 ~ ] # kubectl scale deploy nginx --replicas 3 # scale pods deployment.apps/nginx scaled [ root@master1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-644659db67-szgcg 1 /1 Running 0 1m 10 .244.100.90 worker5 <none> <none> nginx-644659db67-98rcg 1 /1 Running 0 1m 10 .244.100.92 master1 <none> <none> nginx-644659db67-brqdg 1 /1 Running 0 10s 10 .244.100.94 master1 <none> <none> View IP pool: Added 1 to ALLOCATED-IP-COUNT of nginx-ippool-v4 : [ root@master1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE nginx-ippool-v4 4 10 .244.0.0/16 3 5 false false","title":"Create applications"},{"location":"usage/install/underlay/get-started-calico/#conclusion","text":"The test result shows that clients outside the cluster can access Nginx Pods directly via their IP addresses. Nginx Pods can also communicate across cluster nodes, including Calico subnets. In Calico BGP mode, Spiderpool can be integrated with Calico to satisfy the fixed IP requirements for Deployments and other types of applications.","title":"Conclusion"},{"location":"usage/install/underlay/get-started-kind-zh_CN/","text":"Kind Quick Start English | \u7b80\u4f53\u4e2d\u6587 Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u90e8\u7f72\u7b26\u5408\u60a8\u9700\u6c42\u7684\u96c6\u7fa4\uff0c\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002 \u5148\u51b3\u6761\u4ef6 \u5df2\u5b89\u88c5 Go \u514b\u9686 Spiderpool \u4ee3\u7801\u4ed3\u5e93\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002 git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u83b7\u53d6 Spiderpool \u7684\u6700\u65b0\u955c\u50cf tag ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name' | head -n 1 ) \u6267\u884c make dev-doctor \uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\uff0c\u5982\u679c\u7f3a\u5c11\u7ec4\u4ef6\u4f1a\u4e3a\u60a8\u81ea\u52a8\u5b89\u88c5\u3002 Spiderpool \u811a\u672c\u652f\u6301\u7684\u591a\u79cd\u5b89\u88c5\u6a21\u5f0f \u5982\u679c\u60a8\u5728\u4e2d\u56fd\u5927\u9646\uff0c\u5b89\u88c5\u65f6\u53ef\u4ee5\u989d\u5916\u6307\u5b9a\u53c2\u6570 -e E2E_CHINA_IMAGE_REGISTRY=true \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u5b89\u88c5 Spiderpool \u5728 Underlay CNI\uff08Macvlan\uff09 \u96c6\u7fa4 ~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u5b89\u88c5 Spiderpool \u5728 Calico Overlay CNI \u96c6\u7fa4 ~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u5728\u542f\u7528\u4e86 kube-proxy \u7684 Cilium \u96c6\u7fa4\u4e2d\u5b89\u88c5 Spiderpool \u786e\u8ba4\u64cd\u4f5c\u7cfb\u7edf Kernel \u7248\u672c\u53f7\u662f\u662f\u5426 >= 4.9.17\uff0c\u5185\u6838\u8fc7\u4f4e\u65f6\u5c06\u4f1a\u5bfc\u81f4\u5b89\u88c5\u5931\u8d25\uff0c\u63a8\u8350 Kernel 5.10+ \u3002 ~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u5728\u542f\u7528\u4e86 ebpf \u7684 Cilium \u96c6\u7fa4\u4e2d\u5b89\u88c5 Spiderpool \u786e\u8ba4\u64cd\u4f5c\u7cfb\u7edf Kernel \u7248\u672c\u53f7\u662f\u662f\u5426 >= 4.9.17\uff0c\u5185\u6838\u8fc7\u4f4e\u65f6\u5c06\u4f1a\u5bfc\u81f4\u5b89\u88c5\u5931\u8d25\uff0c\u63a8\u8350 Kernel 5.10+ \u3002 ~# make e2e_init_cilium_with_ebpf -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG \u9a8c\u8bc1\u5b89\u88c5 \u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002 ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config \u60a8\u53ef\u4ee5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> \u60a8\u4e5f\u53ef\u4ee5\u624b\u52a8\u521b\u5efa\u5e94\u7528\u9a8c\u8bc1 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> \u901a\u8fc7\u6d4b\u8bd5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u5b83\u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684\u66f4\u591a\u529f\u80fd\u3002 \u5378\u8f7d \u5378\u8f7d Kind \u96c6\u7fa4 \u6267\u884c make clean \u5378\u8f7d Kind \u96c6\u7fa4\u3002 \u5220\u9664\u6d4b\u8bd5\u955c\u50cf ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Kind Quick Start"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#kind-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Kind \u662f\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u7684\u5de5\u5177\u3002Spiderpool \u63d0\u4f9b\u4e86\u5b89\u88c5 Kind \u96c6\u7fa4\u7684\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u90e8\u7f72\u7b26\u5408\u60a8\u9700\u6c42\u7684\u96c6\u7fa4\uff0c\u8fdb\u884c Spiderpool \u7684\u6d4b\u8bd5\u4e0e\u4f53\u9a8c\u3002","title":"Kind Quick Start"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#_1","text":"\u5df2\u5b89\u88c5 Go \u514b\u9686 Spiderpool \u4ee3\u7801\u4ed3\u5e93\u5230\u672c\u5730\u4e3b\u673a\u4e0a\uff0c\u5e76\u8fdb\u5165 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u3002 git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u83b7\u53d6 Spiderpool \u7684\u6700\u65b0\u955c\u50cf tag ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name' | head -n 1 ) \u6267\u884c make dev-doctor \uff0c\u68c0\u67e5\u672c\u5730\u4e3b\u673a\u4e0a\u7684\u5f00\u53d1\u5de5\u5177\u662f\u5426\u6ee1\u8db3\u90e8\u7f72 Kind \u96c6\u7fa4\u4e0e Spiderpool \u7684\u6761\u4ef6\uff0c\u5982\u679c\u7f3a\u5c11\u7ec4\u4ef6\u4f1a\u4e3a\u60a8\u81ea\u52a8\u5b89\u88c5\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool","text":"\u5982\u679c\u60a8\u5728\u4e2d\u56fd\u5927\u9646\uff0c\u5b89\u88c5\u65f6\u53ef\u4ee5\u989d\u5916\u6307\u5b9a\u53c2\u6570 -e E2E_CHINA_IMAGE_REGISTRY=true \uff0c\u4ee5\u5e2e\u52a9\u60a8\u66f4\u5feb\u7684\u62c9\u53d6\u955c\u50cf\u3002","title":"Spiderpool \u811a\u672c\u652f\u6301\u7684\u591a\u79cd\u5b89\u88c5\u6a21\u5f0f"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool-underlay-cnimacvlan","text":"~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5b89\u88c5 Spiderpool \u5728 Underlay CNI\uff08Macvlan\uff09 \u96c6\u7fa4"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#spiderpool-calico-overlay-cni","text":"~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5b89\u88c5 Spiderpool \u5728 Calico Overlay CNI \u96c6\u7fa4"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#kube-proxy-cilium-spiderpool","text":"\u786e\u8ba4\u64cd\u4f5c\u7cfb\u7edf Kernel \u7248\u672c\u53f7\u662f\u662f\u5426 >= 4.9.17\uff0c\u5185\u6838\u8fc7\u4f4e\u65f6\u5c06\u4f1a\u5bfc\u81f4\u5b89\u88c5\u5931\u8d25\uff0c\u63a8\u8350 Kernel 5.10+ \u3002 ~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5728\u542f\u7528\u4e86 kube-proxy \u7684 Cilium \u96c6\u7fa4\u4e2d\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#ebpf-cilium-spiderpool","text":"\u786e\u8ba4\u64cd\u4f5c\u7cfb\u7edf Kernel \u7248\u672c\u53f7\u662f\u662f\u5426 >= 4.9.17\uff0c\u5185\u6838\u8fc7\u4f4e\u65f6\u5c06\u4f1a\u5bfc\u81f4\u5b89\u88c5\u5931\u8d25\uff0c\u63a8\u8350 Kernel 5.10+ \u3002 ~# make e2e_init_cilium_with_ebpf -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"\u5728\u542f\u7528\u4e86 ebpf \u7684 Cilium \u96c6\u7fa4\u4e2d\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#_2","text":"\u5728 Spiderpool \u5de5\u7a0b\u7684\u6839\u76ee\u5f55\u4e0b\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4e3a kubectl \u914d\u7f6e Kind \u96c6\u7fa4\u7684 KUBECONFIG\u3002 ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config \u60a8\u53ef\u4ee5\u770b\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u5185\u5bb9\u8f93\u51fa\uff1a ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false Spiderpool \u63d0\u4f9b\u7684\u5feb\u901f\u5b89\u88c5 Kind \u96c6\u7fa4\u811a\u672c\u4f1a\u81ea\u52a8\u4e3a\u60a8\u521b\u5efa\u4e00\u4e2a\u5e94\u7528\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u662f\u5e94\u7528\u7684\u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> \u60a8\u4e5f\u53ef\u4ee5\u624b\u52a8\u521b\u5efa\u5e94\u7528\u9a8c\u8bc1 Kind \u96c6\u7fa4\u662f\u5426\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c\uff0c\u4ee5\u4e0b\u547d\u4ee4\u4f1a\u521b\u5efa 1 \u4e2a\u526f\u672c Deployment\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> \u901a\u8fc7\u6d4b\u8bd5\uff0cKind \u96c6\u7fa4\u4e00\u5207\u6b63\u5e38\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u5b83\u6d4b\u8bd5\u4e0e\u4f53\u9a8c Spiderpool \u7684\u66f4\u591a\u529f\u80fd\u3002","title":"\u9a8c\u8bc1\u5b89\u88c5"},{"location":"usage/install/underlay/get-started-kind-zh_CN/#_3","text":"\u5378\u8f7d Kind \u96c6\u7fa4 \u6267\u884c make clean \u5378\u8f7d Kind \u96c6\u7fa4\u3002 \u5220\u9664\u6d4b\u8bd5\u955c\u50cf ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"\u5378\u8f7d"},{"location":"usage/install/underlay/get-started-kind/","text":"Kind Quick Start English | \u7b80\u4f53\u4e2d\u6587 Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Spiderpool provides a script to install the Kind cluster, you can use it to deploy a cluster that meets your needs, and test and experience Spiderpool. Prerequisites Go has already been installed. Clone the Spiderpool code repository to the local host and go to the root directory of the Spiderpool project. ~# git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool Get the latest image tag of Spiderpool. ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name' | head -n 1 ) Execute make dev-doctor to check that the development tools on the local host meet the conditions for deploying a Kind cluster with Spiderpool, and that the components are automatically installed for you if they are missing. Various installation modes supported by Spiderpool script If you are mainland user who is not available to access ghcr.io, Additional parameter -e E2E_CHINA_IMAGE_REGISTRY=true can be specified during installation to help you pull images faster. Install Spiderpool in Underlay CNI (Macvlan) cluster ~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Install Spiderpool on Calico Overlay CNI cluster ~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Install Spiderpool in Cilium cluster with kube-proxy enabled Confirm whether the operating system Kernel version number is >= 4.9.17. If the kernel is too low, the installation will fail. Kernel 5.10+ is recommended. ~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Installing Spiderpool in Cilium cluster with ebpf enabled Confirm whether the operating system Kernel version number is >= 4.9.17. If the kernel is too low, the installation will fail. Kernel 5.10+ is recommended. ~# make e2e_init_cilium_with_ebpf -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG Check that everything is working Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl. ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config It should be possible to observe the following: ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application: ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> You can also manually create an application to verify that the cluster is available, the following command will create 1 copy of Deployment: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> As tested, everything works fine with the Kind cluster. You can test and experience more features of Spiderpool based on kind clusters. Uninstall Uninstall a Kind cluster Execute make clean to uninstall the Kind cluster. Delete test's images ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Kind"},{"location":"usage/install/underlay/get-started-kind/#kind-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Spiderpool provides a script to install the Kind cluster, you can use it to deploy a cluster that meets your needs, and test and experience Spiderpool.","title":"Kind Quick Start"},{"location":"usage/install/underlay/get-started-kind/#prerequisites","text":"Go has already been installed. Clone the Spiderpool code repository to the local host and go to the root directory of the Spiderpool project. ~# git clone https://github.com/spidernet-io/spiderpool.git && cd spiderpool Get the latest image tag of Spiderpool. ~# SPIDERPOOL_LATEST_IMAGE_TAG = $( curl -s https://api.github.com/repos/spidernet-io/spiderpool/releases | jq -r '.[].tag_name' | head -n 1 ) Execute make dev-doctor to check that the development tools on the local host meet the conditions for deploying a Kind cluster with Spiderpool, and that the components are automatically installed for you if they are missing.","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-kind/#various-installation-modes-supported-by-spiderpool-script","text":"If you are mainland user who is not available to access ghcr.io, Additional parameter -e E2E_CHINA_IMAGE_REGISTRY=true can be specified during installation to help you pull images faster.","title":"Various installation modes supported by Spiderpool script"},{"location":"usage/install/underlay/get-started-kind/#install-spiderpool-in-underlay-cni-macvlan-cluster","text":"~# make e2e_init_underlay -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Install Spiderpool in Underlay CNI (Macvlan) cluster"},{"location":"usage/install/underlay/get-started-kind/#install-spiderpool-on-calico-overlay-cni-cluster","text":"~# make e2e_init_overlay_calico -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Install Spiderpool on Calico Overlay CNI cluster"},{"location":"usage/install/underlay/get-started-kind/#install-spiderpool-in-cilium-cluster-with-kube-proxy-enabled","text":"Confirm whether the operating system Kernel version number is >= 4.9.17. If the kernel is too low, the installation will fail. Kernel 5.10+ is recommended. ~# make e2e_init_overlay_cilium -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Install Spiderpool in Cilium cluster with kube-proxy enabled"},{"location":"usage/install/underlay/get-started-kind/#installing-spiderpool-in-cilium-cluster-with-ebpf-enabled","text":"Confirm whether the operating system Kernel version number is >= 4.9.17. If the kernel is too low, the installation will fail. Kernel 5.10+ is recommended. ~# make e2e_init_cilium_with_ebpf -e E2E_SPIDERPOOL_TAG = $SPIDERPOOL_LATEST_IMAGE_TAG","title":"Installing Spiderpool in Cilium cluster with ebpf enabled"},{"location":"usage/install/underlay/get-started-kind/#check-that-everything-is-working","text":"Execute the following command in the root directory of the Spiderpool project to configure KUBECONFIG for the Kind cluster for kubectl. ~# export KUBECONFIG = $( pwd ) /test/.cluster/spider/.kube/config It should be possible to observe the following: ~# kubectl get nodes NAME STATUS ROLES AGE VERSION spider-control-plane Ready control-plane 2m29s v1.26.2 spider-worker Ready <none> 2m58s v1.26.2 ~# kubectll get po -n kube-sysem | grep spiderpool NAME READY STATUS RESTARTS AGE spiderpool-agent-4dr97 1 /1 Running 0 3m spiderpool-agent-4fkm4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-c5dk4 1 /1 Running 0 3m spiderpool-controller-7864477fc7-wpgjn 1 /1 Running 0 3m spiderpool-init 0 /1 Completed 0 3m ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT default-v4-ippool 4 172 .18.0.0/16 5 253 true default-v6-ippool 6 fc00:f853:ccd:e793::/64 5 253 true vlan100-v4 4 172 .100.0.0/16 0 2559 false vlan100-v6 6 fd00:172:100::/64 0 65009 false vlan100-v4 4 172 .200.0.0/16 0 2559 false vlan200-v6 6 fd00:172:200::/64 0 65009 false The Quick Install Kind Cluster script provided by Spiderpool will automatically create an application for you to verify that your Kind cluster is working properly and the following is the running state of the application: ~# kubectl get po -l app = test-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-pod-856f9689d-876nm 1 /1 Running 0 5m34s 172 .18.40.63 spider-worker <none> <none> You can also manually create an application to verify that the cluster is available, the following command will create 1 copy of Deployment: cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 1 selector: matchLabels: app: test-app template: metadata: labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-84d5699474-dbtl5 1 /1 Running 0 6m23s 172 .18.40.112 spider-control-plane <none> <none> As tested, everything works fine with the Kind cluster. You can test and experience more features of Spiderpool based on kind clusters.","title":"Check that everything is working"},{"location":"usage/install/underlay/get-started-kind/#uninstall","text":"Uninstall a Kind cluster Execute make clean to uninstall the Kind cluster. Delete test's images ~# docker rmi -f $( docker images | grep spiderpool | awk '{print $3}' ) ~# docker rmi -f $( docker images | grep multus | awk '{print $3}' )","title":"Uninstall"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/","text":"Macvlan Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Macvlan \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5df2\u5b89\u88c5 Helm \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Iface r \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 \u521b\u5efa\u4e0e\u7f51\u7edc\u63a5\u53e3 eth0 \u5728\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u6c60\u4ee5\u4f9b Pod \u4f7f\u7528\uff0c\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF \u9a8c\u8bc1\u5b89\u88c5 ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false \u521b\u5efa CNI \u914d\u7f6e Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528 \u82e5\u6709 vlan \u7684\u9700\u6c42\uff0c\u53ef\u5728 spec.vlanID \u5b57\u6bb5\u4e2d\u6307\u5b9a vlan \u53f7\uff0c\u6211\u4eec\u5c06\u4f1a\u4e3a\u7f51\u5361\u521b\u5efa\u5bf9\u5e94\u7684 vlan \u5b50\u63a5\u53e3 \u6211\u4eec\u8fd8\u63d0\u4f9b\u5bf9\u7f51\u5361 bond \u7684\u652f\u6301\uff0c\u53ea\u9700\u5728 spec.bond.name \u548c spec.bond.mode \u91cc\u6307\u5b9a bond \u7f51\u5361\u7684\u540d\u5b57\u548c\u6a21\u5f0f\u5373\u53ef\u3002\u968f\u540e\u6211\u4eec\u4f1a\u81ea\u52a8\u4e3a\u4f60\u5b9e\u73b0\u591a\u5f20\u7f51\u5361 bond \u6210\u4e00\u5f20\u7f51\u5361\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms \u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a \u67e5\u770b service \u7684 IP\uff1a ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#macvlan-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Macvlan \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u5177\u5907\u591a\u5f20 Underlay \u7f51\u5361\uff0c\u901a\u8fbe\u591a\u4e2a Underlay \u5b50\u7f51 Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#_1","text":"\u51c6\u5907\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u5df2\u5b89\u88c5 Helm \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Iface r \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" \u5982\u679c\u60a8\u7684\u96c6\u7fa4\u672a\u5b89\u88c5 Macvlan CNI, \u53ef\u6307\u5b9a Helm \u53c2\u6570 --set plugins.installCNI=true \u5b89\u88c5 Macvlan \u5230\u6bcf\u4e2a\u8282\u70b9\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 \u521b\u5efa\u4e0e\u7f51\u7edc\u63a5\u53e3 eth0 \u5728\u540c\u4e00\u4e2a\u5b50\u7f51\u7684 IP \u6c60\u4ee5\u4f9b Pod \u4f7f\u7528\uff0c\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF \u9a8c\u8bc1\u5b89\u88c5 ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#cni","text":"Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Macvlan SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 Macvlan \u6240\u9700\u7684\u5bbf\u4e3b\u673a\u7236\u63a5\u53e3\uff0c\u672c\u4f8b\u5b50\u4ee5\u5bbf\u4e3b\u673a eth0 \u7f51\u5361\u4e3a\u4f8b\uff0c\u4ece\u8be5\u7f51\u5361\u521b\u5efa Macvlan \u5b50\u63a5\u53e3\u7ed9 Pod \u4f7f\u7528 \u82e5\u6709 vlan \u7684\u9700\u6c42\uff0c\u53ef\u5728 spec.vlanID \u5b57\u6bb5\u4e2d\u6307\u5b9a vlan \u53f7\uff0c\u6211\u4eec\u5c06\u4f1a\u4e3a\u7f51\u5361\u521b\u5efa\u5bf9\u5e94\u7684 vlan \u5b50\u63a5\u53e3 \u6211\u4eec\u8fd8\u63d0\u4f9b\u5bf9\u7f51\u5361 bond \u7684\u652f\u6301\uff0c\u53ea\u9700\u5728 spec.bond.name \u548c spec.bond.mode \u91cc\u6307\u5b9a bond \u7f51\u5361\u7684\u540d\u5b57\u548c\u6a21\u5f0f\u5373\u53ef\u3002\u968f\u540e\u6211\u4eec\u4f1a\u81ea\u52a8\u4e3a\u4f60\u5b9e\u73b0\u591a\u5f20\u7f51\u5361 bond \u6210\u4e00\u5f20\u7f51\u5361\u3002 MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF \u5728\u672c\u6587\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0a\u914d\u7f6e\uff0c\u521b\u5efa\u5982\u4e0b\u7684 Macvlan SpiderMultusConfig\uff0c\u5c06\u57fa\u4e8e\u5b83\u81ea\u52a8\u751f\u6210\u7684 Multus NetworkAttachmentDefinition CR\uff0c\u5b83\u5bf9\u5e94\u4e86\u5bbf\u4e3b\u673a\u7684 eth0 \u7f51\u5361\u3002 ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"\u521b\u5efa CNI \u914d\u7f6e"},{"location":"usage/install/underlay/get-started-macvlan-zh_CN/#_2","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff1a ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms \u6d4b\u8bd5 Pod \u4e0e service IP \u7684\u901a\u8baf\u60c5\u51b5\uff1a \u67e5\u770b service \u7684 IP\uff1a ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 service \uff1a ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-macvlan/","text":"Macvlan Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Macvlan and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort. Prerequisites Make sure a Kubernetes cluster is ready. Helm has been already installed. If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If Macvlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install Macvlan in your cluster. If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Create a SpiderIPPool instance. Create an IP Pool in the same subnet as the network interface eth0 for Pods to use, the following is an example of creating a related SpiderIPPool: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF Verify installation ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false Create CNI configuration To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating a Macvlan SpiderMultusConfig configuration: Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0. If there is a VLAN requirement, you can specify the VLAN ID in the spec.vlanID field. We will create the corresponding VLAN sub-interface for the network card. We also provide support for network card bonding. Just specify the name of the bond network card and its mode in the spec.bond.name and spec.bond.mode respectively. We will automatically combine multiple network cards into one bonded network card for you. MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF In the example of this article, use the above configuration to create the following Macvlan SpiderMultusConfig, which will automatically generate Multus NetworkAttachmentDefinition CR based on it, which corresponds to the eth0 network card of the host. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m Create applications Create test Pods and service via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the status of Pods: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s Test the communication between Pods: ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms Test the communication between Pods and service IP: ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Macvlan"},{"location":"usage/install/underlay/get-started-macvlan/#macvlan-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , Macvlan and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.","title":"Macvlan Quick Start"},{"location":"usage/install/underlay/get-started-macvlan/#prerequisites","text":"Make sure a Kubernetes cluster is ready. Helm has been already installed. If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-macvlan/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"macvlan-conf\" If Macvlan is not installed in your cluster, you can specify the Helm parameter --set plugins.installCNI=true to install Macvlan in your cluster. If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Create a SpiderIPPool instance. Create an IP Pool in the same subnet as the network interface eth0 for Pods to use, the following is an example of creating a related SpiderIPPool: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/macvlan-conf EOF Verify installation ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE ippool-test 4 172 .18.0.0/16 0 10 false","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-macvlan/#create-cni-configuration","text":"To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating a Macvlan SpiderMultusConfig configuration: Verify the required host parent interface for Macvlan. In this case, a Macvlan sub-interface will be created for Pods from the host parent interface --eth0. If there is a VLAN requirement, you can specify the VLAN ID in the spec.vlanID field. We will create the corresponding VLAN sub-interface for the network card. We also provide support for network card bonding. Just specify the name of the bond network card and its mode in the spec.bond.name and spec.bond.mode respectively. We will automatically combine multiple network cards into one bonded network card for you. MACVLAN_MASTER_INTERFACE = \"eth0\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: macvlan-conf namespace: kube-system spec: cniType: macvlan macvlan: master: - ${MACVLAN_MASTER_INTERFACE} EOF In the example of this article, use the above configuration to create the following Macvlan SpiderMultusConfig, which will automatically generate Multus NetworkAttachmentDefinition CR based on it, which corresponds to the eth0 network card of the host. ~# kubectl get spidermultusconfigs.spiderpool.spidernet.io -n kube-system NAME AGE macvlan-conf 10m ~# kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system NAME AGE macvlan-conf 10m","title":"Create CNI configuration"},{"location":"usage/install/underlay/get-started-macvlan/#create-applications","text":"Create test Pods and service via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/macvlan-conf labels: app: test-app spec: containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: test-app-svc labels: app: test-app spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: test-app EOF Check the status of Pods: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-f9f94688-2srj7 1 /1 Running 0 2m13s 172 .18.30.139 ipv4-worker <none> <none> test-app-f9f94688-8982v 1 /1 Running 0 2m13s 172 .18.30.138 ipv4-control-plane <none> <none> Spiderpool has created fixed IP pools for applications, ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 2 10 false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME test-app-f9f94688-2srj7 eth0 ippool-test 172 .18.30.139/16 ipv4-worker 3m5s test-app-f9f94688-8982v eth0 ippool-test 172 .18.30.138/16 ipv4-control-plane 3m5s Test the communication between Pods: ~# kubectl exec -ti test-app-f9f94688-2srj7 -- ping 172 .18.30.138 -c 2 PING 172 .18.30.138 ( 172 .18.30.138 ) : 56 data bytes 64 bytes from 172 .18.30.138: seq = 0 ttl = 64 time = 1 .524 ms 64 bytes from 172 .18.30.138: seq = 1 ttl = 64 time = 0 .194 ms --- 172 .18.30.138 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .194/0.859/1.524 ms Test the communication between Pods and service IP: ~# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 20h test-app-svc ClusterIP 10 .96.190.4 <none> 80 /TCP 109m ~# kubectl exec -ti test-app-85cf87dc9c-7dm7m -- curl 10 .96.190.4:80 -I HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Thu, 23 Mar 2023 05 :01:04 GMT Content-Type: text/html Content-Length: 4055 Last-Modified: Fri, 23 Sep 2022 02 :53:30 GMT Connection: keep-alive ETag: \"632d1faa-fd7\" Accept-Ranges: bytes","title":"Create applications"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/","text":"Ovs-cni Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Ovs-cni \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002 ovs-cni \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002 \u5148\u51b3\u6761\u4ef6 \u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u5b89\u88c5\u8bf4\u660e \u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002 ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Iface r \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true \u5982\u679c\u672a\u5b89\u88c5 ovs-cni, \u53ef\u4ee5\u901a\u8fc7 Helm \u53c2\u6570 '-set plugins.installOvsCNI=true' \u5b89\u88c5\u5b83\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u4ee5\u5e2e\u52a9\u60a8\u5feb\u901f\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u914d\u7f6e Open vSwitch \u7f51\u6865\u3002 \u521b\u5efa\u7f51\u6865\u5e76\u914d\u7f6e\u7f51\u6865\uff0c\u4ee5 eth0 \u4e3a\u4f8b\u3002 ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801> dev br1 ~# ip link set br1 up ~# ip route add default via <\u9ed8\u8ba4\u7f51\u5173IP> dev br1 \u8bf7\u628a\u4ee5\u4e0a\u547d\u4ee4\u914d\u7f6e\u5728\u7cfb\u7edf\u884c\u52a8\u811a\u672c\u4e2d\uff0c\u4ee5\u5728\u4e3b\u673a\u91cd\u542f\u65f6\u80fd\u591f\u751f\u6548 \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5 IP \u6c60\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5 IP \u6c60\u7684\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Ovs SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u7f51\u6865\u540d\u79f0\uff0c\u672c\u4f8b\u5b50\u4ee5 br1 \u4e3a\u4f8b: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF \u521b\u5efa\u5e94\u7528 \u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool \u4e3a\u5e94\u7528\u5206\u914d\u4e86 IP\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#ovs-cni-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c Underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 Ovs-cni \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u5c06\u53ef\u7528\u7684\u7f51\u6865\u516c\u5f00\u4e3a\u8282\u70b9\u8d44\u6e90\uff0c\u4f9b\u96c6\u7fa4\u4f7f\u7528\u3002 ovs-cni \u662f\u4e00\u4e2a\u57fa\u4e8e Open vSwitch\uff08OVS\uff09\u7684 Kubernetes CNI \u63d2\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728 Kubernetes \u96c6\u7fa4\u4e2d\u4f7f\u7528 OVS \u8fdb\u884c\u7f51\u7edc\u865a\u62df\u5316\u7684\u65b9\u5f0f\u3002","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#_1","text":"\u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u5fc5\u987b\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Open vSwitch\uff0c\u53ef\u53c2\u8003 \u5b98\u65b9\u5b89\u88c5\u8bf4\u660e \u4ee5\u4e0b\u793a\u4f8b\u662f\u57fa\u4e8e Ubuntu 22.04.1\u3002\u4e3b\u673a\u7cfb\u7edf\u4e0d\u540c\uff0c\u5b89\u88c5\u65b9\u5f0f\u53ef\u80fd\u4e0d\u540c\u3002 ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Iface r \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true \u5982\u679c\u672a\u5b89\u88c5 ovs-cni, \u53ef\u4ee5\u901a\u8fc7 Helm \u53c2\u6570 '-set plugins.installOvsCNI=true' \u5b89\u88c5\u5b83\u3002 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u4ee5\u5e2e\u52a9\u60a8\u5feb\u901f\u7684\u62c9\u53d6\u955c\u50cf\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u914d\u7f6e Open vSwitch \u7f51\u6865\u3002 \u521b\u5efa\u7f51\u6865\u5e76\u914d\u7f6e\u7f51\u6865\uff0c\u4ee5 eth0 \u4e3a\u4f8b\u3002 ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP\u5730\u5740>/<\u5b50\u7f51\u63a9\u7801> dev br1 ~# ip link set br1 up ~# ip route add default via <\u9ed8\u8ba4\u7f51\u5173IP> dev br1 \u8bf7\u628a\u4ee5\u4e0a\u547d\u4ee4\u914d\u7f6e\u5728\u7cfb\u7edf\u884c\u52a8\u811a\u672c\u4e2d\uff0c\u4ee5\u5728\u4e3b\u673a\u91cd\u542f\u65f6\u80fd\u591f\u751f\u6548 \u521b\u5efa\u540e\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u5230\u5982\u4e0b\u7684\u7f51\u6865\u4fe1\u606f\uff1a ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5 IP \u6c60\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5 IP \u6c60\u7684\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002\u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF \u9a8c\u8bc1\u5b89\u88c5\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# Spiderpool \u4e3a\u7b80\u5316\u4e66\u5199 JSON \u683c\u5f0f\u7684 Multus CNI \u914d\u7f6e\uff0c\u5b83\u63d0\u4f9b\u4e86 SpiderMultusConfig CR \u6765\u81ea\u52a8\u7ba1\u7406 Multus NetworkAttachmentDefinition CR\u3002\u5982\u4e0b\u662f\u521b\u5efa Ovs SpiderMultusConfig \u914d\u7f6e\u7684\u793a\u4f8b\uff1a \u786e\u8ba4 ovs-cni \u6240\u9700\u7684\u7f51\u6865\u540d\u79f0\uff0c\u672c\u4f8b\u5b50\u4ee5 br1 \u4e3a\u4f8b: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-ovs-zh_CN/#_2","text":"\u4ee5\u4e0b\u7684\u793a\u4f8b Yaml \u4e2d\uff0c \u4f1a\u521b\u5efa 2 \u4e2a\u526f\u672c\u7684 Deployment\uff0c\u5176\u4e2d\uff1a v1.multus-cni.io/default-network \uff1a\u7528\u4e8e\u6307\u5b9a Multus \u7684 NetworkAttachmentDefinition \u914d\u7f6e\uff0c\u4f1a\u57fa\u4e8e\u5b83\u4e3a\u5e94\u7528\u521b\u5efa\u4e00\u5f20\u9ed8\u8ba4\u7f51\u5361\u3002 cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool \u4e3a\u5e94\u7528\u5206\u914d\u4e86 IP\uff0c\u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185\uff1a ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf\u60c5\u51b5\uff0c\u4ee5\u8de8\u8282\u70b9 Pod \u4e3a\u4f8b\uff1a ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-ovs/","text":"Ovs-cni Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus , Ovs-cni , and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster. ovs-cni is a Kubernetes CNI plugin that utilizes Open vSwitch (OVS) to enable network virtualization within a Kubernetes cluster. Prerequisites Make sure a multi-node Kubernetes cluster is ready. Helm has been already installed. Open vSwitch must be installed and running on the host. It could refer to Installation . The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system. ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true If ovs-cni is not installed, you can install it by specifying the Helm parameter --set plugins.installOvsCNI=true . If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. To configure Open vSwitch bridges on each node: Create a bridge and configure it using `eth0`` as an example. ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP address>/<subnet mask> dev br1 ~# ip link set br1 up ~# ip route add default via <default gateway IP> dev br1 Pleade include these commands in your system startup script to ensure they take effect after host restarts. After creating the bridge, you will be able to view its information on each node: ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" Create a SpiderIPPool instance. The Pod will obtain an IP address from the IP pool for underlying network communication, so the subnet of the IP pool needs to correspond to the underlying subnet being accessed. Here is an example of creating a SpiderSubnet instance: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF Verify the installation\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an ovs-cni SpiderMultusConfig configuration: Confirm the bridge name for ovs-cni. Take the host bridge: br1 as an example: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF Create Applications In the following example Yaml, 2 copies of the Deployment are created, of which: v1.multus-cni.io/default-network : used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool assigns an IP to the application, and the application's IP will be automatically fixed within this IP range: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane Testing Pod communication with cross-node Pods: ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Ovs"},{"location":"usage/install/underlay/get-started-ovs/#ovs-cni-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool can be used as a solution to provide fixed IPs in an Underlay network scenario, and this article will use Multus , Ovs-cni , and Spiderpool as examples to build a complete Underlay network solution that exposes the available bridges as node resources for use by the cluster. ovs-cni is a Kubernetes CNI plugin that utilizes Open vSwitch (OVS) to enable network virtualization within a Kubernetes cluster.","title":"Ovs-cni Quick Start"},{"location":"usage/install/underlay/get-started-ovs/#prerequisites","text":"Make sure a multi-node Kubernetes cluster is ready. Helm has been already installed. Open vSwitch must be installed and running on the host. It could refer to Installation . The following examples are based on Ubuntu 22.04.1. installation may vary depending on the host system. ~# sudo apt-get install -y openvswitch-switch ~# sudo systemctl start openvswitch-switch If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-ovs/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.defaultCniCRName = \"ovs-conf\" --set plugins.installOvsCNI = true If ovs-cni is not installed, you can install it by specifying the Helm parameter --set plugins.installOvsCNI=true . If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. To configure Open vSwitch bridges on each node: Create a bridge and configure it using `eth0`` as an example. ~# ovs-vsctl add-br br1 ~# ovs-vsctl add-port br1 eth0 ~# ip addr add <IP address>/<subnet mask> dev br1 ~# ip link set br1 up ~# ip route add default via <default gateway IP> dev br1 Pleade include these commands in your system startup script to ensure they take effect after host restarts. After creating the bridge, you will be able to view its information on each node: ~# ovs-vsctl show ec16d9e1-6187-4b21-9c2f-8b6cb75434b9 Bridge br1 Port eth0 Interface eth0 Port br1 Interface br1 type: internal Port veth97fb4795 Interface veth97fb4795 ovs_version: \"2.17.3\" Create a SpiderIPPool instance. The Pod will obtain an IP address from the IP pool for underlying network communication, so the subnet of the IP pool needs to correspond to the underlying subnet being accessed. Here is an example of creating a SpiderSubnet instance: cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: ips: - \"172.18.30.131-172.18.30.140\" subnet: 172.18.0.0/16 gateway: 172.18.0.1 multusName: - kube-system/ovs-conf EOF Verify the installation\uff1a ~# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m ~# kubectl get sp ippool-test NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT ippool-test 4 172 .18.0.0/16 0 10 false ~# To simplify writing Multus CNI configuration in JSON format, Spiderpool provides SpiderMultusConfig CR to automatically manage Multus NetworkAttachmentDefinition CR. Here is an example of creating an ovs-cni SpiderMultusConfig configuration: Confirm the bridge name for ovs-cni. Take the host bridge: br1 as an example: BRIDGE_NAME = \"br1\" cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: ovs-conf namespace: kube-system spec: cniType: ovs ovs: bridge: \"${BRIDGE_NAME}\" EOF","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-ovs/#create-applications","text":"In the following example Yaml, 2 copies of the Deployment are created, of which: v1.multus-cni.io/default-network : used to specify Multus' NetworkAttachmentDefinition configuration, which will create a default NIC for the application. cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-app spec: replicas: 2 selector: matchLabels: app: test-app template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"ipv4\": [\"ippool-test\"] } v1.multus-cni.io/default-network: kube-system/ovs-conf labels: app: test-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: test-app topologyKey: kubernetes.io/hostname containers: - name: test-app image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP EOF SpiderIPPool assigns an IP to the application, and the application's IP will be automatically fixed within this IP range: ~# kubectl get po -l app = test-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-app-6f8dddd88d-hstg7 1 /1 Running 0 3m37s 172 .18.30.131 ipv4-worker <none> <none> test-app-6f8dddd88d-rj7sm 1 /1 Running 0 3m37s 172 .18.30.132 ipv4-control-plane <none> <none> ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 172 .18.0.0/16 2 2 false false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE test-app-6f8dddd88d-hstg7 eth0 ippool-test 172 .18.30.131/16 ipv4-worker test-app-6f8dddd88d-rj7sm eth0 ippool-test 172 .18.30.132/16 ipv4-control-plane Testing Pod communication with cross-node Pods: ~#kubectl exec -ti test-app-6f8dddd88d-hstg7 -- ping 172 .18.30.132 -c 2 PING 172 .18.30.132 ( 172 .18.30.132 ) : 56 data bytes 64 bytes from 172 .18.30.132: seq = 0 ttl = 64 time = 1 .882 ms 64 bytes from 172 .18.30.132: seq = 1 ttl = 64 time = 0 .195 ms --- 172 .18.30.132 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .195/1.038/1.882 ms","title":"Create Applications"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/","text":"SR-IOV Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 SR-IOV \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u7684\u7f51\u5361\u5177\u6709 SR-IOV \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1 \u5148\u51b3\u6761\u4ef6 \u4e00\u4e2a Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361 \u67e5\u8be2\u7f51\u5361 bus-info\uff1a ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 \u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 Single Root I/O Virtualization (SR-IOV) \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Ifacer \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager \u5b89\u88c5 Spiderpool \u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" \u5e26\u4e0a helm \u9009\u9879 --set sriov.install=true \uff0c \u4f1a\u5b89\u88c5 sriov-network-operator \uff0cresourcePrefix \u9ed8\u8ba4\u4e3a \"spidernet.io\"\uff0c\u53ef\u901a\u8fc7 helm \u9009\u9879 --set sriov.resourcePrefix \u4fee\u6539 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7ed9\u5e0c\u671b\u8fd0\u884c SR-IOV CNI \u7684\u8282\u70b9\uff0c\u6309\u7167\u5982\u4e0b\u547d\u4ee4\u6253\u4e0a label\uff0c\u8fd9\u6837\uff0csriov-network-operator \u624d\u4f1a\u5728\u6307\u5b9a\u7684\u8282\u70b9\u4e0a\u5b89\u88c5\u7ec4\u4ef6 kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" \u5728\u8282\u70b9\u4e0a\u521b\u5efa VF \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u8282\u70b9\u4e0a\u7684\u53ef\u7528\u7f51\u5361 $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded \u5982\u679c SriovNetworkNodeState CRs \u7684\u72b6\u6001\u4e3a InProgress , \u8bf4\u660e sriov-operator \u6b63\u5728\u540c\u6b65\u8282\u70b9\u72b6\u6001\uff0c\u7b49\u5f85\u72b6\u6001\u4e3a Succeeded \u8bf4\u660e\u540c\u6b65\u5b8c\u6210\u3002\u67e5\u770b CR, \u786e\u8ba4 sriov-network-operator \u5df2\u7ecf\u53d1\u73b0\u8282\u70b9\u4e0a\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361\u3002 \u4ece\u4e0a\u9762\u53ef\u77e5\uff0c\u8282\u70b9 node-1 \u4e0a\u7684\u7f51\u5361 enp4s0f0np0 \u5177\u6709 SR-IOV \u529f\u80fd\uff0c\u5e76\u4e14\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\u4e3a 8\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7\u521b\u5efa SriovNetworkNodePolicy CRs \u5e76\u901a\u8fc7 nicSelector.pfNames \u6307\u5b9a PF (Physical function, \u7269\u7406\u7f51\u5361)\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u8282\u70b9\u4e0a\u7684\u8fd9\u4e9b\u7f51\u5361\u521b\u5efa\u51fa VF(Virtual Function): $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # \u6e34\u671b\u7684 VFs \u6570\u91cf resourceName: sriov_netdevice EOF \u4e0b\u53d1\u5982\u4e0a\u547d\u4ee4\u540e, \u56e0\u4e3a\u9700\u8981\u914d\u7f6e\u8282\u70b9\u542f\u7528 SR-IOV \u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u91cd\u542f\u8282\u70b9\u3002\u5982\u6709\u9700\u8981\uff0c\u6307\u5b9a\u5de5\u4f5c\u8282\u70b9\u800c\u975e Master \u8282\u70b9\u3002 resourceName \u4e0d\u80fd\u4e3a\u7279\u6b8a\u5b57\u7b26\uff0c\u652f\u6301\u7684\u5b57\u7b26: [0-9],[a-zA-Z] \u548c \"_\"\u3002 \u5728\u4e0b\u53d1 SriovNetworkNodePolicy CRs \u4e4b\u540e\uff0c\u518d\u6b21\u67e5\u770b SriovNetworkNodeState CRs \u7684\u72b6\u6001, \u53ef\u4ee5\u770b\u89c1 status \u4e2d VF \u5df2\u7ecf\u5f97\u5230\u914d\u7f6e: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... \u67e5\u770b Node \u53d1\u73b0\u540d\u4e3a spidernet.io/sriov_netdevice \u7684 SR-IOV \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d VF \u7684\u6570\u91cf\u4e3a 8: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } sriov-network-config-daemon Pod \u8d1f\u8d23\u5728\u8282\u70b9\u4e0a\u914d\u7f6e VF \uff0c\u5176\u4f1a\u987a\u5e8f\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u5b8c\u6210\u8be5\u5de5\u4f5c\u3002\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u914d\u7f6e VF \u65f6\uff0csriov-network-config-daemon \u4f1a\u5bf9\u8282\u70b9\u4e0a\u7684\u6240\u6709 Pod \u8fdb\u884c\u9a71\u9010\uff0c\u914d\u7f6e VF \uff0c\u5e76\u53ef\u80fd\u91cd\u542f\u8282\u70b9\u3002\u5f53 sriov-network-config-daemon \u9a71\u9010\u67d0\u4e2a Pod \u5931\u8d25\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6240\u6709\u6d41\u7a0b\u90fd\u505c\u6ede\uff0c\u4ece\u800c\u5bfc\u81f4 node \u7684 VF \u6570\u91cf\u4e00\u76f4\u4e3a 0\u3002 \u8fd9\u79cd\u60c5\u51b5\u65f6\uff0csriov-network-config-daemon Pod \u4f1a\u770b\u5230\u5982\u4e0b\u7c7b\u4f3c\u65e5\u5fd7\uff1a error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... \u8be5\u95ee\u9898\u53ef\u53c2\u8003 sriov-network-operator \u793e\u533a\u7684\u7c7b\u4f3c issue \u6b64\u65f6\uff0c\u53ef\u6392\u67e5\u6307\u5b9a Pod \u4e3a\u5565\u65e0\u6cd5\u9a71\u9010\u7684\u539f\u56e0\uff0c\u6709\u5982\u4e0b\u53ef\u80fd\uff1a \uff081\uff09\u8be5\u9a71\u9010\u5931\u8d25\u7684 Pod \u53ef\u80fd\u914d\u7f6e\u4e86 PodDisruptionBudget\uff0c\u5bfc\u81f4\u53ef\u7528\u526f\u672c\u6570\u4e0d\u8db3\u3002\u8bf7\u8c03\u6574 PodDisruptionBudget \uff082\uff09\u96c6\u7fa4\u4e2d\u7684\u53ef\u7528\u8282\u70b9\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6ca1\u6709\u8282\u70b9\u53ef\u4ee5\u8c03\u5ea6 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF \u521b\u5efa SpiderMultusConfig \u5b9e\u4f8b\u3002 \u6ce8\u610f: \u5982\u679c\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u662f\u4f7f\u7528 NetworkManager \u7684 OS\uff0c\u6bd4\u5982 Fedora Centos\u7b49\uff0c\u5f3a\u70c8\u5efa\u8bae\u914d\u7f6e NetworkManager \u7684\u914d\u7f6e\u6587\u4ef6(/etc/NetworkManager/conf.d/spidernet.conf)\uff0c\u907f\u514d NetworkManager \u5e72\u6270 coordinator \u521b\u5efa\u7684 Veth \u865a\u62df\u63a5\u53e3\uff0c\u5f71\u54cd\u901a\u4fe1: ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice EOF SpiderIPPool.Spec.multusName: kube-system/sriov-test \u8981\u548c\u521b\u5efa\u7684 SpiderMultusConfig \u5b9e\u4f8b\u7684 Name \u548c Namespace \u76f8\u5339\u914d resourceName: spidernet.io/sriov_netdevice \u7531\u5b89\u88c5 sriov-operator \u6307\u5b9a\u7684 resourcePrefix: spidernet.io \u548c\u521b\u5efa SriovNetworkNodePolicy CR \u65f6\u6307\u5b9a\u7684 resourceName: sriov_netdevice \u62fc\u63a5\u800c\u6210 \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet.io/sriov_netdevice: '1' limits: spidernet.io/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF \u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a spidernet/sriov_netdevice : \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 SR-IOV \u8d44\u6e90\u3002 v1.multus-cni.io/default-network \uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002 \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3 \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001 ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185: ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms \u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf \u67e5\u770b Service \u7684 IP\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"SR-IOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#sr-iov-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool \u53ef\u7528\u4f5c underlay \u7f51\u7edc\u573a\u666f\u4e0b\u63d0\u4f9b\u56fa\u5b9a IP \u7684\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u5c06\u4ee5 Multus \u3001 SR-IOV \u3001 Spiderpool \u4e3a\u4f8b\uff0c\u642d\u5efa\u4e00\u5957\u5b8c\u6574\u7684 Underlay \u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u4ee5\u4e0b\u5404\u79cd\u529f\u80fd\u9700\u6c42\uff1a \u901a\u8fc7\u7b80\u6613\u8fd0\u7ef4\uff0c\u5e94\u7528\u53ef\u5206\u914d\u5230\u56fa\u5b9a\u7684 Underlay IP \u5730\u5740 Pod \u7684\u7f51\u5361\u5177\u6709 SR-IOV \u7684\u7f51\u7edc\u52a0\u901f\u529f\u80fd Pod \u80fd\u591f\u901a\u8fc7 Pod IP\u3001clusterIP\u3001nodePort \u7b49\u65b9\u5f0f\u901a\u4fe1","title":"SR-IOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#_1","text":"\u4e00\u4e2a Kubernetes \u96c6\u7fa4 Helm \u5de5\u5177 \u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361 \u67e5\u8be2\u7f51\u5361 bus-info\uff1a ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 \u901a\u8fc7 bus-info \u67e5\u8be2\u7f51\u5361\u662f\u5426\u652f\u6301 SR-IOV \u529f\u80fd\uff0c\u51fa\u73b0 Single Root I/O Virtualization (SR-IOV) \u5b57\u6bb5\u8868\u793a\u7f51\u5361\u652f\u6301 SR-IOV \u529f\u80fd\uff1a ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) \u5982\u679c\u60a8\u4f7f\u7528\u5982 Fedora\u3001Centos \u7b49 OS\uff0c \u5e76\u4e14\u4f7f\u7528 NetworkManager \u7ba1\u7406\u548c\u914d\u7f6e\u7f51\u7edc\uff0c\u5728\u4ee5\u4e0b\u573a\u666f\u65f6\u5efa\u8bae\u60a8\u9700\u8981\u914d\u7f6e NetworkManager: \u5982\u679c\u4f60\u4f7f\u7528 Underlay \u6a21\u5f0f\uff0c coordinator \u4f1a\u5728\u4e3b\u673a\u4e0a\u521b\u5efa veth \u63a5\u53e3\uff0c\u4e3a\u4e86\u9632\u6b62 NetworkManager \u5e72\u6270 veth \u63a5\u53e3, \u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 \u5982\u679c\u4f60\u901a\u8fc7 Ifacer \u521b\u5efa Vlan \u548c Bond \u63a5\u53e3\uff0cNetworkManager \u53ef\u80fd\u4f1a\u5e72\u6270\u8fd9\u4e9b\u63a5\u53e3\uff0c\u5bfc\u81f4 Pod \u8bbf\u95ee\u5f02\u5e38\u3002\u6211\u4eec\u9700\u8981\u914d\u7f6e NetworkManager\uff0c\u4f7f\u5176\u4e0d\u7eb3\u7ba1\u8fd9\u4e9b Veth \u63a5\u53e3\u3002 ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#spiderpool","text":"\u5b89\u88c5 Spiderpool\u3002 helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" \u5e26\u4e0a helm \u9009\u9879 --set sriov.install=true \uff0c \u4f1a\u5b89\u88c5 sriov-network-operator \uff0cresourcePrefix \u9ed8\u8ba4\u4e3a \"spidernet.io\"\uff0c\u53ef\u901a\u8fc7 helm \u9009\u9879 --set sriov.resourcePrefix \u4fee\u6539 \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7ed9\u5e0c\u671b\u8fd0\u884c SR-IOV CNI \u7684\u8282\u70b9\uff0c\u6309\u7167\u5982\u4e0b\u547d\u4ee4\u6253\u4e0a label\uff0c\u8fd9\u6837\uff0csriov-network-operator \u624d\u4f1a\u5728\u6307\u5b9a\u7684\u8282\u70b9\u4e0a\u5b89\u88c5\u7ec4\u4ef6 kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" \u5728\u8282\u70b9\u4e0a\u521b\u5efa VF \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u8282\u70b9\u4e0a\u7684\u53ef\u7528\u7f51\u5361 $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded \u5982\u679c SriovNetworkNodeState CRs \u7684\u72b6\u6001\u4e3a InProgress , \u8bf4\u660e sriov-operator \u6b63\u5728\u540c\u6b65\u8282\u70b9\u72b6\u6001\uff0c\u7b49\u5f85\u72b6\u6001\u4e3a Succeeded \u8bf4\u660e\u540c\u6b65\u5b8c\u6210\u3002\u67e5\u770b CR, \u786e\u8ba4 sriov-network-operator \u5df2\u7ecf\u53d1\u73b0\u8282\u70b9\u4e0a\u652f\u6301 SR-IOV \u529f\u80fd\u7684\u7f51\u5361\u3002 \u4ece\u4e0a\u9762\u53ef\u77e5\uff0c\u8282\u70b9 node-1 \u4e0a\u7684\u7f51\u5361 enp4s0f0np0 \u5177\u6709 SR-IOV \u529f\u80fd\uff0c\u5e76\u4e14\u652f\u6301\u7684\u6700\u5927 VF \u6570\u91cf\u4e3a 8\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7\u521b\u5efa SriovNetworkNodePolicy CRs \u5e76\u901a\u8fc7 nicSelector.pfNames \u6307\u5b9a PF (Physical function, \u7269\u7406\u7f51\u5361)\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u8282\u70b9\u4e0a\u7684\u8fd9\u4e9b\u7f51\u5361\u521b\u5efa\u51fa VF(Virtual Function): $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # \u6e34\u671b\u7684 VFs \u6570\u91cf resourceName: sriov_netdevice EOF \u4e0b\u53d1\u5982\u4e0a\u547d\u4ee4\u540e, \u56e0\u4e3a\u9700\u8981\u914d\u7f6e\u8282\u70b9\u542f\u7528 SR-IOV \u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u91cd\u542f\u8282\u70b9\u3002\u5982\u6709\u9700\u8981\uff0c\u6307\u5b9a\u5de5\u4f5c\u8282\u70b9\u800c\u975e Master \u8282\u70b9\u3002 resourceName \u4e0d\u80fd\u4e3a\u7279\u6b8a\u5b57\u7b26\uff0c\u652f\u6301\u7684\u5b57\u7b26: [0-9],[a-zA-Z] \u548c \"_\"\u3002 \u5728\u4e0b\u53d1 SriovNetworkNodePolicy CRs \u4e4b\u540e\uff0c\u518d\u6b21\u67e5\u770b SriovNetworkNodeState CRs \u7684\u72b6\u6001, \u53ef\u4ee5\u770b\u89c1 status \u4e2d VF \u5df2\u7ecf\u5f97\u5230\u914d\u7f6e: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... \u67e5\u770b Node \u53d1\u73b0\u540d\u4e3a spidernet.io/sriov_netdevice \u7684 SR-IOV \u8d44\u6e90\u5df2\u7ecf\u751f\u6548\uff0c\u5176\u4e2d VF \u7684\u6570\u91cf\u4e3a 8: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } sriov-network-config-daemon Pod \u8d1f\u8d23\u5728\u8282\u70b9\u4e0a\u914d\u7f6e VF \uff0c\u5176\u4f1a\u987a\u5e8f\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u5b8c\u6210\u8be5\u5de5\u4f5c\u3002\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u914d\u7f6e VF \u65f6\uff0csriov-network-config-daemon \u4f1a\u5bf9\u8282\u70b9\u4e0a\u7684\u6240\u6709 Pod \u8fdb\u884c\u9a71\u9010\uff0c\u914d\u7f6e VF \uff0c\u5e76\u53ef\u80fd\u91cd\u542f\u8282\u70b9\u3002\u5f53 sriov-network-config-daemon \u9a71\u9010\u67d0\u4e2a Pod \u5931\u8d25\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6240\u6709\u6d41\u7a0b\u90fd\u505c\u6ede\uff0c\u4ece\u800c\u5bfc\u81f4 node \u7684 VF \u6570\u91cf\u4e00\u76f4\u4e3a 0\u3002 \u8fd9\u79cd\u60c5\u51b5\u65f6\uff0csriov-network-config-daemon Pod \u4f1a\u770b\u5230\u5982\u4e0b\u7c7b\u4f3c\u65e5\u5fd7\uff1a error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... \u8be5\u95ee\u9898\u53ef\u53c2\u8003 sriov-network-operator \u793e\u533a\u7684\u7c7b\u4f3c issue \u6b64\u65f6\uff0c\u53ef\u6392\u67e5\u6307\u5b9a Pod \u4e3a\u5565\u65e0\u6cd5\u9a71\u9010\u7684\u539f\u56e0\uff0c\u6709\u5982\u4e0b\u53ef\u80fd\uff1a \uff081\uff09\u8be5\u9a71\u9010\u5931\u8d25\u7684 Pod \u53ef\u80fd\u914d\u7f6e\u4e86 PodDisruptionBudget\uff0c\u5bfc\u81f4\u53ef\u7528\u526f\u672c\u6570\u4e0d\u8db3\u3002\u8bf7\u8c03\u6574 PodDisruptionBudget \uff082\uff09\u96c6\u7fa4\u4e2d\u7684\u53ef\u7528\u8282\u70b9\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6ca1\u6709\u8282\u70b9\u53ef\u4ee5\u8c03\u5ea6 \u521b\u5efa SpiderIPPool \u5b9e\u4f8b\u3002 Pod \u4f1a\u4ece\u8be5\u5b50\u7f51\u4e2d\u83b7\u53d6 IP\uff0c\u8fdb\u884c Underlay \u7684\u7f51\u7edc\u901a\u8baf\uff0c\u6240\u4ee5\u8be5\u5b50\u7f51\u9700\u8981\u4e0e\u63a5\u5165\u7684 Underlay \u5b50\u7f51\u5bf9\u5e94\u3002 \u4ee5\u4e0b\u662f\u521b\u5efa\u76f8\u5173\u7684 SpiderIPPool \u793a\u4f8b cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF \u521b\u5efa SpiderMultusConfig \u5b9e\u4f8b\u3002 \u6ce8\u610f: \u5982\u679c\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u662f\u4f7f\u7528 NetworkManager \u7684 OS\uff0c\u6bd4\u5982 Fedora Centos\u7b49\uff0c\u5f3a\u70c8\u5efa\u8bae\u914d\u7f6e NetworkManager \u7684\u914d\u7f6e\u6587\u4ef6(/etc/NetworkManager/conf.d/spidernet.conf)\uff0c\u907f\u514d NetworkManager \u5e72\u6270 coordinator \u521b\u5efa\u7684 Veth \u865a\u62df\u63a5\u53e3\uff0c\u5f71\u54cd\u901a\u4fe1: ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth* > EOF ~# systemctl restart NetworkManager $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice EOF SpiderIPPool.Spec.multusName: kube-system/sriov-test \u8981\u548c\u521b\u5efa\u7684 SpiderMultusConfig \u5b9e\u4f8b\u7684 Name \u548c Namespace \u76f8\u5339\u914d resourceName: spidernet.io/sriov_netdevice \u7531\u5b89\u88c5 sriov-operator \u6307\u5b9a\u7684 resourcePrefix: spidernet.io \u548c\u521b\u5efa SriovNetworkNodePolicy CR \u65f6\u6307\u5b9a\u7684 resourceName: sriov_netdevice \u62fc\u63a5\u800c\u6210","title":"\u5b89\u88c5 Spiderpool"},{"location":"usage/install/underlay/get-started-sriov-zh_CN/#_2","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6d4b\u8bd5 Pod \u548c Service\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet.io/sriov_netdevice: '1' limits: spidernet.io/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF \u5fc5\u8981\u53c2\u6570\u8bf4\u660e\uff1a spidernet/sriov_netdevice : \u8be5\u53c2\u6570\u8868\u793a\u4f7f\u7528 SR-IOV \u8d44\u6e90\u3002 v1.multus-cni.io/default-network \uff1a\u8be5 annotation \u6307\u5b9a\u4e86\u4f7f\u7528\u7684 Multus \u7684 CNI \u914d\u7f6e\u3002 \u66f4\u591a Multus \u6ce8\u89e3\u4f7f\u7528\u8bf7\u53c2\u8003 Multus \u6ce8\u89e3 \u67e5\u770b Pod \u8fd0\u884c\u72b6\u6001 ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> \u5e94\u7528\u7684 IP \u5c06\u4f1a\u81ea\u52a8\u56fa\u5b9a\u5728\u8be5 IP \u8303\u56f4\u5185: ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 \u6d4b\u8bd5 Pod \u4e0e Pod \u7684\u901a\u8baf ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms \u6d4b\u8bd5 Pod \u4e0e Service \u901a\u8baf \u67e5\u770b Service \u7684 IP\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Pod \u5185\u8bbf\u95ee\u81ea\u8eab\u7684 Service \uff1a ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-sriov/","text":"SR-IOV Quick Start English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , SR-IOV , Veth , and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort. Prerequisites Make sure a Kubernetes cluster is ready Helm has already been installed A SR-IOV-enabled NIC Check the NIC's bus-info: ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 Check whether the NIC supports SR-IOV via bus-info. If the Single Root I/O Virtualization (SR-IOV) field appears, it means that SR-IOV is supported: ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager Install Spiderpool Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" When using the helm option --set sriov.install=true , it will install the sriov-network-operator . The default value for resourcePrefix is \"spidernet.io\" which can be modified via the helm option --set sriov.resourcePrefix . For users in the Chinese mainland, it is recommended to specify the spec --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. To enable the SR-IOV CNI on specific nodes, you need to apply the following command to label those nodes. This will allow the sriov-network-operator to install the components on the designated nodes. kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" Create VFs on the node Use the following command to view the available network interfaces on the node: $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded If the status of SriovNetworkNodeState CRs is InProgress , it indicates that the sriov-operator is currently synchronizing the node state. Wait for the status to become Succeeded to confirm that the synchronization is complete. Check the CR to ensure that the sriov-network-operator has discovered the network interfaces on the node that support SR-IOV. Based on the given information, it is known that the network interface's enp4s0f0np0 on the node node-1 supports SR-IOV capability with a maximum of 8 VFs. Now, let's create SriovNetworkNodePolicy CRs and specify PF (Physical function, physical network interface) through nicSelector.pfNames to generate VFs(Virtual Function) on these network interfaces of the respective nodes: $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # desired number of VFs resourceName: sriov_netdevice EOF After executing the above command, please note that configuring nodes to enable SR-IOV functionality may require a node restart. If needed, specify worker nodes instead of master nodes for this configuration. The resourceName should not contain special characters and is limited to [0-9], [a-zA-Z], and \"_\". After applying the SriovNetworkNodePolicy CRs, you can check the status of the SriovNetworkNodeState CRs again to verify that the VFs have been successfully configured: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... To confirm that the SR-IOV resources named spidernet.io/sriov_netdevice have been successfully enabled on a specific node and that the number of VFs is set to 8, you can use the following command: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } The sriov-network-config-daemon Pod is responsible for configuring VF on nodes, and it will sequentially complete the work on each node. When configuring VF on each node, the SR-IOV network configuration daemon will evict all Pods on the node, configure VF, and possibly restart the node. When SR-IOV network configuration daemon fails to evict a Pod, it will cause all processes to stop, resulting in the vf number of nodes remaining at 0. In this case, the SR-IOV network configuration daemon Pod will see logs similar to the following: error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... This issue can be referred to similar topics in the sriov-network-operator community issue The reason why the designated Pod cannot be expelled can be investigated, which may include the following: (1) The Pod that failed the eviction may have been configured with a PodDisruptionBudget, resulting in a shortage of available replicas. Please adjust the PodDisruptionBudget (2) Insufficient available nodes in the cluster, resulting in no nodes available for scheduling Create a SpiderIPPool instance. The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF Create a SpiderMultusConfig instance. shell $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice SpiderIPPool.Spec.multusName: 'kube-system/sriov-test' must be to match the Name and Namespace of the SpiderMultusConfig instance created. Create applications Create test Pods and Services via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet/sriov_netdevice: '1' limits: spidernet/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF Spec descriptions: spidernet/sriov_netdevice : Sriov resources used. v1.multus-cni.io/default-network : specifies the CNI configuration for Multus. For more information on Multus annotations, refer to Multus Quickstart . Check the status of Pods: ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> Spiderpool ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 Test the communication between Pods: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms Test the communication between Pods and Services: Check Services' IPs\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Access its own service within the Pod: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"SR-IOV"},{"location":"usage/install/underlay/get-started-sriov/#sr-iov-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Spiderpool provides a solution for assigning static IP addresses in underlay networks. In this page, we'll demonstrate how to build a complete underlay network solution using Multus , SR-IOV , Veth , and Spiderpool , which meets the following kinds of requirements: Applications can be assigned static Underlay IP addresses through simple operations. Pods with multiple Underlay NICs connect to multiple Underlay subnets. Pods can communicate in various ways, such as Pod IP, clusterIP, and nodePort.","title":"SR-IOV Quick Start"},{"location":"usage/install/underlay/get-started-sriov/#prerequisites","text":"Make sure a Kubernetes cluster is ready Helm has already been installed A SR-IOV-enabled NIC Check the NIC's bus-info: ~# ethtool -i enp4s0f0np0 | grep bus-info bus-info: 0000 :04:00.0 Check whether the NIC supports SR-IOV via bus-info. If the Single Root I/O Virtualization (SR-IOV) field appears, it means that SR-IOV is supported: ~# lspci -s 0000 :04:00.0 -v | grep SR-IOV Capabilities: [ 180 ] Single Root I/O Virtualization ( SR-IOV ) If your OS is such as Fedora and CentOS and uses NetworkManager to manage network configurations, you need to configure NetworkManager in the following scenarios: If you are using Underlay mode, the coordinator will create veth interfaces on the host. To prevent interference from NetworkManager with the veth interface. It is strongly recommended that you configure NetworkManager. If you create VLAN and Bond interfaces through Ifacer, NetworkManager may interfere with these interfaces, leading to abnormal pod access. It is strongly recommended that you configure NetworkManager. ~# IFACER_INTERFACE = \"<NAME>\" ~# cat << EOF | > /etc/NetworkManager/conf.d/spidernet.conf > [keyfile] > unmanaged-devices=interface-name:^veth*;interface-name:${IFACER_INTERFACE} > EOF ~# systemctl restart NetworkManager","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-sriov/#install-spiderpool","text":"Install Spiderpool. helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set sriov.install = true --set multus.multusCNI.defaultCniCRName = \"sriov-test\" When using the helm option --set sriov.install=true , it will install the sriov-network-operator . The default value for resourcePrefix is \"spidernet.io\" which can be modified via the helm option --set sriov.resourcePrefix . For users in the Chinese mainland, it is recommended to specify the spec --set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pull failures from Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. To enable the SR-IOV CNI on specific nodes, you need to apply the following command to label those nodes. This will allow the sriov-network-operator to install the components on the designated nodes. kubectl label node $NodeName node-role.kubernetes.io/worker = \"\" Create VFs on the node Use the following command to view the available network interfaces on the node: $ kubectl get sriovnetworknodestates -n kube-system NAME SYNC STATUS AGE node-1 Succeeded 24s ... $ kubectl get sriovnetworknodestates -n kube-system node-1 -o yaml apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState spec: ... status: interfaces: - deviceID: \"1017\" driver: mlx5_core linkSpeed: 10000 Mb/s linkType: ETH mac: 04 :3f:72:d0:d2:86 mtu: 1500 name: enp4s0f0np0 pciAddress: \"0000:04:00.0\" totalvfs: 8 vendor: 15b3 syncStatus: Succeeded If the status of SriovNetworkNodeState CRs is InProgress , it indicates that the sriov-operator is currently synchronizing the node state. Wait for the status to become Succeeded to confirm that the synchronization is complete. Check the CR to ensure that the sriov-network-operator has discovered the network interfaces on the node that support SR-IOV. Based on the given information, it is known that the network interface's enp4s0f0np0 on the node node-1 supports SR-IOV capability with a maximum of 8 VFs. Now, let's create SriovNetworkNodePolicy CRs and specify PF (Physical function, physical network interface) through nicSelector.pfNames to generate VFs(Virtual Function) on these network interfaces of the respective nodes: $ cat << EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy1 namespace: sriov-network-operator spec: deviceType: netdevice nodeSelector: kubernetes.io/os: \"linux\" nicSelector: pfNames: - enp4s0f0np0 numVfs: 8 # desired number of VFs resourceName: sriov_netdevice EOF After executing the above command, please note that configuring nodes to enable SR-IOV functionality may require a node restart. If needed, specify worker nodes instead of master nodes for this configuration. The resourceName should not contain special characters and is limited to [0-9], [a-zA-Z], and \"_\". After applying the SriovNetworkNodePolicy CRs, you can check the status of the SriovNetworkNodeState CRs again to verify that the VFs have been successfully configured: $ kubectl get sriovnetworknodestates -n sriov-network-operator node-1 -o yaml ... - Vfs: - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.4 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.5 vendor: \"15b3\" - deviceID: 1018 driver: mlx5_core pciAddress: 0000 :04:00.6 vendor: \"15b3\" deviceID: \"1017\" driver: mlx5_core mtu: 1500 numVfs: 8 pciAddress: 0000 :04:00.0 totalvfs: 8 vendor: \"8086\" ... To confirm that the SR-IOV resources named spidernet.io/sriov_netdevice have been successfully enabled on a specific node and that the number of VFs is set to 8, you can use the following command: ~# kubectl get node node-1 -o json | jq '.status.allocatable' { \"cpu\" : \"24\" , \"ephemeral-storage\" : \"94580335255\" , \"hugepages-1Gi\" : \"0\" , \"hugepages-2Mi\" : \"0\" , \"spidernet.io/sriov_netdevice\" : \"8\" , \"memory\" : \"16247944Ki\" , \"pods\" : \"110\" } The sriov-network-config-daemon Pod is responsible for configuring VF on nodes, and it will sequentially complete the work on each node. When configuring VF on each node, the SR-IOV network configuration daemon will evict all Pods on the node, configure VF, and possibly restart the node. When SR-IOV network configuration daemon fails to evict a Pod, it will cause all processes to stop, resulting in the vf number of nodes remaining at 0. In this case, the SR-IOV network configuration daemon Pod will see logs similar to the following: error when evicting pods/calico-kube-controllers-865d498fd9-245c4 -n kube-system (will retry after 5s) ... This issue can be referred to similar topics in the sriov-network-operator community issue The reason why the designated Pod cannot be expelled can be investigated, which may include the following: (1) The Pod that failed the eviction may have been configured with a PodDisruptionBudget, resulting in a shortage of available replicas. Please adjust the PodDisruptionBudget (2) Insufficient available nodes in the cluster, resulting in no nodes available for scheduling Create a SpiderIPPool instance. The Pod will obtain an IP address from this subnet for underlying network communication, so the subnet needs to correspond to the underlying subnet that is being accessed. Here is an example of creating a SpiderSubnet instance:\uff1a cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: ippool-test spec: default: true ips: - \"10.20.168.190-10.20.168.199\" subnet: 10.20.0.0/16 gateway: 10.20.0.1 multusName: kube-system/sriov-test EOF Create a SpiderMultusConfig instance. shell $ cat <<EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderMultusConfig metadata: name: sriov-test namespace: kube-system spec: cniType: sriov sriov: resourceName: spidernet.io/sriov_netdevice SpiderIPPool.Spec.multusName: 'kube-system/sriov-test' must be to match the Name and Namespace of the SpiderMultusConfig instance created.","title":"Install Spiderpool"},{"location":"usage/install/underlay/get-started-sriov/#create-applications","text":"Create test Pods and Services via the command below\uff1a cat <<EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: sriov-deploy spec: replicas: 2 selector: matchLabels: app: sriov-deploy template: metadata: annotations: v1.multus-cni.io/default-network: kube-system/sriov-test labels: app: sriov-deploy spec: containers: - name: sriov-deploy image: nginx imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP resources: requests: spidernet/sriov_netdevice: '1' limits: spidernet/sriov_netdevice: '1' --- apiVersion: v1 kind: Service metadata: name: sriov-deploy-svc labels: app: sriov-deploy spec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 80 selector: app: sriov-deploy EOF Spec descriptions: spidernet/sriov_netdevice : Sriov resources used. v1.multus-cni.io/default-network : specifies the CNI configuration for Multus. For more information on Multus annotations, refer to Multus Quickstart . Check the status of Pods: ~# kubectl get pod -l app = sriov-deploy -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES sriov-deploy-9b4b9f6d9-mmpsm 1 /1 Running 0 6m54s 10 .20.168.191 worker-12 <none> <none> sriov-deploy-9b4b9f6d9-xfsvj 1 /1 Running 0 6m54s 10 .20.168.190 master-11 <none> <none> Spiderpool ensuring that the applications' IPs are automatically fixed within the defined ranges. ~# kubectl get spiderippool NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DEFAULT DISABLE ippool-test 4 10 .20.0.0/16 2 10 true false ~# kubectl get spiderendpoints NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE sriov-deploy-9b4b9f6d9-mmpsm eth0 ippool-test 10 .20.168.191/16 worker-12 sriov-deploy-9b4b9f6d9-xfsvj eth0 ippool-test 10 .20.168.190/16 master-11 Test the communication between Pods: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- ping 10 .20.168.190 -c 3 PING 10 .20.168.190 ( 10 .20.168.190 ) 56 ( 84 ) bytes of data. 64 bytes from 10 .20.168.190: icmp_seq = 1 ttl = 64 time = 0 .162 ms 64 bytes from 10 .20.168.190: icmp_seq = 2 ttl = 64 time = 0 .138 ms 64 bytes from 10 .20.168.190: icmp_seq = 3 ttl = 64 time = 0 .191 ms --- 10 .20.168.190 ping statistics --- 3 packets transmitted, 3 received, 0 % packet loss, time 2051ms rtt min/avg/max/mdev = 0 .138/0.163/0.191/0.021 ms Test the communication between Pods and Services: Check Services' IPs\uff1a ~# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .43.0.1 <none> 443 /TCP 23d sriov-deploy-svc ClusterIP 10 .43.54.100 <none> 80 /TCP 20m Access its own service within the Pod: ~# kubectl exec -it sriov-deploy-9b4b9f6d9-mmpsm -- curl 10 .43.54.100 -I HTTP/1.1 200 OK Server: nginx/1.23.3 Date: Mon, 27 Mar 2023 08 :22:39 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 13 Dec 2022 15 :53:53 GMT Connection: keep-alive ETag: \"6398a011-267\" Accept-Ranges: bytes","title":"Create applications"},{"location":"usage/install/underlay/get-started-weave-zh_CN/","text":"Weave Quick Start English | \u7b80\u4f53\u4e2d\u6587 Weave \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c Weave \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 IPAM \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 IPAM \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d Weave , \u5728\u4fdd\u7559 Weave \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 Spiderpool \u6269\u5c55 Weave \u7684 IPAM \u80fd\u529b\u3002 \u5148\u51b3\u6761\u4ef6 \u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177 \u5b89\u88c5 \u5b89\u88c5 Weave : kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml \u7b49\u5f85 Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u6240\u4f7f\u7528\u7684 IP \u6c60: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave \u4f7f\u7528 10.32.0.0/12 \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u5185 SpiderIPPool\u3002 \u9a8c\u8bc1\u5b89\u88c5 [ root@node1 ~ ] # kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 0 12901 false \u5207\u6362 Weave \u7684 IPAM \u4e3a Spiderpool \u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: /etc/cni/net.d/10-weave.conflist \u7684 ipam \u5b57\u6bb5: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } \u4fee\u6539\u4e3a: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } \u6216\u53ef\u901a\u8fc7 jq \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 jq \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: # \u4ee5 centos7 \u4e3a\u4f8b yum -y install jq \u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist \u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c \u521b\u5efa\u5e94\u7528 \u4f7f\u7528\u6ce8\u89e3: ipam.spidernet.io/ippool \u6307\u5b9a Pod \u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d IP: [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4ece SpiderIPPool: weave-ippool-v4 \u4e2d\u5206\u914d IP Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool \u4e2d\u5206\u914d IP \u5730\u5740: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false \u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002 \u901a\u8fc7 Spiderpool , \u786e\u5b9e\u6269\u5c55\u4e86 Weave \u7684 IPAM \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528 \uff0c\u4f53\u9a8c Spiderpool \u5176\u4ed6\u7684\u529f\u80fd\u3002","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#weave-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Weave \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848, \u5b83\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u865a\u62df\u7f51\u7edc\u3001\u81ea\u52a8\u53d1\u73b0\u548c\u8fde\u63a5\u4e0d\u540c\u7684\u5bb9\u5668, \u4e3a\u5bb9\u5668\u63d0\u4f9b\u7f51\u7edc\u8fde\u901a\u548c\u7f51\u7edc\u7b56\u7565\u7b49\u80fd\u529b\u3002\u540c\u65f6\u5b83\u53ef\u4f5c\u4e3a Kubernetes \u5bb9\u5668\u7f51\u7edc\u89e3\u51b3\u65b9\u6848(CNI)\u7684\u4e00\u79cd\u9009\u62e9\uff0c Weave \u9ed8\u8ba4\u4f7f\u7528\u5185\u7f6e\u7684 IPAM \u4e3a Pod \u63d0\u4f9b IP \u5206\u914d\u80fd\u529b, \u5176 IPAM \u80fd\u529b\u5bf9\u7528\u6237\u5e76\u4e0d\u53ef\u89c1\uff0c\u7f3a\u4e4f Pod IP \u5730\u5740\u7684\u7ba1\u7406\u5206\u914d\u80fd\u529b\u3002 \u672c\u6587\u5c06\u4ecb\u7ecd Spiderpool \u642d\u914d Weave , \u5728\u4fdd\u7559 Weave \u539f\u6709\u529f\u80fd\u7684\u57fa\u7840\u4e0a, \u7ed3\u5408 Spiderpool \u6269\u5c55 Weave \u7684 IPAM \u80fd\u529b\u3002","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_1","text":"\u51c6\u5907\u597d\u4e00\u4e2a Kubernetes \u96c6\u7fa4, \u6ca1\u6709\u5b89\u88c5\u4efb\u4f55\u7684 CNI Helm\u3001Kubectl\u3001Jq(\u53ef\u9009) \u4e8c\u8fdb\u5236\u5de5\u5177","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_2","text":"\u5b89\u88c5 Weave : kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml \u7b49\u5f85 Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m \u5b89\u88c5 Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false \u5982\u679c\u60a8\u662f\u56fd\u5185\u7528\u6237\uff0c\u53ef\u4ee5\u6307\u5b9a\u53c2\u6570 --set global.imageRegistryOverride=ghcr.m.daocloud.io \u907f\u514d Spiderpool \u7684\u955c\u50cf\u62c9\u53d6\u5931\u8d25\u3002 \u901a\u8fc7 multus.multusCNI.defaultCniCRName \u6307\u5b9a multus \u9ed8\u8ba4\u4f7f\u7528\u7684 CNI \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\u540d\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5b89\u88c5\u540e\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6570\u636e\u4e3a\u7a7a\u7684 NetworkAttachmentDefinition \u5bf9\u5e94\u5b9e\u4f8b\u3002\u5982\u679c multus.multusCNI.defaultCniCRName \u9009\u9879\u4e3a\u7a7a\uff0c\u4f1a\u5c1d\u8bd5\u901a\u8fc7 /etc/cni/net.d \u76ee\u5f55\u4e0b\u7684\u7b2c\u4e00\u4e2a CNI \u914d\u7f6e\u6765\u521b\u5efa\u5bf9\u5e94\u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u5426\u5219\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u540d\u4e3a default \u7684 NetworkAttachmentDefinition \u5b9e\u4f8b\uff0c\u4ee5\u5b8c\u6210 multus \u7684\u5b89\u88c5\u3002 \u7b49\u5f85 Pod Running\uff0c \u521b\u5efa Pod \u6240\u4f7f\u7528\u7684 IP \u6c60: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave \u4f7f\u7528 10.32.0.0/12 \u4f5c\u4e3a\u96c6\u7fa4\u9ed8\u8ba4\u5b50\u7f51\u3002\u6240\u4ee5\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u5b50\u7f51\u5185 SpiderIPPool\u3002 \u9a8c\u8bc1\u5b89\u88c5 [ root@node1 ~ ] # kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1 /1 Running 0 13m spiderpool-agent-kxf27 1 /1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1 /1 Running 0 13m spiderpool-init 0 /1 Completed 0 13m [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 0 12901 false","title":"\u5b89\u88c5"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#weave-ipam-spiderpool","text":"\u4fee\u6539\u6bcf\u4e2a\u8282\u70b9\u4e0a: /etc/cni/net.d/10-weave.conflist \u7684 ipam \u5b57\u6bb5: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } \u4fee\u6539\u4e3a: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } \u6216\u53ef\u901a\u8fc7 jq \u5de5\u5177\u4e00\u952e\u4fee\u6539\u3002\u5982\u6ca1\u6709 jq \u53ef\u5148\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5: # \u4ee5 centos7 \u4e3a\u4f8b yum -y install jq \u4fee\u6539 CNI \u914d\u7f6e\u6587\u4ef6: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist \u6ce8\u610f\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6267\u884c","title":"\u5207\u6362 Weave \u7684 IPAM \u4e3a Spiderpool"},{"location":"usage/install/underlay/get-started-weave-zh_CN/#_3","text":"\u4f7f\u7528\u6ce8\u89e3: ipam.spidernet.io/ippool \u6307\u5b9a Pod \u4ece\u8be5 SpiderIPPool \u4e2d\u5206\u914d IP: [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/ippool \uff1a\u6307\u5b9a Pod \u4ece SpiderIPPool: weave-ippool-v4 \u4e2d\u5206\u914d IP Pod \u6210\u529f\u521b\u5efa, \u5e76\u4e14\u4ece Spiderpool \u4e2d\u5206\u914d IP \u5730\u5740: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false \u6d4b\u8bd5\u8fde\u901a\u6027\uff0c\u4ee5 Pod \u8de8\u8282\u70b9\u901a\u4fe1\u4e3a\u4f8b: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cIP \u5206\u914d\u6b63\u5e38\u3001\u7f51\u7edc\u8fde\u63a5\u6b63\u5e38\u3002 \u901a\u8fc7 Spiderpool , \u786e\u5b9e\u6269\u5c55\u4e86 Weave \u7684 IPAM \u80fd\u529b\u3002\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u53c2\u8003 Spiderpool \u4f7f\u7528 \uff0c\u4f53\u9a8c Spiderpool \u5176\u4ed6\u7684\u529f\u80fd\u3002","title":"\u521b\u5efa\u5e94\u7528"},{"location":"usage/install/underlay/get-started-weave/","text":"Weave Quick Start English | \u7b80\u4f53\u4e2d\u6587 Weave , an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, Weave utilizes the built-in IPAM to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how Weave and Spiderpool can be integrated to extend Weave 's IPAM capabilities while preserving its original functions. Prerequisites A ready Kubernetes cluster without any CNI installed Helm, Kubectl and Jq (optional) Install Install Weave: kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml Wait for Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Wait for Pod Running and create the IPPool used by Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave uses 10.32.0.0/12 as the cluster's default subnet, and thus a SpiderIPPool with the same subnet needs to be created in this case. Verify installation shell [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1/1 Running 0 13m spiderpool-agent-kxf27 1/1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1/1 Running 0 13m spiderpool-init 0/1 Completed 0 13m [root@node1 ~]# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10.32.0.0/12 0 12901 false Switch Weave 's IPAM to Spiderpool Change the ipam field of /etc/cni/net.d/10-weave.conflist on each node: Change the following: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } To: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } Alternatively, it can be changed with jq in one step. If jq is not installed, you can use the following command to install it: # Take centos7 as an example yum -y install jq Change the CNI configuration file: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist Make sure to run this command at each node Create applications Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation ipam.spidernet.io/ippool : [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/subnet : specifies that the Pods will be assigned IPs from SpiderSubnet: weave-ippool-v4 . The Pods have been created and allocated IP addresses from Spiderpool Subnets: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false To test connectivity, let's use inter-node communication between Pods as an example: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms The test results indicate that IP allocation and network connectivity are normal. Spiderpool has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of Spiderpool .","title":"Weave"},{"location":"usage/install/underlay/get-started-weave/#weave-quick-start","text":"English | \u7b80\u4f53\u4e2d\u6587 Weave , an open-source network solution, provides network connectivity and policies for containers by creating a virtual network, automatically discovering and connecting containers. Also known as a Kubernetes Container Network Interface (CNI) solution, Weave utilizes the built-in IPAM to allocate IP addresses for Pods by default, with limited visibility and IPAM capabilities for Pods. This page demonstrates how Weave and Spiderpool can be integrated to extend Weave 's IPAM capabilities while preserving its original functions.","title":"Weave Quick Start"},{"location":"usage/install/underlay/get-started-weave/#prerequisites","text":"A ready Kubernetes cluster without any CNI installed Helm, Kubectl and Jq (optional)","title":"Prerequisites"},{"location":"usage/install/underlay/get-started-weave/#install","text":"Install Weave: kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml Wait for Pod Running: [ root@node1 ~ ] # kubectl get po -n kube-system | grep weave weave-net-ck849 2 /2 Running 4 0 1m weave-net-vhmqx 2 /2 Running 4 0 1m Install Spiderpool helm repo add spiderpool https://spidernet-io.github.io/spiderpool helm repo update spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system --set multus.multusCNI.install = false If you are mainland user who is not available to access ghcr.io\uff0cYou can specify the parameter -set global.imageRegistryOverride=ghcr.m.daocloud.io to avoid image pulling failures for Spiderpool. Specify the name of the NetworkAttachmentDefinition instance for the default CNI used by Multus via multus.multusCNI.defaultCniCRName . If the multus.multusCNI.defaultCniCRName option is provided, an empty NetworkAttachmentDefinition instance will be automatically generated upon installation. Otherwise, Multus will attempt to create a NetworkAttachmentDefinition instance based on the first CNI configuration found in the /etc/cni/net.d directory. If no suitable configuration is found, a NetworkAttachmentDefinition instance named default will be created to complete the installation of Multus. Wait for Pod Running and create the IPPool used by Pod: cat << EOF | kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: weave-ippool-v4 labels: ipam.spidernet.io/subnet-cidr: 10-32-0-0-12 spec: ips: - 10.32.0.100-10.32.50.200 subnet: 10.32.0.0/12 EOF Weave uses 10.32.0.0/12 as the cluster's default subnet, and thus a SpiderIPPool with the same subnet needs to be created in this case. Verify installation shell [root@node1 ~]# kubectl get po -n kube-system | grep spiderpool spiderpool-agent-7hhkz 1/1 Running 0 13m spiderpool-agent-kxf27 1/1 Running 0 13m spiderpool-controller-76798dbb68-xnktr 1/1 Running 0 13m spiderpool-init 0/1 Completed 0 13m [root@node1 ~]# kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10.32.0.0/12 0 12901 false","title":"Install"},{"location":"usage/install/underlay/get-started-weave/#switch-weaves-ipam-to-spiderpool","text":"Change the ipam field of /etc/cni/net.d/10-weave.conflist on each node: Change the following: [ root@node1 ~ ] # cat /etc/cni/net.d/10-weave.conflist { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"hairpinMode\" : true } , { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true } , \"snat\" : true } ] } To: { \"cniVersion\" : \"0.3.0\" , \"name\" : \"weave\" , \"plugins\" : [ { \"name\" : \"weave\" , \"type\" : \"weave-net\" , \"ipam\" : { \"type\" : \"spiderpool\" }, \"hairpinMode\" : true }, { \"type\" : \"portmap\" , \"capabilities\" : { \"portMappings\" : true }, \"snat\" : true } ] } Alternatively, it can be changed with jq in one step. If jq is not installed, you can use the following command to install it: # Take centos7 as an example yum -y install jq Change the CNI configuration file: cat <<< $( jq '.plugins[0].ipam.type = \"spiderpool\" ' /etc/cni/net.d/10-weave.conflist ) > /etc/cni/net.d/10-weave.conflist Make sure to run this command at each node","title":"Switch Weave's IPAM to Spiderpool"},{"location":"usage/install/underlay/get-started-weave/#create-applications","text":"Specify that the Pods will be allocated IPs from that SpiderSubnet via the annotation ipam.spidernet.io/ippool : [ root@node1 ~ ] # cat << EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: annotations: ipam.spidernet.io/ippool: '{\"ipv4\":[\"weave-ippool-v4\"]}' labels: app: nginx spec: containers: - image: nginx imagePullPolicy: IfNotPresent lifecycle: {} name: container-1 EOF spec.template.metadata.annotations.ipam.spidernet.io/subnet : specifies that the Pods will be assigned IPs from SpiderSubnet: weave-ippool-v4 . The Pods have been created and allocated IP addresses from Spiderpool Subnets: [ root@node1 ~ ] # kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5745d9b5d7-2rvn7 1 /1 Running 0 8s 10 .32.22.190 node1 <none> <none> nginx-5745d9b5d7-5ssck 1 /1 Running 0 8s 10 .32.35.87 node2 <none> <none> [ root@node1 ~ ] # kubectl get sp NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE weave-ippool-v4 4 10 .32.0.0/12 2 2 false To test connectivity, let's use inter-node communication between Pods as an example: [ root@node1 ~ ] # kubectl exec nginx-5745d9b5d7-2rvn7 -- ping 10.32.35.87 -c 2 PING 10 .32.35.87 ( 10 .32.35.87 ) : 56 data bytes 64 bytes from 10 .32.35.87: seq = 0 ttl = 64 time = 4 .561 ms 64 bytes from 10 .32.35.87: seq = 1 ttl = 64 time = 0 .632 ms --- 10 .32.35.87 ping statistics --- 2 packets transmitted, 2 packets received, 0 % packet loss round-trip min/avg/max = 0 .632/2.596/4.561 ms The test results indicate that IP allocation and network connectivity are normal. Spiderpool has extended the capabilities of Weave's IPAM. Next, you can go to Spiderpool to explore other features of Spiderpool .","title":"Create applications"}]}